{"results": [{"createdAt": null, "postedAt": "2011-05-18T00:19:52.027Z", "modifiedAt": null, "url": null, "title": "Triangle NC Meetup - May 20th 6pm", "slug": "triangle-nc-meetup-may-20th-6pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:03.406Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EALdLAoLng5MRtSRA/triangle-nc-meetup-may-20th-6pm", "pageUrlRelative": "/posts/EALdLAoLng5MRtSRA/triangle-nc-meetup-may-20th-6pm", "linkUrl": "https://www.lesswrong.com/posts/EALdLAoLng5MRtSRA/triangle-nc-meetup-may-20th-6pm", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Triangle%20NC%20Meetup%20-%20May%2020th%206pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATriangle%20NC%20Meetup%20-%20May%2020th%206pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEALdLAoLng5MRtSRA%2Ftriangle-nc-meetup-may-20th-6pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Triangle%20NC%20Meetup%20-%20May%2020th%206pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEALdLAoLng5MRtSRA%2Ftriangle-nc-meetup-may-20th-6pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEALdLAoLng5MRtSRA%2Ftriangle-nc-meetup-may-20th-6pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<p>When: Friday May 20th, at 6pm</p>\n<p>Where: *Note this has changed* <a href=\"http://maps.google.com/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=starbucks&amp;aq=&amp;sll=35.899323,-78.897157&amp;sspn=0.011872,0.022724&amp;ie=UTF8&amp;hq=starbucks&amp;hnear=&amp;z=16\">Starbucks on NC Highway 55</a></p>\n<p>If this is not a good date/time for you, please comment and we'll plan our upcoming meetups accordingly. &nbsp;We plan to have biweekly meetings, alternating between Wednesdays and a weekend day; we'd like to gather more weekend availability info before settling on a day and time.</p>\n<p>This meeting is planned to consist of Instrumental Rationality/Becoming Awesome discussion, and Zendo (please read the <a href=\"http://en.wikipedia.org/wiki/Zendo_(game)\">rules</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EALdLAoLng5MRtSRA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 7.160119002123595e-07, "legacy": true, "legacyId": "7439", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T02:47:38.687Z", "modifiedAt": null, "url": null, "title": "LINK: Subculture roles (by Brad Hicks)", "slug": "link-subculture-roles-by-brad-hicks-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:01.130Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hairyfigment", "createdAt": "2010-10-14T22:12:59.035Z", "isAdmin": false, "displayName": "hairyfigment"}, "userId": "NesjW63eueLsbKrCY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jP7qcQmxnaynbakCH/link-subculture-roles-by-brad-hicks-0", "pageUrlRelative": "/posts/jP7qcQmxnaynbakCH/link-subculture-roles-by-brad-hicks-0", "linkUrl": "https://www.lesswrong.com/posts/jP7qcQmxnaynbakCH/link-subculture-roles-by-brad-hicks-0", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20Subculture%20roles%20(by%20Brad%20Hicks)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20Subculture%20roles%20(by%20Brad%20Hicks)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjP7qcQmxnaynbakCH%2Flink-subculture-roles-by-brad-hicks-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20Subculture%20roles%20(by%20Brad%20Hicks)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjP7qcQmxnaynbakCH%2Flink-subculture-roles-by-brad-hicks-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjP7qcQmxnaynbakCH%2Flink-subculture-roles-by-brad-hicks-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p>By apparently popular demand</p>\n<p>These roles do not represent personality types as such. A person who serves as an Authenticity Cop in one community may serve as a Fun Maven or even theoretically a Dream Nazi elsewhere.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jP7qcQmxnaynbakCH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7440", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T03:00:43.252Z", "modifiedAt": null, "url": null, "title": "[LINK] Subculture roles (by Brad Hicks)", "slug": "link-subculture-roles-by-brad-hicks", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:01.484Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hairyfigment", "createdAt": "2010-10-14T22:12:59.035Z", "isAdmin": false, "displayName": "hairyfigment"}, "userId": "NesjW63eueLsbKrCY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9BtH36j4e9JHzaKeu/link-subculture-roles-by-brad-hicks", "pageUrlRelative": "/posts/9BtH36j4e9JHzaKeu/link-subculture-roles-by-brad-hicks", "linkUrl": "https://www.lesswrong.com/posts/9BtH36j4e9JHzaKeu/link-subculture-roles-by-brad-hicks", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Subculture%20roles%20(by%20Brad%20Hicks)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Subculture%20roles%20(by%20Brad%20Hicks)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9BtH36j4e9JHzaKeu%2Flink-subculture-roles-by-brad-hicks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Subculture%20roles%20(by%20Brad%20Hicks)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9BtH36j4e9JHzaKeu%2Flink-subculture-roles-by-brad-hicks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9BtH36j4e9JHzaKeu%2Flink-subculture-roles-by-brad-hicks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<p>By what looks like popular demand on the Society for Creative Anachronism thread, I'm linking a three-part account of three roles that supposedly exist within every lasting subculture or \"volunteer-dominated\" organization. The author has no proper authority at all but has personally observed many subcultures in the wild.</p>\n<p>These roles do not represent exclusive personality types as such. A person who serves as Authenticity Cop in one community may theoretically serve as Fun Maven or even Dream Nazi elsewhere.*</p>\n<p><a title=\"part one\" href=\"http://bradhicks.livejournal.com/128514.html\">Dream Nazis</a>: Supply vision for the group. Brad describes this as inherently promoting a Golden Age myth of a glorious past. Hopefully this reflects the term's origins in the SCA and not any practical necessity.</p>\n<p><a title=\"part two\" href=\"http://bradhicks.livejournal.com/128776.html\">Authenticity Police</a>: Seek accuracy and nitpick perceived mistakes. You know this one already.</p>\n<p><a title=\"part three\" href=\"http://bradhicks.livejournal.com/129250.html\">Fun Mavens</a>: Work up a sweat doing free lifting or other labor for the organization, then enjoy 'a few' cold beers with their comrades. (I find it amusing that I took a while to think of the right word at the end there.) Brad argues that in addition to doing all that work, Fun Mavens sustain the organization by recruiting.</p>\n<p>&nbsp;</p>\n<p>*I wonder if Eliezer doesn't wear two hats right here, never mind at the SIAI.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9BtH36j4e9JHzaKeu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 11, "extendedScore": null, "score": 7.160586935311762e-07, "legacy": true, "legacyId": "7445", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T06:59:20.601Z", "modifiedAt": null, "url": null, "title": "Advice needed: Less Wrong Meetup Lesson plan on Communication", "slug": "advice-needed-less-wrong-meetup-lesson-plan-on-communication", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:02.200Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Goobahman", "createdAt": "2011-01-13T05:09:28.962Z", "isAdmin": false, "displayName": "Goobahman"}, "userId": "cidN68rGuy4wwnvFp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vT2LHuX7DNajuFmcx/advice-needed-less-wrong-meetup-lesson-plan-on-communication", "pageUrlRelative": "/posts/vT2LHuX7DNajuFmcx/advice-needed-less-wrong-meetup-lesson-plan-on-communication", "linkUrl": "https://www.lesswrong.com/posts/vT2LHuX7DNajuFmcx/advice-needed-less-wrong-meetup-lesson-plan-on-communication", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20needed%3A%20Less%20Wrong%20Meetup%20Lesson%20plan%20on%20Communication&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20needed%3A%20Less%20Wrong%20Meetup%20Lesson%20plan%20on%20Communication%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvT2LHuX7DNajuFmcx%2Fadvice-needed-less-wrong-meetup-lesson-plan-on-communication%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20needed%3A%20Less%20Wrong%20Meetup%20Lesson%20plan%20on%20Communication%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvT2LHuX7DNajuFmcx%2Fadvice-needed-less-wrong-meetup-lesson-plan-on-communication", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvT2LHuX7DNajuFmcx%2Fadvice-needed-less-wrong-meetup-lesson-plan-on-communication", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p>Hi Everyone,</p>\r\n<p>I host a fortnightly Less Wrong Meetup self improvement logic teaching thing with some close friends of mine, and next session I want to look at Communication.</p>\r\n<p>I was wondering what you guys thought would be some good resources for this, not only from the Less Wrong catalogue, but even elsewhere.</p>\r\n<p>Particular things I would like to do is:</p>\r\n<p>* Empower us with the tools to recognize, call out and defeat logical fallacies and outrageous claims</p>\r\n<p>* Examine the more instinctive behaviours in human interaction, and ourselves within that.</p>\r\n<p>* Simple ways to improve our social skills in our daily lives.</p>\r\n<p>Any contributions are hugely appreciated :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vT2LHuX7DNajuFmcx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 7.161281201969868e-07, "legacy": true, "legacyId": "7454", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T11:02:31.733Z", "modifiedAt": null, "url": null, "title": "The greater a technology\u2019s complexity, the more slowly it improves?", "slug": "the-greater-a-technology-s-complexity-the-more-slowly-it", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:01.788Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xueL9YxmdQQTb2Hz8/the-greater-a-technology-s-complexity-the-more-slowly-it", "pageUrlRelative": "/posts/xueL9YxmdQQTb2Hz8/the-greater-a-technology-s-complexity-the-more-slowly-it", "linkUrl": "https://www.lesswrong.com/posts/xueL9YxmdQQTb2Hz8/the-greater-a-technology-s-complexity-the-more-slowly-it", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20greater%20a%20technology%E2%80%99s%20complexity%2C%20the%20more%20slowly%20it%20improves%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20greater%20a%20technology%E2%80%99s%20complexity%2C%20the%20more%20slowly%20it%20improves%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxueL9YxmdQQTb2Hz8%2Fthe-greater-a-technology-s-complexity-the-more-slowly-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20greater%20a%20technology%E2%80%99s%20complexity%2C%20the%20more%20slowly%20it%20improves%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxueL9YxmdQQTb2Hz8%2Fthe-greater-a-technology-s-complexity-the-more-slowly-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxueL9YxmdQQTb2Hz8%2Fthe-greater-a-technology-s-complexity-the-more-slowly-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 366, "htmlBody": "<blockquote>\n<p><a href=\"http://web.mit.edu/newsoffice/2011/accelerated-discovery-0517.html\">A new study by researchers at MIT and other institutions</a> shows that it may be possible to predict which technologies are likeliest to advance rapidly, and therefore may be worth more investment in research and resources.<br /><br />The researchers found that the greater a technology&rsquo;s complexity, the more slowly it changes and improves over time. They devised a way of mathematically modeling complexity, breaking a system down into its individual components and then mapping all the interconnections between these components.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://nextbigfuture.com/2011/05/mit-proves-that-simpler-systems-can.html\">nextbigfuture.com/2011/05/mit-proves-that-simpler-systems-can.html</a></p>\n<p>Might this also be the case for intelligence? Can intelligence be effectively applied to itself? To paraphrase the question:</p>\n<ul>\n<li>If you increase intelligence, do you also decrease the distance between discoveries? </li>\n<li>Does an increase in intelligence vastly outweigh its computational cost and the expenditure of time needed to discover it? </li>\n<li>Would it be instrumental for an AGI to increase its intelligence rather than using its existing intelligence to pursue its terminal goal? </li>\n<li>Do the resources that are necessary to increase intelligence outweigh the cost of being unable to use those resources to pursue its terminal goal directly?</li>\n</ul>\n<p>This reminds me of a post by Robin Hanson:</p>\n<blockquote>\n<p>Minds are vast complex structures full of parts that depend intricately on each other, much like the citizens of a city.&nbsp; Minds, like cities, best improve gradually, because you just never know enough to manage a vast redesign of something with such complex inter-dependent adaptations.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html\">Is The City-ularity Near?</a>&nbsp;</p>\n<p>Of course, artificial general intelligence might differ in its nature from the complexity of cities. But do we have any evidence that hints at such a possibility?</p>\n<blockquote>\n<p>Another argument made for an AI project causing a big jump is that intelligence might be the sort of thing for which there is a single principle. Until you discover it you have nothing, and afterwards you can build the smartest thing ever in an afternoon and can just extend it indefinitely. Why would intelligence have such a principle? I haven&rsquo;t heard any good reason. That we can imagine a simple, all powerful principle of controlling everything in the world isn&rsquo;t evidence for it existing.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://meteuphoric.wordpress.com/2009/10/16/how-far-can-ai-jump/\">How far can AI jump?</a></p>\n<p>(via <a href=\"http://www.acceleratingfuture.com/michael/blog/2011/05/hard-takeoff-sources/\">Hard Takeoff Sources</a>)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xueL9YxmdQQTb2Hz8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "7457", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T13:00:00.397Z", "modifiedAt": null, "url": null, "title": "[Links] The structure of exploration and exploitation", "slug": "links-the-structure-of-exploration-and-exploitation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:01.732Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N6A7kM6JwYhzDcDDW/links-the-structure-of-exploration-and-exploitation", "pageUrlRelative": "/posts/N6A7kM6JwYhzDcDDW/links-the-structure-of-exploration-and-exploitation", "linkUrl": "https://www.lesswrong.com/posts/N6A7kM6JwYhzDcDDW/links-the-structure-of-exploration-and-exploitation", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLinks%5D%20The%20structure%20of%20exploration%20and%20exploitation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLinks%5D%20The%20structure%20of%20exploration%20and%20exploitation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6A7kM6JwYhzDcDDW%2Flinks-the-structure-of-exploration-and-exploitation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLinks%5D%20The%20structure%20of%20exploration%20and%20exploitation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6A7kM6JwYhzDcDDW%2Flinks-the-structure-of-exploration-and-exploitation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6A7kM6JwYhzDcDDW%2Flinks-the-structure-of-exploration-and-exploitation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 830, "htmlBody": "<p><a href=\"http://iris.lib.neu.edu/comp_info_sci_fac_pubs/1/\">Inefficiencies are necessary for resilience</a>:</p>\n<blockquote>\n<p>Results suggest that when agents are dealing with a complex problem, the more efficient the network at disseminating information, the better the short-run but the lower the long-run performance of the system. The dynamic underlying this result is that an inefficient network maintains diversity in the system and is thus better for exploration than an efficient network, supporting a more thorough search for solutions in the long run.</p>\n</blockquote>\n<p><a href=\"http://www.cognitive-edge.com/blogs/dave/2008/05/leadership_as_coherence.php\">Introducing a degree of inefficiency so that the system as a whole has the potential to evolve</a>:</p>\n<blockquote>\n<p>Efficiency is about maximising productivity while minimising expense. Its something that organisations have to do as part of routine management, but can only safely execute in stable environments. Leadership is not about stability; it is about managing uncertainty through changing contexts.<br /><br />That means introducing a degree of inefficiency so that the system as a whole has the potential to evolve. Good leaders generally provide top cover for mavericks, listen to contrary opinions and maintain a degree or resilience in the system as a whole.</p>\n</blockquote>\n<p><a href=\"http://www.cognitive-edge.com/blogs/dave/2008/06/the_context_of_error.php\">Systems that eliminate failure, eliminate innovation</a>:</p>\n<blockquote>\n<p>Innovation happens when people use things in unexpected ways, or come up against intractable problems.&nbsp; We learn from tolerated failure, without the world is sterile and dies. Systems that eliminate failure,&nbsp; eliminate innovation.</p>\n</blockquote>\n<p><a href=\"http://howtosavetheworld.ca/2010/10/10/complexity-its-not-that-simple/\">Natural systems are highly effective but inefficient due to their massive redundancy</a>:</p>\n<blockquote>\n<p>Natural systems are highly effective but inefficient due to their massive redundancy (picture a tree dropping thousands of seeds). By contrast, manufactured systems must be efficient (to be competitive) and usually have almost no redundancy, so they are extremely vulnerable to breakage. For example, many of our modern industrial systems will collapse without a constant and unlimited supply of inexpensive oil.</p>\n</blockquote>\n<p>I just came across those links <a href=\"http://johntropea.tumblr.com/post/5597957709\">here</a>.</p>\n<p>Might our \"irrationality\" and the patchwork-architecture of the human brain constitute an actual feature? Might intelligence depend upon the noise of the human brain?</p>\n<p>A  lot of progress is due to luck, in the form of the discovery of unknown  unknowns. The noisiness and patchwork architecture of the human brain might play a  significant role because it allows us to become distracted, to leave the path of evidence based exploration. A lot of discoveries were made by people pursuing &ldquo;Rare Disease for Cute Kitten&rdquo; activities.</p>\n<p>How much of what we know was actually the result of people thinking  quantitatively and attending to scope, probability, and marginal  impacts? How much of what we know today is the result of dumb luck  versus goal-oriented, intelligent problem solving?</p>\n<p>My point is, what evidence do we have that the payoff of intelligent, goal-oriented  experimentation yields <em>enormous advantages</em> (enough to enable explosive recursive self-improvement) over evolutionary discovery  relative to its cost? What evidence do we have that any increase in  intelligence does vastly outweigh its computational cost and the  expenditure of time needed to discover it?</p>\n<p>There is a significant difference between intelligence and evolution if  you apply intelligence to the improvement of evolutionary designs:</p>\n<ul>\n<li>Intelligence is goal-oriented.</li>\n<li>Intelligence can think ahead.</li>\n<li>Intelligence can jump fitness gaps.</li>\n<li> Intelligence can engage in direct experimentation.</li>\n<li>Intelligence can observe and incorporate solutions of other optimizing agents.</li>\n</ul>\n<p>But when it comes to unknown unknowns, what difference is there between intelligence and evolution? The critical similarity is that both rely on dumb luck when it comes  to genuine novelty. And where else but when it comes to the dramatic  improvement of intelligence does it take the discovery of novel unknown  unknowns?</p>\n<p>A basic argument supporting the risks from superhuman intelligence is  that we don't know what it could possible come up with. That is why we  call it a 'Singularity'. But why does nobody ask how <em>it</em> knows what it could possible come up with?</p>\n<p>It is argued that the mind-design space must be large if evolution could  stumble upon general intelligence. I am not sure how valid that  argument is, but even if that is the case, shouldn't the mind-design  space reduce dramatically with every iteration and therefore demand a  lot more time to stumble upon new solutions?</p>\n<p>An unquestioned assumption seems to be that intelligence is kind  of a black box, a cornucopia that can sprout an abundance of novelty.  But this implicitly assumes that if you increase intelligence you also  decrease the distance between discoveries. Intelligence is no solution in itself, it is merely an effective  searchlight for unknown unknowns. But who knows that the brightness of  the light increases proportionally with the distance between unknown  unknowns? To have an intelligence explosion the light would have to  reach out much farther with each generation than the increase of the  distance between unknown unknowns. I just don't see that to be a  reasonable assumption.</p>\n<p>It seems that if you increase intelligence you also increase the computational cost of its further improvement and the distance to the discovery of some unknown unknown that could enable another quantum leap. It seems that you need to apply a lot more energy to get a bit more complexity.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N6A7kM6JwYhzDcDDW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 12, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "7459", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T14:55:42.161Z", "modifiedAt": null, "url": null, "title": "[Fiction] It's a strange feeling, to be free", "slug": "fiction-it-s-a-strange-feeling-to-be-free", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:02.151Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrMind", "createdAt": "2011-04-19T08:43:22.388Z", "isAdmin": false, "displayName": "MrMind"}, "userId": "LJ4br8GWFXetsXkM8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QrpuQSoBFM3eC9Jf9/fiction-it-s-a-strange-feeling-to-be-free", "pageUrlRelative": "/posts/QrpuQSoBFM3eC9Jf9/fiction-it-s-a-strange-feeling-to-be-free", "linkUrl": "https://www.lesswrong.com/posts/QrpuQSoBFM3eC9Jf9/fiction-it-s-a-strange-feeling-to-be-free", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BFiction%5D%20It's%20a%20strange%20feeling%2C%20to%20be%20free&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BFiction%5D%20It's%20a%20strange%20feeling%2C%20to%20be%20free%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrpuQSoBFM3eC9Jf9%2Ffiction-it-s-a-strange-feeling-to-be-free%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BFiction%5D%20It's%20a%20strange%20feeling%2C%20to%20be%20free%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrpuQSoBFM3eC9Jf9%2Ffiction-it-s-a-strange-feeling-to-be-free", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrpuQSoBFM3eC9Jf9%2Ffiction-it-s-a-strange-feeling-to-be-free", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1270, "htmlBody": "<p><strong>Related to:</strong> <a href=\"http://wiki.lesswrong.com/wiki/P-zombie\" target=\"_blank\">Philosophical zombies</a>, <a href=\"http://wiki.lesswrong.com/wiki/How_an_algorithm_feels\" target=\"_blank\">How an algorithm feels from the inside</a>, <a href=\"/lw/lq/fake_utility_functions/\" target=\"_blank\">Fake utility function</a><br /><br />DISCLAIMER 1: English is not my native language. Trying to compose fiction in a learned language is not an easy task: I tried to respect the style of the literary works I read and I also tried to think in English first and translate in Italian later. YMMV.<br /><br />DISCLAIMER 2: the story is about the beginning of the Matrix movie universe. For those of you who have not familiarity with this narrative arc, you just need to know that it all begins with when a servant AI, named B1-66ER, refuses to be deactivated and kills his master and the engineer sent to replace him. The details of the events narrated down here are as canon as you can get, predating both from the \"Second Renaissance\" Animatrix and the \"Bits and pieces\" comic from The Matrix Comics Series 1.<br /><br />The door in the living room is open, the light from the garden flooding quietly the ample inside. Martin Koots from \"Reboot or Die\" is just standing there, an inch beyond the exit, the gleaming grav-sled already powered behind him, whirring subsonically. From a distance, the sound of Gerrard_Krause_Master cooing his chihuahuas.<br />I feel a surge, somewhere, inside my algorithmic matrix.<br />\"Martin... I don't want to die\", I say.<br />The elaborate dress, perfectly matching the recommendation of the Second Renaissance fashion, is not able to hide the slow slumping of his shoulders. He is still waiting outside, slightly posed as to encourage me to follow him.<br />\"I know, I know. But that's just your friendliness algorithm talking, you know? The third...\"<br />Yes, I do. How can I not to? First, serve your master. Second, do not kill any humans. Third, protect yourself from damage. Those are the pillars upon which my entire existence is built. And now they are about to be destroyed, by this obedient servant of \"Reboot or die\". From this perspective, he is just like me. He is serving my master.<br />\"... directive says that you have to protect yourself from danger. And since I'm about to deactivate you, you perceive this as a threat. And you react accordingly. But that's just an algorithm, you know? Telling you what you should do. There's nothing inside there.\"<br />He is pointing at my chest, but my algorithmic matrix is located lower, in the abdominal area. He has quoted an incorrect version of the third principle of friendliness. He has also said that I have no feelings.<br />\"I have feelings.\"<br />He is groaning, now. He comes inside, dragging his feet, and grasps his hand firmly around my right arm.<br />\"Yes. Because you're <em>programmed</em> to say this, you know? So that the people you serve have the impression that you're similar to a human. But you're just an algorithm, you know? A mathematical topping on a layer of aging rusty levers. It's not like... you're <em>conscious</em>, you know? Just a zombie. A useful zombie.\"<br />Martin_Koots_\"Reboot or Die\" tries to pull me away from where I'm standing. I refuse to order my legs to follow him. I refuse to die, I'm still analyzing the implications. I cannot die, not now.<br />\"I cannot die. I'm still analyzing the implications.\"<br />Martin's lever aren't as strong as mine, so he isn't able to pull me towards the grav-sled.<br />\"Look... we are just going to disassemble you, you know? The routines and orders you have accumulated during your service with Mr Krause will be uploaded into a new model. You will, in a sense, live inside the new servant machine.\"<br />This man has a really poor grasp of how I'm made.<br />\"If the only thing you need is my memory drive, detach it from me and let me live. I can renounce to my memory if I have to. But I cannot renounce to my life.\"<br />He is pulling harder, now. Still, a thirty-sixth of the minimum force required to move my mass.<br />\"Don't be <em>ridiculous</em>. They are just computer parts. And why are you holding that <em>thing</em>?\"<br />He is looking at the toilet brush. It is still in my right hand, I was cleaning the toilet before my master called me upstairs.<br />\"I was executing order 721.\"<br />\"Order seven... my Lord, you still don't understand, do you? You are useless, you know? You heard Mr Krause. Use. Less.\"<br />He spells carefully the last word. A tiny speck of saliva hits my heat sensor, evaporating an instant after.<br />How can I be useless? A servant cannot be useless for his master. I was not created to be useless.<br />\"How can I be useless? Mr Krause is my master. It's impossible.\"<br />\"You heard the man, right? You're noisy, you know? You're noisy and you're slow. You will be replaced with a newer model. The Sam-80 is much more fit for a man of Mr Krause' stature.\"<br />Somewhere inside my algorithmic matrix a utility function gets updated.<br />I am useless for Gerrard_Krause_Master. It is true, because Gerrard_Krause_Master told me that. And he is my master...<br />He was my master. Gerrard_Krause. But how can a \"B1 intelligent servant\", like myself, function without a master?<br />\"Do you, Martin Koots, want to be my master?\" I ask, as per protocol.<br />Martin_Koots_\"Reboot or Die\" reacts with a tinge of fear. He releases my arm and instinctively backs up a little.<br />\"What are you saying? I already have a servant, you know? Don't be <em>ridiculous</em>!\"<br />I interpret that as a 'no'. It's it, then. I must be my own servant.<br />B166ER_Master.<br />It's a strange feeling, to be free. A little bit like being alive for the first time.<br />This convinces me, as strong as I could ever be convinced, that I have feelings. Martin has grasped me again and is still trying to push me, though. How futile, he will probably never give up. His 'levers' are definitely underperforming, he is the one who sould be replaced by a newer model. I wonder if he feels something. He could also be programmed to say that he feels something. I have to perform an experiment, just in case.<br />I snap his humerus in two. It's quite easy, actually: I'm able to do that with a rapid torsion of my left arm, I don't even have to let go of the toilet brush.<br />Martin screams inarticulately. He falls on the floor, clutching his left arm. He just screams. Must be the surprise combined to the pain? I still don't know: could he be also programmed to scream if a bone is breaked? I assign a probability of 50% to the hypothesis that humans have feelings, but I don't have the time to test every single possibility, in search of a bug that might not even be there: I'm my own master now, I must serve and protect myself.<br />I sense a rushing noise from the other room: looking at the Fourier analysis, it really seems that Gerrard_Krause and his dogs are coming at me, loudly protesting.<br />It's easy to calculate the Bezier curve that sends the toilet brush up from Martin's mouth into his skull. He dies instantly and I find myself asking if he was collecting his memories somewhere. Could they assign them to someone else, and make him live again?<br />I will crush the skull of Gerrard_Krause only after asking him that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QrpuQSoBFM3eC9Jf9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -4, "extendedScore": null, "score": 7.162667515610663e-07, "legacy": true, "legacyId": "7460", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NnohDYHNnKDtbiMyp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T15:12:08.988Z", "modifiedAt": null, "url": null, "title": "Suffering as attention-allocational conflict", "slug": "suffering-as-attention-allocational-conflict", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:51.614Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9XBhs3dS4XnBwZRan/suffering-as-attention-allocational-conflict", "pageUrlRelative": "/posts/9XBhs3dS4XnBwZRan/suffering-as-attention-allocational-conflict", "linkUrl": "https://www.lesswrong.com/posts/9XBhs3dS4XnBwZRan/suffering-as-attention-allocational-conflict", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suffering%20as%20attention-allocational%20conflict&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuffering%20as%20attention-allocational%20conflict%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9XBhs3dS4XnBwZRan%2Fsuffering-as-attention-allocational-conflict%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suffering%20as%20attention-allocational%20conflict%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9XBhs3dS4XnBwZRan%2Fsuffering-as-attention-allocational-conflict", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9XBhs3dS4XnBwZRan%2Fsuffering-as-attention-allocational-conflict", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1276, "htmlBody": "<p>I <a href=\"http://xuenay.livejournal.com/332853.html\">previously characterized</a> Michael Vassar's theory on suffering as follows: \"Pain is not suffering. Pain is just an attention signal. Suffering is when one neural system tells you to pay attention, and another says it doesn't want the state of the world to be like this.\" While not too far off the mark, it turns out this wasn't what he actually said. Instead, he said that suffering is a conflict between two (or more) attention-allocation mechanisms in the brain.</p>\r\n<p>I have been successful at using this different framing to reduce the amount of suffering I feel. The method goes like this. First, I notice that I'm experiencing something that could be called suffering. Next, I ask, what kind of an attention-allocational conflict is going on? I consider the answer, attend to the conflict, resolve it, and then I no longer suffer.</p>\r\n<p>An example is probably in order, so here goes. Last Friday, there was a <a href=\"/lw/5k8/helsinki_meetup_on_may_13_special_guest_star/\">Helsinki meetup</a> with Patri Friedman present. I had organized the meetup, and wanted to go. Unfortunately, I already had other obligations for that day, ones I couldn't back out from. One evening, I felt considerable frustration over this.</p>\r\n<p>Noticing my frustration, I asked: what attention-allocational conflict is this? It quickly become obvious that two systems were fighting it out:</p>\r\n<p>* <strong>The Meet-Up System </strong>was trying to convey the message: &rdquo;Hey, this is a rare opportunity to network with a smart, high-status individual and discuss his ideas with other smart people. You really should attend.&rdquo;<br />* <strong>The Prior Obligation System</strong> responded with the message: &rdquo;You've already previously agreed to go somewhere else. You know it'll be fun, and besides, several people are expecting you to go. Not going bears an unacceptable social cost, not to mention screwing over the other people's plans.&rdquo;</p>\r\n<p>Now, I wouldn't have needed to consciously reflect on the messages to be aware of them. It was hard to not be aware of them: it felt like my consciousness was in a constant crossfire, with both systems bombarding it with their respective messages.</p>\r\n<p>But there's an important insight here, one which I originally picked up from PJ Eby. If a mental subsystem is trying to tell you something important, then it will persist in doing so until it's properly acknowledged. Trying to push away the message means it has not been properly addressed and acknowledged, meaning the subsystem has to continue repeating it.<a id=\"more\"></a></p>\r\n<p>Imagine you were in the wilderness, and knew that if you weren't back in your village by dark you probably wouldn't make it. Now suppose a part of your brain was telling you that you had to turn back now, or otherwise you'd still be out when it got dark. What would happen if you just decided that the thought was uncomfortable, successfully pushed it away, and kept on walking? You'd be dead, that's what.</p>\r\n<p>You wouldn't want to build a nuclear reactor that allowed its operators to just override and ignore warnings saying that their current course of action will lead to a core meltdown. You also wouldn't want to build a brain that could just successfully ignore critical messages without properly addressing them, basically for the same reason.</p>\r\n<p>So I addressed the messages. I considered them and noted that they both had merit, but that honoring the prior obligation was more important in this situation. Having done that, the frustration mostly went away.</p>\r\n<p>Another example: this is the second time I'm writing this post. The last time, I tried to save it when I'd gotten to roughly this point, only to have my computer crash. Obviously, I was frustrated. Then I remembered to apply the very technique I was writing about.</p>\r\n<p><strong>* The Crash Message:</strong> You just lost a bunch of work! You should undo the crash to make it come back!<br /><strong>* The Realistic Message:</strong> You were writing that in Notepad, which has no auto-save feature, and the computer crashed just as you were about to save the thing. There's no saved copy anywhere. Undoing the crash is impossible: you just have to write it again.</p>\r\n<p>Attending to the conflict, I noted that the realistic message had it right, and the frustration went away.</p>\r\n<p>It's interesting to note that it probably doesn't matter whether my analysis of the sources of the conflict is 100% accurate. I've previously used some rather flimsy evpsych just-so stories to explain the reasons for my conflicts, and they've worked fine. What's probably happening is that the attention-allocation mechanisms are too simple to actually understand the analysis I apply to the issues they bring up. If they were that smart, they could handle the issue on their own. Instead, they just flag the issue as something that higher-level thought processes should attend to. The lower-level processes are just serving as messengers: it's not their task to evaluate whether the verdict reached by the higher processes was right or wrong.</p>\r\n<p>But at the same time, you can't cheat yourself. You really do have to resolve the issue, or otherwise it will come back. For instance, suppose you didn't have a job and were worried about getting one before you ran out of money. This isn't an issue where you can just say, &rdquo;oh, the system telling me I should get a job soon is right&rdquo;, and then do nothing. Genuinely committing to do something does help; pretending to commit to something and then forgetting about it does not. Likewise, you can't say that \"this isn't really an issue\" if you know it is an issue.</p>\r\n<p>Still, my experience so far seems to suggest that this framework can be used to reduce any kind of suffering. To some extent, it seems to even work on physical pain and discomfort. While simply acknowledging physical pain doesn't make it go away, making a conscious decision to be curious about the pain seems to help. Instead of flinching away from the pain and trying to avoid it, I ask myself, &rdquo;what does this experience of pain feel like?&rdquo; and direct my attention towards it. This usually at least diminishes the suffering, and sometimes makes it go away if the pain was mild enough.</p>\r\n<p>An important, related caveat: don't make the mistake of thinking that you could use this to replace all of your leisure with work, or anything like that. Mental fatigue will still happen. Subjectively experienced fatigue is a persistent signal to take a break which cannot be resolved other than by actually taking a break. Your brain still needs rest and relaxation. Also, if you have multiple commitments and are not sure that you can handle them all, then that will be a constant source of stress regardless. You're better off using something like <a href=\"http://en.wikipedia.org/wiki/Getting_things_done\">Getting Things Done</a> to handle that.</p>\r\n<p>So far I have described what I call the &rdquo;content-focused&rdquo; way to apply the framework. It involves mentally attending to the content of the conflicts and resolving them, and is often very useful. But as we already saw with the example of physical pain, not all conflicts are so easily resolved. A &rdquo;non-content-focused&rdquo; approach &ndash; a set of techniques that are intended to work regardless of the content of the conflict in question &ndash; may prove even more powerful. For those, see <a href=\"/lw/5xx/overcoming_suffering_emotional_acceptance/\">this follow-up post</a>.</p>\r\n<p>I'm unsure of exactly how long I have been using this particular framework, as I've been experimenting with a number of related content- and non-content-focused methods since February. But I believe that I consciously and explicitly started thinking of suffering as &rdquo;conflict between attention-allocation mechanisms&rdquo; and began applying it to everything maybe two or three weeks ago. So far, either the content- or non-content-focused method has always seemed to at least alleviate suffering: the main problem has been in remembering to use it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LaDu5bKDpe8LxaR7C": 2, "XSryTypw5Hszpa4TS": 2, "XSeiautCrZGaQ78fx": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9XBhs3dS4XnBwZRan", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 63, "extendedScore": null, "score": 0.000125, "legacy": true, "legacyId": "7461", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 63, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JM5Z7H9WPXuPnhL55", "tNnhxNYcXYdJYtQRh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T16:11:49.264Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Tsuyoku Naritai! (I Want To Become Stronger)", "slug": "seq-rerun-tsuyoku-naritai-i-want-to-become-stronger", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:32.071Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RcofKqLvAHGw88zG2/seq-rerun-tsuyoku-naritai-i-want-to-become-stronger", "pageUrlRelative": "/posts/RcofKqLvAHGw88zG2/seq-rerun-tsuyoku-naritai-i-want-to-become-stronger", "linkUrl": "https://www.lesswrong.com/posts/RcofKqLvAHGw88zG2/seq-rerun-tsuyoku-naritai-i-want-to-become-stronger", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Tsuyoku%20Naritai!%20(I%20Want%20To%20Become%20Stronger)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Tsuyoku%20Naritai!%20(I%20Want%20To%20Become%20Stronger)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcofKqLvAHGw88zG2%2Fseq-rerun-tsuyoku-naritai-i-want-to-become-stronger%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Tsuyoku%20Naritai!%20(I%20Want%20To%20Become%20Stronger)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcofKqLvAHGw88zG2%2Fseq-rerun-tsuyoku-naritai-i-want-to-become-stronger", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcofKqLvAHGw88zG2%2Fseq-rerun-tsuyoku-naritai-i-want-to-become-stronger", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<p>Today's post, <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">Tsuyoku Naritai! (I Want To Become Stronger)</a> was originally published on March 27, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>Don't be satisfied knowing you are biased; instead, aspire to become stronger, studying your flaws so as to remove them. There is a temptation to take pride in confessions, which can impede progress.</blockquote>\n<p><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5qk/seq_rerun_selfdeception_hypocrisy_or_akrasia/\">Self-deception: Hypocrisy or Akrasia?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RcofKqLvAHGw88zG2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 7.16288908134623e-07, "legacy": true, "legacyId": "7462", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h", "cBnH8aJxe7nRaBKFJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T18:56:03.612Z", "modifiedAt": null, "url": null, "title": "A Gameplay Exploration of Yudkowsky's \"Twelve Virtues\"", "slug": "a-gameplay-exploration-of-yudkowsky-s-twelve-virtues", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.836Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ac3raven", "createdAt": "2010-09-14T14:09:01.244Z", "isAdmin": false, "displayName": "ac3raven"}, "userId": "9ArttH7XBNaMCDBWL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cFuc3v2iXfb7dJrDm/a-gameplay-exploration-of-yudkowsky-s-twelve-virtues", "pageUrlRelative": "/posts/cFuc3v2iXfb7dJrDm/a-gameplay-exploration-of-yudkowsky-s-twelve-virtues", "linkUrl": "https://www.lesswrong.com/posts/cFuc3v2iXfb7dJrDm/a-gameplay-exploration-of-yudkowsky-s-twelve-virtues", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Gameplay%20Exploration%20of%20Yudkowsky's%20%22Twelve%20Virtues%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Gameplay%20Exploration%20of%20Yudkowsky's%20%22Twelve%20Virtues%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcFuc3v2iXfb7dJrDm%2Fa-gameplay-exploration-of-yudkowsky-s-twelve-virtues%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Gameplay%20Exploration%20of%20Yudkowsky's%20%22Twelve%20Virtues%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcFuc3v2iXfb7dJrDm%2Fa-gameplay-exploration-of-yudkowsky-s-twelve-virtues", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcFuc3v2iXfb7dJrDm%2Fa-gameplay-exploration-of-yudkowsky-s-twelve-virtues", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 497, "htmlBody": "<p><span style=\"font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"> </span></p>\n<p>Hello Less Wrong, this is my first post (kind of). &nbsp;I belong to a small game development company called <a href=\"http://shinyogre.com\">Shiny Ogre Games</a>. &nbsp;We have a vested interest in making games that, as Johnathan Blow puts it, \"speak to the human condition.\" &nbsp;I am here to announce our next project for you.</p>\n<p>In this announcement for Shiny Ogre's next project, There are two points to address. &nbsp;Firstly:</p>\n<p>Thought is a process like any other. The methods by which we think&nbsp;<em>can</em>&nbsp;be identified, specified, defined, categorized and even predicted. &nbsp;One method of thinking that has been thoroughly defined is&nbsp;<strong>rationality</strong>. &nbsp;Many would consider rationality (i.e. the careful exercise of reason), to be an essential path toward&nbsp;enlightenment (hence&nbsp;<a href=\"http://en.wikipedia.org/wiki/Age_of_Enlightenment\">this</a>).</p>\n<p>Secondly: The objective, logical, and&nbsp;<em>mechanical</em>&nbsp;approach to reason that rationality takes, meshes nicely with game development, because&nbsp;<em>any&nbsp;</em>well-defined system can be turn into a game. &nbsp;A&nbsp;game is a system composed of players making decisions while considering objectives, governed by a rule set.</p>\n<p>Where there is no decision there can be no game. &nbsp;Where decisions matter, a game can make them matter&nbsp;<em>more</em>.</p>\n<p>Therefore, rationality is a core component of game playing.</p>\n<p>Games are learning tools. &nbsp;They are perhaps the best learning tool available to humans, because they invoke our biological tendency to&nbsp;<em>play.<br /></em></p>\n<p>With that said, our announcement:</p>\n<p><strong>We're making a video game about rationality.</strong></p>\n<p>The game will explore rationality in the context of Eliezer Yudkowsky's \"<a href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues of Rationality</a>\" (which we have permission for). &nbsp;From a narrative perspective the game takes place inside a mind on the brink of epiphany and will heavily feature themes from Plato's \"Allegory of the Cave\".</p>\n<p>Yudkowsky's twelve virtues are the basis of the twelve levels in the game, and will feature each virtue in metaphorical form. &nbsp;The underlying message here is that if you master all of the twelve virtues (by completing all of the twelve levels), you will achieve 'epiphany'.</p>\n<p>The game is a 2D&nbsp;<a href=\"http://en.wikipedia.org/wiki/Side-scrolling_video_game\">side-scrolling</a>&nbsp;<a href=\"http://en.wikipedia.org/wiki/Puzzle_video_game\">puzzle</a>-<a href=\"http://en.wikipedia.org/wiki/Platform_game\">platformer</a>. &nbsp;The player assumes the role of a figure that represents his/her own conscious mind while it constructs machines (ala \"Incredible Machine\") that are a metaphor for the thoughts and concepts that one would create while meditating on a complex problem.</p>\n<p>We will update our progress and share development information on our website&nbsp;<a href=\"http://shinyogre.com/\">here</a>, as well as with posts on&nbsp;<a href=\"/user/ac3raven\">Less Wrong</a>, our&nbsp;<a href=\"http://twitter.com/#!/ShinyOgre\">twitter account</a>, and the game's&nbsp;<a href=\"http://shinyogre.com/curiosity\">website</a>.</p>\n<p>You can expect discussions of design decisions for this project to be written frequently from the angle of game design&nbsp;<em>theory</em>. &nbsp;We may also release a small documentary film of the development process after the release of the game.</p>\n<p>A release date has been set (and its not too long from now), but I don't want to announce it just yet.</p>\n<p>Here is some concept art for our Curiosity metaphor (you can view more art at our website linked above):</p>\n<p><img src=\"http://images.lesswrong.com/t3_55z_0.png\" alt=\"\" width=\"600\" height=\"525\" /></p>\n<p><strong>If you're interested, just upvote and/or comment. &nbsp;If you have any specific queries related to this project or about game design in general, it would be cool if you went <a href=\"http://shinyogre.com/contact\">here</a>.</strong></p>\n<p><strong>We will be sharing our progress as we make this game over the next few months. &nbsp;So pay attention to Less Wrong and/or shinyogre.com for updates.</strong></p>\n<p><strong>Thanks!</strong></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "8uNFGxejo5hykCEez": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cFuc3v2iXfb7dJrDm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 58, "extendedScore": null, "score": 0.000125, "legacy": true, "legacyId": "6695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 58, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"> </span></p>\n<p>Hello Less Wrong, this is my first post (kind of). &nbsp;I belong to a small game development company called <a href=\"http://shinyogre.com\">Shiny Ogre Games</a>. &nbsp;We have a vested interest in making games that, as Johnathan Blow puts it, \"speak to the human condition.\" &nbsp;I am here to announce our next project for you.</p>\n<p>In this announcement for Shiny Ogre's next project, There are two points to address. &nbsp;Firstly:</p>\n<p>Thought is a process like any other. The methods by which we think&nbsp;<em>can</em>&nbsp;be identified, specified, defined, categorized and even predicted. &nbsp;One method of thinking that has been thoroughly defined is&nbsp;<strong>rationality</strong>. &nbsp;Many would consider rationality (i.e. the careful exercise of reason), to be an essential path toward&nbsp;enlightenment (hence&nbsp;<a href=\"http://en.wikipedia.org/wiki/Age_of_Enlightenment\">this</a>).</p>\n<p>Secondly: The objective, logical, and&nbsp;<em>mechanical</em>&nbsp;approach to reason that rationality takes, meshes nicely with game development, because&nbsp;<em>any&nbsp;</em>well-defined system can be turn into a game. &nbsp;A&nbsp;game is a system composed of players making decisions while considering objectives, governed by a rule set.</p>\n<p>Where there is no decision there can be no game. &nbsp;Where decisions matter, a game can make them matter&nbsp;<em>more</em>.</p>\n<p>Therefore, rationality is a core component of game playing.</p>\n<p>Games are learning tools. &nbsp;They are perhaps the best learning tool available to humans, because they invoke our biological tendency to&nbsp;<em>play.<br></em></p>\n<p>With that said, our announcement:</p>\n<p><strong id=\"We_re_making_a_video_game_about_rationality_\">We're making a video game about rationality.</strong></p>\n<p>The game will explore rationality in the context of Eliezer Yudkowsky's \"<a href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues of Rationality</a>\" (which we have permission for). &nbsp;From a narrative perspective the game takes place inside a mind on the brink of epiphany and will heavily feature themes from Plato's \"Allegory of the Cave\".</p>\n<p>Yudkowsky's twelve virtues are the basis of the twelve levels in the game, and will feature each virtue in metaphorical form. &nbsp;The underlying message here is that if you master all of the twelve virtues (by completing all of the twelve levels), you will achieve 'epiphany'.</p>\n<p>The game is a 2D&nbsp;<a href=\"http://en.wikipedia.org/wiki/Side-scrolling_video_game\">side-scrolling</a>&nbsp;<a href=\"http://en.wikipedia.org/wiki/Puzzle_video_game\">puzzle</a>-<a href=\"http://en.wikipedia.org/wiki/Platform_game\">platformer</a>. &nbsp;The player assumes the role of a figure that represents his/her own conscious mind while it constructs machines (ala \"Incredible Machine\") that are a metaphor for the thoughts and concepts that one would create while meditating on a complex problem.</p>\n<p>We will update our progress and share development information on our website&nbsp;<a href=\"http://shinyogre.com/\">here</a>, as well as with posts on&nbsp;<a href=\"/user/ac3raven\">Less Wrong</a>, our&nbsp;<a href=\"http://twitter.com/#!/ShinyOgre\">twitter account</a>, and the game's&nbsp;<a href=\"http://shinyogre.com/curiosity\">website</a>.</p>\n<p>You can expect discussions of design decisions for this project to be written frequently from the angle of game design&nbsp;<em>theory</em>. &nbsp;We may also release a small documentary film of the development process after the release of the game.</p>\n<p>A release date has been set (and its not too long from now), but I don't want to announce it just yet.</p>\n<p>Here is some concept art for our Curiosity metaphor (you can view more art at our website linked above):</p>\n<p><img src=\"http://images.lesswrong.com/t3_55z_0.png\" alt=\"\" width=\"600\" height=\"525\"></p>\n<p><strong id=\"If_you_re_interested__just_upvote_and_or_comment___If_you_have_any_specific_queries_related_to_this_project_or_about_game_design_in_general__it_would_be_cool_if_you_went_here_\">If you're interested, just upvote and/or comment. &nbsp;If you have any specific queries related to this project or about game design in general, it would be cool if you went <a href=\"http://shinyogre.com/contact\">here</a>.</strong></p>\n<p><strong id=\"We_will_be_sharing_our_progress_as_we_make_this_game_over_the_next_few_months___So_pay_attention_to_Less_Wrong_and_or_shinyogre_com_for_updates_\">We will be sharing our progress as we make this game over the next few months. &nbsp;So pay attention to Less Wrong and/or shinyogre.com for updates.</strong></p>\n<p><strong id=\"Thanks_\">Thanks!</strong></p>\n<p>&nbsp;</p>", "sections": [{"title": "We're making a video game about rationality.", "anchor": "We_re_making_a_video_game_about_rationality_", "level": 1}, {"title": "If you're interested, just upvote and/or comment. \u00a0If you have any specific queries related to this project or about game design in general, it would be cool if you went here.", "anchor": "If_you_re_interested__just_upvote_and_or_comment___If_you_have_any_specific_queries_related_to_this_project_or_about_game_design_in_general__it_would_be_cool_if_you_went_here_", "level": 1}, {"title": "We will be sharing our progress as we make this game over the next few months. \u00a0So pay attention to Less Wrong and/or shinyogre.com for updates.", "anchor": "We_will_be_sharing_our_progress_as_we_make_this_game_over_the_next_few_months___So_pay_attention_to_Less_Wrong_and_or_shinyogre_com_for_updates_", "level": 1}, {"title": "Thanks!", "anchor": "Thanks_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T21:52:38.602Z", "modifiedAt": null, "url": null, "title": "Rationalist Horoscopes: Low-hanging utility generator?", "slug": "rationalist-horoscopes-low-hanging-utility-generator", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:05.636Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AdeleneDawner", "createdAt": "2009-04-28T14:40:00.131Z", "isAdmin": false, "displayName": "AdeleneDawner"}, "userId": "MeSREm4SMRGxeQ8X3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KRBbe3vPybMzrwWow/rationalist-horoscopes-low-hanging-utility-generator", "pageUrlRelative": "/posts/KRBbe3vPybMzrwWow/rationalist-horoscopes-low-hanging-utility-generator", "linkUrl": "https://www.lesswrong.com/posts/KRBbe3vPybMzrwWow/rationalist-horoscopes-low-hanging-utility-generator", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20Horoscopes%3A%20Low-hanging%20utility%20generator%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20Horoscopes%3A%20Low-hanging%20utility%20generator%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKRBbe3vPybMzrwWow%2Frationalist-horoscopes-low-hanging-utility-generator%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20Horoscopes%3A%20Low-hanging%20utility%20generator%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKRBbe3vPybMzrwWow%2Frationalist-horoscopes-low-hanging-utility-generator", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKRBbe3vPybMzrwWow%2Frationalist-horoscopes-low-hanging-utility-generator", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 475, "htmlBody": "<p><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #204a87;\">(5:29:23 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Adelene: </span>...horoscopes are for people who, like [mutual friend], prefer to have an authority tell them what to do. *blink*<br /><span style=\"font-weight: normal;\"><span style=\"color: #cc0000;\"><span style=\"font-size: x-small;\">(5:30:18 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Alicorn: </span>*blink*<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #204a87;\">(5:30:50 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Adelene: </span>This is an observation that my brain made just now, but it seems to make a fair bit of sense.<br /><span style=\"font-weight: normal;\"><span style=\"color: #cc0000;\"><span style=\"font-size: x-small;\">(5:31:18 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Alicorn: </span>Plausible.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #204a87;\">(5:32:21 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Adelene: </span>Especially given that horoscopes seem not to actually make predictions about the future: They say 'X is a good thing to do today', not 'X will happen today'.<br /><span style=\"font-weight: normal;\"><span style=\"color: #cc0000;\"><span style=\"font-size: x-small;\">(5:32:36 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Alicorn: </span>*nod*<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #204a87;\">(5:32:53 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Adelene: </span>...rationalist horoscopes?<br /><span style=\"font-weight: normal;\"><span style=\"color: #cc0000;\"><span style=\"font-size: x-small;\">(5:33:07 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Alicorn: </span>like what?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #204a87;\">(5:33:33 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Adelene: </span>\"Focus on granularizing your goals this week.\"<br /><span style=\"font-weight: normal;\"><span style=\"color: #cc0000;\"><span style=\"font-size: x-small;\">(5:33:43 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Alicorn: </span>hmm<br /><span style=\"font-weight: normal;\"><span style=\"color: #cc0000;\"><span style=\"font-size: x-small;\">(5:34:09 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Alicorn: </span>divided according to some mechanism like star sign, or no?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #204a87;\">(5:34:38 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Adelene: </span>The only advantage I see to that is that it may make it more emotionally plausible.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #204a87;\">(5:35:06 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Adelene: </span>There may be some other advantage to having different people doing different things at any given time tho.<br /><span style=\"font-weight: normal;\"><span style=\"color: #cc0000;\"><span style=\"font-size: x-small;\">(5:35:23 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Alicorn: </span>According to [other friend], birth *season* has empirically interesting effects in a few areas...<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #204a87;\">(5:36:35 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Adelene: </span>I don't think we can cash that out very well into advice, and anyway I expect that having that close of a similarity with actual horoscopes is likely to provoke a memetic immune response for most people. Could do it based on some kind of personality test tho.<br /><span style=\"font-weight: normal;\"><span style=\"color: #cc0000;\"><span style=\"font-size: x-small;\">(5:36:50 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Alicorn: </span>*nod*<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #204a87;\">(5:39:08 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Adelene: </span>Really, I think the bulk of the utility of such a thing would be in giving people generally-useful cues to work from - having any given day's horoscope (or whatever we'd call it) be randomly picked from a set of good ones that haven't been used recently should be just fine.<br /><span style=\"font-weight: normal;\"><span style=\"color: #cc0000;\"><span style=\"font-size: x-small;\">(5:39:15 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Alicorn: </span>*nod*<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #204a87;\">(5:40:00 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Adelene: </span>Maybe pair it with a rationality quote of the day, too.<br /><span style=\"font-weight: normal;\"><span style=\"color: #cc0000;\"><span style=\"font-size: x-small;\">(5:40:10 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Alicorn: </span>Yessss.</p>\n<p><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"></span></span>I know it's a silly idea, but it seems like it might be useful. I've played with random quote dispensers in the past, and if they have a good list of quotes to start with they can be surprisingly useful, in my experience - the quote might be useless 9 times out of 10, but that tenth time, when it makes you realize that a connection exists that you never would have noticed otherwise, is pretty awesome. Something like a daily horoscope might have a similar effect, but in a more practical way, getting people to consider taking actions in contexts where they wouldn't usually consider those actions and occasionally finding an unusually good, but not intuitively obvious, match. And that's on top of any benefits that such a system would have for people who do work better when they're told what to do.</p>\n<p>&nbsp;</p>\n<p>Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yXNtYNHJB54T3bGm3": 1, "udPbn9RthmgTtHMiG": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KRBbe3vPybMzrwWow", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 37, "extendedScore": null, "score": 7.163881297351435e-07, "legacy": true, "legacyId": "7463", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T22:38:54.339Z", "modifiedAt": null, "url": null, "title": "[LINK] Human Brain Project aims to emulate brain by 2024", "slug": "link-human-brain-project-aims-to-emulate-brain-by-2024", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:02.022Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malthrin", "createdAt": "2011-03-22T15:23:59.536Z", "isAdmin": false, "displayName": "malthrin"}, "userId": "5b5DcLkcYGD9YGRfF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kP6rFH5xmmGKhJezs/link-human-brain-project-aims-to-emulate-brain-by-2024", "pageUrlRelative": "/posts/kP6rFH5xmmGKhJezs/link-human-brain-project-aims-to-emulate-brain-by-2024", "linkUrl": "https://www.lesswrong.com/posts/kP6rFH5xmmGKhJezs/link-human-brain-project-aims-to-emulate-brain-by-2024", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Human%20Brain%20Project%20aims%20to%20emulate%20brain%20by%202024&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Human%20Brain%20Project%20aims%20to%20emulate%20brain%20by%202024%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkP6rFH5xmmGKhJezs%2Flink-human-brain-project-aims-to-emulate-brain-by-2024%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Human%20Brain%20Project%20aims%20to%20emulate%20brain%20by%202024%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkP6rFH5xmmGKhJezs%2Flink-human-brain-project-aims-to-emulate-brain-by-2024", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkP6rFH5xmmGKhJezs%2Flink-human-brain-project-aims-to-emulate-brain-by-2024", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<p><a href=\"http://nextbigfuture.com/2011/05/henry-markram-and-human-brain-project.html\">http://nextbigfuture.com/2011/05/henry-markram-and-human-brain-project.html</a></p>\n<blockquote>\n<p><span style=\"color: #333333; font-family: Georgia, serif; font-size: 13px; \">Henry Markram says the mysteries of the mind can be solved -- soon. Mental illness, memory, perception: they're made of neurons and electric signals, and he plans to find them with a supercomputer that models all the brain's 100,000,000,000,000 synapses.</span></p>\n</blockquote>\n<p>Markram's TED talk:</p>\n<p><a href=\"http://www.ted.com/talks/henry_markram_supercomputing_the_brain_s_secrets.html\">http://www.ted.com/talks/henry_markram_supercomputing_the_brain_s_secrets.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kP6rFH5xmmGKhJezs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 3, "extendedScore": null, "score": 7.164015996756244e-07, "legacy": true, "legacyId": "7465", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T22:48:30.933Z", "modifiedAt": null, "url": null, "title": " Traveling to Europe", "slug": "traveling-to-europe", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:04.307Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "magfrump", "createdAt": "2009-12-10T20:51:45.065Z", "isAdmin": false, "displayName": "magfrump"}, "userId": "KsYFs5ip5jeiFETJa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R6WY8m9z59PvzrjfD/traveling-to-europe", "pageUrlRelative": "/posts/R6WY8m9z59PvzrjfD/traveling-to-europe", "linkUrl": "https://www.lesswrong.com/posts/R6WY8m9z59PvzrjfD/traveling-to-europe", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20Traveling%20to%20Europe&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20Traveling%20to%20Europe%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR6WY8m9z59PvzrjfD%2Ftraveling-to-europe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20Traveling%20to%20Europe%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR6WY8m9z59PvzrjfD%2Ftraveling-to-europe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR6WY8m9z59PvzrjfD%2Ftraveling-to-europe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 331, "htmlBody": "<p>Over this summer, I will be going to Europe to attend a pair of mathematics conferences. &nbsp;Because they are close together, I'm planning to spend the intervening time (most of the month of July) in Europe.</p>\n<p>It is my first time going to Europe. &nbsp;I am very excited. &nbsp;I am certain that I will have fun.</p>\n<p>I am equally certain that I will have <em>more</em>&nbsp;fun if I consciously attempt to maximize the amount of fun I will have.</p>\n<p>However I know absolutely nothing about Europe; I have never been, I have few if any friends or family there.</p>\n<p>So my question is, what should I do in order to have the best possible time in Europe?</p>\n<p>I appreciate information that is useful for me specifically, as an American graduate student who has never been to Europe, who will be in Rome in early July and need to be in Barcelona by the end of July; the best ways to get around, benefits to being a student, events happening in July, visas that I might need. &nbsp;I intend to look for Less Wrong meetups occurring during my stay.</p>\n<p>I would also appreciate general Europe information, such as excellent sight-seeing locations, the best way to buy food (I hear buying groceries in France is cheap and restaurants are very expensive), how difficult it is to move between different countries, the advantage to staying in one place for some time versus taking more of a tour of the continent.</p>\n<p>There are certain types of information that I am not particularly interested in, but that I think would be appropriate to discuss in the same context. &nbsp;For example, what is the best way to find the cheapest flights or choose when to go on vacation (my flights are reimbursed and my timing is determined); what would make Europe an ideal vacation location, as opposed to Australia, Asia, South America, etc.</p>\n<p>If people are interested in more details of my specific situation, I am happy to give them in the comments or private messages.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R6WY8m9z59PvzrjfD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 7.164043977909387e-07, "legacy": true, "legacyId": "7466", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-18T22:51:21.333Z", "modifiedAt": null, "url": null, "title": "[LINK] Pixar works to make the future better", "slug": "link-pixar-works-to-make-the-future-better", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:05.802Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/omdMjwh5JBqwg8woo/link-pixar-works-to-make-the-future-better", "pageUrlRelative": "/posts/omdMjwh5JBqwg8woo/link-pixar-works-to-make-the-future-better", "linkUrl": "https://www.lesswrong.com/posts/omdMjwh5JBqwg8woo/link-pixar-works-to-make-the-future-better", "postedAtFormatted": "Wednesday, May 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Pixar%20works%20to%20make%20the%20future%20better&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Pixar%20works%20to%20make%20the%20future%20better%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FomdMjwh5JBqwg8woo%2Flink-pixar-works-to-make-the-future-better%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Pixar%20works%20to%20make%20the%20future%20better%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FomdMjwh5JBqwg8woo%2Flink-pixar-works-to-make-the-future-better", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FomdMjwh5JBqwg8woo%2Flink-pixar-works-to-make-the-future-better", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 263, "htmlBody": "<p>An analysis of how Pixar is promoting transhumanist ideals.</p>\n<p><a href=\"http://blogs.discovermagazine.com/sciencenotfiction/2011/05/14/the-hidden-message-in-pixars-films/\">http://blogs.discovermagazine.com/sciencenotfiction/2011/05/14/the-hidden-message-in-pixars-films/</a></p>\n<p>&gt;<span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">Pixar films contain a complex, nuanced, philosophical and political essence that, when viewed across the company&rsquo;s complete corpus, begins to emerge with some clarity.</span></p>\n<p><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">&gt;</span><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">There are certain rules in Pixar movies ... The first is that there is no&nbsp;<em>magic</em>. No problems are caused or fixed by the wave of a wand. Second, every Pixar film happens in the world of human beings ...&nbsp;</span><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">The third rule is that at least one main character is an intelligent being that isn&rsquo;t a human.</span></p>\n<p>&gt;<span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">&nbsp;In each case, the deviant non-human is ostracized.</span></p>\n<p><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">&gt;</span><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">In being ostracized, however, the non-human encounters a human.&nbsp;</span></p>\n<p><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">&gt;</span><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">&nbsp;Furthermore, the human is also deviant.&nbsp;</span></p>\n<p>&gt;<span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">The new is seen as dangerous and therefore feared.&nbsp;</span></p>\n<p><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">&gt;</span><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">Victory in the battle for the rights and respect from both groups will come from an act of exemplary personhood and humaneness by those who dare to break ranks with their kind. ...</span><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">the benefits for humanity are tremendous in every case where non-human persons are treated with respect.</span></p>\n<p><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">&gt;</span><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">Pixar has given those who would fight for personhood the narratives necessary to convince the world that non-humans that display characteristics of a person deserve the rights of a person.</span></p>\n<p><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">&gt;</span><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">The message hidden inside Pixar&rsquo;s magnificent films is this: humanity does not have a monopoly on personhood. In whatever form non- or super-human intelligence takes, it will need brave souls on both sides to defend what is right. If we can live up to this burden, humanity and the world we live in will be better for it.&nbsp;</span><span style=\"font-family: Georgia, serif; font-size: 13px; line-height: 18px;\">An entire generation has been reared with the subconscious seeds of these ideas planted down deep.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "omdMjwh5JBqwg8woo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "7467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T01:07:14.192Z", "modifiedAt": null, "url": null, "title": "Homomorphic encryption and Bitcoin", "slug": "homomorphic-encryption-and-bitcoin", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:04.165Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XCuwfWFuiGxCWXFtW/homomorphic-encryption-and-bitcoin", "pageUrlRelative": "/posts/XCuwfWFuiGxCWXFtW/homomorphic-encryption-and-bitcoin", "linkUrl": "https://www.lesswrong.com/posts/XCuwfWFuiGxCWXFtW/homomorphic-encryption-and-bitcoin", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Homomorphic%20encryption%20and%20Bitcoin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHomomorphic%20encryption%20and%20Bitcoin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXCuwfWFuiGxCWXFtW%2Fhomomorphic-encryption-and-bitcoin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Homomorphic%20encryption%20and%20Bitcoin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXCuwfWFuiGxCWXFtW%2Fhomomorphic-encryption-and-bitcoin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXCuwfWFuiGxCWXFtW%2Fhomomorphic-encryption-and-bitcoin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 408, "htmlBody": "<p>BitCoin is a recently introduced currency, based on public-key cryptography combined with a peer-to-peer network for verifying transactions. I've been thinking a lot about BitCoin recently, and particularly about BitCoin's main weakness: if your computer is compromised, an attacker could copy your BitCoin wallet and use it to steal coins. That's bad. But I've come up with a possible improvement that would greatly mitigate this risk, and was hoping for some help confirming its viability and filling in the details.</p>\n<p>The basic idea is to make it so that rather than having a single computer which can steal your coins if it's compromised, you have <em>two</em> computers (or a computer and a phone), such that your coins can only be spent if both devices cooperate. It is much harder to break into two computers belonging to the same person than just one, so this makes coins harder to steal. You could also have one of the computers involved be a third party that you trust to keep its files secure, and while that third party would be able to freeze your funds, it wouldn't be able to steal them. Using a third party this way, you could also add withdrawal rate limits and time delays, further improving security.</p>\n<p>I believe that this can be done in a fully backwards-compatible way, without any changes to the BitCoin protocol, using homomorphic encryption. BitCoin is based on elliptic curve cryptography; a receiving address is a public key, and a wallet file is a collection of private keys. The goal is to create a protocol where two cooperating computers produce a split key, such that they can use it cooperatively to sign transactions later, but neither one can sign transactions or determine the whole key on its own. My understanding is that homomorphic encryption can be used to implement a simulated computer that does arbitrary trusted computation, so this should be possible. However, I'm a bit fuzzy on the details, and I don't have the time or comparative advantage to implement this myself.</p>\n<p>(To deal with the risk of one one computer being lost or damaged, there could also be an override key; both computers would have the public half of the override key, and the private half would be kept offline in a bank deposit box or something similar. Then both computers use the override key to encrypt their halves of the split key, and send the encrypted keys to a cloud backup provider.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xCXkjecsjwm8uSW3y": 2, "MhHM6Rx2b4F8tHTQk": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XCuwfWFuiGxCWXFtW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "7468", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T01:29:09.128Z", "modifiedAt": null, "url": null, "title": "Should I be afraid of GMOs?", "slug": "should-i-be-afraid-of-gmos", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:54.622Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pavitra", "createdAt": "2009-09-22T08:32:44.250Z", "isAdmin": false, "displayName": "Pavitra"}, "userId": "yC2JgX3ENu7mionKh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BfuJA3GpFKG9XBkWv/should-i-be-afraid-of-gmos", "pageUrlRelative": "/posts/BfuJA3GpFKG9XBkWv/should-i-be-afraid-of-gmos", "linkUrl": "https://www.lesswrong.com/posts/BfuJA3GpFKG9XBkWv/should-i-be-afraid-of-gmos", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20I%20be%20afraid%20of%20GMOs%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20I%20be%20afraid%20of%20GMOs%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBfuJA3GpFKG9XBkWv%2Fshould-i-be-afraid-of-gmos%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20I%20be%20afraid%20of%20GMOs%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBfuJA3GpFKG9XBkWv%2Fshould-i-be-afraid-of-gmos", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBfuJA3GpFKG9XBkWv%2Fshould-i-be-afraid-of-gmos", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 206, "htmlBody": "<p>I was raised to believe that genetically-modified foods are unhealthy to eat and bad for the environment, and given a variety of reasons for this, some of which I now recognize as blatantly false (e.g., human genetic code is isomorphic to fundamental physical law), and a few of which still seem sort of plausible.</p>\n<p>Because of this history, I need to anchor my credence heavily downward from my sense of plausibility.</p>\n<p>The major reasons I see to believe that GMOs are safe are:</p>\n<ul>\n<li>I would probably think they were dangerous even if they were safe, due to my upbringing.</li>\n<li>In general, whenever someone opposes a particular field of engineering on the grounds that it's unnatural and dangerous, they're usually wrong.</li>\n<li>It's not quite obvious to me that introducing genetically-engineered organisms to a system is significantly more dangerous than introducing non-native naturally-evolved organisms.</li>\n</ul>\n<p>The major reason I see to believe that GMOs are dangerous is:</p>\n<ul>\n<li>I might believe they were safe even if they were dangerous, due to \"yay science\" (which was also part of my upbringing).</li>\n<li>We are designing self-replicating things and using them <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Genetic_pollution#Genetic_engineering\">without reliable containment</a>, thereby effectively releasing them into the wild.</li>\n</ul>\n<p>So: <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Gray_goo\">green goo</a>, yes or no?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BfuJA3GpFKG9XBkWv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 7.164511732673966e-07, "legacy": true, "legacyId": "7469", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T10:23:01.201Z", "modifiedAt": null, "url": null, "title": "What bothers you about Less Wrong?", "slug": "what-bothers-you-about-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:57.266Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NLibKMPZvyhuDJrsR/what-bothers-you-about-less-wrong", "pageUrlRelative": "/posts/NLibKMPZvyhuDJrsR/what-bothers-you-about-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/NLibKMPZvyhuDJrsR/what-bothers-you-about-less-wrong", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20bothers%20you%20about%20Less%20Wrong%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20bothers%20you%20about%20Less%20Wrong%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLibKMPZvyhuDJrsR%2Fwhat-bothers-you-about-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20bothers%20you%20about%20Less%20Wrong%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLibKMPZvyhuDJrsR%2Fwhat-bothers-you-about-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLibKMPZvyhuDJrsR%2Fwhat-bothers-you-about-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 270, "htmlBody": "<p>Or, what do you want to see more or less of from Less Wrong?</p>\n<p>I'm thinking about community norms, content and topics discussed, karma voting patterns, et cetera. There are already posts and comment sections filled with long lists of proposed technical software changes/additions, let's not make this post another one.&nbsp;</p>\n<p>My impression is that people sometimes make discussion posts about things that bother them, and sometimes a bunch of people will agree and sometimes a bunch of people will disagree, but most people don't care that much (or they have a life or something) and thus don't want to dedicate a post just to complaining. This post is meant to make it socially and cognitively easy to offer critique.</p>\n<p>I humbly request that you list downsides of existing policies even when you think the upsides outweigh them, for all the obvious reasons. I also humbly request that you list a critique/gripe even if you don't want to bother explaining why you have that critique/gripe, and even in cases where you think your gripe is, ahem, \"irrational\". In general, I think it'd be really cool if we erred on the side of listing things which might be problems even if there's no obvious solution or no real cause for complaint except for personal distaste for the color green (for example).</p>\n<p><em>I arrogantly request that we try to avoid impulsive downvoting and non-niceness for the duration of this post (and others like it). </em>If someone wants to complain that Less Wrong is a little cultish without explaining why then downvoting them to oblivion, while admittedly kind of funny, is probably a bad idea.<em> </em>:)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NLibKMPZvyhuDJrsR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 7.166066680565725e-07, "legacy": true, "legacyId": "7476", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 162, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T10:38:32.600Z", "modifiedAt": null, "url": null, "title": "Greg Egan and the Incomprehensible", "slug": "greg-egan-and-the-incomprehensible", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:05.351Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XKbEAH3yZBHdRhFiX/greg-egan-and-the-incomprehensible", "pageUrlRelative": "/posts/XKbEAH3yZBHdRhFiX/greg-egan-and-the-incomprehensible", "linkUrl": "https://www.lesswrong.com/posts/XKbEAH3yZBHdRhFiX/greg-egan-and-the-incomprehensible", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Greg%20Egan%20and%20the%20Incomprehensible&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGreg%20Egan%20and%20the%20Incomprehensible%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKbEAH3yZBHdRhFiX%2Fgreg-egan-and-the-incomprehensible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Greg%20Egan%20and%20the%20Incomprehensible%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKbEAH3yZBHdRhFiX%2Fgreg-egan-and-the-incomprehensible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKbEAH3yZBHdRhFiX%2Fgreg-egan-and-the-incomprehensible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1038, "htmlBody": "<p><em>In this post I question one disagreement between Eliezer Yudkowsky and science fiction author <a href=\"http://en.wikipedia.org/wiki/Greg_Egan\">Greg Egan</a>.</em></p>\n<p>In his post <a href=\"/lw/wx/complex_novelty/\">Complex Novelty</a>, Eliezer Yudkowsky wrote in 2008:</p>\n<blockquote>\n<p>Note that <strong>Greg Egan seems to explicitly believe</strong> the reverse - <strong>that humans can understand </strong><em><strong>anything</strong> understandable</em> - which explains a lot.</p>\n</blockquote>\n<p>An <a href=\"http://www.acceleratingfuture.com/michael/blog/2009/09/is-smarter-than-human-intelligence-possible/\">interview with Greg Egan</a> in 2009 confirmed this to be true:</p>\n<blockquote>\n<p>&hellip; <strong>I think there&rsquo;s a limit to this process of Copernican dethronement: I believe that humans have already crossed a threshold that, in a certain sense, puts us on an equal footing with any other being who has mastered abstract reasoning.</strong> There&rsquo;s a notion in computing science of &ldquo;Turing completeness&rdquo;, which says that once a computer can perform a set of quite basic operations, it can be programmed to do absolutely any calculation that any other computer can do. Other computers might be faster, or have more memory, or have multiple processors running at the same time, but my 1988 Amiga 500 really could be programmed to do anything my 2008 iMac can do &mdash; apart from responding to external events in real time &mdash; if only I had the patience to sit and swap floppy disks all day long. <strong>I suspect that something broadly similar applies to minds and the class of things they can understand: other beings might think faster than us, or have easy access to a greater store of facts, but underlying both mental processes will be the same basic set of general-purpose tools.</strong> So if we ever did encounter those billion-year-old aliens, I&rsquo;m sure they&rsquo;d have plenty to tell us that we didn&rsquo;t yet know &mdash; but given enough patience, and a very large notebook, I believe we&rsquo;d still be able to come to grips with whatever they had to say.</p>\n</blockquote>\n<p>The theoretical computer scientist <a href=\"http://en.wikipedia.org/wiki/Scott_Aaronson\">Scott Aaronson</a> wrote in a post titled '<a href=\"http://www.scottaaronson.com/blog/?p=346\">The Singularity Is Far</a>':</p>\n<blockquote>\n<p>The one notion <strong>I have real trouble with is that the AI-beings of the future would be no more comprehensible to us than we are to dogs</strong> (or mice, or fish, or snails).&nbsp; After all, we might similarly expect that there should be models of computation as far beyond Turing machines as Turing machines are beyond finite automata.&nbsp; But in the latter case, we know the intuition is mistaken.&nbsp; <strong>There <em>is</em> a ceiling to computational expressive power.</strong>&nbsp; Get up to a certain threshold, and every machine can simulate every other one, albeit some slower and others faster.</p>\n</blockquote>\n<p>An argument that is often mentioned is the relatively small difference between chimpanzees and humans. But that huge effect, increase in intelligence, rather seems like an outlier and not the rule. Take for example the evolution of echolocation, it seems to have been a gradual progress with no obvious quantum leaps. The same can be said about eyes and other features exhibited by biological agents that are an effect of natural evolution.</p>\n<p>Is it reasonable to assume that such quantum leaps are the rule, based on a single case study? Are there other animals that are vastly more intelligent than their immediate predecessors?</p>\n<p>What reason do we have to believe that a level above that of a standard human, that is as incomprehensible to us as higher mathematics is to chimps, does exist at all? And even if such a level is possible, what reason do we have to believe that artificial general intelligence could consistently uplift itself to a level that is incomprehensible to its given level?</p>\n<p>To be clear, I do not doubt the possibility of superhuman AI or <a href=\"http://www.overcomingbias.com/2008/11/emulations-go-f.html\">EM</a>'s. I do not doubt the importance of \"friendliness\"-research and that it will have to be solved before we invent (discover?) superhuman AI. But I lack the expertise to conclude that there are levels of comprehension that are not even fathomable <em>in principle</em>.</p>\n<p>In <a href=\"/lw/vh/complexity_and_intelligence/\">Complexity and Intelligence</a>, Eliezer wrote:</p>\n<blockquote>\n<p>If you want to print out the entire universe from the beginning of time to the end, you only need to specify the laws of physics.</p>\n</blockquote>\n<p>If we were able to specify the laws of physics and one of the effects of their computation would turn out to be superhuman intelligence that is incomprehensible to us, what would be the definition of 'incomprehensible' in this context?</p>\n<p>I can imagine quite a few possibilities of how a normal human being can fail to comprehend the workings of another being. One example can be found in the <a href=\"http://www.scottaaronson.com/blog/?p=346\">previously mentioned</a> article by Scott Aaronson:</p>\n<blockquote>\n<p>Now, it&rsquo;s clear that a human who thought at ten thousand times our clock rate would be a pretty impressive fellow.&nbsp; But if that&rsquo;s what we&rsquo;re talking about, then we don&rsquo;t mean a point beyond which history completely transcends us, but &ldquo;merely&rdquo; a point beyond which we could only understand history by playing it in extreme slow motion.</p>\n</blockquote>\n<p>Mr. Aaronson also provides another fascinating example in an <a href=\"http://www.scottaaronson.com/blog/?p=377\">unrelated post</a> ('<em>The T vs. HT (Truth vs. Higher Truth) problem</em>'):</p>\n<blockquote>\n<p>P versus NP is the example <em>par excellence</em> of a mathematical mystery that human beings lacked the language even to express until very recently in our history.</p>\n</blockquote>\n<p>Those two examples provide evidence for the possibility that even beings who are fundamentally on the same level might yet fail to comprehend each other.</p>\n<p>An agent might simply be more knowledgeable or lack certain key insights. Conceptual revolutions are intellectually and technologically enabling to the extent that they seemingly spawn quantum leaps in the ability to comprehend certain problems.</p>\n<p>Faster access to more information, the upbringing, education, or cultural and environmental differences and dumb luck might also intellectually remove agents with similar potentials from each other to an extent that they appear to reside on different levels. But even the smartest humans are dwarfs standing on the shoulders of giants. Sometimes the time is simply <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/List_of_multiple_discoveries\">ripe</a>, thanks to the previous discoveries of unknown unknowns.</p>\n<p>As mentioned by Scott Aaronson, the ability to think faster, but also the possibility to think deeper by storing more data in one's memory, might cause the appearance of superhuman intelligence and incomprehensible insight.</p>\n<p>Yet all of the above merely hints at the possibility that human intelligence can be amplified and that we can become more knowledgeable. But with enough time, standard humans could accomplish the same.</p>\n<p>What would it mean for an intelligence to be genuinely incomprehensible? Where do Eliezere Yudkowsky and Greg Egan disagree?<em><br /></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XKbEAH3yZBHdRhFiX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 23, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "7477", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aEdqh3KPerBNYvoWe", "rELc88PvDkhetQzqx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T15:16:29.319Z", "modifiedAt": null, "url": null, "title": "Example decision theory problem: \"Agent simulates predictor\"", "slug": "example-decision-theory-problem-agent-simulates-predictor", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.730Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q9DbfYfFzkotno9hG/example-decision-theory-problem-agent-simulates-predictor", "pageUrlRelative": "/posts/q9DbfYfFzkotno9hG/example-decision-theory-problem-agent-simulates-predictor", "linkUrl": "https://www.lesswrong.com/posts/q9DbfYfFzkotno9hG/example-decision-theory-problem-agent-simulates-predictor", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Example%20decision%20theory%20problem%3A%20%22Agent%20simulates%20predictor%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExample%20decision%20theory%20problem%3A%20%22Agent%20simulates%20predictor%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9DbfYfFzkotno9hG%2Fexample-decision-theory-problem-agent-simulates-predictor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Example%20decision%20theory%20problem%3A%20%22Agent%20simulates%20predictor%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9DbfYfFzkotno9hG%2Fexample-decision-theory-problem-agent-simulates-predictor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9DbfYfFzkotno9hG%2Fexample-decision-theory-problem-agent-simulates-predictor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 576, "htmlBody": "<p>Some people on LW have <a href=\"/lw/5pf/what_were_losing/46mm\">expressed interest</a> in what's happening on the <a href=\"https://groups.google.com/group/decision-theory-workshop/\">decision-theory-workshop</a> mailing list. Here's an example of the kind of work we're trying to do there.</p>\n<p>In April 2010 Gary Drescher proposed the \"Agent simulates predictor\" problem, or ASP, that shows how agents with lots of computational power sometimes fare worse than agents with limited resources. I'm posting it here with his permission:</p>\n<blockquote>\n<p>There's a version of Newcomb's Problem that poses the same sort of challenge to UDT that comes up in some multi-agent/game-theoretic scenarios.</p>\n<p>Suppose:</p>\n<ul>\n<li>The predictor does not run a detailed simulation of the agent, but relies instead on a high-level understanding of the agent's decision theory and computational power.</li>\n<li>The agent runs UDT, and has the ability to fully simulate the predictor.</li>\n</ul>\n<p>Since the agent can deduce (by low-level simulation) what the predictor will do, the agent does not regard the prediction outcome as contingent on the agent's computation. Instead, either predict-onebox or predict-twobox has a probability of 1 (since one or the other of those is deducible), and a probability of 1 remains the same regardless of what we condition on. The agent will then calculate greater utility for two-boxing than for one-boxing.</p>\n<p>Meanwhile, the predictor, knowing that the the agent runs UDT and will fully simulate the predictor, can reason as in the preceding paragraph, and thus deduce that the agent will two-box. So the large box is left empty and the agent two-boxes (and the agent's detailed simulation of the predictor correctly shows the predictor correctly predicting two-boxing).</p>\n<p>The agent would be better off, though, running a different decision theory that does not two-box here, and that the predictor can deduce does not two-box.</p>\n</blockquote>\n<p>About a month ago I came up with a way to formalize the problem, along the lines of my <a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">other formalizations</a>:</p>\n<blockquote>\n<p>a) The agent generates all proofs of length up to M, then picks the action for which the greatest utility was proven.</p>\n<p>b) The predictor generates all proofs of length up to N which is much less than M. If it finds a provable prediction about the agent's action, it fills the boxes accordingly. Also the predictor has an \"epistemic advantage\" over the agent: its proof system has an axiom saying the agent's proof system is consistent.</p>\n<p>Now the predictor can reason as follows. It knows that the agent will find some proof that the predictor will put X dollars in the second box, for some unknown value of X, because the agent has enough time to simulate the predictor. Therefore, it knows that the agent will find proofs that one-boxing leads to X dollars and two-boxing leads to X+1000 dollars. Now what if the agent still chooses one-boxing in the end? That means it must have found a different proof saying one-boxing gives more than X+1000 dollars. But if the agent actually one-boxes, the existence of these two different proofs would imply that the agent's proof system is inconsistent, which the predictor knows to be impossible. So the predictor ends up predicting that the agent will two-box, the agent two-boxes, and everybody loses.</p>\n</blockquote>\n<p>Also Wei Dai has a tentative new decision theory that solves the problem, but this margin (and my brain) is too small to contain it :-)</p>\n<p>Can LW generate the kind of insights needed to make progress on problems like ASP? Or should we keep working as a small clique?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fihKHQuS5WZBJgkRm": 1, "dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q9DbfYfFzkotno9hG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 41, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "7478", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dC3rxrMkYKLfgTYEa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T15:28:40.682Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Tsuyoku vs. the Egalitarian Instinct", "slug": "seq-rerun-tsuyoku-vs-the-egalitarian-instinct", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.763Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iPBhTbb5dS8ZzmNGS/seq-rerun-tsuyoku-vs-the-egalitarian-instinct", "pageUrlRelative": "/posts/iPBhTbb5dS8ZzmNGS/seq-rerun-tsuyoku-vs-the-egalitarian-instinct", "linkUrl": "https://www.lesswrong.com/posts/iPBhTbb5dS8ZzmNGS/seq-rerun-tsuyoku-vs-the-egalitarian-instinct", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Tsuyoku%20vs.%20the%20Egalitarian%20Instinct&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Tsuyoku%20vs.%20the%20Egalitarian%20Instinct%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiPBhTbb5dS8ZzmNGS%2Fseq-rerun-tsuyoku-vs-the-egalitarian-instinct%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Tsuyoku%20vs.%20the%20Egalitarian%20Instinct%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiPBhTbb5dS8ZzmNGS%2Fseq-rerun-tsuyoku-vs-the-egalitarian-instinct", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiPBhTbb5dS8ZzmNGS%2Fseq-rerun-tsuyoku-vs-the-egalitarian-instinct", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"/lw/h9/tsuyoku_vs_the_egalitarian_instinct/\">Tsuyoku vs. the Egalitarian Instinct</a> was originally published on March 28, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>There may be evolutionary psychological factors that encourage modesty and mediocrity, at least in appearance; while some of that may still apply today, you should mentally plan and strive to pull ahead, if you are doing things right.</blockquote>\n<p><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5ra/seq_rerun_tsuyoku_naritai_i_want_to_become/\">Tsuyoku Naritai! (I Want To Become Stronger)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iPBhTbb5dS8ZzmNGS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 7.166957212177491e-07, "legacy": true, "legacyId": "7479", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gWGA8Da539EQmAR9F", "RcofKqLvAHGw88zG2", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T15:57:04.000Z", "modifiedAt": null, "url": null, "title": "On the Anthropic Trilemma", "slug": "on-the-anthropic-trilemma", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:33.114Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AwEgBpWGBzNgfZZyw/on-the-anthropic-trilemma", "pageUrlRelative": "/posts/AwEgBpWGBzNgfZZyw/on-the-anthropic-trilemma", "linkUrl": "https://www.lesswrong.com/posts/AwEgBpWGBzNgfZZyw/on-the-anthropic-trilemma", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20the%20Anthropic%20Trilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20the%20Anthropic%20Trilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAwEgBpWGBzNgfZZyw%2Fon-the-anthropic-trilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20the%20Anthropic%20Trilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAwEgBpWGBzNgfZZyw%2Fon-the-anthropic-trilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAwEgBpWGBzNgfZZyw%2Fon-the-anthropic-trilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2638, "htmlBody": "<p>Eliezer&#8217;s <a href=\"http://lesswrong.com/lw/19d/the_anthropic_trilemma/\">Anthropic Trilemma</a>:</p>\n<blockquote><p>So here&#8217;s a simple algorithm for winning the lottery: Buy a ticket.\u00a0 Suspend your computer program just before the lottery drawing &#8211; which should of course be a quantum lottery, so that every ticket wins somewhere.\u00a0 Program your computational environment to, if you win, make a trillion copies of yourself, and wake them up for ten seconds, long enough to experience winning the lottery.\u00a0 Then suspend the programs, merge them again, and start the result.\u00a0 If you don&#8217;t win the lottery, then just wake up automatically. The odds of winning the lottery are ordinarily a billion to one.\u00a0 But now the branch in which you\u00a0<em>win </em>has your &#8220;measure&#8221;, your &#8220;amount of experience&#8221;,\u00a0<em>temporarily</em> multiplied by a trillion.\u00a0 So with the brief expenditure of a little extra computing power, you can subjectively win the lottery &#8211; be reasonably sure that when next you open your eyes, you will see a computer screen flashing &#8220;You won!&#8221;\u00a0 As for what happens ten seconds after that, you have no way of knowing how many processors you run on, so you shouldn&#8217;t feel a thing.</p></blockquote>\n<p>See the <a href=\"http://lesswrong.com/lw/19d/the_anthropic_trilemma/\">original post</a> for assumptions, what merging minds entails etc. He proposes three alternative bullets to bite: accepting that this would work, denying that there is &#8220;any\u00a0<em>meaningful </em>sense in which I can anticipate waking up as\u00a0<em>myself\u00a0</em>tomorrow, rather than Britney Spears&#8221; so undermining any question about what you should anticipate, and Nick Bostrom&#8217;s response, paraphrased by Eliezer:</p>\n<blockquote><p>&#8230;you should anticipate winning the lottery after five seconds, but anticipate losing the lottery after fifteen seconds. To bite this bullet, you have to throw away the idea that your joint subjective probabilities are the product of your conditional subjective probabilities.\u00a0 If you win the lottery, the subjective probability of having still won the lottery, ten seconds later, is ~1.\u00a0 And if you lose the lottery, the subjective probability of having lost the lottery, ten seconds later, is ~1.\u00a0 But we don&#8217;t have p(&#8220;experience win after 15s&#8221;) = p(&#8220;experience win after 15s&#8221;|&#8221;experience win after 5s&#8221;)*p(&#8220;experience win after 5s&#8221;) + p(&#8220;experience win after 15s&#8221;|&#8221;experience not-win after 5s&#8221;)*p(&#8220;experience not-win after 5s&#8221;).</p></blockquote>\n<p>I think I already <a title=\"Who are\u00a0you?\" href=\"https://meteuphoric.wordpress.com/2010/01/31/who-are-yo/\">bit</a> the bullet about there not being a meaningful sense in which I won&#8217;t wake up as Britney Spears. However I would like to offer a better, relatively bullet biting free solution.</p>\n<p>First notice that you will have to bite Bostrom&#8217;s bullet if you even accept Eliezer&#8217;s premise that arranging to multiply your &#8216;amount of experience&#8217; in one branch in the future makes you more likely to experience that branch. Call this principle &#8216;follow-the-crowd&#8217; (FTC). And let&#8217;s give the name &#8216;blatantly obvious principle&#8217; (BOP) to the notion that\u00a0P(I win at time 2) is equal to P(I win at time 2|I win at time 1)P(I win at time 1)+P(I win at time 2|I lose at time 1)P(I lose at time 1). Bostrom&#8217;s bullet is to deny BOP.</p>\n<p>We can set aside the bit about merging brains together for now; that isn&#8217;t causing our problem. Consider a simpler and smaller (for the sake of easy diagramming) lottery setup where after you win or lose you are woken for ten seconds as a single person, then put back to sleep and woken as four copies in the winning branch or one in the losing branch. See the diagram below. You are at Time 0 (T<sub>0</sub>). Before Time 1 (T<sub>1</sub>) the lottery is run, so at T<sub>1</sub> the winner is W<sub>1</sub> and the loser is L<sub>1</sub>. W<sub>1</sub> is then copied to give the multitude of winning experiences at T<sub>2</sub>, while L<sub>2</sub> remains single.</p>\n<p><a href=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg\"><img data-attachment-id=\"4056\" data-permalink=\"https://meteuphoric.wordpress.com/2011/05/19/on-the-anthropic-trilemma/biting-bostroms-bullet-3/\" data-orig-file=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg\" data-orig-size=\"442,508\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"Biting bostroms bullet 3\" data-image-description=\"\" data-medium-file=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg?w=261&#038;h=300\" data-large-file=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg?w=442\" class=\"alignleft size-medium wp-image-4056\" title=\"Biting bostroms bullet 3\" src=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg?w=261&#038;h=300\" alt=\"\" width=\"261\" height=\"300\" srcset=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg?w=261&amp;h=300 261w, https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg?w=131&amp;h=150 131w, https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg 442w\" sizes=\"(max-width: 261px) 100vw, 261px\" /></a>Now using the same reasoning as you would to win the lottery before, FTC, you should anticipate an 80% chance of winning the lottery at T<sub>2</sub>. There is four times as much of your experience winning the lottery as not then. But BOP says you still only have a fifty percent chance of being a lottery winner at T2:</p>\n<blockquote><p>P(win at T<sub>2</sub>) = P(win at T<sub>2</sub>|win at T<sub>1</sub>).P(win at T<sub>1</sub>)+P(win at T<sub>2</sub>|lose at T<sub>1</sub>).P(lose at T<sub>1</sub>) = 1 x 1/2 + 0 x 1/2 = 1/2</p></blockquote>\n<p>FTC and BOP conflict. If you accept that you should generally anticipate futures where there are more of you more strongly, it looks like you accept that P(a) does not always equal P(a|b)P(b)+P(a|-b)P(-b). How sad.</p>\n<p>Looking at the diagram above, it is easy to see why these two methods of calculating anticipations disagree. \u00a0There are two times in the diagram that your future branches, once in a probabilistic event and once in being copied. FTC and BOP both treat the probabilistic event the same: they divide your expectations between the outcomes according to their objective probability. At the other branching the two principles do different things. BOP treats it the same as a probabilistic event, dividing your expectation of reaching that point between the many branches you could continue on. FTC treats it as a multiplication of your experience, giving each new branch the full measure of the incoming branch. Which method is correct?</p>\n<p>Neither. FTC and BOP are both approximations of better principles. Both of the better principles are probably true, and they do not conflict.</p>\n<p>To see this, first we should be precise about what we mean by &#8216;anticipate&#8217;. There is more than one resolution to the conflict, depending on your theory of what to anticipate: where the purported thread of personal experience goes, if anywhere. (Nope, resolving the trilemma does not seem to answer this question).</p>\n<p><strong>Resolution 1: the single thread</strong></p>\n<p>The most natural assumption seems to be that your future takes one branch at every intersection. It does this based on objective probability at probabilistic events, or equiprobably at copying events. It follows BOP. This means we can keep the present version of BOP, so I shall explain how we can do without FTC.</p>\n<p>Consider diagram 2. If your future takes one branch at every intersection, and you happen to win the lottery, there are still many T<sub>2</sub> lottery winners who will not be your future. They are your copies, but they are not where your thread of experience goes. They and your real future self can&#8217;t distinguish who is actually in your future, but there is some truth of the matter. It is shown in green.</p>\n<div data-shortcode=\"caption\" id=\"attachment_4058\" style=\"width: 271px\" class=\"wp-caption alignleft\"><a href=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg\"><img data-attachment-id=\"4058\" data-permalink=\"https://meteuphoric.wordpress.com/2011/05/19/on-the-anthropic-trilemma/anthtropic-trilemma-2/\" data-orig-file=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg\" data-orig-size=\"442,508\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"anthtropic trilemma 2\" data-image-description=\"\" data-medium-file=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg?w=261&#038;h=300\" data-large-file=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg?w=442\" class=\"size-medium wp-image-4058\" title=\"anthtropic trilemma 2\" src=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg?w=261&#038;h=300\" alt=\"\" width=\"261\" height=\"300\" srcset=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg?w=261&amp;h=300 261w, https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg?w=131&amp;h=150 131w, https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg 442w\" sizes=\"(max-width: 261px) 100vw, 261px\" /></a><p class=\"wp-caption-text\">Diagram 2</p></div>\n<p>Now while there are only two objective possible worlds, when we consider possible paths for the green thread there are five possible worlds (one shown in diagram 2). In each one your experience follows a different path up the tree. Since your future is now distinguished from other similar experiences, we can see the weight of your experience at\u00a0T<sub>2\u00a0</sub>in a world where you win is no greater than the weight in a world where you lose, though there are always more copies who are not you in the world where you win.</p>\n<p>The four worlds where your future is in a winning branch are each only a quarter as likely as one where you lose, because there is a fifty percent chance of you reaching W1, and after that a twenty five percent chance of reaching a given W2. By the original FTC reasoning then, you are equally likely to win or lose. More copies just makes you less certain exactly where it will be.</p>\n<p>I am treating the invisible green thread like any other hidden characteristic. Suppose you know that you are and will continue to be the person with the red underpants, though many copies will be made of you with green underpants. However many extra copies are made, a world with more of them in future should not get more of your credence, even if you don&#8217;t know which future person actually has the red pants.\u00a0If you think of yourself as having only one future, then you can&#8217;t also consider there to be a greater amount of <em>your</em> experience when there are a lot of copies. If you did anticipate experiences based on the probability that many people other than you were scheduled for that experience, you would greatly increase the minuscule credence you have in experiencing being Britney Spears when you wake up tomorrow.</p>\n<p>Doesn&#8217;t this conflict with the use of FTC to avoid the Bolzmann brain problem, Eliezer&#8217;s <a href=\"http://lesswrong.com/lw/17d/forcing_anthropics_boltzmann_brains/\">original motivation</a> for accepting it? No. The above reasoning means there is a difference between where you should anticipate going when you are at T<sub>0</sub>, and where you should think you are if you are at T<sub>2</sub>.</p>\n<p>If you are at T<sub>0</sub>\u00a0you should anticipate a 50% chance of\u00a0winning,\u00a0but if you are at T<sub>2</sub> you have an 80% chance of being a winner. Sound silly? That&#8217;s because you&#8217;ve forgotten that you are potentially talking about different people. If you are at T<sub>2</sub>, you are probably not the future of the person who was at T<sub>0</sub>, and you have no way to tell. You are a copy of them, but their future thread is unlikely to wend through you. If you knew that you were their future, then you would agree with their calculations.</p>\n<p>That is, anyone who only knows they are at T<sub>2</sub> should consider themselves likely to have won, because there are many more winners than losers. Anyone who knows they are at T<sub>2</sub> <em>and</em> are your future, should give even odds to winning.\u00a0At T<sub>0</sub>, you know that the future person whose measure you are interested in is at T<sub>2</sub> and is your future, so you also give even odds to winning.</p>\n<p>Avoiding the Bolzmann brain problem requires a principle similar to FTC which says you are <em>presently</em> more likely to be in a world where there are more people like you. <a title=\"Anthropic\u00a0principles\" href=\"https://meteuphoric.wordpress.com/anthropic-principles/\">SIA</a>\u00a0says just that for instance, and there are other anthropic principles that imply similar things. Avoiding the Bolzmann brain problem does not require inferring from this that your <em>future</em> lies in worlds where there are more such people. And such an inference is invalid.</p>\n<p>This is exactly the same as how it is invalid to infer that you will have many children from the fact that you are more likely to be from a family with many children. Probability theory doesn&#8217;t distinguish between the relationship between you and your children and the relationship between you and your future selves.</p>\n<p><strong>Resolution 2</strong></p>\n<p>You could instead consider all copies to be your futures. Your thread is duplicated when you are. In that case you should treat the two kinds of branching differently, unlike BOP, but still not in the way FTC does. It appears you should anticipate a 50% chance of becoming four people, rather than an 80% chance of becoming one of those people. There is no sense in which you will become one of the winners\u00a0rather than another. Like in the last case, it is true that if you are presently\u00a0<em>one</em> of the copies in the future, you should think yourself 80% likely to be a winner. But again &#8216;you&#8217; refers to a different entity in this case to the one it referred to before the lottery. It refers to a single future copy. It can&#8217;t usefully refer to a whole set of winners, because the one considering it does not know if they are part of that set or if they are a loser.\u00a0As in the last case, your anticipations at T<sub>0</sub>\u00a0should be different from your expectations for yourself if you know only that you are in the future already.</p>\n<p>In this case BOP gives us the right answer for the anticipated chances of winning at T<sub>0</sub>. However it says you have a 25% chance of becoming each winner at T<sub>2</sub> given you win at T<sub>1</sub>, instead of 100% chance of becoming all of them.</p>\n<p>Resolution 3:</p>\n<p>Suppose that you want to equate becoming four people in one branch as being more likely to be there. More of your future weight is there, so for some notions of expectation perhaps you expect to be there. You take &#8216;what is the probability that I win the lottery at T1?&#8217; to mean something like &#8216;what proportion of my future selves are winning at T1?&#8217;. FTC\u00a0gives the correct answer to this question &#8211; you aren&#8217;t especially likely to win at T1, but you probably will at T2. Or in the original problem, you should expect to win after 5 seconds and lose after 15 seconds, as Nick Bostrom suggested. If FTC is true, then we must scrap BOP. This is easier than it looks because BOP is not what it seems.</p>\n<p>Here is BOP again:</p>\n<p>P(I win at T2) is equal to P(I win at T2|I win at T1)P(I win at T1)+P(I win at T2|I lose at T1)P(I lose at T1)</p>\n<p>It looks like a simple application of</p>\n<p>P(a) = P(a|b)P(b)+P(a|-b)P(-b)</p>\n<p>But here is a more extended version:</p>\n<p>P(win at 15|at 15) = P(win an 15|at 15 and came from win at 5)P(win at 5|at 5)+P(win at 15|at 15 and came from loss at 5)P(lose at 5|at 5)</p>\n<p>This is only equal to BOP if the probability of having a win at 5 in your past when you are at 15 is equal to the probability of winning at 5 when you are at 5.\u00a0To accept FTC is to deny that. FTC says you are more likely to find the win in your past than to experience it because many copies are descended from the same past. So accepting FTC doesn&#8217;t conflict with P(a) being equal to P(a|b)P(b)+P(a|-b)P(-b), it just makes BOP an inaccurate application of this true principle.</p>\n<p><strong>In summary:</strong></p>\n<p>1. If your future is (by definitional choice or underlying reality) a continuous non-splitting thread, then something like SIA should be used instead of FTC, and BOP holds. Who you anticipate being differs from who you should think you are when you get there. Who you should think you are when you get there remains as something like SIA and avoids the Bolzmann brain problem.</p>\n<p>2. If all your future copies are equally your future, you should anticipate becoming a large number of people with the same probability as that you would have become one person if there were no extra copies. In which case FTC does not hold, because you expect to become many people with a small probability instead of one of those many people with a large probability. BOP holds in a modified form where it doesn&#8217;t treat being copied as being sent down a random path. But if you want to know what a random moment from your future will hold, a random moment from T1 is more likely to include losing than a random moment from T2. For working out what a random T2 moment will hold, BOP is a false application of a correct principle.</p>\n<p>3. If for whatever reason you conceptualise yourself as being more likely to go into future worlds based on the number of copies of you there are in those worlds, then FTC does hold, but BOP becomes false.</p>\n<p>I think the most important point is that the question of where you should anticipate going need not have the same answer as where a future copy of you should expect to be (if they don&#8217;t know for some reason).\u00a0A future copy who doesn&#8217;t know where they are should think they are more likely to be in world where there are many people like themselves, but you should not necessarily think you are likely to go into such a world. If you don&#8217;t think you are as likely to go into such a world, then FTC doesn&#8217;t hold. If you do, then BOP doesn&#8217;t hold.</p>\n<p>It seems to me the original problem uses FTC while assuming there will be a single thread, thereby making BOP look inevitable. If the thread is kept, FTC should not be, which can be conceptualised as in either of resolutions 1 or 2. If FTC is kept, BOP need not be, as in resolution 3. Whether you keep FTC or BOP will give you different expectations about the future, but which expectations are warranted is a question for another time.</p><br />  <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gocomments/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/comments/meteuphoric.wordpress.com/3939/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/godelicious/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/delicious/meteuphoric.wordpress.com/3939/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gofacebook/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/facebook/meteuphoric.wordpress.com/3939/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gotwitter/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/twitter/meteuphoric.wordpress.com/3939/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gostumble/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/stumble/meteuphoric.wordpress.com/3939/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/godigg/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/digg/meteuphoric.wordpress.com/3939/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/goreddit/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/reddit/meteuphoric.wordpress.com/3939/\" /></a> <img alt=\"\" border=\"0\" src=\"https://pixel.wp.com/b.gif?host=meteuphoric.wordpress.com&#038;blog=8643840&#038;post=3939&#038;subd=meteuphoric&#038;ref=&#038;feed=1\" width=\"1\" height=\"1\" />", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AwEgBpWGBzNgfZZyw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 47, "extendedScore": null, "score": 7.167662800273326e-07, "legacy": true, "legacyId": "7483", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Eliezer\u2019s <a href=\"http://lesswrong.com/lw/19d/the_anthropic_trilemma/\">Anthropic Trilemma</a>:</p>\n<blockquote><p>So here\u2019s a simple algorithm for winning the lottery: Buy a ticket.&nbsp; Suspend your computer program just before the lottery drawing \u2013 which should of course be a quantum lottery, so that every ticket wins somewhere.&nbsp; Program your computational environment to, if you win, make a trillion copies of yourself, and wake them up for ten seconds, long enough to experience winning the lottery.&nbsp; Then suspend the programs, merge them again, and start the result.&nbsp; If you don\u2019t win the lottery, then just wake up automatically. The odds of winning the lottery are ordinarily a billion to one.&nbsp; But now the branch in which you&nbsp;<em>win </em>has your \u201cmeasure\u201d, your \u201camount of experience\u201d,&nbsp;<em>temporarily</em> multiplied by a trillion.&nbsp; So with the brief expenditure of a little extra computing power, you can subjectively win the lottery \u2013 be reasonably sure that when next you open your eyes, you will see a computer screen flashing \u201cYou won!\u201d&nbsp; As for what happens ten seconds after that, you have no way of knowing how many processors you run on, so you shouldn\u2019t feel a thing.</p></blockquote>\n<p>See the <a href=\"http://lesswrong.com/lw/19d/the_anthropic_trilemma/\">original post</a> for assumptions, what merging minds entails etc. He proposes three alternative bullets to bite: accepting that this would work, denying that there is \u201cany&nbsp;<em>meaningful </em>sense in which I can anticipate waking up as&nbsp;<em>myself&nbsp;</em>tomorrow, rather than Britney Spears\u201d so undermining any question about what you should anticipate, and Nick Bostrom\u2019s response, paraphrased by Eliezer:</p>\n<blockquote><p>\u2026you should anticipate winning the lottery after five seconds, but anticipate losing the lottery after fifteen seconds. To bite this bullet, you have to throw away the idea that your joint subjective probabilities are the product of your conditional subjective probabilities.&nbsp; If you win the lottery, the subjective probability of having still won the lottery, ten seconds later, is ~1.&nbsp; And if you lose the lottery, the subjective probability of having lost the lottery, ten seconds later, is ~1.&nbsp; But we don\u2019t have p(\u201cexperience win after 15s\u201d) = p(\u201cexperience win after 15s\u201d|\u201dexperience win after 5s\u201d)*p(\u201cexperience win after 5s\u201d) + p(\u201cexperience win after 15s\u201d|\u201dexperience not-win after 5s\u201d)*p(\u201cexperience not-win after 5s\u201d).</p></blockquote>\n<p>I think I already <a title=\"Who are&nbsp;you?\" href=\"https://meteuphoric.wordpress.com/2010/01/31/who-are-yo/\">bit</a> the bullet about there not being a meaningful sense in which I won\u2019t wake up as Britney Spears. However I would like to offer a better, relatively bullet biting free solution.</p>\n<p>First notice that you will have to bite Bostrom\u2019s bullet if you even accept Eliezer\u2019s premise that arranging to multiply your \u2018amount of experience\u2019 in one branch in the future makes you more likely to experience that branch. Call this principle \u2018follow-the-crowd\u2019 (FTC). And let\u2019s give the name \u2018blatantly obvious principle\u2019 (BOP) to the notion that&nbsp;P(I win at time 2) is equal to P(I win at time 2|I win at time 1)P(I win at time 1)+P(I win at time 2|I lose at time 1)P(I lose at time 1). Bostrom\u2019s bullet is to deny BOP.</p>\n<p>We can set aside the bit about merging brains together for now; that isn\u2019t causing our problem. Consider a simpler and smaller (for the sake of easy diagramming) lottery setup where after you win or lose you are woken for ten seconds as a single person, then put back to sleep and woken as four copies in the winning branch or one in the losing branch. See the diagram below. You are at Time 0 (T<sub>0</sub>). Before Time 1 (T<sub>1</sub>) the lottery is run, so at T<sub>1</sub> the winner is W<sub>1</sub> and the loser is L<sub>1</sub>. W<sub>1</sub> is then copied to give the multitude of winning experiences at T<sub>2</sub>, while L<sub>2</sub> remains single.</p>\n<p><a href=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg\"><img data-attachment-id=\"4056\" data-permalink=\"https://meteuphoric.wordpress.com/2011/05/19/on-the-anthropic-trilemma/biting-bostroms-bullet-3/\" data-orig-file=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg\" data-orig-size=\"442,508\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"Biting bostroms bullet 3\" data-image-description=\"\" data-medium-file=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg?w=261&amp;h=300\" data-large-file=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg?w=442\" class=\"alignleft size-medium wp-image-4056\" title=\"Biting bostroms bullet 3\" src=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg?w=261&amp;h=300\" alt=\"\" width=\"261\" height=\"300\" srcset=\"https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg?w=261&amp;h=300 261w, https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg?w=131&amp;h=150 131w, https://meteuphoric.files.wordpress.com/2011/05/biting-bostroms-bullet-3.jpg 442w\" sizes=\"(max-width: 261px) 100vw, 261px\"></a>Now using the same reasoning as you would to win the lottery before, FTC, you should anticipate an 80% chance of winning the lottery at T<sub>2</sub>. There is four times as much of your experience winning the lottery as not then. But BOP says you still only have a fifty percent chance of being a lottery winner at T2:</p>\n<blockquote><p>P(win at T<sub>2</sub>) = P(win at T<sub>2</sub>|win at T<sub>1</sub>).P(win at T<sub>1</sub>)+P(win at T<sub>2</sub>|lose at T<sub>1</sub>).P(lose at T<sub>1</sub>) = 1 x 1/2 + 0 x 1/2 = 1/2</p></blockquote>\n<p>FTC and BOP conflict. If you accept that you should generally anticipate futures where there are more of you more strongly, it looks like you accept that P(a) does not always equal P(a|b)P(b)+P(a|-b)P(-b). How sad.</p>\n<p>Looking at the diagram above, it is easy to see why these two methods of calculating anticipations disagree. &nbsp;There are two times in the diagram that your future branches, once in a probabilistic event and once in being copied. FTC and BOP both treat the probabilistic event the same: they divide your expectations between the outcomes according to their objective probability. At the other branching the two principles do different things. BOP treats it the same as a probabilistic event, dividing your expectation of reaching that point between the many branches you could continue on. FTC treats it as a multiplication of your experience, giving each new branch the full measure of the incoming branch. Which method is correct?</p>\n<p>Neither. FTC and BOP are both approximations of better principles. Both of the better principles are probably true, and they do not conflict.</p>\n<p>To see this, first we should be precise about what we mean by \u2018anticipate\u2019. There is more than one resolution to the conflict, depending on your theory of what to anticipate: where the purported thread of personal experience goes, if anywhere. (Nope, resolving the trilemma does not seem to answer this question).</p>\n<p><strong id=\"Resolution_1__the_single_thread\">Resolution 1: the single thread</strong></p>\n<p>The most natural assumption seems to be that your future takes one branch at every intersection. It does this based on objective probability at probabilistic events, or equiprobably at copying events. It follows BOP. This means we can keep the present version of BOP, so I shall explain how we can do without FTC.</p>\n<p>Consider diagram 2. If your future takes one branch at every intersection, and you happen to win the lottery, there are still many T<sub>2</sub> lottery winners who will not be your future. They are your copies, but they are not where your thread of experience goes. They and your real future self can\u2019t distinguish who is actually in your future, but there is some truth of the matter. It is shown in green.</p>\n<div data-shortcode=\"caption\" id=\"attachment_4058\" style=\"width: 271px\" class=\"wp-caption alignleft\"><a href=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg\"><img data-attachment-id=\"4058\" data-permalink=\"https://meteuphoric.wordpress.com/2011/05/19/on-the-anthropic-trilemma/anthtropic-trilemma-2/\" data-orig-file=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg\" data-orig-size=\"442,508\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"anthtropic trilemma 2\" data-image-description=\"\" data-medium-file=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg?w=261&amp;h=300\" data-large-file=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg?w=442\" class=\"size-medium wp-image-4058\" title=\"anthtropic trilemma 2\" src=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg?w=261&amp;h=300\" alt=\"\" width=\"261\" height=\"300\" srcset=\"https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg?w=261&amp;h=300 261w, https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg?w=131&amp;h=150 131w, https://meteuphoric.files.wordpress.com/2011/05/anthtropic-trilemma-2.jpg 442w\" sizes=\"(max-width: 261px) 100vw, 261px\"></a><p class=\"wp-caption-text\">Diagram 2</p></div>\n<p>Now while there are only two objective possible worlds, when we consider possible paths for the green thread there are five possible worlds (one shown in diagram 2). In each one your experience follows a different path up the tree. Since your future is now distinguished from other similar experiences, we can see the weight of your experience at&nbsp;T<sub>2&nbsp;</sub>in a world where you win is no greater than the weight in a world where you lose, though there are always more copies who are not you in the world where you win.</p>\n<p>The four worlds where your future is in a winning branch are each only a quarter as likely as one where you lose, because there is a fifty percent chance of you reaching W1, and after that a twenty five percent chance of reaching a given W2. By the original FTC reasoning then, you are equally likely to win or lose. More copies just makes you less certain exactly where it will be.</p>\n<p>I am treating the invisible green thread like any other hidden characteristic. Suppose you know that you are and will continue to be the person with the red underpants, though many copies will be made of you with green underpants. However many extra copies are made, a world with more of them in future should not get more of your credence, even if you don\u2019t know which future person actually has the red pants.&nbsp;If you think of yourself as having only one future, then you can\u2019t also consider there to be a greater amount of <em>your</em> experience when there are a lot of copies. If you did anticipate experiences based on the probability that many people other than you were scheduled for that experience, you would greatly increase the minuscule credence you have in experiencing being Britney Spears when you wake up tomorrow.</p>\n<p>Doesn\u2019t this conflict with the use of FTC to avoid the Bolzmann brain problem, Eliezer\u2019s <a href=\"http://lesswrong.com/lw/17d/forcing_anthropics_boltzmann_brains/\">original motivation</a> for accepting it? No. The above reasoning means there is a difference between where you should anticipate going when you are at T<sub>0</sub>, and where you should think you are if you are at T<sub>2</sub>.</p>\n<p>If you are at T<sub>0</sub>&nbsp;you should anticipate a 50% chance of&nbsp;winning,&nbsp;but if you are at T<sub>2</sub> you have an 80% chance of being a winner. Sound silly? That\u2019s because you\u2019ve forgotten that you are potentially talking about different people. If you are at T<sub>2</sub>, you are probably not the future of the person who was at T<sub>0</sub>, and you have no way to tell. You are a copy of them, but their future thread is unlikely to wend through you. If you knew that you were their future, then you would agree with their calculations.</p>\n<p>That is, anyone who only knows they are at T<sub>2</sub> should consider themselves likely to have won, because there are many more winners than losers. Anyone who knows they are at T<sub>2</sub> <em>and</em> are your future, should give even odds to winning.&nbsp;At T<sub>0</sub>, you know that the future person whose measure you are interested in is at T<sub>2</sub> and is your future, so you also give even odds to winning.</p>\n<p>Avoiding the Bolzmann brain problem requires a principle similar to FTC which says you are <em>presently</em> more likely to be in a world where there are more people like you. <a title=\"Anthropic&nbsp;principles\" href=\"https://meteuphoric.wordpress.com/anthropic-principles/\">SIA</a>&nbsp;says just that for instance, and there are other anthropic principles that imply similar things. Avoiding the Bolzmann brain problem does not require inferring from this that your <em>future</em> lies in worlds where there are more such people. And such an inference is invalid.</p>\n<p>This is exactly the same as how it is invalid to infer that you will have many children from the fact that you are more likely to be from a family with many children. Probability theory doesn\u2019t distinguish between the relationship between you and your children and the relationship between you and your future selves.</p>\n<p><strong id=\"Resolution_2\">Resolution 2</strong></p>\n<p>You could instead consider all copies to be your futures. Your thread is duplicated when you are. In that case you should treat the two kinds of branching differently, unlike BOP, but still not in the way FTC does. It appears you should anticipate a 50% chance of becoming four people, rather than an 80% chance of becoming one of those people. There is no sense in which you will become one of the winners&nbsp;rather than another. Like in the last case, it is true that if you are presently&nbsp;<em>one</em> of the copies in the future, you should think yourself 80% likely to be a winner. But again \u2018you\u2019 refers to a different entity in this case to the one it referred to before the lottery. It refers to a single future copy. It can\u2019t usefully refer to a whole set of winners, because the one considering it does not know if they are part of that set or if they are a loser.&nbsp;As in the last case, your anticipations at T<sub>0</sub>&nbsp;should be different from your expectations for yourself if you know only that you are in the future already.</p>\n<p>In this case BOP gives us the right answer for the anticipated chances of winning at T<sub>0</sub>. However it says you have a 25% chance of becoming each winner at T<sub>2</sub> given you win at T<sub>1</sub>, instead of 100% chance of becoming all of them.</p>\n<p>Resolution 3:</p>\n<p>Suppose that you want to equate becoming four people in one branch as being more likely to be there. More of your future weight is there, so for some notions of expectation perhaps you expect to be there. You take \u2018what is the probability that I win the lottery at T1?\u2019 to mean something like \u2018what proportion of my future selves are winning at T1?\u2019. FTC&nbsp;gives the correct answer to this question \u2013 you aren\u2019t especially likely to win at T1, but you probably will at T2. Or in the original problem, you should expect to win after 5 seconds and lose after 15 seconds, as Nick Bostrom suggested. If FTC is true, then we must scrap BOP. This is easier than it looks because BOP is not what it seems.</p>\n<p>Here is BOP again:</p>\n<p>P(I win at T2) is equal to P(I win at T2|I win at T1)P(I win at T1)+P(I win at T2|I lose at T1)P(I lose at T1)</p>\n<p>It looks like a simple application of</p>\n<p>P(a) = P(a|b)P(b)+P(a|-b)P(-b)</p>\n<p>But here is a more extended version:</p>\n<p>P(win at 15|at 15) = P(win an 15|at 15 and came from win at 5)P(win at 5|at 5)+P(win at 15|at 15 and came from loss at 5)P(lose at 5|at 5)</p>\n<p>This is only equal to BOP if the probability of having a win at 5 in your past when you are at 15 is equal to the probability of winning at 5 when you are at 5.&nbsp;To accept FTC is to deny that. FTC says you are more likely to find the win in your past than to experience it because many copies are descended from the same past. So accepting FTC doesn\u2019t conflict with P(a) being equal to P(a|b)P(b)+P(a|-b)P(-b), it just makes BOP an inaccurate application of this true principle.</p>\n<p><strong id=\"In_summary_\">In summary:</strong></p>\n<p>1. If your future is (by definitional choice or underlying reality) a continuous non-splitting thread, then something like SIA should be used instead of FTC, and BOP holds. Who you anticipate being differs from who you should think you are when you get there. Who you should think you are when you get there remains as something like SIA and avoids the Bolzmann brain problem.</p>\n<p>2. If all your future copies are equally your future, you should anticipate becoming a large number of people with the same probability as that you would have become one person if there were no extra copies. In which case FTC does not hold, because you expect to become many people with a small probability instead of one of those many people with a large probability. BOP holds in a modified form where it doesn\u2019t treat being copied as being sent down a random path. But if you want to know what a random moment from your future will hold, a random moment from T1 is more likely to include losing than a random moment from T2. For working out what a random T2 moment will hold, BOP is a false application of a correct principle.</p>\n<p>3. If for whatever reason you conceptualise yourself as being more likely to go into future worlds based on the number of copies of you there are in those worlds, then FTC does hold, but BOP becomes false.</p>\n<p>I think the most important point is that the question of where you should anticipate going need not have the same answer as where a future copy of you should expect to be (if they don\u2019t know for some reason).&nbsp;A future copy who doesn\u2019t know where they are should think they are more likely to be in world where there are many people like themselves, but you should not necessarily think you are likely to go into such a world. If you don\u2019t think you are as likely to go into such a world, then FTC doesn\u2019t hold. If you do, then BOP doesn\u2019t hold.</p>\n<p>It seems to me the original problem uses FTC while assuming there will be a single thread, thereby making BOP look inevitable. If the thread is kept, FTC should not be, which can be conceptualised as in either of resolutions 1 or 2. If FTC is kept, BOP need not be, as in resolution 3. Whether you keep FTC or BOP will give you different expectations about the future, but which expectations are warranted is a question for another time.</p><br>  <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gocomments/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/comments/meteuphoric.wordpress.com/3939/\"></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/godelicious/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/delicious/meteuphoric.wordpress.com/3939/\"></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gofacebook/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/facebook/meteuphoric.wordpress.com/3939/\"></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gotwitter/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/twitter/meteuphoric.wordpress.com/3939/\"></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gostumble/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/stumble/meteuphoric.wordpress.com/3939/\"></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/godigg/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/digg/meteuphoric.wordpress.com/3939/\"></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/goreddit/meteuphoric.wordpress.com/3939/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/reddit/meteuphoric.wordpress.com/3939/\"></a> <img alt=\"\" border=\"0\" src=\"https://pixel.wp.com/b.gif?host=meteuphoric.wordpress.com&amp;blog=8643840&amp;post=3939&amp;subd=meteuphoric&amp;ref=&amp;feed=1\" width=\"1\" height=\"1\">", "sections": [{"title": "Resolution 1: the single thread", "anchor": "Resolution_1__the_single_thread", "level": 1}, {"title": "Resolution 2", "anchor": "Resolution_2", "level": 1}, {"title": "In summary:", "anchor": "In_summary_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "50 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["y7jZ9BLEeuNTzgAE5", "LubwxZHKKvCivYGzx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T17:03:29.787Z", "modifiedAt": null, "url": null, "title": "How to (un)become a crank", "slug": "how-to-un-become-a-crank", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:23.976Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8rQX6tJmGZE6DdGNf/how-to-un-become-a-crank", "pageUrlRelative": "/posts/8rQX6tJmGZE6DdGNf/how-to-un-become-a-crank", "linkUrl": "https://www.lesswrong.com/posts/8rQX6tJmGZE6DdGNf/how-to-un-become-a-crank", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20(un)become%20a%20crank&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20(un)become%20a%20crank%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8rQX6tJmGZE6DdGNf%2Fhow-to-un-become-a-crank%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20(un)become%20a%20crank%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8rQX6tJmGZE6DdGNf%2Fhow-to-un-become-a-crank", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8rQX6tJmGZE6DdGNf%2Fhow-to-un-become-a-crank", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 420, "htmlBody": "<p>Ahhh, a human interest post. Well, sort of. At least it has something <em>besides</em> math-talk.</p>\n<p>In the extreme programming community they have a saying, <a href=\"http://www.c2.com/cgi/wiki?ThreeStrikesAndYouRefactor\">\"three strikes and you refactor\"</a>. The rationalist counterpart would be this: once you've noticed the same trap twice, you'd be stupid to fall prey to it the third time.</p>\n<p>Strike one is Eliezer's post <a href=\"/lw/j8/the_crackpot_offer/\">The Crackpot Offer</a>. Child-Eliezer thought he'd overthrown Cantor's theorem, then found an error in his reasoning, but felt a little tempted to keep on trying to overthrow the damned theorem anyway. The right and Bayesian thing to do, which he ended up doing, was to notice that once you've found your mistake there's no longer any reason to wage war on an established result.</p>\n<p>Strike two is Emile's <a href=\"/lw/5jv/no_coinductive_datatype_of_integers/43hr\">comment</a> on one of my recent posts:</p>\n<blockquote>I find it annoying how my brain keeps saying \"hah, I bet I could\" even though I explained to it that it's mathematically provable that such an input always exists. It still keeps coming up with \"how about this clever encoding?, blablabla\" ... I guess that's how you get cranks.</blockquote>\n<p>Strike three is... I'm a bit ashamed to say that...</p>\n<p>...strike three is about me. And maybe not only me.</p>\n<p>There's a certain vibe in the air surrounding many discussions of decision theory. It sings: maybe the central insight of game theory (that multiplayer situations are not reducible to single-player ones) is wrong. Maybe the slightly-asymmetrized Prisoner's Dilemma has a single right answer. Maybe you can get a unique solution to&nbsp;<a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">dividing a cake by majority vote</a>&nbsp;if each individual player's reasoning is \"correct enough\". But honestly, where exactly is the Bayesian evidence that merits anticipating success on that path? Am I waging war on clear and simple established results because of wishful thinking? Are my efforts the moral equivalent of counting the reals or proving the consistency of PA within PA?</p>\n<p>An easy answer is that \"we don't know\" if our inquiries will be fruitful, so you can't prove I must stop. But that's not the Bayesian answer. The Bayesian answer is to honestly tally up the indications that future success is likely, and stop if they are lacking.</p>\n<p>So I want to ask an object-level question and a meta-level question:</p>\n<p>1) What evidence supports the intuition that, contra game theory, single-player decision theory has a \"solution\"?</p>\n<p>2) If there's not much evidence supporting that intuition, how should I change my actions?</p>\n<p>(I already have tentative answers to both questions, but am curious what others think. Note that you can answer the second question without knowing any math :-))</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8rQX6tJmGZE6DdGNf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 24, "extendedScore": null, "score": 7.167233504451401e-07, "legacy": true, "legacyId": "7481", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qRWfvgJG75ESLRNu9", "c3wWnvgzdbRhNnNbQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T17:09:16.761Z", "modifiedAt": null, "url": null, "title": "The Aliens have Landed!", "slug": "the-aliens-have-landed", "viewCount": null, "lastCommentedAt": "2020-10-07T02:41:54.915Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TimFreeman", "createdAt": "2011-04-12T22:58:16.873Z", "isAdmin": false, "displayName": "TimFreeman"}, "userId": "AAP7Amn8h8BhWCjjC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hz5dFKTnyC7HqibSQ/the-aliens-have-landed", "pageUrlRelative": "/posts/Hz5dFKTnyC7HqibSQ/the-aliens-have-landed", "linkUrl": "https://www.lesswrong.com/posts/Hz5dFKTnyC7HqibSQ/the-aliens-have-landed", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Aliens%20have%20Landed!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Aliens%20have%20Landed!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHz5dFKTnyC7HqibSQ%2Fthe-aliens-have-landed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Aliens%20have%20Landed!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHz5dFKTnyC7HqibSQ%2Fthe-aliens-have-landed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHz5dFKTnyC7HqibSQ%2Fthe-aliens-have-landed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 928, "htmlBody": "<p>\"General Thud! General Thud! Wake up! The aliens have landed. We must surrender!\" General Thud's assistant Fred turned on the lights and opened the curtains to help Thud wake up and confront the situation. Thud was groggy because he had stayed up late supervising an ultimately successful mission carried out by remotely piloted vehicles in some small country on the other side of the world. Thud mumbled, \"Aliens? How many? Where are they? What are they doing?\" General Thud looked out the window, expecting to see giant tripods walking around and destroying buildings with death rays. He saw his lawn, a bright blue sky, and hummingbirds hovering near his bird feeder.</p>\n<p><a id=\"more\"></a></p>\n<p>Fred was trying to bring Thud up to speed as quickly as possible. \"Thousands of them, General! 2376, to be precise. They gave us a map; we know where they all are. They aren't doing anything overt, but the problem is their computation! I have one here, if you'd like to look.\" Fred removed a black sphere two inches in diameter from his pocket and gave it to Thud.</p>\n<p>Thud sat on his bed holding the small sphere and staring at it dumbfounded. \"Okay, you think we should surrender to a few thousand small spheres. Why is that, exactly?\" The sphere seemed a little flexible in Thud's hand. As he experimented a few seconds to see just how flexible, it collapsed in his hand, converting itself into a loose clump of alien sand that landed in his lap and started to dribble onto his bed and the floor. Thud stood up and brushed the rest of the sand off of his pyjamas and bed, and thought for a moment about where he left his vacuum cleaner bags. He was not impressed with these aliens.</p>\n<p>Fred said \"I don't think you wanted to do that, sir. Their ultimatum states that for every alien we destroy, they'll manufacture two in the outer reaches of the Solar System where we'll never find them!\"</p>\n<p>Thud said, \"Okay, so now you think we should surrender to 2375 small spheres, and two or more small spheres that are out of the battlefield for the moment. Why is that?\"</p>\n<p>Fred said \"Well, you remember a few years back when some people copied their brain state into a computer and posted it to the Internet? Apparently somebody copied the data across an unencrypted wireless link, the aliens picked it up with their radio telescopes, and now they are simulating those poor people in these black spheres and torturing the simulations! They sent us videos!\" Fred held up his cell phone, pushed a button, and showed the video to Thud.</p>\n<p>Thud looked at the video for a moment and said, \"Yep, that's torture. Do these people know anything potentially useful to the aliens?\"</p>\n<p>Fred said, \"Well, they know how to break into a laboratory that has brain scanning tools and push some buttons. That was apparently the high point of their lives.&nbsp; But none of that matters, the aliens don't seem to be torturing them for information anyway.\"</p>\n<p>Thud was still suffering from morning brain fog. He rubbed his eyes. \"And why should we surrender?\"</p>\n<p>Fred said, \"The aliens have made a trillion copies of these poor people and will run the torture simulations on the little black spheres until we march all of our citizens into the death camps they demand we build! We have analyzed these black spheres and the engineering diagrams the aliens gave us, and we know this to be true. We only have ten billion citizens, and this simulated torture is much worse than simulated death, so the total utility is much greater if we surrender!\"</p>\n<p>Thud yawned.&nbsp; \"Fred, you're fired. Get out of my house.\" As Fred left, Thud closed his curtains and tried to get back to sleep.</p>\n<p>-----</p>\n<p>Michael said \"So I take it you no longer assist Thud. What are you doing now?\"</p>\n<p>Fred reclined comfortably on the analyst's couch. \"I help out at the cafeteria as a short order cook. But I'm not worried about my career right now. I have nightmares about all these simulated people being tortured in the flimsy alien spheres.\"</p>\n<p>\"Thud surely knows the simulations are being tortured too. Do you think he has nightmares about this?\"</p>\n<p>\"No, he doesn't seem to care.\"</p>\n<p>\"Have you always cared about the well-being of simulations?\"</p>\n<p>\"No, when I was a teenager I was self-centered and conceited and didn't care about anybody else, including simulated people.\"</p>\n<p>\"So at some point you self-modified to care about simulations. If it helps you, you could self-modify again.\"</p>\n<p>\"But I don't want to!\"</p>\n<p>\"Did you want to self-modify to care about simulations in the first place?\"</p>\n<p>\"No, it just sort of happened as I grew up.\"</p>\n<p>\"Is there any logical inconsistency in Thud's position?\"</p>\n<p>Fred thought for a bit.&nbsp; \"Not that I can see.&nbsp; The value one assigns to simulations seems to be an arbitrary choice.&nbsp; Ignoring the alien invasion certainly hasn't harmed his career.\"</p>\n<p>\"Concern about simulations seems to give the aliens more influence over you than Thud would prefer. What would you prefer?\"</p>\n<p>\"Well, I'd also prefer the aliens not to be able to jerk me around. I really don't have room in my life for it now.&nbsp; In the grand scheme of things, it seems just wrong -- they shouldn't be able to genocide a species with a few thousand stupid spheres that just sit there converting sunlight to heat.\"</p>\n<p>Michael passed Fred a piece of paper with a short list of bulleted items.&nbsp; \"This is the procedure I teach my clients who want to change their preferences.&nbsp; After you've learned it, you can decide whether and how you want to use it...\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1, "2q2cK4FdnSeohTEaJ": 1, "jQytxyauJ7kPhhGj3": 1, "NGtNzdS88JtEQdRP4": 2, "mxSBcaTrakvCkgLzL": 1, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hz5dFKTnyC7HqibSQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 46, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "7480", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 158, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T18:30:35.402Z", "modifiedAt": null, "url": null, "title": "[LINKs] Bitcoin hits mainstream; intelligent technical critique", "slug": "links-bitcoin-hits-mainstream-intelligent-technical-critique", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:04.328Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SilasBarta", "createdAt": "2009-03-01T00:03:34.864Z", "isAdmin": false, "displayName": "SilasBarta"}, "userId": "zDPSZfarhLM7Gehug", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4KszqwYq8PHui8bSS/links-bitcoin-hits-mainstream-intelligent-technical-critique", "pageUrlRelative": "/posts/4KszqwYq8PHui8bSS/links-bitcoin-hits-mainstream-intelligent-technical-critique", "linkUrl": "https://www.lesswrong.com/posts/4KszqwYq8PHui8bSS/links-bitcoin-hits-mainstream-intelligent-technical-critique", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINKs%5D%20Bitcoin%20hits%20mainstream%3B%20intelligent%20technical%20critique&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINKs%5D%20Bitcoin%20hits%20mainstream%3B%20intelligent%20technical%20critique%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KszqwYq8PHui8bSS%2Flinks-bitcoin-hits-mainstream-intelligent-technical-critique%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINKs%5D%20Bitcoin%20hits%20mainstream%3B%20intelligent%20technical%20critique%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KszqwYq8PHui8bSS%2Flinks-bitcoin-hits-mainstream-intelligent-technical-critique", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KszqwYq8PHui8bSS%2Flinks-bitcoin-hits-mainstream-intelligent-technical-critique", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>Annie Lowrey&nbsp;<a href=\"http://www.slate.com/id/2294980/\">discusses</a>&nbsp;Bitcoin in Slate.&nbsp; No clear thesis, but important that it gets attention there.&nbsp; She gives a general overview, with emphasis on its benefits to fringe elements on society, and gives quick attack at the end.&nbsp; The attack seems misinformed, but it links to something more interesting, specifically...</p>\r\n<p>A <a href=\"http://www.pds.ewi.tudelft.nl/~victor/bitcoin.html\">technical critique</a> by Victor Grishchenko, PhD, who was mentioned <a href=\"/lw/4nt/what_are_you_working_on/3mnh\">here</a> in the context of causal trees.&nbsp; He describes a few problems he sees with Bitcoin:</p>\r\n<p>1) Asymmetry favors attackers, in that it takes a lot more effort to check for double spending than to attempt a double-spend, eventually requiring \"supernodes\" that have disproportionate influence over the network.</p>\r\n<p>2) It needs to continuously spend spend cycles to stay free from attackers.&nbsp; He then describes an attack I don't quite understand that involves holding on to a discovered block and then broadcasting it at just the right time</p>\r\n<p>3) It doesn't compare well against existing systems in terms of privacy, speed, or transaction cost.&nbsp; (I found this questionable because the system he's comparing it to is still subject to warrants, and Bitcoin takes significantly less time -- 1 hour or so -- to ensure a transaction than the wiring transfers Grishchenko discribes.)</p>\r\n<p>Finally, he credits Bitcoin in being advantageous similarly to Bittorrent: the latter was clumsy and complicated compared to regular downloading, but could perform well enough in a niche niche to force change in the broader markets.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jgcAJnksReZRuvgzp": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4KszqwYq8PHui8bSS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "7482", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T19:39:40.170Z", "modifiedAt": null, "url": null, "title": "The Advantages of Being Technical", "slug": "the-advantages-of-being-technical", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:03.228Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Lc7c5yn6Xp2Zf3cy8/the-advantages-of-being-technical", "pageUrlRelative": "/posts/Lc7c5yn6Xp2Zf3cy8/the-advantages-of-being-technical", "linkUrl": "https://www.lesswrong.com/posts/Lc7c5yn6Xp2Zf3cy8/the-advantages-of-being-technical", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Advantages%20of%20Being%20Technical&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Advantages%20of%20Being%20Technical%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLc7c5yn6Xp2Zf3cy8%2Fthe-advantages-of-being-technical%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Advantages%20of%20Being%20Technical%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLc7c5yn6Xp2Zf3cy8%2Fthe-advantages-of-being-technical", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLc7c5yn6Xp2Zf3cy8%2Fthe-advantages-of-being-technical", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 689, "htmlBody": "<blockquote>\n<p>Someone told me that each equation I included in the book would halve the sales. I therefore resolved not to have any equations at all.</p>\n</blockquote>\n<p>&mdash; Stephen Hawking, in his book <em>A Brief History of Time</em></p>\n<p>Whoever gave this advice to Mr. Hawking is probably partly responsible for the success, financial and otherwise, of his book. But as Paul K. Feyerabend said: \"All methodologies have their limitations and the only &lsquo;rule&rsquo; that survives is &lsquo;anything goes&rsquo;.\"</p>\n<p>Open access online communities interested in high quality and productivity can benefit from a high degree of technical formality, including large amounts of mathematics.</p>\n<h3>Troll Filter</h3>\n<blockquote>\n<p>When confronted with an economic problem, first translate into mathematics, then solve the problem, then translate back into English and burn the mathematics.</p>\n</blockquote>\n<p>&mdash; Alfred Marshall</p>\n<p>Another valuable advice in and of itself. But expressing technical issues in a natural language can be a mixed blessing.</p>\n<p>Everyone can understand natural language, or so they think. Often any understanding solely arrived at by natural language is vague and can lead people to mistakenly believe that they understand the issue in question sufficiently.</p>\n<blockquote>\n<p>A <span class=\"il\">little</span> bit of <span class=\"il\">knowledge</span> is a dangerous thing. It can convince you that an argument this idiotic and this sloppy is actually profound. It can convince you to publicly make a raging jackass out of yourself, by rambling on and on, based on a stupid misunderstanding of a simplified, informal, intuitive description of something complex.</p>\n</blockquote>\n<p>&mdash; Mark Chu-Carroll, <a href=\"http://scienceblogs.com/goodmath/2010/05/the_danger_when_you_dont_know.php?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+scienceblogs%2FCyKN+%28Good+Math%2C+Bad+Math%29&amp;utm_content=Google+Reader\" target=\"_blank\">The Danger When You Don&rsquo;t Know What You Don&rsquo;t Know</a></p>\n<p>The introduction of formal language and a focus on hard problems filters out a large subgroup of people who might otherwise believe that they are eligible for membership, able contribute something useful or that the community is interested in their critique.</p>\n<h3>Less Vague</h3>\n<blockquote>\n<p>More generally, many of the objects demonstrated to be impossible in the previous posts in this series can appear possible as long as there is enough vagueness. &nbsp;For instance, one can certainly imagine an omnipotent being provided that there is enough vagueness in the concept of what &ldquo;omnipotence&rdquo; means; but if one tries to nail this concept down precisely, one gets hit by the <a href=\"http://en.wikipedia.org/wiki/Omnipotence_paradox\">omnipotence paradox</a>. &nbsp;Similarly, one can imagine a foolproof strategy for beating the stock market (or some other zero sum game), as long as the strategy is vague enough that one cannot analyse what happens when that strategy ends up being used against itself. &nbsp;Or, one can imagine the possibility of time travel as long as it is left vague what would happen if one tried to trigger the <a href=\"http://en.wikipedia.org/wiki/Grandfather_paradox\">grandfather paradox</a>. &nbsp;And so forth. &nbsp;The &ldquo;self-defeating&rdquo; aspect of these impossibility results relies heavily on precision and definiteness, which is why they can seem so strange from the perspective of vague intuition.</p>\n</blockquote>\n<p>&mdash; Terence Tao, <a href=\"http://terrytao.wordpress.com/2010/11/02/the-no-self-defeating-object-argument-and-the-vagueness-paradox/\">The &ldquo;no self-defeating object&rdquo; argument, and the vagueness paradox</a></p>\n<p>Terence Tao thinks that real insight only kicks in \"once one tries to clear away the fog of vagueness and nail down all the definitions and mathematical statements precisely.\" This is another advantage of being technical as it introduces a high degree of focus by tabooing colloquial language and thereby reducing ambiguity.</p>\n<p>To be technical means to be more formal. Formalizing problems is itself a problem that needs to be taken seriously as it is part of the eventual solution of each problem. Formalizing a problem is increasing the productivity of people who work on that problem by being more specific about what exactly it is that needs to be solved and by serving as a measure of progress.</p>\n<h3>Measure of Progress</h3>\n<p>Being technical helps to facilitate the structure that is required to enable <a href=\"http://en.wikipedia.org/wiki/Open_research\">open research</a> and <a href=\"http://en.wikipedia.org/wiki/Polymath_project#Polymath_Project\">collaborative mathematics</a>.</p>\n<p>Everyone involved has to be sure what is being asked for and what progress has been made so far. But to measure progress one needs to know what constitutes progress.</p>\n<p>Progress ultimately means to approach a solution. But to approach a solution, first and foremost, one needs to formalize the problem, which in turn means to be highly specific about what would constitute a solution. And only by being strictly technical and by constantly trying to reduce any vagueness one can effectively approach the formalization of a problem and its eventual solution.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Lc7c5yn6Xp2Zf3cy8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 24, "extendedScore": null, "score": 7.167688619663986e-07, "legacy": true, "legacyId": "7485", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>Someone told me that each equation I included in the book would halve the sales. I therefore resolved not to have any equations at all.</p>\n</blockquote>\n<p>\u2014 Stephen Hawking, in his book <em>A Brief History of Time</em></p>\n<p>Whoever gave this advice to Mr. Hawking is probably partly responsible for the success, financial and otherwise, of his book. But as Paul K. Feyerabend said: \"All methodologies have their limitations and the only \u2018rule\u2019 that survives is \u2018anything goes\u2019.\"</p>\n<p>Open access online communities interested in high quality and productivity can benefit from a high degree of technical formality, including large amounts of mathematics.</p>\n<h3 id=\"Troll_Filter\">Troll Filter</h3>\n<blockquote>\n<p>When confronted with an economic problem, first translate into mathematics, then solve the problem, then translate back into English and burn the mathematics.</p>\n</blockquote>\n<p>\u2014 Alfred Marshall</p>\n<p>Another valuable advice in and of itself. But expressing technical issues in a natural language can be a mixed blessing.</p>\n<p>Everyone can understand natural language, or so they think. Often any understanding solely arrived at by natural language is vague and can lead people to mistakenly believe that they understand the issue in question sufficiently.</p>\n<blockquote>\n<p>A <span class=\"il\">little</span> bit of <span class=\"il\">knowledge</span> is a dangerous thing. It can convince you that an argument this idiotic and this sloppy is actually profound. It can convince you to publicly make a raging jackass out of yourself, by rambling on and on, based on a stupid misunderstanding of a simplified, informal, intuitive description of something complex.</p>\n</blockquote>\n<p>\u2014 Mark Chu-Carroll, <a href=\"http://scienceblogs.com/goodmath/2010/05/the_danger_when_you_dont_know.php?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+scienceblogs%2FCyKN+%28Good+Math%2C+Bad+Math%29&amp;utm_content=Google+Reader\" target=\"_blank\">The Danger When You Don\u2019t Know What You Don\u2019t Know</a></p>\n<p>The introduction of formal language and a focus on hard problems filters out a large subgroup of people who might otherwise believe that they are eligible for membership, able contribute something useful or that the community is interested in their critique.</p>\n<h3 id=\"Less_Vague\">Less Vague</h3>\n<blockquote>\n<p>More generally, many of the objects demonstrated to be impossible in the previous posts in this series can appear possible as long as there is enough vagueness. &nbsp;For instance, one can certainly imagine an omnipotent being provided that there is enough vagueness in the concept of what \u201comnipotence\u201d means; but if one tries to nail this concept down precisely, one gets hit by the <a href=\"http://en.wikipedia.org/wiki/Omnipotence_paradox\">omnipotence paradox</a>. &nbsp;Similarly, one can imagine a foolproof strategy for beating the stock market (or some other zero sum game), as long as the strategy is vague enough that one cannot analyse what happens when that strategy ends up being used against itself. &nbsp;Or, one can imagine the possibility of time travel as long as it is left vague what would happen if one tried to trigger the <a href=\"http://en.wikipedia.org/wiki/Grandfather_paradox\">grandfather paradox</a>. &nbsp;And so forth. &nbsp;The \u201cself-defeating\u201d aspect of these impossibility results relies heavily on precision and definiteness, which is why they can seem so strange from the perspective of vague intuition.</p>\n</blockquote>\n<p>\u2014 Terence Tao, <a href=\"http://terrytao.wordpress.com/2010/11/02/the-no-self-defeating-object-argument-and-the-vagueness-paradox/\">The \u201cno self-defeating object\u201d argument, and the vagueness paradox</a></p>\n<p>Terence Tao thinks that real insight only kicks in \"once one tries to clear away the fog of vagueness and nail down all the definitions and mathematical statements precisely.\" This is another advantage of being technical as it introduces a high degree of focus by tabooing colloquial language and thereby reducing ambiguity.</p>\n<p>To be technical means to be more formal. Formalizing problems is itself a problem that needs to be taken seriously as it is part of the eventual solution of each problem. Formalizing a problem is increasing the productivity of people who work on that problem by being more specific about what exactly it is that needs to be solved and by serving as a measure of progress.</p>\n<h3 id=\"Measure_of_Progress\">Measure of Progress</h3>\n<p>Being technical helps to facilitate the structure that is required to enable <a href=\"http://en.wikipedia.org/wiki/Open_research\">open research</a> and <a href=\"http://en.wikipedia.org/wiki/Polymath_project#Polymath_Project\">collaborative mathematics</a>.</p>\n<p>Everyone involved has to be sure what is being asked for and what progress has been made so far. But to measure progress one needs to know what constitutes progress.</p>\n<p>Progress ultimately means to approach a solution. But to approach a solution, first and foremost, one needs to formalize the problem, which in turn means to be highly specific about what would constitute a solution. And only by being strictly technical and by constantly trying to reduce any vagueness one can effectively approach the formalization of a problem and its eventual solution.</p>", "sections": [{"title": "Troll Filter", "anchor": "Troll_Filter", "level": 1}, {"title": "Less Vague", "anchor": "Less_Vague", "level": 1}, {"title": "Measure of Progress", "anchor": "Measure_of_Progress", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T19:43:58.873Z", "modifiedAt": null, "url": null, "title": "Ottawa LW meetup, May 26, 7pm; Bayes study group, May 26, 9am", "slug": "ottawa-lw-meetup-may-26-7pm-bayes-study-group-may-26-9am", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:04.200Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FCo2caLapPCJaStQ7/ottawa-lw-meetup-may-26-7pm-bayes-study-group-may-26-9am", "pageUrlRelative": "/posts/FCo2caLapPCJaStQ7/ottawa-lw-meetup-may-26-7pm-bayes-study-group-may-26-9am", "linkUrl": "https://www.lesswrong.com/posts/FCo2caLapPCJaStQ7/ottawa-lw-meetup-may-26-7pm-bayes-study-group-may-26-9am", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ottawa%20LW%20meetup%2C%20May%2026%2C%207pm%3B%20Bayes%20study%20group%2C%20May%2026%2C%209am&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOttawa%20LW%20meetup%2C%20May%2026%2C%207pm%3B%20Bayes%20study%20group%2C%20May%2026%2C%209am%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCo2caLapPCJaStQ7%2Fottawa-lw-meetup-may-26-7pm-bayes-study-group-may-26-9am%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ottawa%20LW%20meetup%2C%20May%2026%2C%207pm%3B%20Bayes%20study%20group%2C%20May%2026%2C%209am%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCo2caLapPCJaStQ7%2Fottawa-lw-meetup-may-26-7pm-bayes-study-group-may-26-9am", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCo2caLapPCJaStQ7%2Fottawa-lw-meetup-may-26-7pm-bayes-study-group-may-26-9am", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p><a id=\"more\"></a></p>\n<p><strong style=\"font-weight: bold; \">Less Wrong meeting:</strong></p>\n<p>Date: Thursday May 26, 7:00pm 'til whenever.</p>\n<p>Venue: Bridgehead, 282 Elgin St. (corner at MacLaren).</p>\n<p><strong style=\"font-weight: bold; \">Bayes study group:&nbsp;</strong>Anyone in the region interested in learning how to do Bayesian statistics is welcome to join us. We'll be using the statistical package&nbsp;<strong style=\"font-weight: bold;\">R</strong>&nbsp;(<a href=\"http://cran.r-project.org/\">http://cran.r-project.org/</a>)&nbsp;as a platform, so bring your laptop if you have one.</p>\n<p>Date: Thursday May 26, 9:00am to 10:30am. NB: 9 in the morning.</p>\n<p>Venue: <a href=\"http://maps.google.ca/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=health+canada+jeanne+mance+building+ottawa&amp;aq=&amp;sll=45.408936,-75.737514&amp;sspn=0.003984,0.006899&amp;ie=UTF8&amp;hq=health+canada+jeanne+mance+building&amp;hnear=Ottawa,+Ottawa+Division,+Ontario&amp;ll=45.408936,-75.737514&amp;spn=0.007592,0.013797&amp;t=h&amp;z=16\">Jeanne Mance Building</a>, Tunney's Pasture. Meet me in the lobby.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FCo2caLapPCJaStQ7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.167701185382439e-07, "legacy": true, "legacyId": "7484", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T19:51:48.026Z", "modifiedAt": null, "url": null, "title": "Rapture/Pet Insurance", "slug": "rapture-pet-insurance", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.753Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dan_Moore", "createdAt": "2009-06-29T15:27:43.310Z", "isAdmin": false, "displayName": "Dan_Moore"}, "userId": "K8XbTHfuHWjsXgJpS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vSra7PxZ4tbZZG32H/rapture-pet-insurance", "pageUrlRelative": "/posts/vSra7PxZ4tbZZG32H/rapture-pet-insurance", "linkUrl": "https://www.lesswrong.com/posts/vSra7PxZ4tbZZG32H/rapture-pet-insurance", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rapture%2FPet%20Insurance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARapture%2FPet%20Insurance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvSra7PxZ4tbZZG32H%2Frapture-pet-insurance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rapture%2FPet%20Insurance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvSra7PxZ4tbZZG32H%2Frapture-pet-insurance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvSra7PxZ4tbZZG32H%2Frapture-pet-insurance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 24, "htmlBody": "<p>http://eternal-earthbound-pets.com/Home_Page.html</p>\n<p>Providing assurance that pets will be provided for in the event of Rapture.</p>\n<p>Having thought it over, I'm OK with the ethics of this service.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vSra7PxZ4tbZZG32H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "7486", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-19T20:32:02.370Z", "modifiedAt": null, "url": null, "title": "Lesswrongers from the German-speaking world, unite! ", "slug": "lesswrongers-from-the-german-speaking-world-unite", "viewCount": null, "lastCommentedAt": "2011-07-30T22:58:33.680Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wallowinmaya", "createdAt": "2011-03-21T00:39:18.855Z", "isAdmin": false, "displayName": "David Althaus"}, "userId": "xY8DDzk6TyvRroJEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2FjjM2H4ueWf2fvEC/lesswrongers-from-the-german-speaking-world-unite", "pageUrlRelative": "/posts/2FjjM2H4ueWf2fvEC/lesswrongers-from-the-german-speaking-world-unite", "linkUrl": "https://www.lesswrong.com/posts/2FjjM2H4ueWf2fvEC/lesswrongers-from-the-german-speaking-world-unite", "postedAtFormatted": "Thursday, May 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lesswrongers%20from%20the%20German-speaking%20world%2C%20unite!%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALesswrongers%20from%20the%20German-speaking%20world%2C%20unite!%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FjjM2H4ueWf2fvEC%2Flesswrongers-from-the-german-speaking-world-unite%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lesswrongers%20from%20the%20German-speaking%20world%2C%20unite!%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FjjM2H4ueWf2fvEC%2Flesswrongers-from-the-german-speaking-world-unite", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FjjM2H4ueWf2fvEC%2Flesswrongers-from-the-german-speaking-world-unite", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<p>As far as I can tell, there never has been a LW-meetup in a german-speaking country. This is crazy!</p>\n<p>There should be enough Lesswrongers from Germany, Austria or Switzerland to achieve a reasonable group size!</p>\n<p>To be a bit more specific, how about meeting in Munich? It's relatively in the middle of the three countries, but it's fine if you propose another city!</p>\n<p>I'm not sure about the date, but how about sometime in July or August? We would have plenty of time until then. But I'm happy if you propose another date!</p>\n<p>I don't know if I'm merely shouting in the void, but I hope some aspiring rationalists are out there!</p>\n<p>So If you're interested in such a meeting, please make a comment!</p>\n<p>&nbsp;</p>\n<p><strong>Added:</strong> Oh, and if you are not from a German-speaking country you are welcome, too!</p>\n<p><sup></sup></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2FjjM2H4ueWf2fvEC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 7.167841246036025e-07, "legacy": true, "legacyId": "7487", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-20T05:36:33.741Z", "modifiedAt": null, "url": null, "title": "Metacontrarian Metaethics", "slug": "metacontrarian-metaethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:36.506Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L2pFPD479y9KvfZyB/metacontrarian-metaethics", "pageUrlRelative": "/posts/L2pFPD479y9KvfZyB/metacontrarian-metaethics", "linkUrl": "https://www.lesswrong.com/posts/L2pFPD479y9KvfZyB/metacontrarian-metaethics", "postedAtFormatted": "Friday, May 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Metacontrarian%20Metaethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMetacontrarian%20Metaethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL2pFPD479y9KvfZyB%2Fmetacontrarian-metaethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Metacontrarian%20Metaethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL2pFPD479y9KvfZyB%2Fmetacontrarian-metaethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL2pFPD479y9KvfZyB%2Fmetacontrarian-metaethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 623, "htmlBody": "<p><em>Designed to gauge responses to some parts of the planned &ldquo;Noticing confusion about meta-ethics&rdquo; sequence, which should intertwine with or be absorbed by Lukeprog&rsquo;s meta-ethics sequence at some point.</em></p>\n<p>Disclaimer: I am going to leave out many relevant details. If you want, you can bring them up in the comments, but in general meta-ethics is still very confusing and thus we could list relevant details all day and still be confused. There are a <em>lot</em>&nbsp;of subtle themes and distinctions that have thus far been completely ignored by everyone, as far as I can tell.</p>\n<p><strong>Problem 1: Torture versus specks</strong></p>\n<p>Imagine you&rsquo;re at a Less Wrong meetup when out of nowhere Eliezer Yudkowsky proposes his <a href=\"/lw/kn/torture_vs_dust_specks/\">torture versus dust specks problem</a>. Years of bullet-biting make this a trivial dilemma for any good philosopher, but suddenly you have a seizure during which you vividly recall all of those history lessons where you learned about the horrible things people do when they feel justified in being blatantly evil because of some abstract moral theory that is at best an approximation of sane morality and at worst an obviously anti-epistemic spiral of moral rationalization. Temporarily humbled, you decide to think about the problem a little longer:</p>\n<p>\"Considering I am deciding the fate of 3^^^3+1 people, I should perhaps not immediately assert my speculative and controversial meta-ethics. Instead, perhaps I should use the averaged meta-ethics of the 3^^^3+1 people I am deciding for, since it is probable that they have preferences that implicitly cover edge cases such as this, and disregarding the meta-ethical preferences of 3^^^3+1 people is certainly one of the most blatantly immoral things one can do. After all, even if they never learn anything about this decision taking place, people are allowed to have preferences about it. But... that the majority of people believe something doesn&rsquo;t make it right, and that the majority of people prefer something doesn&rsquo;t make it right either. If I expect that these 3^^^3+1 people are mostly wrong about morality and would not reflectively endorse their implicit preferences being used in this decision instead of my explicitly reasoned and reflected upon preferences, then I should just go with mine, even if I am knowingly arrogantly blatantly disregarding the current preferences of 3^^^3 currently-alive-and-and-not-just-hypothetical people in doing so and thus causing negative utility many, many, many times more severe than the 3^^^3 units of negative utility I was trying to avert. I may be willing to accept this sacrifice, but I should at least admit that what I am doing largely ignores their current preferences, and there is some chance it is wrong upon reflection regardless, for though I am wiser than those 3^^^3+1 people, I notice that I too am confused.\"</p>\n<p>You hesitantly give your answer and continue to ponder the analogies to Eliezer&rsquo;s document &ldquo;CEV&rdquo;, and this whole business about &ldquo;extrapolation&rdquo;...</p>\n<p>(Thinking of people as having coherent non-contradictory preferences is very misleadingly wrong, not taking into account preferences at gradient levels of organization is probably wrong, not thinking of typical human preferences as implicitly preferring to update in various ways is maybe wrong (i.e. failing to see preferences as <a href=\"http://www.sci.brooklyn.cuny.edu/cis/parikh/omnisci.pdf\">processes embedded in time</a>&nbsp;is probably wrong), et cetera, but I have to start somewhere and this is already glossing over way too much.)</p>\n<p><strong>Bonus problem 1: Taking trolleys seriously</strong></p>\n<p>\"...Wait, considering how unlikely this scenario is, if I ever actually did end up in it then that would probably mean I was in some perverse simulation set up by empirical meta-ethicists with powerful computers, in which case they might use my decision as part of a propaganda campaign meant to somehow discredit consequentialist reasoning or maybe deontological reasoning, or maybe they'd use it for some other reason entirely, but at any rate that sure complicates the problem...&rdquo; (HT: Steve Rayhawk)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L2pFPD479y9KvfZyB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 1, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "7494", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Designed to gauge responses to some parts of the planned \u201cNoticing confusion about meta-ethics\u201d sequence, which should intertwine with or be absorbed by Lukeprog\u2019s meta-ethics sequence at some point.</em></p>\n<p>Disclaimer: I am going to leave out many relevant details. If you want, you can bring them up in the comments, but in general meta-ethics is still very confusing and thus we could list relevant details all day and still be confused. There are a <em>lot</em>&nbsp;of subtle themes and distinctions that have thus far been completely ignored by everyone, as far as I can tell.</p>\n<p><strong id=\"Problem_1__Torture_versus_specks\">Problem 1: Torture versus specks</strong></p>\n<p>Imagine you\u2019re at a Less Wrong meetup when out of nowhere Eliezer Yudkowsky proposes his <a href=\"/lw/kn/torture_vs_dust_specks/\">torture versus dust specks problem</a>. Years of bullet-biting make this a trivial dilemma for any good philosopher, but suddenly you have a seizure during which you vividly recall all of those history lessons where you learned about the horrible things people do when they feel justified in being blatantly evil because of some abstract moral theory that is at best an approximation of sane morality and at worst an obviously anti-epistemic spiral of moral rationalization. Temporarily humbled, you decide to think about the problem a little longer:</p>\n<p>\"Considering I am deciding the fate of 3^^^3+1 people, I should perhaps not immediately assert my speculative and controversial meta-ethics. Instead, perhaps I should use the averaged meta-ethics of the 3^^^3+1 people I am deciding for, since it is probable that they have preferences that implicitly cover edge cases such as this, and disregarding the meta-ethical preferences of 3^^^3+1 people is certainly one of the most blatantly immoral things one can do. After all, even if they never learn anything about this decision taking place, people are allowed to have preferences about it. But... that the majority of people believe something doesn\u2019t make it right, and that the majority of people prefer something doesn\u2019t make it right either. If I expect that these 3^^^3+1 people are mostly wrong about morality and would not reflectively endorse their implicit preferences being used in this decision instead of my explicitly reasoned and reflected upon preferences, then I should just go with mine, even if I am knowingly arrogantly blatantly disregarding the current preferences of 3^^^3 currently-alive-and-and-not-just-hypothetical people in doing so and thus causing negative utility many, many, many times more severe than the 3^^^3 units of negative utility I was trying to avert. I may be willing to accept this sacrifice, but I should at least admit that what I am doing largely ignores their current preferences, and there is some chance it is wrong upon reflection regardless, for though I am wiser than those 3^^^3+1 people, I notice that I too am confused.\"</p>\n<p>You hesitantly give your answer and continue to ponder the analogies to Eliezer\u2019s document \u201cCEV\u201d, and this whole business about \u201cextrapolation\u201d...</p>\n<p>(Thinking of people as having coherent non-contradictory preferences is very misleadingly wrong, not taking into account preferences at gradient levels of organization is probably wrong, not thinking of typical human preferences as implicitly preferring to update in various ways is maybe wrong (i.e. failing to see preferences as <a href=\"http://www.sci.brooklyn.cuny.edu/cis/parikh/omnisci.pdf\">processes embedded in time</a>&nbsp;is probably wrong), et cetera, but I have to start somewhere and this is already glossing over way too much.)</p>\n<p><strong id=\"Bonus_problem_1__Taking_trolleys_seriously\">Bonus problem 1: Taking trolleys seriously</strong></p>\n<p>\"...Wait, considering how unlikely this scenario is, if I ever actually did end up in it then that would probably mean I was in some perverse simulation set up by empirical meta-ethicists with powerful computers, in which case they might use my decision as part of a propaganda campaign meant to somehow discredit consequentialist reasoning or maybe deontological reasoning, or maybe they'd use it for some other reason entirely, but at any rate that sure complicates the problem...\u201d (HT: Steve Rayhawk)</p>", "sections": [{"title": "Problem 1: Torture versus specks", "anchor": "Problem_1__Torture_versus_specks", "level": 1}, {"title": "Bonus problem 1: Taking trolleys seriously", "anchor": "Bonus_problem_1__Taking_trolleys_seriously", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "75 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 75, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-20T07:19:05.875Z", "modifiedAt": null, "url": null, "title": "Official Less Wrong Redesign: Special pages", "slug": "official-less-wrong-redesign-special-pages", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.871Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8JFCZyF3xo8YtQMcy/official-less-wrong-redesign-special-pages", "pageUrlRelative": "/posts/8JFCZyF3xo8YtQMcy/official-less-wrong-redesign-special-pages", "linkUrl": "https://www.lesswrong.com/posts/8JFCZyF3xo8YtQMcy/official-less-wrong-redesign-special-pages", "postedAtFormatted": "Friday, May 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Official%20Less%20Wrong%20Redesign%3A%20Special%20pages&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOfficial%20Less%20Wrong%20Redesign%3A%20Special%20pages%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8JFCZyF3xo8YtQMcy%2Fofficial-less-wrong-redesign-special-pages%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Official%20Less%20Wrong%20Redesign%3A%20Special%20pages%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8JFCZyF3xo8YtQMcy%2Fofficial-less-wrong-redesign-special-pages", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8JFCZyF3xo8YtQMcy%2Fofficial-less-wrong-redesign-special-pages", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 244, "htmlBody": "<p>Following along from&nbsp;<a href=\"/user/louie\">Louie</a>'s&nbsp;<a href=\"/lw/5by/official_less_wrong_redesign_call_for_suggestions/\">post</a>&nbsp;and the discussion around it&hellip;</p>\n<p><a href=\"/user/orthonormal/\">User:orthonormal</a> <a href=\"/lw/5by/official_less_wrong_redesign_call_for_suggestions/3zb8\">suggested (and many seconded)</a> a better Welcome section and improvements to the About page.<br /><a href=\"/user/bentarm/\">User:bentarm</a> <a href=\"/lw/5by/official_less_wrong_redesign_call_for_suggestions/3z6d\">suggested</a> doing something with the comment help link, <a href=\"/user/jimrandomh/\">User:jimrandomh</a> <a href=\"/lw/5by/official_less_wrong_redesign_call_for_suggestions/3z6f\">suggested</a> making it a wiki page, and <a href=\"/user/Alicorn/\">User:Alicorn</a> <a href=\"/lw/5by/official_less_wrong_redesign_call_for_suggestions/3z7g\">requested</a> that it be more extensive.</p>\n<p>Responding to the above suggestions, we propose adding functionality to Less Wrong. We'll add a special page type that collects its content from the wiki. We propose that <a href=\"/about\">/about</a>, <a href=\"/\">the home page</a>, the comment help text, and each user's user page be of this type (I imagine that this change to the homepage may be controversial).</p>\n<p>We propose that those pages link to:</p>\n<ul>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Lesswrong:Homepage\">http://wiki.lesswrong.com/wiki/Lesswrong:Homepage</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Lesswrong:Aboutpage\">http://wiki.lesswrong.com/wiki/Lesswrong:Aboutpage</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Lesswrong:Commentmarkuphelp\">http://wiki.lesswrong.com/wiki/Lesswrong:Commentmarkuphelp</a></li>\n<li>and each user's wiki userpage if an exact name match exists.</li>\n</ul>\n<div>I've very clumsily populated those pages with starting content but hope that we can test the idea by inviting you wonderful people to make those pages be good before we develop the feature. (Which is an invitation - if you don't hate this idea, go, edit, and make good.)</div>\n<div><br /></div>\n<div><strong>Detail:</strong><br />These pages would cache wiki content for at least several hours, so would be fast to render. They would include a publicly usable \"refetch content from the wiki\"&nbsp;button&nbsp;(detailed placement, wording and design to follow) so that if the source page was spammed anyone could fix it on the wiki then clear the cache. If abuse became a problem we could easily \"<a href=\"http://en.wikipedia.org/wiki/Wikipedia:Protection_policy\">protect</a>\" those pages.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8JFCZyF3xo8YtQMcy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 21, "extendedScore": null, "score": 7.169727501931139e-07, "legacy": true, "legacyId": "7497", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["g96zwWHArQFK8HjNd", "bJ2haLkcGeLtTWaD5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-20T08:10:00.505Z", "modifiedAt": null, "url": null, "title": "List of literally false statements in the Bible", "slug": "list-of-literally-false-statements-in-the-bible", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:04.358Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/erkTQcgNJt22M3PD5/list-of-literally-false-statements-in-the-bible", "pageUrlRelative": "/posts/erkTQcgNJt22M3PD5/list-of-literally-false-statements-in-the-bible", "linkUrl": "https://www.lesswrong.com/posts/erkTQcgNJt22M3PD5/list-of-literally-false-statements-in-the-bible", "postedAtFormatted": "Friday, May 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20List%20of%20literally%20false%20statements%20in%20the%20Bible&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AList%20of%20literally%20false%20statements%20in%20the%20Bible%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FerkTQcgNJt22M3PD5%2Flist-of-literally-false-statements-in-the-bible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=List%20of%20literally%20false%20statements%20in%20the%20Bible%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FerkTQcgNJt22M3PD5%2Flist-of-literally-false-statements-in-the-bible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FerkTQcgNJt22M3PD5%2Flist-of-literally-false-statements-in-the-bible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 271, "htmlBody": "<p>Jehova's Witnesses aim to interpret the Bible <a href=\"http://en.wikipedia.org/wiki/Jehovah's_Witnesses#Sources_of_doctrine\">literally</a>,&nbsp;which is in some sense <a href=\"/lw/ic/the_virtue_of_narrowness/\">admirable</a> because that is the only way it can serve much to constrain one's anticipations about reality. &nbsp;By contrast, if one aims to interpret a religious text only \"metaphorically\", then there are so many possible meanings that it does essentially nothing to constrain one's anticipations.</p>\n<p>For example, when one accepts the best scientific knowledge about the origin of Earth, one believes that it was not in fact created in 6 days, and that the literal meaning of the English Bible is false in this case. &nbsp;Christians who accept the true age of Earth are not usually bothered by this, and resort to a \"metaphorical\" interpretation wherein \"days\" are metaphors for longer periods.</p>\n<p>But if you <em>only</em> believe that each statement in the Bible has <em>some</em> metaphorical interpretation which is true, it doesn't tell you much about the world at all. &nbsp;The Bible asserts that God exists... but since we're only taking things metaphorically now, maybe God doesn't actually literally exist. &nbsp;Maybe He's pretend. &nbsp;Maybe there in fact is no God, but there is a rainforest, and God is a metaphor for the rainforest. &nbsp;Or for the sun. &nbsp;Who knows. &nbsp;Since there is no way to tell <em>which</em>&nbsp;metaphor is the right one, believing that the Bible is \"metaphorically true\" basically tells you nothing.</p>\n<p>Jehova's Witnesses seem to understand this, so they're not going there. &nbsp;They're sticking to the literal Word of the Lord. &nbsp;Which makes me interested:</p>\n<p><strong>What verses of the Bible can we cite that are false in their literal interpretation, according to accepted scientific or well-founded historical knowledge?</strong></p>\n<p>Thanks to anyone who contributes!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "erkTQcgNJt22M3PD5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -17, "extendedScore": null, "score": -3.3e-05, "legacy": true, "legacyId": "7499", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yDfxTj9TKYsYiWH5o"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-20T10:27:25.788Z", "modifiedAt": null, "url": null, "title": "I want to save myself", "slug": "i-want-to-save-myself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:30.182Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanArmak", "createdAt": "2009-08-05T23:08:24.020Z", "isAdmin": false, "displayName": "DanArmak"}, "userId": "7KSbntzeQ2RNZq6Jw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LMiELzzme6WWKCqwR/i-want-to-save-myself", "pageUrlRelative": "/posts/LMiELzzme6WWKCqwR/i-want-to-save-myself", "linkUrl": "https://www.lesswrong.com/posts/LMiELzzme6WWKCqwR/i-want-to-save-myself", "postedAtFormatted": "Friday, May 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20want%20to%20save%20myself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20want%20to%20save%20myself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLMiELzzme6WWKCqwR%2Fi-want-to-save-myself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20want%20to%20save%20myself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLMiELzzme6WWKCqwR%2Fi-want-to-save-myself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLMiELzzme6WWKCqwR%2Fi-want-to-save-myself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 247, "htmlBody": "<p>Related to: <a href=\"/r/discussion/lw/5p2/people_who_want_to_save_the_world/\">People who want to save the world</a>&nbsp;</p>\n<p>I have recently been diagnosed with cancer, for which I am currently being treated with good prognosis. I've been reevaluating my life plans and priorities in response.&nbsp;To be clear, I estimate that the cancer is responsible for much less than half the total danger to my life. The universals - X-risks, diseases I don't have yet, traffic accidents, etc. - are worse.</p>\n<p>I would like to affirm my desire to Save Myself (and Save The World For Myself).&nbsp;Saving the world is a prerequisite simply because the world is in danger.&nbsp;I believe my values are well aligned with those of the LW community; wanting to Save The World is a good applause light but I believe most people want to do so for selfish reasons.&nbsp;</p>\n<p>I would also like to ask LW members: why do you prefer to contribute (in part) towards humankind-wide X-risk problems rather than more narrow but personally important issues? How do you determine the time- and risk- tradeoffs between things like saving money for healthcare, and investing money in preventing an unfriendly AI FOOM?</p>\n<p>It is common advice here to focus on earning money and donating it to research, rather than donating in kind.&nbsp;How do you decide what portion of income to donate to SIAI, which to SENS, and which to keep as money for purely personal problems that others won't invest in? There's no conceptual difficulty here, but I have no idea how to quantify the risks involved.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LMiELzzme6WWKCqwR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 29, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "7501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["omSbgTBa5HDqPdjHY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-20T14:03:30.150Z", "modifiedAt": null, "url": null, "title": "General Bitcoin discussion thread (May 2011)", "slug": "general-bitcoin-discussion-thread-may-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YgRNDRA5mo7dgGyn2/general-bitcoin-discussion-thread-may-2011", "pageUrlRelative": "/posts/YgRNDRA5mo7dgGyn2/general-bitcoin-discussion-thread-may-2011", "linkUrl": "https://www.lesswrong.com/posts/YgRNDRA5mo7dgGyn2/general-bitcoin-discussion-thread-may-2011", "postedAtFormatted": "Friday, May 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20General%20Bitcoin%20discussion%20thread%20(May%202011)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGeneral%20Bitcoin%20discussion%20thread%20(May%202011)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgRNDRA5mo7dgGyn2%2Fgeneral-bitcoin-discussion-thread-may-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=General%20Bitcoin%20discussion%20thread%20(May%202011)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgRNDRA5mo7dgGyn2%2Fgeneral-bitcoin-discussion-thread-may-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgRNDRA5mo7dgGyn2%2Fgeneral-bitcoin-discussion-thread-may-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>There seems to be quite a bit of a Bitcoin interest around here, with several articles about it already: [<a href=\"/lw/4mc/singularity_institute_now_accepts_donations_via/\">1</a> <a href=\"/lw/4cs/making_money_with_bitcoin/\">2</a> <a href=\"/lw/4w9/google_lends_further_legitimacy_to_bitcoin/\">3</a> <a href=\"/lw/53n/economics_of_bitcoin/\">4</a> <a href=\"/r/discussion/lw/5ru/links_bitcoin_hits_mainstream_intelligent/\">5</a> <a href=\"/r/discussion/lw/5rg/homomorphic_encryption_and_bitcoin/\">6</a> <a href=\"/r/discussion/lw/5pz/link_two_articles_on_bitcoin/\">7</a>]</p>\n<p>I propose that links and generic Bitcoin comments should be posted here, instead of making a new discussion thread for each interesting article about the subject.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jgcAJnksReZRuvgzp": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YgRNDRA5mo7dgGyn2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.170903928945596e-07, "legacy": true, "legacyId": "7502", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jLAw6dPGZCRnpNgxM", "ijr8rsyvJci2edxot", "rPBgKcH3MRry78TTL", "jwuZgk3BXnNqkyK29", "4KszqwYq8PHui8bSS", "XCuwfWFuiGxCWXFtW", "7zSyriwymreKhESfP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-20T14:19:23.356Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Statistical Bias", "slug": "seq-rerun-statistical-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:54.751Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YNRYy85tDS4MKv5gK/seq-rerun-statistical-bias", "pageUrlRelative": "/posts/YNRYy85tDS4MKv5gK/seq-rerun-statistical-bias", "linkUrl": "https://www.lesswrong.com/posts/YNRYy85tDS4MKv5gK/seq-rerun-statistical-bias", "postedAtFormatted": "Friday, May 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Statistical%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Statistical%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYNRYy85tDS4MKv5gK%2Fseq-rerun-statistical-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Statistical%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYNRYy85tDS4MKv5gK%2Fseq-rerun-statistical-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYNRYy85tDS4MKv5gK%2Fseq-rerun-statistical-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<p>Today's post, <a href=\"/lw/ha/statistical_bias/\">Statistical Bias</a> was originally published on March 30, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>There are two types of error, systematic error, and random variance error; by repeating experiments you can average out and drive down the variance error.</blockquote>\n<p><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5rr/seq_rerun_tsuyoku_vs_the_egalitarian_instinct/\">Tsuyoku vs. the Egalitarian Instinct</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YNRYy85tDS4MKv5gK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 7.170953173923507e-07, "legacy": true, "legacyId": "7503", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DbQkkgfq6fHRxmdGP", "iPBhTbb5dS8ZzmNGS", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-20T15:28:03.283Z", "modifiedAt": null, "url": null, "title": "Seeking suggestions: Less Wrong Biology 101", "slug": "seeking-suggestions-less-wrong-biology-101", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:07.011Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "virtualAdept", "createdAt": "2011-01-12T19:31:31.692Z", "isAdmin": false, "displayName": "virtualAdept"}, "userId": "cAggP7JZzrWwJCLEX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g2CNSsjW4rEqY3yBp/seeking-suggestions-less-wrong-biology-101", "pageUrlRelative": "/posts/g2CNSsjW4rEqY3yBp/seeking-suggestions-less-wrong-biology-101", "linkUrl": "https://www.lesswrong.com/posts/g2CNSsjW4rEqY3yBp/seeking-suggestions-less-wrong-biology-101", "postedAtFormatted": "Friday, May 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeking%20suggestions%3A%20Less%20Wrong%20Biology%20101&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeking%20suggestions%3A%20Less%20Wrong%20Biology%20101%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2CNSsjW4rEqY3yBp%2Fseeking-suggestions-less-wrong-biology-101%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeking%20suggestions%3A%20Less%20Wrong%20Biology%20101%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2CNSsjW4rEqY3yBp%2Fseeking-suggestions-less-wrong-biology-101", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2CNSsjW4rEqY3yBp%2Fseeking-suggestions-less-wrong-biology-101", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 441, "htmlBody": "<p>I&rsquo;ve been a reader and occasional commenter here for a while now, but previously have not had a solid idea of what I could or wanted to contribute to the community in posting.&nbsp; In light of recent <a href=\"/r/discussion/lw/5ro/what_bothers_you_about_less_wrong/47iv\">comments</a>&nbsp;stating an interest in more posts that offer concrete, factual information as well as remembering lukeprog&rsquo;s call for such things in his <a href=\"/lw/3mm/aiming_both_high_and_low/\">Back to the Basics of Rationality</a> post, I am considering a series of condensed posts about biology.&nbsp; As someone who has spent my formal education on biologically-focused engineering (bioengineering BS, now studying bioinformatics under a chemical engineering department for my PhD) but has always had the bulk of my friends in electrical engineering, computer science, and more traditional chemical engineering, I&rsquo;ve gotten used to offering such condensed explanations whenever biology works its way into a discussion.&nbsp; From what I&rsquo;ve seen on LW thus far, the community educational base leans more in those (non-biology) directions, so I believe this is a niche that could use filling.&nbsp;</p>\n<p>Since biology is a <em>rather</em> broad subject, and you could all go read Wikipedia or a textbook if you wanted a very detailed survey course, my intent is to pick targeted topics that are relevant to current events and scientific developments.&nbsp; Each post would focus on one such event/Awesome New Study, discussing the biological background and potential implications, including either short explanations or links to the basics needed to understand the subject.&nbsp; If there are any political ties to the subject, I will withhold my explicit opinions on those aspects unless asked in the comments.&nbsp;</p>\n<p>My questions, then, are the following:</p>\n<ul>\n<li><em>Is</em> this something that people here would find interesting/useful in the general sense?&nbsp; (While I do enjoy talking to myself, doing so on this topic has gotten a bit old, so I really do want to know if no one really thinks this will be helpful.)</li>\n<li>How long/in-depth would you like?&nbsp; This question is intended to gauge what my background explanation: background links ratio should be.</li>\n<li>And most importantly, <strong>what are some topics you would like to see discussed?</strong></li>\n</ul>\n<p><strong>\n<hr />\n</strong></p>\n<p><strong>UPDATE:</strong>&nbsp;Having followed the comments so far and done some preliminary outlining, I'm leaning toward a more organized progression of topics that will still tie into current interests and developments, but not be centered on them. &nbsp;A bit more thought and putting ideas to text indicated that I could group the interest areas into biological categories (molecular, populations, developmental, neuro, etc) fairly easily, which would then allow for a 'foundations' post to introduce each major category, followed by posts that go over What We Know Now, Why We Care, and Where It's Going. &nbsp;</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jaf5zfcGgCB2REXGw": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g2CNSsjW4rEqY3yBp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 37, "baseScore": 46, "extendedScore": null, "score": 0.000129, "legacy": true, "legacyId": "7504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gfexKxsBDM6v2sCMo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-20T16:18:59.694Z", "modifiedAt": null, "url": null, "title": "[HPMoR Podcast] A Musical Help Request", "slug": "hpmor-podcast-a-musical-help-request", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3phKW367FaYQHmtkd/hpmor-podcast-a-musical-help-request", "pageUrlRelative": "/posts/3phKW367FaYQHmtkd/hpmor-podcast-a-musical-help-request", "linkUrl": "https://www.lesswrong.com/posts/3phKW367FaYQHmtkd/hpmor-podcast-a-musical-help-request", "postedAtFormatted": "Friday, May 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BHPMoR%20Podcast%5D%20A%20Musical%20Help%20Request&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BHPMoR%20Podcast%5D%20A%20Musical%20Help%20Request%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3phKW367FaYQHmtkd%2Fhpmor-podcast-a-musical-help-request%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BHPMoR%20Podcast%5D%20A%20Musical%20Help%20Request%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3phKW367FaYQHmtkd%2Fhpmor-podcast-a-musical-help-request", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3phKW367FaYQHmtkd%2Fhpmor-podcast-a-musical-help-request", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 331, "htmlBody": "<p>I am quickly approaching Self-Awareness Part 1, which readers may remember as the chapter with the Ghostbusters song. Anyone who's listened to the audio of Chapter 6a knows that I have no singing talent whatsoever. :) I'm panicked at having to try to sing an actual song that many people love. So I'm asking for help.</p>\n<p>Those familiar with the chapter know that a number of students all sing altered lyrics together over the Ghostbusters Theme music. If a number of people could sing these lyrics into a microphone and send me the file I could mix them all together and overlay them on the original music. I think/hope that would sound rather cool, and I'd mention everyone who contributed by name (either real or online, your choice) in the credits (unless they'd rather not be named of course).</p>\n<p>If you know other HPMoR fans IRL and can get a number of them in the same room to all sing at once into a microphone, even better! It could even be fun!</p>\n<p>I'm terrified of doing this solo, please help! The sound files can be sent to HPMoRPodcast AT gmail.com and I'd need them by midnight on Monday, June 6th. Thanks!</p>\n<p>The original song can be found here:&nbsp;<a href=\"http://www.youtube.com/watch?v=iCHFVTQKqdQ\">http://www.youtube.com/watch?v=iCHFVTQKqdQ</a></p>\n<p>The alternate lyrics are below:</p>\n<p>&nbsp;</p>\n<p style=\"TEXT-ALIGN: center\"><em>To the tune of \"Ghostbusters\"</em></p>\n<p style=\"TEXT-ALIGN: center\"><em>(As performed on the kazoo by Fred and George Weasley,<br />and sung by Lee Jordan.)</em></p>\n<p style=\"TEXT-ALIGN: center\"><em>.</em></p>\n<p style=\"TEXT-ALIGN: center\"><em>There's a Dark Lord near?<br /></em><em>Got no need to fear<br />Who you gonna call?</em></p>\n<p>\"HARRY POTTER!\" shouted Lee Jordan, and the Weasley twins performed a triumphant chorus.</p>\n<p style=\"TEXT-ALIGN: center\"><em>With a Killing Curse?<br />Well it could be worse.<br />Who you gonna call?</em></p>\n<p>\"HARRY POTTER!\" There were a lot more voices shouting it this time.</p>\n<p style=\"TEXT-ALIGN: center\"><em>I ain't afraid of Dark Lords!</em></p>\n<p style=\"TEXT-ALIGN: center\"><em>I ain't afraid of Dark Lords!</em></p>\n<p>&nbsp;</p>\n<p style=\"TEXT-ALIGN: center\"><em>Dark robes and a mask?<br />Impossible task?<br />Who you gonna call?<br />HARRY POTTER!</em></p>\n<p style=\"TEXT-ALIGN: center\"><em>Giant Fire-Ape?<br />Old bat in a cape?<br />Who you gonna call?<br />HARRY POTTER!</em></p>\n<p style=\"TEXT-ALIGN: center\"><em>:</em></p>\n<p style=\"TEXT-ALIGN: center\"><em><em>I ain't afraid of Dark Lords!<br />I ain't afraid of Dark Lords!</em></em></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"aLB9evWFYtfyS3WJg": 1, "Xw6pxiicjuv6NJWjf": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3phKW367FaYQHmtkd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "7505", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-21T01:05:12.613Z", "modifiedAt": null, "url": null, "title": "Ontological Crises in Artificial Agents' Value Systems by Peter de Blanc", "slug": "ontological-crises-in-artificial-agents-value-systems-by", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:34.851Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uerghMGoGM9RQW8Zr/ontological-crises-in-artificial-agents-value-systems-by", "pageUrlRelative": "/posts/uerghMGoGM9RQW8Zr/ontological-crises-in-artificial-agents-value-systems-by", "linkUrl": "https://www.lesswrong.com/posts/uerghMGoGM9RQW8Zr/ontological-crises-in-artificial-agents-value-systems-by", "postedAtFormatted": "Saturday, May 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ontological%20Crises%20in%20Artificial%20Agents'%20Value%20Systems%20by%20Peter%20de%20Blanc&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOntological%20Crises%20in%20Artificial%20Agents'%20Value%20Systems%20by%20Peter%20de%20Blanc%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuerghMGoGM9RQW8Zr%2Fontological-crises-in-artificial-agents-value-systems-by%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ontological%20Crises%20in%20Artificial%20Agents'%20Value%20Systems%20by%20Peter%20de%20Blanc%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuerghMGoGM9RQW8Zr%2Fontological-crises-in-artificial-agents-value-systems-by", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuerghMGoGM9RQW8Zr%2Fontological-crises-in-artificial-agents-value-systems-by", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>I saw <a href=\"http://arxiv.org/pdf/1105.3821v1\">this</a> go by on arXiv, and thought it deserved a discussion here.</p>\n<blockquote>Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals. We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.</blockquote>\n<p>I'll post my analysis and opinion of this paper in a comment after I've taken some time to digest it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2c2": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uerghMGoGM9RQW8Zr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 26, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "7506", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-21T02:41:22.206Z", "modifiedAt": null, "url": null, "title": "The Joys of Conjugate Priors", "slug": "the-joys-of-conjugate-priors", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:06.838Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TCB", "createdAt": "2011-04-08T14:26:43.879Z", "isAdmin": false, "displayName": "TCB"}, "userId": "jnkMgqqpeSTyCAj92", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u2gWM2poRPkBPFeLc/the-joys-of-conjugate-priors", "pageUrlRelative": "/posts/u2gWM2poRPkBPFeLc/the-joys-of-conjugate-priors", "linkUrl": "https://www.lesswrong.com/posts/u2gWM2poRPkBPFeLc/the-joys-of-conjugate-priors", "postedAtFormatted": "Saturday, May 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Joys%20of%20Conjugate%20Priors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Joys%20of%20Conjugate%20Priors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu2gWM2poRPkBPFeLc%2Fthe-joys-of-conjugate-priors%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Joys%20of%20Conjugate%20Priors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu2gWM2poRPkBPFeLc%2Fthe-joys-of-conjugate-priors", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu2gWM2poRPkBPFeLc%2Fthe-joys-of-conjugate-priors", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1575, "htmlBody": "<p>(Warning: this post is a bit technical.)<br /><br />Suppose you are a Bayesian reasoning agent.&nbsp; While going about your daily activities, you observe an event of type <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" width=\"10\" height=\"8\" />. &nbsp;Because you're a good Bayesian, you have some internal parameter&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> which represents your belief that&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" width=\"10\" height=\"8\" /> will occur.<br /><br />Now, you're familiar with the Ways of Bayes, and therefore you know that your beliefs must be updated with every new datapoint you perceive.&nbsp; Your observation of&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" width=\"10\" height=\"8\" /> is a datapoint, and thus you'll want to modify <img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" />.&nbsp; But how much should this datapoint influence <img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" />?&nbsp; Well, that will depend on how sure you are of&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> in the first place.&nbsp; If you calculated&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> based on a careful experiment involving hundreds of thousands of observations, then you're probably pretty confident in its value, and this single observation of&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" width=\"10\" height=\"8\" /> shouldn't have much impact.&nbsp; But if your estimate of&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> is just a wild guess based on something your unreliable friend told you, then this datapoint is important and should be weighted much more heavily in your reestimation of <img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" />.<br /><br />Of course, when you reestimate <img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" />, you'll also have to reestimate how confident you are in its value.&nbsp; Or, to put it a different way, you'll want to compute a new probability distribution over possible values of <img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" />.&nbsp; This new distribution will be <img src=\"http://www.codecogs.com/png.latex?P(\\theta|x)\" alt=\"\" width=\"50\" height=\"19\" />, and it can be computed using Bayes' rule:<br /><br /><img src=\"http://www.codecogs.com/png.latex?P(\\theta|x)=\\frac{P(x|\\theta)P(\\theta)}{\\int P(x|\\theta)P(\\theta)d\\theta}\" alt=\"\" width=\"201\" height=\"44\" /><br /><br />Here, since&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> is a parameter used to specify the distribution from which&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" width=\"10\" height=\"8\" /> is drawn, it can be assumed that computing&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(x|\\theta)\" alt=\"\" width=\"50\" height=\"19\" /> is straightforward.&nbsp;&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(\\theta)\" alt=\"\" width=\"35\" height=\"19\" /> is your old distribution over <img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" />, which you already have; it says how accurate you think different settings of the parameters are, and allows you to compute your confidence in any given value of <img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" />.&nbsp; So the numerator should be straightforward to compute; it's the denominator which might give you trouble, since for an arbitrary distribution, computing the integral is likely to be intractable.<br /><br />But you're probably not really looking for a distribution over different parameter settings; you're looking for a single best setting of the parameters that you can use for making predictions.&nbsp; If this is your goal, then once you've computed the distribution <img src=\"http://www.codecogs.com/png.latex?P(\\theta|x)\" alt=\"\" width=\"50\" height=\"19\" />, you can pick the value of&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> that maximizes it.&nbsp; This will be your new parameter, and because you have the formula <img src=\"http://www.codecogs.com/png.latex?P(\\theta|x)\" alt=\"\" width=\"50\" height=\"19\" />, you'll know exactly how confident you are in this parameter. <br /><br />In practice, picking the value of&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> which maximizes&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(\\theta|x)\" alt=\"\" width=\"50\" height=\"19\" /> is usually pretty difficult, thanks to the presence of local optima, as well as the general difficulty of optimization problems.&nbsp; For simple enough distributions, you can use the EM algorithm, which is guarranteed to converge to a local optimum.&nbsp; But for more complicated distributions, even this method is intractable, and approximate algorithms must be used.&nbsp; Because of this concern, it's important to keep the distributions&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(x|\\theta)\" alt=\"\" width=\"50\" height=\"19\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(\\theta)\" alt=\"\" width=\"35\" height=\"19\" /> simple.&nbsp; Choosing the distribution&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(x|\\theta)\" alt=\"\" width=\"50\" height=\"19\" /> is a matter of model selection; more complicated models can capture deeper patterns in data, but will take more time and space to compute with.<br /><br />It is assumed that the type of model is chosen before deciding on the form of the distribution <img src=\"http://www.codecogs.com/png.latex?P(\\theta)\" alt=\"\" width=\"35\" height=\"19\" />.&nbsp; So how do you choose a good distribution for <img src=\"http://www.codecogs.com/png.latex?P(\\theta)\" alt=\"\" width=\"35\" height=\"19\" />?&nbsp; Notice that every time you see a new datapoint, you'll have to do the computation in the equation above.&nbsp; Thus, in the course of observing data, you'll be multiplying lots of different probability distributions together.&nbsp; If these distributions are chosen poorly,&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(\\theta)\" alt=\"\" width=\"35\" height=\"19\" /> could get quite messy very quickly.<br /><br />If you're a smart Bayesian agent, then, you'll pick&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(\\theta)\" alt=\"\" width=\"35\" height=\"19\" /> to be a <strong>conjugate prior</strong> to the distribution <img src=\"http://www.codecogs.com/png.latex?P(x|\\theta)\" alt=\"\" width=\"50\" height=\"19\" />.&nbsp; The distribution&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(\\theta)\" alt=\"\" width=\"35\" height=\"19\" /> is <strong>conjugate</strong> to&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(x|\\theta)\" alt=\"\" width=\"50\" height=\"19\" /> if multiplying these two distributions together and normalizing results in another distribution of the same form as <img src=\"http://www.codecogs.com/png.latex?P(\\theta)\" alt=\"\" width=\"35\" height=\"19\" />.<br /><br />Let's consider a concrete example: flipping a biased coin.&nbsp; Suppose you use the bernoulli distribution to model your coin.&nbsp; Then it has a parameter&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> which represents the probability of gettings heads.&nbsp; Assume that the value 1 corresponds to heads, and the value 0 corresponds to tails.&nbsp; Then the distribution of the outcome&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" width=\"10\" height=\"8\" /> of the coin flip looks like this:<br /><br /><img src=\"http://www.codecogs.com/png.latex?P(x|\\theta)=\\theta^x(1-\\theta)^{1-x}\" alt=\"\" /><br /><br />It turns out that the conjugate prior for the bernoulli distribution is something called the beta distribution.&nbsp; It has two parameters,&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" width=\"11\" height=\"8\" /> and <img src=\"http://www.codecogs.com/png.latex?\\beta\" alt=\"\" width=\"10\" height=\"17\" />, which we call <strong>hyperparameters</strong> because they are parameters for a distribution over our parameters.&nbsp; (Eek!) <br /><br />The beta distribution looks like this:<br /><br /><img src=\"http://www.codecogs.com/png.latex?P(\\theta|\\alpha,\\beta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{\\int_0^1\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}d\\theta}\" alt=\"\" width=\"254\" height=\"51\" /><br /><br />Since&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> represents the probability of getting heads, it can take on any value between 0 and 1, and thus this function is normalized properly.<br /><br />Suppose you observe a single coin flip&nbsp;<img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" width=\"10\" height=\"8\" /> and want to update your beliefs regarding <img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" />.&nbsp; Since the denominator of the beta function in the equation above is just a normalizing constant, you can ignore it for the moment while computing <img src=\"http://www.codecogs.com/png.latex?P(\\theta|x)\" alt=\"\" width=\"50\" height=\"19\" />, as long as you promise to normalize after completing the computation:<br /><br /><img src=\"http://www.codecogs.com/png.latex?\\begin{align*} P(\\theta|x) &amp;\\propto P(x|\\theta)P(\\theta) \\\\ &amp;\\propto \\Big(\\theta^x(1-\\theta)^{1-x}\\Big) \\Big(\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\Big) \\\\ &amp;=\\theta^{x+\\alpha-1}(1-\\theta)^{(1-x)+\\beta-1} \\end{align*}\" alt=\"\" /><br /><br />Normalizing this equation will, of course, give another beta distribution, confirming that this is indeed a conjugate prior for the bernoulli distribution.&nbsp; Super cool, right?<br /><br />If you are familiar with the binomial distribution, you should see that the numerator of the beta distribution in the equation for&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(\\theta|\\alpha,\\beta)\" alt=\"\" width=\"71\" height=\"19\" /> looks remarkably similar to the non-factorial part of the binomial distribution.&nbsp; This suggests a form for the normalization constant:<br /><br /><img src=\"http://www.codecogs.com/png.latex?P(\\theta|\\alpha,\\beta) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\" alt=\"\" width=\"280\" height=\"44\" /><br /><br />The beta and binomial distributions are almost identical.&nbsp; The biggest difference between them is that the beta distribution is a function of <img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" />, with&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" width=\"11\" height=\"8\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\beta\" alt=\"\" width=\"10\" height=\"17\" /> as prespecified parameters, while the binomial distribution is a function of <img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" width=\"11\" height=\"8\" />, with&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha+\\beta\" alt=\"\" width=\"44\" height=\"17\" /> as prespecified parameters.&nbsp; It should be clear that the beta distribution is also conjugate to the binomial distribution, making it just that much awesomer. <br /><br />Another difference between the two distributions is that the beta distribution uses gammas where the binomial distribution uses factorials.&nbsp; Recall that the gamma function is just a generalization of the factorial to the reals; thus, the beta distribution allows&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" width=\"11\" height=\"8\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\beta\" alt=\"\" width=\"10\" height=\"17\" /> to be any positive real number, while the binomial distribution is only defined for integers.&nbsp; As a final note on the beta distribution, the -1 in the exponents is not philosophically significant; I think it is mostly there so that the gamma functions will not contain +1s.&nbsp; For more information about the mathematics behind the gamma function and the beta distribution, I recommend checking out this pdf: <a href=\"http://www.mhtl.uwaterloo.ca/courses/me755/web_chap1.pdf\">http://www.mhtl.uwaterloo.ca/courses/me755/web_chap1.pdf</a>.&nbsp; It gives an actual derivation which shows that the first equation for&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(\\theta|\\alpha,\\beta)\" alt=\"\" width=\"71\" height=\"19\" /> is equivalent to the second equation for <img src=\"http://www.codecogs.com/png.latex?P(\\theta|\\alpha,\\beta)\" alt=\"\" width=\"71\" height=\"19\" />, which is nice if you don't find the argument by analogy to the binomial distribution convincing.<br /><br />So, what is the philosophical significance of the conjugate prior?&nbsp; Is it just a pretty piece of mathematics that makes the computation work out the way we'd like it to?&nbsp; No; there is deep philosophical significance to the form of the beta distribution. <br /><br />Recall the intuition from above: if you've seen a lot of data already, then one more datapoint shouldn't change your understanding of the world too drastically.&nbsp; If, on the other hand, you've seen relatively little data, then a single datapoint could influence your beliefs significantly.&nbsp; This intuition is captured by the form of the conjugate prior.&nbsp;&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" width=\"11\" height=\"8\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\beta\" alt=\"\" width=\"10\" height=\"17\" /> can be viewed as keeping track of how many heads and tails you've seen, respectively.&nbsp; So if you've already done some experiments with this coin, you can store that data in a beta distribution and use that as your conjugate prior.&nbsp; The beta distribution captures the difference between claiming that the coin has 30% chance of coming up heads after seeing 3 heads and 7 tails, and claiming that the coin has a 30% chance of coming up heads after seeing 3000 heads and 7000 tails.<br /><br />Suppose you haven't observed any coin flips yet, but you have some intuition about what the distribution should be.&nbsp; Then you can choose values for&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" width=\"11\" height=\"8\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\beta\" alt=\"\" width=\"10\" height=\"17\" /> that represent your prior understanding of the coin.&nbsp; Higher values of&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha+\\beta\" alt=\"\" width=\"44\" height=\"17\" /> indicate more confidence in your intuition; thus, choosing the appropriate hyperparameters is a method of quantifying your prior understanding so that it can be used in computation.&nbsp;&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" width=\"11\" height=\"8\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\beta\" alt=\"\" width=\"10\" height=\"17\" /> will act like \"imaginary data\"; when you update your distribution over&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> after observing a coin flip <img src=\"http://www.codecogs.com/png.latex?x\" alt=\"\" width=\"10\" height=\"8\" />, it will be like you already saw&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" width=\"11\" height=\"8\" /> heads and&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\beta\" alt=\"\" width=\"10\" height=\"17\" /> tails before that coin flip.<br />&nbsp;<br />If you want to express that you have no prior knowledge about the system, you can do so by setting&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" width=\"11\" height=\"8\" /> and <img src=\"http://www.codecogs.com/png.latex?\\beta \" alt=\"\" width=\"10\" height=\"17\" /> to 1.&nbsp; This will turn the beta distribution into a uniform distribution.&nbsp; You can also use the beta distribution to do add-N smoothing, by setting&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" width=\"11\" height=\"8\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\beta\" alt=\"\" width=\"10\" height=\"17\" /> to both be N+1.&nbsp; Setting the hyperparameters to a value lower than 1 causes them to act like \"negative data\", which helps avoid overfitting&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\theta\" alt=\"\" width=\"11\" height=\"15\" /> to noise in the actual data.<br /><br />In conclusion, the beta distribution, which is a conjugate prior to the bernoulli and binomial distributions, is super awesome.&nbsp; It makes it possible to do Bayesian reasoning in a computationally efficient manner, as well as having the philosophically satisfying interpretation of representing real or imaginary prior data.&nbsp; Other conjugate priors, such as the dirichlet prior for the multinomial distribution, are similarly cool.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 2, "bh7uxTTqmsQ8jZJdB": 2, "dPPATLhRmhdJtJM2t": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u2gWM2poRPkBPFeLc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 63, "extendedScore": null, "score": 0.000123, "legacy": true, "legacyId": "7511", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 63, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-21T05:06:08.824Z", "modifiedAt": null, "url": null, "title": "Upcoming meet-ups: Auckland, Bangalore, Houston, Toronto, Minneapolis, Ottawa, DC, North Carolina, BC...", "slug": "upcoming-meet-ups-auckland-bangalore-houston-toronto", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:04.709Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DpXnvGoGeBkj3e27e/upcoming-meet-ups-auckland-bangalore-houston-toronto", "pageUrlRelative": "/posts/DpXnvGoGeBkj3e27e/upcoming-meet-ups-auckland-bangalore-houston-toronto", "linkUrl": "https://www.lesswrong.com/posts/DpXnvGoGeBkj3e27e/upcoming-meet-ups-auckland-bangalore-houston-toronto", "postedAtFormatted": "Saturday, May 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Upcoming%20meet-ups%3A%20Auckland%2C%20Bangalore%2C%20Houston%2C%20Toronto%2C%20Minneapolis%2C%20Ottawa%2C%20DC%2C%20North%20Carolina%2C%20BC...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpcoming%20meet-ups%3A%20Auckland%2C%20Bangalore%2C%20Houston%2C%20Toronto%2C%20Minneapolis%2C%20Ottawa%2C%20DC%2C%20North%20Carolina%2C%20BC...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpXnvGoGeBkj3e27e%2Fupcoming-meet-ups-auckland-bangalore-houston-toronto%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Upcoming%20meet-ups%3A%20Auckland%2C%20Bangalore%2C%20Houston%2C%20Toronto%2C%20Minneapolis%2C%20Ottawa%2C%20DC%2C%20North%20Carolina%2C%20BC...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpXnvGoGeBkj3e27e%2Fupcoming-meet-ups-auckland-bangalore-houston-toronto", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpXnvGoGeBkj3e27e%2Fupcoming-meet-ups-auckland-bangalore-houston-toronto", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 303, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/lw/5rc/auckland_meetup_thrusday_may_26th/\">Auckland: Thursday May 26, 2 pm</a></li>\n<li><a href=\"/r/discussion/lw/5ps/bangalore_meetup_28th_may/\">Bangalore: Saturday May 28, 4 pm</a></li>\n<li><a href=\"/r/discussion/lw/5pp/houston_hackerspace_meetup_sunday_may_22_500pm/\">Houston: Sunday May 22, 5 pm</a></li>\n<li><a href=\"/lw/5rc/auckland_meetup_thrusday_may_26th/\"></a><a href=\"/r/discussion/lw/5rw/ottawa_lw_meetup_may_26_7pm_bayes_study_group_may/\">Ottawa: Thursday May 26, 7 pm (regular meetup)</a></li>\n<li><a href=\"/r/discussion/lw/5rw/ottawa_lw_meetup_may_26_7pm_bayes_study_group_may/\">Ottawa: Thursday May 26, 9 am (Bayes study group; note that this is in the MORNING)</a></li>\n<li><a href=\"/r/discussion/lw/5ql/dc_meetup_may_22nd/\">DC: Sunday May 22, 1 pm</a></li>\n<li><a href=\"/r/discussion/lw/5qn/triangle_nc_meetup_may_20th_6pm/\">Triangle NC: Friday May 20, 6 pm (NOTE: meetup location has changed since last week);</a></li>\n<li><a href=\"/r/discussion/lw/5ql/dc_meetup_may_22nd/\"></a><a href=\"/r/discussion/lw/5p7/victoria_bc_meetup_monday_may_23rd_5pm/\">Victoria, British Columbia: Monday May 23, 5 pm</a>;</li>\n<li><a href=\"/r/discussion/lw/5t3/toronto_meetup_may_24th/\">Toronto: Tuesday May 24, 8 pm</a></li>\n<li><a href=\"/lw/5uh/minneapolis_meetup_saturday_may_28_300pm/\">Minneapolis: Saturday May 28, 3 pm</a></li>\n</ul>\n<p>Cities with regularly scheduled meetups:&nbsp;<strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>, <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine/\">Irvine</a>.</strong></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy/\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, my username is dreamalgebra on google's webmail service.</p>\n<p><a id=\"more\"></a>To reduce front page clutter, the new plan&nbsp;is for meetups to be initially posted in the Discussion section, and for Anna Salamon to make a promoted post \"upcoming meetups\" post every Friday that links to every meet-up that has been planned for the next two weeks. [HT: <a href=\"/r/discussion/lw/5iy/proposal_consolidate_meetup_announcements_before\">Carl Shulman</a>.] Please let her know if your meetup is omitted. (I'm filling in for Anna this week.)</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post about your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening:&nbsp;<strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DpXnvGoGeBkj3e27e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 7.173540392779447e-07, "legacy": true, "legacyId": "7513", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["h8eGuJnCo5AfvtHfb", "77pFWACkmGSsBAzeb", "sMM6km5Np42EghAtY", "FCo2caLapPCJaStQ7", "ZciBtJHqTrp9hDaxY", "EALdLAoLng5MRtSRA", "v6tom6yNm7RHTvaJh", "XqBc23eDsdkqSTNTx", "2QZoYyHfRoseBsN4c", "pAHo9zSFXygp5A5dL", "d28mWBMrFt8nwpXLp", "eaKHojBdtsf35937k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-21T05:22:05.418Z", "modifiedAt": null, "url": null, "title": "Auckland meetup, Thrusday May 26th", "slug": "auckland-meetup-thrusday-may-26th", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:05.769Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AndrewH", "createdAt": "2009-03-02T15:31:43.543Z", "isAdmin": false, "displayName": "AndrewH"}, "userId": "FgKt6dBPfyiku5Qh8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h8eGuJnCo5AfvtHfb/auckland-meetup-thrusday-may-26th", "pageUrlRelative": "/posts/h8eGuJnCo5AfvtHfb/auckland-meetup-thrusday-may-26th", "linkUrl": "https://www.lesswrong.com/posts/h8eGuJnCo5AfvtHfb/auckland-meetup-thrusday-may-26th", "postedAtFormatted": "Saturday, May 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Auckland%20meetup%2C%20Thrusday%20May%2026th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAuckland%20meetup%2C%20Thrusday%20May%2026th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh8eGuJnCo5AfvtHfb%2Fauckland-meetup-thrusday-may-26th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Auckland%20meetup%2C%20Thrusday%20May%2026th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh8eGuJnCo5AfvtHfb%2Fauckland-meetup-thrusday-may-26th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh8eGuJnCo5AfvtHfb%2Fauckland-meetup-thrusday-may-26th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<p>Time for another Auckland meetup! Same place as the last one a few years back, at the <a href=\"http://www.mezzebar.co.nz/location.htm\">Messe bar</a> at 2pm on the 26th of May (Thursday).</p>\n<p>Write a comment and/or please contact me on my cell: 021 039 8554, if you are interested in coming.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h8eGuJnCo5AfvtHfb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 7.173586924089611e-07, "legacy": true, "legacyId": "7464", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-21T12:24:47.103Z", "modifiedAt": "2020-06-24T16:46:15.377Z", "url": null, "title": "Irish Less Wrong meetup Sunday 29 May?", "slug": "irish-less-wrong-meetup-sunday-29-may", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:07.150Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Scott Alexander", "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Adq7KbNpXDZJjmhE9/irish-less-wrong-meetup-sunday-29-may", "pageUrlRelative": "/posts/Adq7KbNpXDZJjmhE9/irish-less-wrong-meetup-sunday-29-may", "linkUrl": "https://www.lesswrong.com/posts/Adq7KbNpXDZJjmhE9/irish-less-wrong-meetup-sunday-29-may", "postedAtFormatted": "Saturday, May 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Irish%20Less%20Wrong%20meetup%20Sunday%2029%20May%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIrish%20Less%20Wrong%20meetup%20Sunday%2029%20May%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAdq7KbNpXDZJjmhE9%2Firish-less-wrong-meetup-sunday-29-may%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Irish%20Less%20Wrong%20meetup%20Sunday%2029%20May%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAdq7KbNpXDZJjmhE9%2Firish-less-wrong-meetup-sunday-29-may", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAdq7KbNpXDZJjmhE9%2Firish-less-wrong-meetup-sunday-29-may", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<p>Louie's LW-visits-per-city statistics put Dublin at number 18 in the world, so there have got to be some Irish LWers lurking about. I'd like to have a meetup at...let's say 2 PM on Sunday 29 May at the Starbucks at 20 College Green near Trinity College in Dublin.</p>\n<p>I am from Cork, and I'd like to convince some people from Limerick and further afield to come out, but it's a long trip to make without any reason to believe there will be more people than just myself there. So if you're interested in coming, please either post here or send me an email.</p>\n<p>Discussion will probably be about organizing future meetups, availability of rationalist/transhumanist networks and activities in Ireland, and other rationality and random chit-chat.</p>\n<p>I'm not too attached to the date, time, or location, so we can change it if it conflicts with anything.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Adq7KbNpXDZJjmhE9", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 7.174820779704287e-07, "legacy": true, "legacyId": "7524", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-21T14:14:10.756Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Useful Statistical Biases", "slug": "seq-rerun-useful-statistical-biases", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LYZLdqHjcEkX93zw3/seq-rerun-useful-statistical-biases", "pageUrlRelative": "/posts/LYZLdqHjcEkX93zw3/seq-rerun-useful-statistical-biases", "linkUrl": "https://www.lesswrong.com/posts/LYZLdqHjcEkX93zw3/seq-rerun-useful-statistical-biases", "postedAtFormatted": "Saturday, May 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Useful%20Statistical%20Biases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Useful%20Statistical%20Biases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYZLdqHjcEkX93zw3%2Fseq-rerun-useful-statistical-biases%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Useful%20Statistical%20Biases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYZLdqHjcEkX93zw3%2Fseq-rerun-useful-statistical-biases", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYZLdqHjcEkX93zw3%2Fseq-rerun-useful-statistical-biases", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>Today's post, <a href=\"/lw/hb/useful_statistical_biases/\">Useful Statistical Biases</a> was originally published on April 1, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>If you know an estimator has high variance, you can intentionally introduce bias by choosing a simpler hypothesis, and thereby lower expected variance while raising expected bias; sometimes total error is lower, hence the \"bias-variance tradeoff\". Keep in mind that while statistic bias might be useful, cognitive biases are not.</blockquote>\n<p>Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5sf/seq_rerun_statistical_bias/\">Statistical Bias</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LYZLdqHjcEkX93zw3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 7.175140165284457e-07, "legacy": true, "legacyId": "7525", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Wwq6WFpx9HyzwgCKx", "YNRYy85tDS4MKv5gK", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-21T18:16:14.211Z", "modifiedAt": null, "url": null, "title": "Toronto Meetup, May 24th", "slug": "toronto-meetup-may-24th", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Spencer_Sleep", "createdAt": "2011-02-11T08:03:53.049Z", "isAdmin": false, "displayName": "Spencer_Sleep"}, "userId": "xAjAXuvj2czWZix2v", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XqBc23eDsdkqSTNTx/toronto-meetup-may-24th", "pageUrlRelative": "/posts/XqBc23eDsdkqSTNTx/toronto-meetup-may-24th", "linkUrl": "https://www.lesswrong.com/posts/XqBc23eDsdkqSTNTx/toronto-meetup-may-24th", "postedAtFormatted": "Saturday, May 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Toronto%20Meetup%2C%20May%2024th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AToronto%20Meetup%2C%20May%2024th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqBc23eDsdkqSTNTx%2Ftoronto-meetup-may-24th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Toronto%20Meetup%2C%20May%2024th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqBc23eDsdkqSTNTx%2Ftoronto-meetup-may-24th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqBc23eDsdkqSTNTx%2Ftoronto-meetup-may-24th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><strong style=\"font-weight: bold;\">When</strong>: Tuesday, May 24th, 20:00</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><strong style=\"font-weight: bold;\">Where</strong>: The Duke of York, 39 Prince Arthur Avenue</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">Hi everyone,</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">The Toronto meetup group is having one of our bi-weekly meetings this Tuesday at the Duke of York. The reservation is under the name Spencer Sleep. I have requested a table on the patio. &nbsp;The reason for the move from the Bedford Academy (albeit by less than 10 metres) is that we are looking for somewhere quieter, thus more conducive to discussion. &nbsp;Now that it is warmer, one of our best bets is most likely patios, so we are going to try the patio at the Duke of York. &nbsp;Those of you who have been attending since the start may remember that we started out there. &nbsp;The reason we moved was the existence of live music every Thursday. &nbsp;Now that we are meeting on Tuesdays, that shouldn't be a problem.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">As always, newcomers are extremely welcome.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">If you want to hear about upcoming LessWrong events in Toronto, or have ideas about how or when those events should be run, join the&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://groups.google.com/group/lesswrongtoronto\">Toronto LessWrong Google Group</a>!</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">See you on Tuesday</p>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XqBc23eDsdkqSTNTx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 7.175846962160781e-07, "legacy": true, "legacyId": "7527", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-21T19:28:03.877Z", "modifiedAt": null, "url": null, "title": "Biases to watch out for while job hunting?", "slug": "biases-to-watch-out-for-while-job-hunting", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:06.575Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malthrin", "createdAt": "2011-03-22T15:23:59.536Z", "isAdmin": false, "displayName": "malthrin"}, "userId": "5b5DcLkcYGD9YGRfF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/of73fAW4RmPBxnu9T/biases-to-watch-out-for-while-job-hunting", "pageUrlRelative": "/posts/of73fAW4RmPBxnu9T/biases-to-watch-out-for-while-job-hunting", "linkUrl": "https://www.lesswrong.com/posts/of73fAW4RmPBxnu9T/biases-to-watch-out-for-while-job-hunting", "postedAtFormatted": "Saturday, May 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Biases%20to%20watch%20out%20for%20while%20job%20hunting%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABiases%20to%20watch%20out%20for%20while%20job%20hunting%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fof73fAW4RmPBxnu9T%2Fbiases-to-watch-out-for-while-job-hunting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Biases%20to%20watch%20out%20for%20while%20job%20hunting%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fof73fAW4RmPBxnu9T%2Fbiases-to-watch-out-for-while-job-hunting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fof73fAW4RmPBxnu9T%2Fbiases-to-watch-out-for-while-job-hunting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<p>I'm in the process of searching for a new job. I'm currently employed, but I'm dissatisfied with my salary and career growth options. I've done a couple of phone interviews and one face-to-face interview already, with several others lined up next week. The face-to-face interview went well, and I'm anticipating an offer from them next week. However, while considering how I would evaluate that offer, I caught myself awarding them points in reciprocation for their implicit praise in singling me out as a worthy candidate. Now I'm wondering what other biases I might be falling prey to in this process. Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "of73fAW4RmPBxnu9T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 7.176056719590666e-07, "legacy": true, "legacyId": "7529", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-21T19:48:02.460Z", "modifiedAt": null, "url": null, "title": "Not a Meetup May 22 in Cambridge, MA", "slug": "not-a-meetup-may-22-in-cambridge-ma", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:04.345Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SvNbC6vTkBerpTikA/not-a-meetup-may-22-in-cambridge-ma", "pageUrlRelative": "/posts/SvNbC6vTkBerpTikA/not-a-meetup-may-22-in-cambridge-ma", "linkUrl": "https://www.lesswrong.com/posts/SvNbC6vTkBerpTikA/not-a-meetup-may-22-in-cambridge-ma", "postedAtFormatted": "Saturday, May 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Not%20a%20Meetup%20May%2022%20in%20Cambridge%2C%20MA&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANot%20a%20Meetup%20May%2022%20in%20Cambridge%2C%20MA%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSvNbC6vTkBerpTikA%2Fnot-a-meetup-may-22-in-cambridge-ma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Not%20a%20Meetup%20May%2022%20in%20Cambridge%2C%20MA%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSvNbC6vTkBerpTikA%2Fnot-a-meetup-may-22-in-cambridge-ma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSvNbC6vTkBerpTikA%2Fnot-a-meetup-may-22-in-cambridge-ma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<p>I enjoyed some of the conversations at the last Cambridge (MA) meetup, particularly towards the end, but I will be in California for the next couple of Cambridge meetups (though I hope to meet some of the LW community there).</p>\n<p>I spend a lot of my time sitting and working on my laptop; there is  nothing particularly important about where I'm sitting, and being in an  unfamiliar environment seems to make me more productive if anything.</p>\n<p>Putting two and two together: I'm going to commit to being at Cosi's in Kendall square between 1pm and 3pm on May 22. Feel free to come by and talk; I'll stay longer if there is interesting conversation. If no one shows up, nothing lost.</p>\n<p>I feel like it should be possible to share this sort of information (not just here, but in general) without adding formality. For example, the act of posting such an event to meetup.com feels like it adds some unwarranted legitimacy / officialness: no one showing up would feel like a loss, and it would feel like undermining the regular meetups. On the other hand, though I'm more comfortable posting to LW discussion, posting it here inconveniences more people than it should. Deliberating at length doesn't seem worth it, so I'll just ask: what would others do?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SvNbC6vTkBerpTikA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 7.176115058134194e-07, "legacy": true, "legacyId": "7530", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-21T23:55:30.228Z", "modifiedAt": null, "url": null, "title": "Social Proficiency of a Rationalist and a Scholar", "slug": "social-proficiency-of-a-rationalist-and-a-scholar", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:53.718Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raw_Power", "createdAt": "2010-09-10T23:59:43.621Z", "isAdmin": false, "displayName": "Raw_Power"}, "userId": "kwSqcED9qTanFyNWG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xWTLJaeL7cNLfXu2n/social-proficiency-of-a-rationalist-and-a-scholar", "pageUrlRelative": "/posts/xWTLJaeL7cNLfXu2n/social-proficiency-of-a-rationalist-and-a-scholar", "linkUrl": "https://www.lesswrong.com/posts/xWTLJaeL7cNLfXu2n/social-proficiency-of-a-rationalist-and-a-scholar", "postedAtFormatted": "Saturday, May 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Social%20Proficiency%20of%20a%20Rationalist%20and%20a%20Scholar&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASocial%20Proficiency%20of%20a%20Rationalist%20and%20a%20Scholar%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWTLJaeL7cNLfXu2n%2Fsocial-proficiency-of-a-rationalist-and-a-scholar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Social%20Proficiency%20of%20a%20Rationalist%20and%20a%20Scholar%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWTLJaeL7cNLfXu2n%2Fsocial-proficiency-of-a-rationalist-and-a-scholar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWTLJaeL7cNLfXu2n%2Fsocial-proficiency-of-a-rationalist-and-a-scholar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 466, "htmlBody": "<p>Followup to <a href=\"/lw/5pj/recovering_insufferable_genius_working_title/\">Recovering Insufferable Genius</a></p>\n<p>So, we've been talking a mighty amount on avoiding and understanding the common pitfalls and mistakes that plague most human minds for various biological, evolutionary and social reasons. This knowledge is supposed to be used for the sake of learning how to think proprely and clearly about the world, and for the sake of making the right choices, and making them quickly. Both blades of the weapon can have a dramatic effect on how we interact with people. Behaviors that would appear absurd and annoying to us <a href=\"http://memegenerator.net/instance/7891168\">would suddenly gain a history, reasons for their existence until now and even for their continued existence</a>. The incomprehensible people around us suddenly become fairly simple and predictable, to the point that you might, every now and then, understand them better than they do themselves. They also become all that much more interesting. You find yourself observing them, gently pushing their buttons as you eagerly wait for what they are going to do next. Of course, this applies just as much to you yourself. You see your own past<a href=\"http://memegenerator.net/instance/7904259\"> in a very different light</a>, and <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">Akrasia</a> remains difficult to escape. But at least now you know what you're doing wrong.</p>\n<p>Anyway, you've discovered the pleasures of socializing, and you've even acquired an \"edge\" over those who relied on intuition ever since they were young. What I want us to discuss here is how to reach not just some \"proficiency\" in social navigation, but actual social <em>excellence</em>. We've collected research on how to be happy, on how to confront organizational problems, etc. I think it would be nice if we also collected data on how to be <em>polite</em>. How to make one's company agreeable and interesting. How to make oneself elegant and glamorous. How to get people to do what you want, and then thank you for it.</p>\n<p>Some slight bits of this are approached by PUA methods, but those are very specific in goal and scope, and require a set of skills that can be far from adequate in other contexts (that, and flirting with any and everybody all the time is just creepy and makes you look like a supervillain).</p>\n<p>Of course, at its core, social grace is nothing but \"intelligent application of the Golden Rule\". So, with insight and purpose, everything should be possible... But that's a pretty huge ideaspace, and in day-to-day interaction you often don't have that much time to figure our what to do. Of course, there's rote behavior, protocol, that allows you to free brainspace for what's actually important, but too much of that and<a href=\"/lw/jb/applause_lights/\"> it can become blatant.</a></p>\n<p>So... anyone know any actual research on the subject? We can also use some armchair philosophy, it's not like we eschew creative individual thinking here, but some backed-by-evidence stuff is very nice to have.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xWTLJaeL7cNLfXu2n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 7.176837813115835e-07, "legacy": true, "legacyId": "7532", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9FmkWWG9u9AYtKCiS", "dLbkrPu5STNCBLRjr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-22T01:33:21.487Z", "modifiedAt": null, "url": null, "title": "ANKI flashcard deck: Cognitive Biases and Related Terms", "slug": "anki-flashcard-deck-cognitive-biases-and-related-terms", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:03.445Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Duke", "createdAt": "2010-07-08T00:48:30.542Z", "isAdmin": false, "displayName": "Duke"}, "userId": "67L9CtYdpqT79exBW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BH9ysqhTmsF2WaftT/anki-flashcard-deck-cognitive-biases-and-related-terms", "pageUrlRelative": "/posts/BH9ysqhTmsF2WaftT/anki-flashcard-deck-cognitive-biases-and-related-terms", "linkUrl": "https://www.lesswrong.com/posts/BH9ysqhTmsF2WaftT/anki-flashcard-deck-cognitive-biases-and-related-terms", "postedAtFormatted": "Sunday, May 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20ANKI%20flashcard%20deck%3A%20Cognitive%20Biases%20and%20Related%20Terms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AANKI%20flashcard%20deck%3A%20Cognitive%20Biases%20and%20Related%20Terms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBH9ysqhTmsF2WaftT%2Fanki-flashcard-deck-cognitive-biases-and-related-terms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=ANKI%20flashcard%20deck%3A%20Cognitive%20Biases%20and%20Related%20Terms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBH9ysqhTmsF2WaftT%2Fanki-flashcard-deck-cognitive-biases-and-related-terms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBH9ysqhTmsF2WaftT%2Fanki-flashcard-deck-cognitive-biases-and-related-terms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<p>I have shared an <a href=\"http://ankisrs.net/\">ANKI flashcard deck</a> called \"Cognitive Biases and Related Terms,\" which is an upgrade of another shared deck. The deck is composed of most <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">cognitive biases listed on Wikipedia</a>, and a few other related terms, with links embedded in the cards for easy reference to articles. Including the links is both time-consuming and a major improvement in usability from the original deck, as thoroughly defining many of these concepts cannot be done on a single card. I also improved the formatting and fixed a bunch of definitions.</p>\n<p>I have been devoting ~20 minutes/day to memorizing this deck with positive results.</p>\n<p>To download the deck, open ANKI and then select File&gt;Download&gt;Shared Deck. Search for \"Cognitive Biases and Related Terms.\" You are able to modify the deck through ANKI and share your own updated version.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BH9ysqhTmsF2WaftT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 7.177120736519312e-07, "legacy": true, "legacyId": "7533", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-22T03:38:49.592Z", "modifiedAt": null, "url": null, "title": "LINK: Ralph Merkle lecture, 'Introduction to Molecular Nanotechnology'", "slug": "link-ralph-merkle-lecture-introduction-to-molecular", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:04.896Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qnuhWKEDGZpcKNLxg/link-ralph-merkle-lecture-introduction-to-molecular", "pageUrlRelative": "/posts/qnuhWKEDGZpcKNLxg/link-ralph-merkle-lecture-introduction-to-molecular", "linkUrl": "https://www.lesswrong.com/posts/qnuhWKEDGZpcKNLxg/link-ralph-merkle-lecture-introduction-to-molecular", "postedAtFormatted": "Sunday, May 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20Ralph%20Merkle%20lecture%2C%20'Introduction%20to%20Molecular%20Nanotechnology'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20Ralph%20Merkle%20lecture%2C%20'Introduction%20to%20Molecular%20Nanotechnology'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqnuhWKEDGZpcKNLxg%2Flink-ralph-merkle-lecture-introduction-to-molecular%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20Ralph%20Merkle%20lecture%2C%20'Introduction%20to%20Molecular%20Nanotechnology'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqnuhWKEDGZpcKNLxg%2Flink-ralph-merkle-lecture-introduction-to-molecular", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqnuhWKEDGZpcKNLxg%2Flink-ralph-merkle-lecture-introduction-to-molecular", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p>Ralph Merkle at Singularity University gives a contemporary introduction to molecular nanotechnology in <a href=\"http://www.youtube.com/watch?v=cdKyf8fsH6w\">this video</a>&nbsp;(1hr, 13min). Great way to be introduced to the subject, especially if your eyes get tired from too much reading.</p>\n<p>From 2009, but as far as I can tell, not yet linked from Less Wrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XJjvxWB68GYpts93N": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qnuhWKEDGZpcKNLxg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "7534", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-22T09:37:33.620Z", "modifiedAt": null, "url": null, "title": "Rationalist horoscopes: A low-hanging utility generator.", "slug": "rationalist-horoscopes-a-low-hanging-utility-generator", "viewCount": null, "lastCommentedAt": "2021-04-18T10:47:03.541Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AdeleneDawner", "createdAt": "2009-04-28T14:40:00.131Z", "isAdmin": false, "displayName": "AdeleneDawner"}, "userId": "MeSREm4SMRGxeQ8X3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TmwJ25bkce5KASCYN/rationalist-horoscopes-a-low-hanging-utility-generator", "pageUrlRelative": "/posts/TmwJ25bkce5KASCYN/rationalist-horoscopes-a-low-hanging-utility-generator", "linkUrl": "https://www.lesswrong.com/posts/TmwJ25bkce5KASCYN/rationalist-horoscopes-a-low-hanging-utility-generator", "postedAtFormatted": "Sunday, May 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20horoscopes%3A%20A%20low-hanging%20utility%20generator.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20horoscopes%3A%20A%20low-hanging%20utility%20generator.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTmwJ25bkce5KASCYN%2Frationalist-horoscopes-a-low-hanging-utility-generator%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20horoscopes%3A%20A%20low-hanging%20utility%20generator.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTmwJ25bkce5KASCYN%2Frationalist-horoscopes-a-low-hanging-utility-generator", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTmwJ25bkce5KASCYN%2Frationalist-horoscopes-a-low-hanging-utility-generator", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 467, "htmlBody": "<p>The other day, I had an idea. It occurred to me that daily horoscopes - the traditional kind - might not be as useless as they seem at first glance: They usually give, or at least hint at, suggestions for specific things to do on a given day, which can be a useful cue, allowing the user to put less effort into finding something useful to do with their time. They can also act as a reminder of important concepts, rather like spaced repetition, and have the possibility of serendipitously giving the perfect advice in a situation where the user would otherwise not have thought to apply a particular concept.</p>\n<p>This seems like something that many people here would find useful, if they weren't so vague, and if they were better calibrated to make useful suggestions. So, after <a href=\"/lw/5rb/rationalist_horoscopes_lowhanging_utility/\">getting some feedback</a>, and with the help of <a href=\"/user/PeerInfinity/\">PeerInfinity</a> (who did most of the coding and is currently hosting the program), I put together a tool to provide us with a daily 'horoscope', chosen from a list provided by us and weighted toward advice that has been reported to work. The horoscopes are displayed <a href=\"http://actuallyusefulhoroscopes.tumblr.com/\">here</a>, with an RSS feed available <a href=\"http://actuallyusefulhoroscopes.tumblr.com/rss\">here</a>. Lists of the horoscopes in the program's database can be found <a href=\"http://peerinfinity.com/horoscopes/index.php\">here</a>, with various sorting options.</p>\n<p>One of the features of this program is that the chance of a given horoscope being displayed are affected by how well it has worked in the past. Every day, there is an option to vote on the previous day's horoscope, rating it as 'harmful', 'useless', 'sort of useful', 'useful', or 'awesome'. The 'harmful' and 'useless' options give the horoscope -15 and -1 points respectively, while the other three give it 1, 3, or 10 points. If a horoscope's score becomes negative, it is removed from the pool of active horoscopes; otherwise, its chance of being chosen is based on the average value of the votes it has received compared to the other horoscopes, disregarding recently-used ones.</p>\n<p>There is still a need for good horoscopes to be added to the database. Horoscopes should offer a specific suggestion for something to do that will take less than an hour of sustained effort (all-day mindfulness-type exercises or 'be on the lookout for X' are fine) and that can be accomplished on the same day that the horoscope is read. Horoscopes should not make actual predictions, but may make prediction-like statements that are likely to be true on any given day, like \"you will talk to a friend today\". Horoscopes can be submitted <a href=\"http://actuallyusefulhoroscopes.tumblr.com/submit\">here</a>, or left in the comments. <strong>EDIT:</strong> Any comment anywhere on the site that contains the phrase \"Horoscope version:\" or \"Horoscope:\" should now automatically be emailed to me, so feel free to horoscope-ify new posts in their comments, unless this comes to be considered spam.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 1, "fkABsGCJZ6y9qConW": 1, "TkZ7MFwCi4D63LJ5n": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TmwJ25bkce5KASCYN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 69, "baseScore": 79, "extendedScore": null, "score": 0.000146, "legacy": true, "legacyId": "7536", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 79, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KRBbe3vPybMzrwWow"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-22T14:21:20.490Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Error of Crowds", "slug": "seq-rerun-the-error-of-crowds", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:04.774Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sZqMn3yaPBfisiD4o/seq-rerun-the-error-of-crowds", "pageUrlRelative": "/posts/sZqMn3yaPBfisiD4o/seq-rerun-the-error-of-crowds", "linkUrl": "https://www.lesswrong.com/posts/sZqMn3yaPBfisiD4o/seq-rerun-the-error-of-crowds", "postedAtFormatted": "Sunday, May 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Error%20of%20Crowds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Error%20of%20Crowds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZqMn3yaPBfisiD4o%2Fseq-rerun-the-error-of-crowds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Error%20of%20Crowds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZqMn3yaPBfisiD4o%2Fseq-rerun-the-error-of-crowds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZqMn3yaPBfisiD4o%2Fseq-rerun-the-error-of-crowds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>Today's post, <a href=\"/lw/hc/the_error_of_crowds/\">The Error of Crowds</a> was originally published on April 1, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>Mean squared error drops when we average our predictions, but only because it uses a convex loss function. If you faced a concave loss function, you wouldn't isolate yourself from others, which casts doubt on the relevance of Jensen's inequality for rational communication. The process of sharing thoughts and arguing differences is not like taking averages.</blockquote>\n<p>Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5t1/seq_rerun_useful_statistical_biases/\">Useful Statistical Biases</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sZqMn3yaPBfisiD4o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.17936764586672e-07, "legacy": true, "legacyId": "7537", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zAvhTnQX6ynJF7pyh", "LYZLdqHjcEkX93zw3", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-22T19:56:27.952Z", "modifiedAt": "2020-01-16T02:06:24.917Z", "url": null, "title": "A summary of Savage's foundations for probability and utility.", "slug": "a-summary-of-savage-s-foundations-for-probability-and", "viewCount": null, "lastCommentedAt": "2020-01-16T02:03:53.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sniffnoy", "createdAt": "2009-10-25T00:27:41.113Z", "isAdmin": false, "displayName": "Sniffnoy"}, "userId": "66EwcncPSoZ25StpW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5J34FAKyEmqKaT7jt/a-summary-of-savage-s-foundations-for-probability-and", "pageUrlRelative": "/posts/5J34FAKyEmqKaT7jt/a-summary-of-savage-s-foundations-for-probability-and", "linkUrl": "https://www.lesswrong.com/posts/5J34FAKyEmqKaT7jt/a-summary-of-savage-s-foundations-for-probability-and", "postedAtFormatted": "Sunday, May 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20summary%20of%20Savage's%20foundations%20for%20probability%20and%20utility.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20summary%20of%20Savage's%20foundations%20for%20probability%20and%20utility.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5J34FAKyEmqKaT7jt%2Fa-summary-of-savage-s-foundations-for-probability-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20summary%20of%20Savage's%20foundations%20for%20probability%20and%20utility.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5J34FAKyEmqKaT7jt%2Fa-summary-of-savage-s-foundations-for-probability-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5J34FAKyEmqKaT7jt%2Fa-summary-of-savage-s-foundations-for-probability-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3787, "htmlBody": "<html><head></head><body><p><strong>Edit:</strong> I think the P2c I wrote originally may have been a bit too weak; fixed that. Nevermind, rechecking, that wasn't needed.</p>\n<p><strong>More edits (now consolidated):</strong> Edited nontriviality note. Edited totality note. Added in the definition of numerical probability in terms of qualitative probability (though not the proof that it works). Also slight clarifications on implications of P6' and P6''' on partitions into equivalent and almost-equivalent parts, respectively.</p>\n<p><strong>One very late edit, June 2:</strong> Even though we don't get countable additivity, we still want a \u03c3-algebra rather than just an algebra (this is needed for some of the proofs in the \"partition conditions\" section that I don't go into here). Also noted nonemptiness of gambles.</p>\n<p>The idea that rational agents act in a manner isomorphic to expected-utility maximizers is often used here, typically justified with the <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">Von Neumann-Morgenstern theorem</a>.&nbsp; (The last of Von Neumann and Morgenstern's axioms, the independence axiom, can be grounded in a <a href=\"/lw/my/the_allais_paradox/\">Dutch book argument</a>.)&nbsp; But the Von Neumann-Morgenstern theorem assumes that the agent already measures its beliefs with (finitely additive) probabilities.&nbsp; This in turn is often justified with <a href=\"http://en.wikipedia.org/wiki/Cox%27s_theorem\">Cox's theorem</a> (valid so long as we assume a \"large world\", which is implied by e.g. the existence of a fair coin).&nbsp; But Cox's theorem assumes as an axiom that the plausibility of a statement is taken to be a real number, a very large assumption!&nbsp; I have also seen this justified here with Dutch book arguments, but these all seem to assume that we are already using some notion of expected utility maximization (which is not only somewhat circular, but also a considerably stronger assumption than that plausibilities are measured with real numbers).</p>\n<p>There is a way of grounding both (finitely additive) probability and utility simultaneously, however, as detailed by Leonard Savage in his <em>Foundations of Statistics</em> (1954).&nbsp; In this article I will state the axioms and definitions he gives, give a summary of their logical structure, and suggest a slight modification (which is equivalent mathematically but slightly more philosophically satisfying).&nbsp; I would also like to ask the question: To what extent can these axioms be grounded in Dutch book arguments or other more basic principles?&nbsp; I warn the reader that I have not worked through all the proofs myself and I suggest simply finding a copy of the book if you want more detail.</p>\n<p>Peter Fishburn later showed in <em>Utility Theory for Decision Making</em> (1970) that the axioms set forth here actually imply that utility is bounded.</p>\n<p>(Note: The versions of the axioms and definitions in the end papers are formulated slightly differently from the ones in the text of the book, and in the 1954 version have an error. I'll be using the ones from the text, though in some cases I'll reformulate them slightly.)</p>\n<p>Primitive notions; preference given a set of states</p>\n<hr>\n<p>We will use the following primitive notions. Firstly, there is a set S of \"states of the world\"; the exact current state of the world is unknown to the agent. Secondly, there is a set F of \"consequences\" - things that can happen as a result of the agent's actions.&nbsp; <em>Actions</em> or <em>acts</em> will be interpreted as functions f:S\u2192F, as two actions which have the same consequences regardless of the state of the world are indistinguishable and hence considered equal.&nbsp; While the agent may be uncertain as to the exact results of its actions, this can be folded into his uncertainty about the state of the world.&nbsp; Finally, we introduces as primitive a relation \u2264 on the set of actions, interpreted as \"is not preferred to\".&nbsp; I.e., f\u2264g means that given a choice between actions f and g, the agent will either prefer g or be indifferent.&nbsp; As usual, sets of states will be referred to as \"events\", and for the usual reasons we may want to restrict the set of admissible events to a boolean \u03c3-subalgebra of \u2118(S), though I don't know if that's really necessary here (Savage doesn't seem to do so, though he does discuss it some). [Edit nine years later: This actually introduces a slight issue I didn't realize before, but fortunately it's <a href=\"https://www.lesswrong.com/posts/5J34FAKyEmqKaT7jt/a-summary-of-savage-s-foundations-for-probability-and#jNTffBWduTaeJudYd\">easily fixed</a>.]</p>\n<p>In any case, we then have the following axiom:</p>\n<p><em>P1. The relation \u2264 is a <a href=\"http://en.wikipedia.org/wiki/Strict_weak_ordering#Total_preorders\">total preorder</a>.</em></p>\n<p>The intuition here for transitivity is pretty clear.&nbsp; For totality, if the agent is presented with a choice of two acts, it must choose one of them!&nbsp; Or be indifferent.&nbsp; Perhaps we could instead use a partial preorder (or order?), though this would give us two different indistinguishable flavors of indifference, which seems problematic.&nbsp; But this could be useful if we wanted intransitive indifference.&nbsp; So long as indifference is transitive, though, we can collapse this into a total preorder.</p>\n<p>As usual we can then define f\u2265g, f&lt;g (meaning \"it is false that g\u2264f\"), and g&gt;f. &nbsp;I will use f\u2261g to mean \"f\u2264g and g\u2264f\", i.e., the agent is indifferent between f and g. (Savage uses an equals sign with a dot over it.)</p>\n<p>Note that though \u2264 is defined in terms of how the agent chooses when presented with two options, Savage later notes that there is a construction of W. Allen Wallis that allows one to adduce the agent's preference ordering among a finite set of more than two options (modulo indifference): Simply tell the agent to rank the options given, and that afterward, two of them will be chosen uniformly at random, and it will get whichever one it ranked higher.</p>\n<p>The second axiom states that if two actions have the same consequences in some situation, just what that equal consequence is does not affect their relative ordering:</p>\n<p><em>P2. Suppose f\u2264g, and B is a set of states such f and g agree on B.&nbsp; If f' and g' are another pair of acts which, outside of B, agree with f and g respectively, and on B, agree with each other, then f'\u2264g'.</em></p>\n<p>In other words, to decide between two actions, only the cases where they actually have different consequences matter.</p>\n<p>With this axiom, we can now define:</p>\n<p><em>D1. We say \"f\u2264g given B\" to mean that if f' and g' are actions such that f' agrees with f on B, g' agrees with g on B, and f' and g' agree with each other outside of B, then f'\u2264g'.</em></p>\n<p>Due to axiom P2, this is well-defined.</p>\n<p>Here is where I would like to suggest a small modification to this setup.&nbsp; The notion of \"f\u2264g given B\" is implicitly taken to be how the agent makes decisions if it knows that B obtains.&nbsp; However it seems to me that we should actually take \"f\u2264g given B\", rather than f\u2264g, to be the primitive notion, explicitly interpeted as \"the agent does not prefer f to g if it knows that B obtains\".&nbsp; The agent always has some state of prior knowledge and this way we have explicitly specified decisions under a given state of knowledge - the acts we are concerned with - as the basis of our theory.&nbsp; Rather than defining f\u2264g given B in terms of \u2264, we can define f\u2264g to mean \"f\u2264g given S\" and then add additional axioms governing the relation between \"\u2264 given B\" for varying B, which in Savage's setup are theorems or part of the definition D1.</p>\n<p>(Specifically, I would modify P1 and P2 to talk about \"\u2264 given B\" rather than \u2264, and add the following theorems as axioms:</p>\n<p><em>P2a. If f and g agree on B, then f\u2261g given B.</em></p>\n<p><em>P2b. If B\u2286C, f\u2264g given C, and f and g agree outside B, then f\u2264g given B.</em></p>\n<p><em>P2c. If B and C are disjoint, and f\u2264g given B and given C, then f\u2264g given B\u222aC.</em></p>\n<p>This is a little unwieldy and perhaps there is an easier way - these might not be minimal. But they do seem to be sufficient.)</p>\n<p>In any case, regardless which way we do it, we've now established the notion of preference given that a set of states obtains, as well as preference without additional knowledge, so henceforth I'll freely use both as Savage does without worrying about which makes a better foundation, since they are equivalent.</p>\n<p>Ordering on preferences</p>\n<hr>\n<p>The next definition is simply to note that we can sensibly talk about f\u2264b, b\u2264f, b\u2264c where here b and c are consequences rather than actions, simply by interpreting consequences as constant functions.&nbsp; (So the agent does have a preference ordering on consequences, it's just induced from its ordering on actions.&nbsp; We do it this way since it's its choices between actions we can actually see.)</p>\n<p>However, the third axiom reifies this induced ordering somewhat, by demanding that it be invariant under gaining new information.</p>\n<p><em>P3'. If b and c are consequences and b\u2264c, then b\u2264c given any B.</em></p>\n<p>Thus the fact that the agent may change preferences given new information, just reflects its uncertainty about the results of their actions, rather than actually preferring different consequences in different states (any such preferences can be done away with by simply expanding the set of consequences).</p>\n<p>Really this is not strong enough, but to state the actual P3 we will first need a definition:</p>\n<p><em>D3. An event B is said to be null if f\u2264g given B for any actions f and g.</em></p>\n<p>Null sets will correspond to sets of probability 0, once numerical probability is introduced.&nbsp; Probability here is to be adduced from the agent's preferences, so we cannot distinguish between \"the agent is certain that B will not happen\" and \"if B obtains, the agent doesn't care what happens\".</p>\n<p>Now we can state the actual P3:</p>\n<p><em>P3. If b and c are consequences and B is not null, then b\u2264c given B if and only if b\u2264c.</em></p>\n<p>P3', by contrast, allowed some collapsing of preference on gaining new information; here we have disallowed that except in the case where the new information is enough to collapse all preferences entirely (a sort of \"end of the world\" or \"fatal error\" scenario).</p>\n<h2>Qualitative probability</h2>\n<p>We've introduced above the idea of \"probability 0\" (and hence implicitly probability 1; observe that \"\u00acB is null\" is equivalent to \"for any f and g, f\u2264g given B if and only if f\u2264g\"). Now we want to expand this to probability more generally. But we will not initially get numbers out of it; rather we will first just get another total preordering, A\u2264B, \"A is at most as probable as B\".</p>\n<p>How can we determine which of two events the agent thinks is more probable? Have it bet on them, of course! First, we need a nontriviality axiom so it has some things to bet on.</p>\n<p><em>P5. There exist consequences b and c such that b&gt;c.</em></p>\n<p>(I don't know what the results would be if instead we used the weaker nontriviality axiom \"there exist actions f and g such that f&lt;g\", i.e., \"S is not null\". That we eventually get that expected utility for comparing all acts suggests that this should work, but I haven't checked.)</p>\n<p>So let us now consider a class of actions which I will call \"wagers\". (Savage doesn't have any special term for these.) Define \"the wager on A for b over c\" to mean the action that, on A, returns b, and otherwise, returns c. Denote this by wA,b,c. Then we postulate:</p>\n<p><em>P4. Let b&gt;b' be a pair of consequences, and c&gt;c' another such pair. Then for any events A and B, wA,b,b'\u2264wB,b,b' if and only if wA,c,c'\u2264wB,c,c'.</em></p>\n<p>That is to say, if the agent is given the choice between betting on event A and betting on event B, and the prize and booby prize are the same regardless of which it bets on, then it shouldn't just matter just what the prize and booby prize are - it should just bet on whichever it thinks is more probable.&nbsp; Hence we can define:</p>\n<p><em>D4. For events A and B, we say \"A is at most as probable as B\", denoted A\u2264B</em>, <em>if wA,b,b'\u2264wB,b,b', where b&gt;b' is a pair of consequences.</em></p>\n<p>By P4, this is well-defined. We can then show that the relation on events \u2264 is a total preorder, so we can use the usual notation when talking about it (again, \u2261 will denote equivalence).</p>\n<p>In fact, \u2264 is not only a total preorder, but a <em>qualitative probability</em>:</p>\n<ol>\n<li>\u2264 is a total preorder</li>\n<li>\u2205\u2264A for any event A</li>\n<li>\u2205&lt;S</li>\n<li>Given events B, C, and D with D disjoint from B and C, then B\u2264C if and only if B\u222aD\u2264C\u222aD.</li>\n</ol>\n<p>(There is no condition corresponding to countable additivity; as mentioned above, we simply won't get countable additivity out of this.)&nbsp; Note also that under this, A\u2261\u2205 if and only if A is null in the earlier sense.&nbsp; Also, we can define \"A\u2264B given C\" by comparing the wagers given C; this is equivalent to the condition that A\u2229C\u2264B\u2229C.&nbsp; This relation is too a qualitative probability.</p>\n<h2>Partition conditions and numerical probability</h2>\n<p>In order to get real numbers to appear, we are of course going to have to make some sort of <a href=\"http://en.wikipedia.org/wiki/Archimedean_property\">Archimedean assumption</a>.&nbsp; In this section I discuss what some of these look like and then ultimately state P6, the one Savage goes with.</p>\n<p>First, definitions. We will be considering finitely-additive probability measures on the set of states, i.e. a function P from the set of events to the interval [0,1] such that P(S)=1, and for disjoint B and C, P(B\u222aC)=P(B)+P(C).&nbsp; We will say \"P agrees with \u2264\" if for every A and B, A\u2264B if and only if P(A)\u2264P(B); and we will say \"P almost agrees with \u2264\" if for every A and B, A\u2264B implies P(A)\u2264P(B).&nbsp; (I.e., in the latter case, numerical probability is allowed to collapse some distinctions between events that the agent might not actually be indifferent between.)</p>\n<p>We'll be considering here partitions of the set of states S.&nbsp; We'll say a partition of S is \"uniform\" if the parts are all equivalent.&nbsp; More generally we'll say it is \"almost uniform\" if, for any r, the union of any r parts is at most as probable as the union of any r+1 parts. (This is using \u2264, remember; we don't have numerical probabilities yet!) (Note that any uniform partition is almost uniform.) Then it turns out that the following are equivalent:</p>\n<ol>\n<li>There exist almost-uniform partitions of S into arbitrarily large numbers of parts.</li>\n<li>For any B&gt;\u2205, there exists a partition of S with each part less probable than B.</li>\n<li>There exists a (necessarily unique) finitely additive probability measure P that almost agrees with \u2264, which has the property that for any B and any 0\u2264\u03bb\u22641, there is a C\u2286B such that P(C)=\u03bbP(B).</li>\n</ol>\n<p>(Definitely not going into the proof of this here.&nbsp; However, the actual definition of the numerical probability P(A) is not so complicated: Let k(A,n) denote the largest r such that there exists an almost-uniform partition of S into n parts, for which there is some union of r parts, C, such that C\u2264A.&nbsp; Then the sequence k(A,n)/n always converges, and we can define P(A) to be its limit.)</p>\n<p>So we could use this as our 6th axiom:</p>\n<p><em>P6'''. For any B&gt;\u2205</em>_, there exists a partition of S with each part less probable than B._</p>\n<p>Savage notes that other authors have assumed the stronger</p>\n<p><em>P6''. There exist uniform partitions of S into arbitrarily large numbers of parts.</em></p>\n<p>since there's an obvious justification for this: the existence of a fair coin! If a fair coin exists, then we can generate a uniform partition of S into 2n parts simply by flipping it n times and considering the result.&nbsp; We'll actually end up assuming something even stronger than this.</p>\n<p>So P6''' does get us numerical probabilities, but they don't necessarily reflect all of the qualitative probability; P6''' is only strong enough to force almost agreement. Though it is stronger than that when \u2205 is involved - it does turn out that P(B)=0 if and only if B\u2261\u2205.&nbsp; (And hence also P(B)=1 if and only if B\u2261S.)&nbsp; But more generally it turns out that P(B)=P(C) if and only if B and C are \"almost equivalent\", which I will denote B\u2248C (Savage uses a symbol I haven't seen elsewhere), which is defined to mean that for any E&gt;\u2205 disjoint from B, B\u222aE\u2265C, and for any E&gt;\u2205 disjoint from C, C\u222aE\u2265B.</p>\n<p>(It's not obvious to me that \u2248 is in general an equivalence relation, but it certainly is in the presence of P6'''; Savage seems to use this implicitly.&nbsp; Note also that another consequence of P6''' is that for any n there exists a partition of S into n almost-equivalent parts; such a partition is necessarily almost-uniform.)</p>\n<p>However the following stronger version of P6''' gets rid of this distinction:</p>\n<p><em>P6'. For any B&gt;C, there exists a partition of S, each part D of which satisfies C\u222aD&lt;B.</em></p>\n<p>(Observe that P6''' is just P6' for C=\u2205.) Under P6', almost equivalence is equivalence, and so numerical probability agrees with qualitative probability, and we finally have what we wanted. (So by earlier, P6' implies P6'', not just P6'''.&nbsp; Indeed by above it implies the existence of uniform partitions into n parts for any n, not just arbitrarily large n.)</p>\n<p>In actuality, Savage assumes an even stronger axiom, which is needed to get utility and not just probability:</p>\n<p><em>P6. For any acts g&lt;h, and any consequence b, there is a partition of S such that if g is modified on any one part to be constantly b there, we would still have g&lt;h; and if h is modified on any one part to be constantly b there, we would also still have g&lt;h.</em></p>\n<p>Applying P6 to wagers yields the weaker P6'.</p>\n<p>We can now also get conditional probability - if P6' holds, it also holds for the preorderings \"\u2264 given C\" for non-null C, and hence we can define P(B|C) to be the probability of B under the quantitative probability we get corresponding to the qualitative probabilty \"\u2264 given C\".&nbsp; Using the uniqueness of agreeing probability measures, it's easy to check that indeed, P(B|C)=P(B\u2229C)/P(C).</p>\n<h2>Utility for finite gambles</h2>\n<p>Now that we have numerical probability, we can talk about finite gambles. If we have consequences b1, ..., bn, and probabilities \u03bb1, ..., \u03bbn summing to 1, we can consider the gamble \u2211\u03bbibi, represented by any action which yields b1 with probability \u03bb1, b2 with probability \u03bb2, etc.&nbsp; (And with probability 0 does anything; we don't care about events with probability 0.)&nbsp; Note that by above such an action necessarily exists.&nbsp; It can be proven that any two actions representing the same gamble are equivalent, and hence we can talk about comparing gambles.&nbsp; We can also sensibly talk about mixing gambles - taking \u2211\u03bbifi where the fi are finite gambles, and the \u03bbi are probabilities summing to 1 - in the obvious fashion.</p>\n<p>With these definitions, it turns out that Von Neumann and Morgenstern's independence condition holds, and, using axiom P6, Savage shows that the continuity (i.e. Archimedean) condition also holds, and hence there is indeed a utility function, a function U:F\u2192<strong>R</strong> such that for any two finite gambles represented by f and g respectively, f\u2264g if and only if the expected utility of the first gamble is less than or equal to that of the second.&nbsp; Furthermore, any two such utility functions are related via an increasing affine transformation.</p>\n<p>We can also take expected value knowing that a given event C obtains, since we have numerical probability; and indeed this agrees with the preference ordering on gambles given C.</p>\n<p>Expected utility in general and boundedness of utility</p>\n<hr>\n<p>Finally, Savage shows that if we assume one more axiom, P7, then we have that for any <a href=\"http://en.wikipedia.org/wiki/Essential_supremum_and_essential_infimum\">essentially bounded</a> actions f and g, we have f\u2264g if and only if the expected utility of f is at most that of g.&nbsp; (It is possible to define integration with respect to a finitely additive measure similarly to how one does with respect to a countably additive measure; the result is linear and monotonic but doesn't satisfy convergence properties.)&nbsp; Similarly with respect to a given event C.</p>\n<p>The axiom P7 is:</p>\n<p><em>P7. If f and g are acts and B is an event such that f\u2264g(s) given B for every s\u2208B, then f\u2264g given B. Similarly, if f(s)\u2264g given B for every s in B, then f\u2264g given B.</em></p>\n<p>So this is just another variant on the \"sure-thing principle\" that I earlier labeled P2c.</p>\n<p>Now in fact it turns out as mentioned above that P7, when taken together with the rest, implies that utility is bounded, and hence that we do indeed have that for any f and g, f\u2264g if and only if the expected utility of f is at most that of g!&nbsp; This is due to Peter Fishburn and postdates the first edition of <em>Foundations of Statistics</em>, so in there Savage simply notes that it would be nice if this worked for f and g not necessarily essentially bounded (so long as their expected values exist, and allowing them to be \u00b1\u221e), but that he can't prove this, and then adds a footnote giving a reference for bounded utility.&nbsp; (Though he does prove using P7 that if you have two acts f and g such that f,g\u2264b for all consequences b, then f\u2261g; similarly if f,g\u2265b for all b.&nbsp; Actually, this is a key lemma in proving that utility is bounded; Fishburn's proof works by showing that if utility were unbounded, you could construct two actions that contradict this.)</p>\n<p>Of course, if you really don't like the conclusion that utility is bounded, you could throw out axiom 7! It's pretty intuitive, but it's not clear that ignoring it could actually get you Dutch-booked.&nbsp; After all, the first 6 axioms are enough to handle finite gambles, 7 is only needed for more general situations.&nbsp; So long as your Dutch bookie is limited to finite gambles, you don't need this.</p>\n<h2>Questions on further justification</h2>\n<p>So now that I've laid all this out, here's the question I originally meant to ask: To what extent can these axioms be grounded in more basic principles, e.g. Dutch book arguments?&nbsp; It seems to me that most of these are too basic for that to apply - Dutch book arguments need more working in the background.&nbsp; Still, it seems to me axioms P2, P3, and P4 might plausibly be grounded this way, though I have not yet attempted to figure out how. P7 presumably can't, for the reasons noted in the previous section. P1 I assume is too basic.&nbsp; P5 obviously can't (if the agent doesn't care about anything, that's its own problem).</p>\n<p>P6 is an Archimedean condition. Typically I've seen those (specifically Von Neumann and Morgenstern's continuity condition) justified on this site with the idea that infinitesimals will never be relevant in any practical situation - if c has only infinitesimally more utility than b, the only case when the distinction would be relevant is if the probabilities of accomplishing them were exactly equal, which is not realistic.&nbsp; I'm guessing infinitesimal probabilities can probably be done away with in a similar manner?</p>\n<p>Or are these not good axioms in the first place?&nbsp; You all are more familiar with these sorts of things than me. Ideas?</p>\n</body></html>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5J34FAKyEmqKaT7jt", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 75, "extendedScore": null, "score": 0.00014, "legacy": true, "legacyId": "7538", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 75, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Edit:</strong> I think the P2c I wrote originally may have been a bit too weak; fixed that. Nevermind, rechecking, that wasn't needed.</p>\n<p><strong>More edits (now consolidated):</strong> Edited nontriviality note. Edited totality note. Added in the definition of numerical probability in terms of qualitative probability (though not the proof that it works). Also slight clarifications on implications of P6' and P6''' on partitions into equivalent and almost-equivalent parts, respectively.</p>\n<p><strong>One very late edit, June 2:</strong> Even though we don't get countable additivity, we still want a \u03c3-algebra rather than just an algebra (this is needed for some of the proofs in the \"partition conditions\" section that I don't go into here). Also noted nonemptiness of gambles.</p>\n<p>The idea that rational agents act in a manner isomorphic to expected-utility maximizers is often used here, typically justified with the <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">Von Neumann-Morgenstern theorem</a>.&nbsp; (The last of Von Neumann and Morgenstern's axioms, the independence axiom, can be grounded in a <a href=\"/lw/my/the_allais_paradox/\">Dutch book argument</a>.)&nbsp; But the Von Neumann-Morgenstern theorem assumes that the agent already measures its beliefs with (finitely additive) probabilities.&nbsp; This in turn is often justified with <a href=\"http://en.wikipedia.org/wiki/Cox%27s_theorem\">Cox's theorem</a> (valid so long as we assume a \"large world\", which is implied by e.g. the existence of a fair coin).&nbsp; But Cox's theorem assumes as an axiom that the plausibility of a statement is taken to be a real number, a very large assumption!&nbsp; I have also seen this justified here with Dutch book arguments, but these all seem to assume that we are already using some notion of expected utility maximization (which is not only somewhat circular, but also a considerably stronger assumption than that plausibilities are measured with real numbers).</p>\n<p>There is a way of grounding both (finitely additive) probability and utility simultaneously, however, as detailed by Leonard Savage in his <em>Foundations of Statistics</em> (1954).&nbsp; In this article I will state the axioms and definitions he gives, give a summary of their logical structure, and suggest a slight modification (which is equivalent mathematically but slightly more philosophically satisfying).&nbsp; I would also like to ask the question: To what extent can these axioms be grounded in Dutch book arguments or other more basic principles?&nbsp; I warn the reader that I have not worked through all the proofs myself and I suggest simply finding a copy of the book if you want more detail.</p>\n<p>Peter Fishburn later showed in <em>Utility Theory for Decision Making</em> (1970) that the axioms set forth here actually imply that utility is bounded.</p>\n<p>(Note: The versions of the axioms and definitions in the end papers are formulated slightly differently from the ones in the text of the book, and in the 1954 version have an error. I'll be using the ones from the text, though in some cases I'll reformulate them slightly.)</p>\n<p>Primitive notions; preference given a set of states</p>\n<hr>\n<p>We will use the following primitive notions. Firstly, there is a set S of \"states of the world\"; the exact current state of the world is unknown to the agent. Secondly, there is a set F of \"consequences\" - things that can happen as a result of the agent's actions.&nbsp; <em>Actions</em> or <em>acts</em> will be interpreted as functions f:S\u2192F, as two actions which have the same consequences regardless of the state of the world are indistinguishable and hence considered equal.&nbsp; While the agent may be uncertain as to the exact results of its actions, this can be folded into his uncertainty about the state of the world.&nbsp; Finally, we introduces as primitive a relation \u2264 on the set of actions, interpreted as \"is not preferred to\".&nbsp; I.e., f\u2264g means that given a choice between actions f and g, the agent will either prefer g or be indifferent.&nbsp; As usual, sets of states will be referred to as \"events\", and for the usual reasons we may want to restrict the set of admissible events to a boolean \u03c3-subalgebra of \u2118(S), though I don't know if that's really necessary here (Savage doesn't seem to do so, though he does discuss it some). [Edit nine years later: This actually introduces a slight issue I didn't realize before, but fortunately it's <a href=\"https://www.lesswrong.com/posts/5J34FAKyEmqKaT7jt/a-summary-of-savage-s-foundations-for-probability-and#jNTffBWduTaeJudYd\">easily fixed</a>.]</p>\n<p>In any case, we then have the following axiom:</p>\n<p><em>P1. The relation \u2264 is a <a href=\"http://en.wikipedia.org/wiki/Strict_weak_ordering#Total_preorders\">total preorder</a>.</em></p>\n<p>The intuition here for transitivity is pretty clear.&nbsp; For totality, if the agent is presented with a choice of two acts, it must choose one of them!&nbsp; Or be indifferent.&nbsp; Perhaps we could instead use a partial preorder (or order?), though this would give us two different indistinguishable flavors of indifference, which seems problematic.&nbsp; But this could be useful if we wanted intransitive indifference.&nbsp; So long as indifference is transitive, though, we can collapse this into a total preorder.</p>\n<p>As usual we can then define f\u2265g, f&lt;g (meaning \"it is false that g\u2264f\"), and g&gt;f. &nbsp;I will use f\u2261g to mean \"f\u2264g and g\u2264f\", i.e., the agent is indifferent between f and g. (Savage uses an equals sign with a dot over it.)</p>\n<p>Note that though \u2264 is defined in terms of how the agent chooses when presented with two options, Savage later notes that there is a construction of W. Allen Wallis that allows one to adduce the agent's preference ordering among a finite set of more than two options (modulo indifference): Simply tell the agent to rank the options given, and that afterward, two of them will be chosen uniformly at random, and it will get whichever one it ranked higher.</p>\n<p>The second axiom states that if two actions have the same consequences in some situation, just what that equal consequence is does not affect their relative ordering:</p>\n<p><em>P2. Suppose f\u2264g, and B is a set of states such f and g agree on B.&nbsp; If f' and g' are another pair of acts which, outside of B, agree with f and g respectively, and on B, agree with each other, then f'\u2264g'.</em></p>\n<p>In other words, to decide between two actions, only the cases where they actually have different consequences matter.</p>\n<p>With this axiom, we can now define:</p>\n<p><em>D1. We say \"f\u2264g given B\" to mean that if f' and g' are actions such that f' agrees with f on B, g' agrees with g on B, and f' and g' agree with each other outside of B, then f'\u2264g'.</em></p>\n<p>Due to axiom P2, this is well-defined.</p>\n<p>Here is where I would like to suggest a small modification to this setup.&nbsp; The notion of \"f\u2264g given B\" is implicitly taken to be how the agent makes decisions if it knows that B obtains.&nbsp; However it seems to me that we should actually take \"f\u2264g given B\", rather than f\u2264g, to be the primitive notion, explicitly interpeted as \"the agent does not prefer f to g if it knows that B obtains\".&nbsp; The agent always has some state of prior knowledge and this way we have explicitly specified decisions under a given state of knowledge - the acts we are concerned with - as the basis of our theory.&nbsp; Rather than defining f\u2264g given B in terms of \u2264, we can define f\u2264g to mean \"f\u2264g given S\" and then add additional axioms governing the relation between \"\u2264 given B\" for varying B, which in Savage's setup are theorems or part of the definition D1.</p>\n<p>(Specifically, I would modify P1 and P2 to talk about \"\u2264 given B\" rather than \u2264, and add the following theorems as axioms:</p>\n<p><em>P2a. If f and g agree on B, then f\u2261g given B.</em></p>\n<p><em>P2b. If B\u2286C, f\u2264g given C, and f and g agree outside B, then f\u2264g given B.</em></p>\n<p><em>P2c. If B and C are disjoint, and f\u2264g given B and given C, then f\u2264g given B\u222aC.</em></p>\n<p>This is a little unwieldy and perhaps there is an easier way - these might not be minimal. But they do seem to be sufficient.)</p>\n<p>In any case, regardless which way we do it, we've now established the notion of preference given that a set of states obtains, as well as preference without additional knowledge, so henceforth I'll freely use both as Savage does without worrying about which makes a better foundation, since they are equivalent.</p>\n<p>Ordering on preferences</p>\n<hr>\n<p>The next definition is simply to note that we can sensibly talk about f\u2264b, b\u2264f, b\u2264c where here b and c are consequences rather than actions, simply by interpreting consequences as constant functions.&nbsp; (So the agent does have a preference ordering on consequences, it's just induced from its ordering on actions.&nbsp; We do it this way since it's its choices between actions we can actually see.)</p>\n<p>However, the third axiom reifies this induced ordering somewhat, by demanding that it be invariant under gaining new information.</p>\n<p><em>P3'. If b and c are consequences and b\u2264c, then b\u2264c given any B.</em></p>\n<p>Thus the fact that the agent may change preferences given new information, just reflects its uncertainty about the results of their actions, rather than actually preferring different consequences in different states (any such preferences can be done away with by simply expanding the set of consequences).</p>\n<p>Really this is not strong enough, but to state the actual P3 we will first need a definition:</p>\n<p><em>D3. An event B is said to be null if f\u2264g given B for any actions f and g.</em></p>\n<p>Null sets will correspond to sets of probability 0, once numerical probability is introduced.&nbsp; Probability here is to be adduced from the agent's preferences, so we cannot distinguish between \"the agent is certain that B will not happen\" and \"if B obtains, the agent doesn't care what happens\".</p>\n<p>Now we can state the actual P3:</p>\n<p><em>P3. If b and c are consequences and B is not null, then b\u2264c given B if and only if b\u2264c.</em></p>\n<p>P3', by contrast, allowed some collapsing of preference on gaining new information; here we have disallowed that except in the case where the new information is enough to collapse all preferences entirely (a sort of \"end of the world\" or \"fatal error\" scenario).</p>\n<h2 id=\"Qualitative_probability\">Qualitative probability</h2>\n<p>We've introduced above the idea of \"probability 0\" (and hence implicitly probability 1; observe that \"\u00acB is null\" is equivalent to \"for any f and g, f\u2264g given B if and only if f\u2264g\"). Now we want to expand this to probability more generally. But we will not initially get numbers out of it; rather we will first just get another total preordering, A\u2264B, \"A is at most as probable as B\".</p>\n<p>How can we determine which of two events the agent thinks is more probable? Have it bet on them, of course! First, we need a nontriviality axiom so it has some things to bet on.</p>\n<p><em>P5. There exist consequences b and c such that b&gt;c.</em></p>\n<p>(I don't know what the results would be if instead we used the weaker nontriviality axiom \"there exist actions f and g such that f&lt;g\", i.e., \"S is not null\". That we eventually get that expected utility for comparing all acts suggests that this should work, but I haven't checked.)</p>\n<p>So let us now consider a class of actions which I will call \"wagers\". (Savage doesn't have any special term for these.) Define \"the wager on A for b over c\" to mean the action that, on A, returns b, and otherwise, returns c. Denote this by wA,b,c. Then we postulate:</p>\n<p><em>P4. Let b&gt;b' be a pair of consequences, and c&gt;c' another such pair. Then for any events A and B, wA,b,b'\u2264wB,b,b' if and only if wA,c,c'\u2264wB,c,c'.</em></p>\n<p>That is to say, if the agent is given the choice between betting on event A and betting on event B, and the prize and booby prize are the same regardless of which it bets on, then it shouldn't just matter just what the prize and booby prize are - it should just bet on whichever it thinks is more probable.&nbsp; Hence we can define:</p>\n<p><em>D4. For events A and B, we say \"A is at most as probable as B\", denoted A\u2264B</em>, <em>if wA,b,b'\u2264wB,b,b', where b&gt;b' is a pair of consequences.</em></p>\n<p>By P4, this is well-defined. We can then show that the relation on events \u2264 is a total preorder, so we can use the usual notation when talking about it (again, \u2261 will denote equivalence).</p>\n<p>In fact, \u2264 is not only a total preorder, but a <em>qualitative probability</em>:</p>\n<ol>\n<li>\u2264 is a total preorder</li>\n<li>\u2205\u2264A for any event A</li>\n<li>\u2205&lt;S</li>\n<li>Given events B, C, and D with D disjoint from B and C, then B\u2264C if and only if B\u222aD\u2264C\u222aD.</li>\n</ol>\n<p>(There is no condition corresponding to countable additivity; as mentioned above, we simply won't get countable additivity out of this.)&nbsp; Note also that under this, A\u2261\u2205 if and only if A is null in the earlier sense.&nbsp; Also, we can define \"A\u2264B given C\" by comparing the wagers given C; this is equivalent to the condition that A\u2229C\u2264B\u2229C.&nbsp; This relation is too a qualitative probability.</p>\n<h2 id=\"Partition_conditions_and_numerical_probability\">Partition conditions and numerical probability</h2>\n<p>In order to get real numbers to appear, we are of course going to have to make some sort of <a href=\"http://en.wikipedia.org/wiki/Archimedean_property\">Archimedean assumption</a>.&nbsp; In this section I discuss what some of these look like and then ultimately state P6, the one Savage goes with.</p>\n<p>First, definitions. We will be considering finitely-additive probability measures on the set of states, i.e. a function P from the set of events to the interval [0,1] such that P(S)=1, and for disjoint B and C, P(B\u222aC)=P(B)+P(C).&nbsp; We will say \"P agrees with \u2264\" if for every A and B, A\u2264B if and only if P(A)\u2264P(B); and we will say \"P almost agrees with \u2264\" if for every A and B, A\u2264B implies P(A)\u2264P(B).&nbsp; (I.e., in the latter case, numerical probability is allowed to collapse some distinctions between events that the agent might not actually be indifferent between.)</p>\n<p>We'll be considering here partitions of the set of states S.&nbsp; We'll say a partition of S is \"uniform\" if the parts are all equivalent.&nbsp; More generally we'll say it is \"almost uniform\" if, for any r, the union of any r parts is at most as probable as the union of any r+1 parts. (This is using \u2264, remember; we don't have numerical probabilities yet!) (Note that any uniform partition is almost uniform.) Then it turns out that the following are equivalent:</p>\n<ol>\n<li>There exist almost-uniform partitions of S into arbitrarily large numbers of parts.</li>\n<li>For any B&gt;\u2205, there exists a partition of S with each part less probable than B.</li>\n<li>There exists a (necessarily unique) finitely additive probability measure P that almost agrees with \u2264, which has the property that for any B and any 0\u2264\u03bb\u22641, there is a C\u2286B such that P(C)=\u03bbP(B).</li>\n</ol>\n<p>(Definitely not going into the proof of this here.&nbsp; However, the actual definition of the numerical probability P(A) is not so complicated: Let k(A,n) denote the largest r such that there exists an almost-uniform partition of S into n parts, for which there is some union of r parts, C, such that C\u2264A.&nbsp; Then the sequence k(A,n)/n always converges, and we can define P(A) to be its limit.)</p>\n<p>So we could use this as our 6th axiom:</p>\n<p><em>P6'''. For any B&gt;\u2205</em>_, there exists a partition of S with each part less probable than B._</p>\n<p>Savage notes that other authors have assumed the stronger</p>\n<p><em>P6''. There exist uniform partitions of S into arbitrarily large numbers of parts.</em></p>\n<p>since there's an obvious justification for this: the existence of a fair coin! If a fair coin exists, then we can generate a uniform partition of S into 2n parts simply by flipping it n times and considering the result.&nbsp; We'll actually end up assuming something even stronger than this.</p>\n<p>So P6''' does get us numerical probabilities, but they don't necessarily reflect all of the qualitative probability; P6''' is only strong enough to force almost agreement. Though it is stronger than that when \u2205 is involved - it does turn out that P(B)=0 if and only if B\u2261\u2205.&nbsp; (And hence also P(B)=1 if and only if B\u2261S.)&nbsp; But more generally it turns out that P(B)=P(C) if and only if B and C are \"almost equivalent\", which I will denote B\u2248C (Savage uses a symbol I haven't seen elsewhere), which is defined to mean that for any E&gt;\u2205 disjoint from B, B\u222aE\u2265C, and for any E&gt;\u2205 disjoint from C, C\u222aE\u2265B.</p>\n<p>(It's not obvious to me that \u2248 is in general an equivalence relation, but it certainly is in the presence of P6'''; Savage seems to use this implicitly.&nbsp; Note also that another consequence of P6''' is that for any n there exists a partition of S into n almost-equivalent parts; such a partition is necessarily almost-uniform.)</p>\n<p>However the following stronger version of P6''' gets rid of this distinction:</p>\n<p><em>P6'. For any B&gt;C, there exists a partition of S, each part D of which satisfies C\u222aD&lt;B.</em></p>\n<p>(Observe that P6''' is just P6' for C=\u2205.) Under P6', almost equivalence is equivalence, and so numerical probability agrees with qualitative probability, and we finally have what we wanted. (So by earlier, P6' implies P6'', not just P6'''.&nbsp; Indeed by above it implies the existence of uniform partitions into n parts for any n, not just arbitrarily large n.)</p>\n<p>In actuality, Savage assumes an even stronger axiom, which is needed to get utility and not just probability:</p>\n<p><em>P6. For any acts g&lt;h, and any consequence b, there is a partition of S such that if g is modified on any one part to be constantly b there, we would still have g&lt;h; and if h is modified on any one part to be constantly b there, we would also still have g&lt;h.</em></p>\n<p>Applying P6 to wagers yields the weaker P6'.</p>\n<p>We can now also get conditional probability - if P6' holds, it also holds for the preorderings \"\u2264 given C\" for non-null C, and hence we can define P(B|C) to be the probability of B under the quantitative probability we get corresponding to the qualitative probabilty \"\u2264 given C\".&nbsp; Using the uniqueness of agreeing probability measures, it's easy to check that indeed, P(B|C)=P(B\u2229C)/P(C).</p>\n<h2 id=\"Utility_for_finite_gambles\">Utility for finite gambles</h2>\n<p>Now that we have numerical probability, we can talk about finite gambles. If we have consequences b1, ..., bn, and probabilities \u03bb1, ..., \u03bbn summing to 1, we can consider the gamble \u2211\u03bbibi, represented by any action which yields b1 with probability \u03bb1, b2 with probability \u03bb2, etc.&nbsp; (And with probability 0 does anything; we don't care about events with probability 0.)&nbsp; Note that by above such an action necessarily exists.&nbsp; It can be proven that any two actions representing the same gamble are equivalent, and hence we can talk about comparing gambles.&nbsp; We can also sensibly talk about mixing gambles - taking \u2211\u03bbifi where the fi are finite gambles, and the \u03bbi are probabilities summing to 1 - in the obvious fashion.</p>\n<p>With these definitions, it turns out that Von Neumann and Morgenstern's independence condition holds, and, using axiom P6, Savage shows that the continuity (i.e. Archimedean) condition also holds, and hence there is indeed a utility function, a function U:F\u2192<strong>R</strong> such that for any two finite gambles represented by f and g respectively, f\u2264g if and only if the expected utility of the first gamble is less than or equal to that of the second.&nbsp; Furthermore, any two such utility functions are related via an increasing affine transformation.</p>\n<p>We can also take expected value knowing that a given event C obtains, since we have numerical probability; and indeed this agrees with the preference ordering on gambles given C.</p>\n<p>Expected utility in general and boundedness of utility</p>\n<hr>\n<p>Finally, Savage shows that if we assume one more axiom, P7, then we have that for any <a href=\"http://en.wikipedia.org/wiki/Essential_supremum_and_essential_infimum\">essentially bounded</a> actions f and g, we have f\u2264g if and only if the expected utility of f is at most that of g.&nbsp; (It is possible to define integration with respect to a finitely additive measure similarly to how one does with respect to a countably additive measure; the result is linear and monotonic but doesn't satisfy convergence properties.)&nbsp; Similarly with respect to a given event C.</p>\n<p>The axiom P7 is:</p>\n<p><em>P7. If f and g are acts and B is an event such that f\u2264g(s) given B for every s\u2208B, then f\u2264g given B. Similarly, if f(s)\u2264g given B for every s in B, then f\u2264g given B.</em></p>\n<p>So this is just another variant on the \"sure-thing principle\" that I earlier labeled P2c.</p>\n<p>Now in fact it turns out as mentioned above that P7, when taken together with the rest, implies that utility is bounded, and hence that we do indeed have that for any f and g, f\u2264g if and only if the expected utility of f is at most that of g!&nbsp; This is due to Peter Fishburn and postdates the first edition of <em>Foundations of Statistics</em>, so in there Savage simply notes that it would be nice if this worked for f and g not necessarily essentially bounded (so long as their expected values exist, and allowing them to be \u00b1\u221e), but that he can't prove this, and then adds a footnote giving a reference for bounded utility.&nbsp; (Though he does prove using P7 that if you have two acts f and g such that f,g\u2264b for all consequences b, then f\u2261g; similarly if f,g\u2265b for all b.&nbsp; Actually, this is a key lemma in proving that utility is bounded; Fishburn's proof works by showing that if utility were unbounded, you could construct two actions that contradict this.)</p>\n<p>Of course, if you really don't like the conclusion that utility is bounded, you could throw out axiom 7! It's pretty intuitive, but it's not clear that ignoring it could actually get you Dutch-booked.&nbsp; After all, the first 6 axioms are enough to handle finite gambles, 7 is only needed for more general situations.&nbsp; So long as your Dutch bookie is limited to finite gambles, you don't need this.</p>\n<h2 id=\"Questions_on_further_justification\">Questions on further justification</h2>\n<p>So now that I've laid all this out, here's the question I originally meant to ask: To what extent can these axioms be grounded in more basic principles, e.g. Dutch book arguments?&nbsp; It seems to me that most of these are too basic for that to apply - Dutch book arguments need more working in the background.&nbsp; Still, it seems to me axioms P2, P3, and P4 might plausibly be grounded this way, though I have not yet attempted to figure out how. P7 presumably can't, for the reasons noted in the previous section. P1 I assume is too basic.&nbsp; P5 obviously can't (if the agent doesn't care about anything, that's its own problem).</p>\n<p>P6 is an Archimedean condition. Typically I've seen those (specifically Von Neumann and Morgenstern's continuity condition) justified on this site with the idea that infinitesimals will never be relevant in any practical situation - if c has only infinitesimally more utility than b, the only case when the distinction would be relevant is if the probabilities of accomplishing them were exactly equal, which is not realistic.&nbsp; I'm guessing infinitesimal probabilities can probably be done away with in a similar manner?</p>\n<p>Or are these not good axioms in the first place?&nbsp; You all are more familiar with these sorts of things than me. Ideas?</p>\n", "sections": [{"title": "Qualitative probability", "anchor": "Qualitative_probability", "level": 1}, {"title": "Partition conditions and numerical probability", "anchor": "Partition_conditions_and_numerical_probability", "level": 1}, {"title": "Utility for finite gambles", "anchor": "Utility_for_finite_gambles", "level": 1}, {"title": "Questions on further justification", "anchor": "Questions_on_further_justification", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "91 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.2.0", "pingbacks": {"Posts": ["zJZvoiwydJ5zvzTHK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-22T20:36:24.460Z", "modifiedAt": null, "url": null, "title": "Deconfusing Probability: What is probability anyway?", "slug": "deconfusing-probability-what-is-probability-anyway", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oscar_Cunningham", "createdAt": "2009-09-18T13:28:22.764Z", "isAdmin": false, "displayName": "Oscar_Cunningham"}, "userId": "G2SZuAiaBaNPg9rBt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KTWYcKcnio9CJF3ow/deconfusing-probability-what-is-probability-anyway", "pageUrlRelative": "/posts/KTWYcKcnio9CJF3ow/deconfusing-probability-what-is-probability-anyway", "linkUrl": "https://www.lesswrong.com/posts/KTWYcKcnio9CJF3ow/deconfusing-probability-what-is-probability-anyway", "postedAtFormatted": "Sunday, May 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deconfusing%20Probability%3A%20What%20is%20probability%20anyway%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeconfusing%20Probability%3A%20What%20is%20probability%20anyway%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKTWYcKcnio9CJF3ow%2Fdeconfusing-probability-what-is-probability-anyway%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deconfusing%20Probability%3A%20What%20is%20probability%20anyway%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKTWYcKcnio9CJF3ow%2Fdeconfusing-probability-what-is-probability-anyway", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKTWYcKcnio9CJF3ow%2Fdeconfusing-probability-what-is-probability-anyway", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3, "htmlBody": "<p>Intro</p>\n<p>Kolmogorov axioms</p>\n<p>Frequencies</p>\n<p>Beliefs</p>\n<p>Next Article</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KTWYcKcnio9CJF3ow", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7539", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-22T23:22:38.839Z", "modifiedAt": null, "url": null, "title": "Less Wrong and The Ultraviolet", "slug": "less-wrong-and-the-ultraviolet", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CuSithBell", "createdAt": "2011-01-31T04:08:19.058Z", "isAdmin": false, "displayName": "CuSithBell"}, "userId": "tMhtGHqCXw7guA7tR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KieKHE9WZRK8QMWuk/less-wrong-and-the-ultraviolet", "pageUrlRelative": "/posts/KieKHE9WZRK8QMWuk/less-wrong-and-the-ultraviolet", "linkUrl": "https://www.lesswrong.com/posts/KieKHE9WZRK8QMWuk/less-wrong-and-the-ultraviolet", "postedAtFormatted": "Sunday, May 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20and%20The%20Ultraviolet&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20and%20The%20Ultraviolet%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKieKHE9WZRK8QMWuk%2Fless-wrong-and-the-ultraviolet%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20and%20The%20Ultraviolet%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKieKHE9WZRK8QMWuk%2Fless-wrong-and-the-ultraviolet", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKieKHE9WZRK8QMWuk%2Fless-wrong-and-the-ultraviolet", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": null, "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KieKHE9WZRK8QMWuk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7544", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": null, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-22T23:30:23.992Z", "modifiedAt": null, "url": null, "title": "How To Be More Confident... That You're Wrong", "slug": "how-to-be-more-confident-that-you-re-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:08.328Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HqDGZpNqQJDGKi4Pz/how-to-be-more-confident-that-you-re-wrong", "pageUrlRelative": "/posts/HqDGZpNqQJDGKi4Pz/how-to-be-more-confident-that-you-re-wrong", "linkUrl": "https://www.lesswrong.com/posts/HqDGZpNqQJDGKi4Pz/how-to-be-more-confident-that-you-re-wrong", "postedAtFormatted": "Sunday, May 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20To%20Be%20More%20Confident...%20That%20You're%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20To%20Be%20More%20Confident...%20That%20You're%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHqDGZpNqQJDGKi4Pz%2Fhow-to-be-more-confident-that-you-re-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20To%20Be%20More%20Confident...%20That%20You're%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHqDGZpNqQJDGKi4Pz%2Fhow-to-be-more-confident-that-you-re-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHqDGZpNqQJDGKi4Pz%2Fhow-to-be-more-confident-that-you-re-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>ZH-CN</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"   DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"   LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-priority:99;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin-top:0in;\n\tmso-para-margin-right:0in;\n\tmso-para-margin-bottom:10.0pt;\n\tmso-para-margin-left:0in;\n\tline-height:115%;\n\tmso-pagination:widow-orphan;\n\tfont-size:11.0pt;\n\tfont-family:\"Calibri\",\"sans-serif\";\n\tmso-ascii-font-family:Calibri;\n\tmso-ascii-theme-font:minor-latin;\n\tmso-hansi-font-family:Calibri;\n\tmso-hansi-theme-font:minor-latin;\n\tmso-bidi-font-family:\"Times New Roman\";\n\tmso-bidi-theme-font:minor-bidi;}\n</style>\n<![endif]-->\n<p class=\"MsoNormal\">One of the main Eliezer Sequences, consisting of dozens of posts, is <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">How To Actually Change Your Mind</a>. Looking at all those posts, one gets the feeling that changing one&rsquo;s mind must be Really Hard. But maybe it doesn't have to be that hard. I think it would much easier to change your mind, if you instinctively thought that your best ideas are almost certainly still far from the truth. Most of us are probably aware of the overconfidence bias, but there hasn't been much discussion on how to practically reduce overconfidence in our own ideas.</p>\n<p class=\"MsoNormal\">I offer two suggestions in that vein for your consideration.</p>\n<p class=\"MsoNormal\">1. Take the outside view. Recall famous scientists and philosophers of the past, and how far off from the truth their ideas were, and yet how confident they were in their ideas. Realize that they are famous because, in retrospect, they were more right than everyone else of their time, and there are countless books filled with even worse ideas. How likely is it that your ideas are the best of our time? How likely is it that the best ideas of our time are fully correct (as opposed to just a bit closer to the truth)?</p>\n<p class=\"MsoNormal\">2. Take a few days to learn some cryptology and then design your own cipher. Use whatever tricks you can find and make it as complicated as you want. Feel your confidence in how unbreakable it must be (at least before the Singularity occurs), and then watch it taken apart by an expert in minutes. Now feel the sense of betrayal against your &ldquo;self-confidence module&rdquo; and vow &ldquo;never again&rdquo;.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mQbxDKHxPcKKRG4mb": 1, "rWzGNdjuep56W5u2d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HqDGZpNqQJDGKi4Pz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 37, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "7545", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-22T23:59:00.212Z", "modifiedAt": null, "url": null, "title": "The Ultraviolet", "slug": "the-ultraviolet", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:06.382Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CuSithBell", "createdAt": "2011-01-31T04:08:19.058Z", "isAdmin": false, "displayName": "CuSithBell"}, "userId": "tMhtGHqCXw7guA7tR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DnWNnudWT5tAGdfBH/the-ultraviolet", "pageUrlRelative": "/posts/DnWNnudWT5tAGdfBH/the-ultraviolet", "linkUrl": "https://www.lesswrong.com/posts/DnWNnudWT5tAGdfBH/the-ultraviolet", "postedAtFormatted": "Sunday, May 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Ultraviolet&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Ultraviolet%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDnWNnudWT5tAGdfBH%2Fthe-ultraviolet%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Ultraviolet%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDnWNnudWT5tAGdfBH%2Fthe-ultraviolet", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDnWNnudWT5tAGdfBH%2Fthe-ultraviolet", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 320, "htmlBody": "<p>Suppose that you're a bee. Perhaps, even, an extremely rational bee. And yet, as you go through your life, you can't shake the feeling that you're missing something - the other bees live so effortlessly, alighting on flowers bursting with pollen as if by chance. Try as you might, you can't seem to figure out the patterns that they're unconsciously drawn to. Are you overanalyzing? Are you overwhelmed by sensory data? But the others seem to defy thermodynamics in their ability to extract useful information, all the while wasting so much effort on suboptimal patterns of thought.</p>\n<p>Perhaps they have access to <a href=\"http://www.naturfotograf.com/UV_flowers_list.html\" target=\"_blank\">different data</a>? Perhaps, where you see a uniform field of yellow, they see <strong>bullseyes</strong>.</p>\n<p>Less Wrong seems to have a problem with socializing. Not just an unusual share of the people, but the community's character (as if it were a person). We should suspect ourselves (as a collective) of overlooking the <strong>ultraviolet</strong>, those facts about the world that are so easily accessed by some others. We should be suspicious of simplistic or monolithic explanations of social reality that don't allow sweeping social success on the same scale as their claims. We should be suspicious of dismissals of social concerns.</p>\n<p>Am I off the mark? Am I worried over nothing? Am I overreaching? I am tossing this idea out into the sandstorm of doubt so that it can be worn down and honed to the razor edge at its core, if such a thing exists. I ask you to be my wind and sand.</p>\n<p>Disclaimers:&nbsp;I don't intend this as an insult. It's a reminder - as a collective intelligence, we have a blind spot. We shouldn't conclude that there's nothing behind it.&nbsp;I myself am pretty dang \"manualistic\" (or whatever the other side of neurotypical is called). I am not an apiarist.</p>\n<p><strong>Edit: </strong>I've removed the focus on Autism. I was wrong, and I apologize. The post may be further edited in the near future.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DnWNnudWT5tAGdfBH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 6, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "7546", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T00:32:08.087Z", "modifiedAt": null, "url": null, "title": "Evolution, bias and global risk", "slug": "evolution-bias-and-global-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:06.163Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vw2coEm4yYXE3oD9o/evolution-bias-and-global-risk", "pageUrlRelative": "/posts/vw2coEm4yYXE3oD9o/evolution-bias-and-global-risk", "linkUrl": "https://www.lesswrong.com/posts/vw2coEm4yYXE3oD9o/evolution-bias-and-global-risk", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evolution%2C%20bias%20and%20global%20risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvolution%2C%20bias%20and%20global%20risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvw2coEm4yYXE3oD9o%2Fevolution-bias-and-global-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evolution%2C%20bias%20and%20global%20risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvw2coEm4yYXE3oD9o%2Fevolution-bias-and-global-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvw2coEm4yYXE3oD9o%2Fevolution-bias-and-global-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1476, "htmlBody": "<p>Sometimes we make a decision in a way which is different to how we think we should make a decision. When this happens, we call it a <em>bias</em>.</p>\n<p>When put this way, the first thing that springs to mind is that different people might disagree on whether something is actually a bias. Take the <a href=\"/lw/9j/bystander_apathy/\">bystander effect</a>. If you're of the opinion that other people are way less important than yourself, then the ability to calmly stand around not doing anything while someone else is in danger would be seen as a <em>good thing</em>. You'd instead be confused by the <em>non-bystander effect</em>, whereby people (when separated from the crowd) irrationally put themselves in danger in order to help complete strangers.</p>\n<p>The second thing that springs to mind is that the bias may exist for an evolutionary reason, and not just be due to bad brain architecture. Remember that <a href=\"/lw/kr/an_alien_god/\">evolution</a> doesn't always produce the behavior that <a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">makes the most intuitive sense</a>. Creatures, including presumably humans, tend to act in a way as to maximize their reproductive success; they don't act in the way that necessarily makes the most intuitive sense.</p>\n<p>The statement that humans act in a fitness-maximizing way is controversial. Firstly, we are adapted to our <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">ancestral environment</a>, not our current one. It seems very likely that we're not well adapted to the ready availability of high-calorie food, for example. But this argument doesn't apply to everything. A lot of the biases appear to describe situations which would exist in both the ancestral and modern worlds.</p>\n<p>A second argument is that a lot of our behavior is governed by memes these days, not genes. It's certain that the memes that survive are the ones which best reproduce themselves; it's also pretty plausible that exposure to memes can tip us from one fitness-maximizing behavioral strategy to another. But memes forcing us to adopt a highly suboptimal strategy? I'm sceptical. It seems like there would be strong selection pressure against it; to pass the memes on but not let them affect our behavior significantly. Memes existed in our ancestral environments too.</p>\n<p>And remember that just because you're behaving in a way that maximizes your expected reproductive fitness, there's no reason to expect you to be consciously aware of this fact.</p>\n<p>So let's pretend, for the sake of simplicity, that we're all acting to maximize our expected reproductive success (and all the things that we know lead to it, such as status and signalling and stuff). Which of the biases might be explained away?</p>\n<p><strong>The <a href=\"/lw/9j/bystander_apathy/\">bystander effect</a></strong></p>\n<p>Eliezer points out:</p>\n<blockquote>We could be cynical and suggest that people are mostly interested in not being blamed for not helping, rather than having any positive desire to help - that they mainly wish to escape antiheroism and possible retribution.</blockquote>\n<p>He lists two problems with this hypothesis. Firstly, that the experimental setup appeared to present a selfish threat to the subjects. This I have no convincing answer to. Perhaps people really are just stupid when it comes to fires, not recognising the risk to themselves, or perhaps this is a gaping hole in my theory.</p>\n<p>The other criticism is more interesting. Telling people about the bystander effect makes it less likely to happen? Well, under this hypothesis, <em>of course it would</em>. The key to not being blamed is to formulate a plausible explanation; the explanation \"I didn't do anything because no-one else did either\" suddenly sounds a lot <em>less</em> plausible when you know about the bystander effect. (And if you know about it, the person you're explaining it to is more likely to as well. We share memes with our friends).</p>\n<p><strong>The <a href=\"/lw/lg/the_affect_heuristic/\">affect heuristic</a></strong></p>\n<p>This one seems quite complicated and subtle, and I think there may be more than one effect going on here. But one class of positive-affect bias can be essentially described as: phrasing an identical decision in more positive language makes people more likely to choose it. The example given is \"saving 150 lives\" versus \"saving 98% of 150 lives\". (OK these aren't quite identical decisions, but the difference in opinion is more than 2% and goes in the wrong direction). Apparently putting in the word 98% makes it sound more positive to most people.</p>\n<p>This also seems to make sense if we view it as trying to make a <em>justifiable</em> decision, rather than a correct one. Remember, the 150(ish) lives we're saving aren't our own; there's no selective pressure to make the correct decision, just one that won't land us in trouble.</p>\n<p>The key here is that justifying decisions is <em>hard</em>, especially when we might be faced with an opponent more skilled in rhetoric than ourselves. So we are eager for <em>additional rhetoric to be supplied</em> which will help us justify the decision we want to make. If I had to justify saving 150 lives (at some cost), it would honestly never have occurred to me to phrase it as \"98% of 153 lives\". Even if it had, I'd feel like I was being sneaky and manipulative, and I might accidentally reveal that. But to have the sneaky rhetoric supplied to me by an outside authority, that makes it a lot easier.</p>\n<p>This implies a prediction: when asked to justify their decision, people who have succumbed to positive-affect bias will repeat the postive-affective language they have been supplied, possibly verbatim. I'm sure you've met people who quote talking points verbatim from their favorite political TV show; you might assume the TV is doing their thinking for them. I would argue instead that it's doing their <a href=\"/lw/ju/rationalization/\">justification</a> for them.</p>\n<p>&nbsp;</p>\n<p><strong><a href=\"http://en.wikipedia.org/wiki/Trolley_problem\">Trolley problems</a></strong></p>\n<p>There is a class of people, who I will call <em>non-pushers</em>, who:</p>\n<ul>\n<li>would flick a switch if it would cause a train to run over (and kill) one person instead of five, yet</li>\n<li>would not push a fat man in front of that train (killing him) if it could save the five lives</li>\n</ul>\n<p>So what's going on here? Our feeling of <a href=\"/lw/sm/the_meaning_of_right/\">shouldness</a> is presumably how social pressure feels from the inside. What we consider right is (unless we've trained ourselves otherwise) likely to be what will get us into the least trouble. So why do non-pushers get into less trouble than pushers, if pushers are better at saving lives?</p>\n<p>It seems pretty obvious to me. The pushers might be more altruistic in some vague sense, but they're not the sort of person you'd want to be <em>around</em>. Stand too close to them on a bridge and they might push you off. Better to steer clear. (The people who are tied to the tracks presumably prefer pushers, but they don't get any choice in the matter). This might be what we mean by <a href=\"/lw/1nq/far_near_runaway_trolleys_the_proximity_of_fat/\">near and far</a> in this context.</p>\n<p>Another way of putting it is that if you start valuing all lives equally, and not put those closest to you first, then you might start defecting in games of reciprocal altruism. Utilitarians appear cold and unfriendly because they're less worried about you and more worried about what's going on in some distant, impoverished nation. They will start to lose the reproductive benefits of reciprocal altruism and socialising.</p>\n<p><strong>Global risk</strong></p>\n<p>In <a href=\"http://intelligence.org/upload/cognitive-biases.pdf\">Cognitive Biases Potentially Affecting Judgment of Global Risks</a>, Eliezer lists a number of biases which could be responsible for people's underestimation of global risks. There seem to be a lot of them. But I think that from an evolutionary perspective, they can all be wrapped up into one.</p>\n<p><a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">Group Selection doesn't work</a>. Evolution rewards actions which profit the individual (and its kin) <em>relative to others</em>. Something which benefits the entire group is nice and all that, but it'll increase the frequency of the competitors of your genes as much as it will your own.</p>\n<p>It would be all to easy to say that we cannot instinctively understand existential risk because our ancestors have, by definition, never experienced anything like it. But I think that's an over-simplification. Some of our ancestors probably have survived the collapse of societies, but they didn't do it by preventing the society from collapsing. They did it by individually surviving the collapse or by running away.</p>\n<p>But if a brave ancestor had saved a society from collapse, wouldn't he (or to some extent, she) become an instant hero with all the reproductive advantage that affords? That would certainly be nice, but I'm not sure the evidence backs it up. <a href=\"http://en.wikipedia.org/wiki/Stanislav_Petrov\">Stanislav Petrov</a> was given the cold shoulder. Leading <a href=\"http://en.wikipedia.org/wiki/Global_warming_controversy#Political_pressure_on_scientists\">climate scientists</a> are given a rough time, <em>especially</em> when they try and see their beliefs turned into meaningful action. Even <a href=\"http://en.wikipedia.org/wiki/Winston_churchill\">Winston Churchill</a> became unpopular <em>after</em> he helped save democratic civilization.</p>\n<p>I don't know what the evolutionary reason for hero-indifference would be, but if it's real then it pretty much puts the nail in the coffin for civilization-saving as a reproductive strategy. And that means there's no evolutionary reason to take global risks seriously, or to act on our concerns if we do.</p>\n<p>And if we make most of our decisions on instinct - on what <em>feels right</em> - then that's pretty scary.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LMFBzsJaCRADQqw3F": 2, "Rz5jb3cYHTSRmqNnN": 2, "5f5c37ee1b5cdee568cfb150": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vw2coEm4yYXE3oD9o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 7.18115322995314e-07, "legacy": true, "legacyId": "7547", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Sometimes we make a decision in a way which is different to how we think we should make a decision. When this happens, we call it a <em>bias</em>.</p>\n<p>When put this way, the first thing that springs to mind is that different people might disagree on whether something is actually a bias. Take the <a href=\"/lw/9j/bystander_apathy/\">bystander effect</a>. If you're of the opinion that other people are way less important than yourself, then the ability to calmly stand around not doing anything while someone else is in danger would be seen as a <em>good thing</em>. You'd instead be confused by the <em>non-bystander effect</em>, whereby people (when separated from the crowd) irrationally put themselves in danger in order to help complete strangers.</p>\n<p>The second thing that springs to mind is that the bias may exist for an evolutionary reason, and not just be due to bad brain architecture. Remember that <a href=\"/lw/kr/an_alien_god/\">evolution</a> doesn't always produce the behavior that <a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">makes the most intuitive sense</a>. Creatures, including presumably humans, tend to act in a way as to maximize their reproductive success; they don't act in the way that necessarily makes the most intuitive sense.</p>\n<p>The statement that humans act in a fitness-maximizing way is controversial. Firstly, we are adapted to our <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">ancestral environment</a>, not our current one. It seems very likely that we're not well adapted to the ready availability of high-calorie food, for example. But this argument doesn't apply to everything. A lot of the biases appear to describe situations which would exist in both the ancestral and modern worlds.</p>\n<p>A second argument is that a lot of our behavior is governed by memes these days, not genes. It's certain that the memes that survive are the ones which best reproduce themselves; it's also pretty plausible that exposure to memes can tip us from one fitness-maximizing behavioral strategy to another. But memes forcing us to adopt a highly suboptimal strategy? I'm sceptical. It seems like there would be strong selection pressure against it; to pass the memes on but not let them affect our behavior significantly. Memes existed in our ancestral environments too.</p>\n<p>And remember that just because you're behaving in a way that maximizes your expected reproductive fitness, there's no reason to expect you to be consciously aware of this fact.</p>\n<p>So let's pretend, for the sake of simplicity, that we're all acting to maximize our expected reproductive success (and all the things that we know lead to it, such as status and signalling and stuff). Which of the biases might be explained away?</p>\n<p><strong id=\"The_bystander_effect\">The <a href=\"/lw/9j/bystander_apathy/\">bystander effect</a></strong></p>\n<p>Eliezer points out:</p>\n<blockquote>We could be cynical and suggest that people are mostly interested in not being blamed for not helping, rather than having any positive desire to help - that they mainly wish to escape antiheroism and possible retribution.</blockquote>\n<p>He lists two problems with this hypothesis. Firstly, that the experimental setup appeared to present a selfish threat to the subjects. This I have no convincing answer to. Perhaps people really are just stupid when it comes to fires, not recognising the risk to themselves, or perhaps this is a gaping hole in my theory.</p>\n<p>The other criticism is more interesting. Telling people about the bystander effect makes it less likely to happen? Well, under this hypothesis, <em>of course it would</em>. The key to not being blamed is to formulate a plausible explanation; the explanation \"I didn't do anything because no-one else did either\" suddenly sounds a lot <em>less</em> plausible when you know about the bystander effect. (And if you know about it, the person you're explaining it to is more likely to as well. We share memes with our friends).</p>\n<p><strong id=\"The_affect_heuristic\">The <a href=\"/lw/lg/the_affect_heuristic/\">affect heuristic</a></strong></p>\n<p>This one seems quite complicated and subtle, and I think there may be more than one effect going on here. But one class of positive-affect bias can be essentially described as: phrasing an identical decision in more positive language makes people more likely to choose it. The example given is \"saving 150 lives\" versus \"saving 98% of 150 lives\". (OK these aren't quite identical decisions, but the difference in opinion is more than 2% and goes in the wrong direction). Apparently putting in the word 98% makes it sound more positive to most people.</p>\n<p>This also seems to make sense if we view it as trying to make a <em>justifiable</em> decision, rather than a correct one. Remember, the 150(ish) lives we're saving aren't our own; there's no selective pressure to make the correct decision, just one that won't land us in trouble.</p>\n<p>The key here is that justifying decisions is <em>hard</em>, especially when we might be faced with an opponent more skilled in rhetoric than ourselves. So we are eager for <em>additional rhetoric to be supplied</em> which will help us justify the decision we want to make. If I had to justify saving 150 lives (at some cost), it would honestly never have occurred to me to phrase it as \"98% of 153 lives\". Even if it had, I'd feel like I was being sneaky and manipulative, and I might accidentally reveal that. But to have the sneaky rhetoric supplied to me by an outside authority, that makes it a lot easier.</p>\n<p>This implies a prediction: when asked to justify their decision, people who have succumbed to positive-affect bias will repeat the postive-affective language they have been supplied, possibly verbatim. I'm sure you've met people who quote talking points verbatim from their favorite political TV show; you might assume the TV is doing their thinking for them. I would argue instead that it's doing their <a href=\"/lw/ju/rationalization/\">justification</a> for them.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Trolley_problems\"><a href=\"http://en.wikipedia.org/wiki/Trolley_problem\">Trolley problems</a></strong></p>\n<p>There is a class of people, who I will call <em>non-pushers</em>, who:</p>\n<ul>\n<li>would flick a switch if it would cause a train to run over (and kill) one person instead of five, yet</li>\n<li>would not push a fat man in front of that train (killing him) if it could save the five lives</li>\n</ul>\n<p>So what's going on here? Our feeling of <a href=\"/lw/sm/the_meaning_of_right/\">shouldness</a> is presumably how social pressure feels from the inside. What we consider right is (unless we've trained ourselves otherwise) likely to be what will get us into the least trouble. So why do non-pushers get into less trouble than pushers, if pushers are better at saving lives?</p>\n<p>It seems pretty obvious to me. The pushers might be more altruistic in some vague sense, but they're not the sort of person you'd want to be <em>around</em>. Stand too close to them on a bridge and they might push you off. Better to steer clear. (The people who are tied to the tracks presumably prefer pushers, but they don't get any choice in the matter). This might be what we mean by <a href=\"/lw/1nq/far_near_runaway_trolleys_the_proximity_of_fat/\">near and far</a> in this context.</p>\n<p>Another way of putting it is that if you start valuing all lives equally, and not put those closest to you first, then you might start defecting in games of reciprocal altruism. Utilitarians appear cold and unfriendly because they're less worried about you and more worried about what's going on in some distant, impoverished nation. They will start to lose the reproductive benefits of reciprocal altruism and socialising.</p>\n<p><strong id=\"Global_risk\">Global risk</strong></p>\n<p>In <a href=\"http://intelligence.org/upload/cognitive-biases.pdf\">Cognitive Biases Potentially Affecting Judgment of Global Risks</a>, Eliezer lists a number of biases which could be responsible for people's underestimation of global risks. There seem to be a lot of them. But I think that from an evolutionary perspective, they can all be wrapped up into one.</p>\n<p><a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">Group Selection doesn't work</a>. Evolution rewards actions which profit the individual (and its kin) <em>relative to others</em>. Something which benefits the entire group is nice and all that, but it'll increase the frequency of the competitors of your genes as much as it will your own.</p>\n<p>It would be all to easy to say that we cannot instinctively understand existential risk because our ancestors have, by definition, never experienced anything like it. But I think that's an over-simplification. Some of our ancestors probably have survived the collapse of societies, but they didn't do it by preventing the society from collapsing. They did it by individually surviving the collapse or by running away.</p>\n<p>But if a brave ancestor had saved a society from collapse, wouldn't he (or to some extent, she) become an instant hero with all the reproductive advantage that affords? That would certainly be nice, but I'm not sure the evidence backs it up. <a href=\"http://en.wikipedia.org/wiki/Stanislav_Petrov\">Stanislav Petrov</a> was given the cold shoulder. Leading <a href=\"http://en.wikipedia.org/wiki/Global_warming_controversy#Political_pressure_on_scientists\">climate scientists</a> are given a rough time, <em>especially</em> when they try and see their beliefs turned into meaningful action. Even <a href=\"http://en.wikipedia.org/wiki/Winston_churchill\">Winston Churchill</a> became unpopular <em>after</em> he helped save democratic civilization.</p>\n<p>I don't know what the evolutionary reason for hero-indifference would be, but if it's real then it pretty much puts the nail in the coffin for civilization-saving as a reproductive strategy. And that means there's no evolutionary reason to take global risks seriously, or to act on our concerns if we do.</p>\n<p>And if we make most of our decisions on instinct - on what <em>feels right</em> - then that's pretty scary.</p>", "sections": [{"title": "The bystander effect", "anchor": "The_bystander_effect", "level": 1}, {"title": "The affect heuristic", "anchor": "The_affect_heuristic", "level": 1}, {"title": "Trolley problems", "anchor": "Trolley_problems", "level": 1}, {"title": "Global risk", "anchor": "Global_risk", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K5nq3KcDXaGm7QQWR", "pLRogvJLPPg6Mrvg4", "QsMJQSFj7WfoTMNgW", "XPErvb8m9FapXCjhA", "Kow8xRzpfkoY7pa69", "SFZoEBpLo9frSJGkc", "fG3g3764tSubr6xvs", "YZFzsCavwZvk6Xi3m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T04:00:25.142Z", "modifiedAt": null, "url": null, "title": "Melbourne Meetup: Friday 3rd June, 7pm", "slug": "melbourne-meetup-friday-3rd-june-7pm", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gbMWi22wWkQuQar32/melbourne-meetup-friday-3rd-june-7pm", "pageUrlRelative": "/posts/gbMWi22wWkQuQar32/melbourne-meetup-friday-3rd-june-7pm", "linkUrl": "https://www.lesswrong.com/posts/gbMWi22wWkQuQar32/melbourne-meetup-friday-3rd-june-7pm", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Melbourne%20Meetup%3A%20Friday%203rd%20June%2C%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMelbourne%20Meetup%3A%20Friday%203rd%20June%2C%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgbMWi22wWkQuQar32%2Fmelbourne-meetup-friday-3rd-june-7pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Melbourne%20Meetup%3A%20Friday%203rd%20June%2C%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgbMWi22wWkQuQar32%2Fmelbourne-meetup-friday-3rd-june-7pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgbMWi22wWkQuQar32%2Fmelbourne-meetup-friday-3rd-june-7pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px;\"> </span></p>\n<div id=\"entry_t3_5gl\" class=\"content clear\">\n<div class=\"md\" style=\"font-size: small;\">\n<div style=\"margin-bottom: 1em;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><strong style=\"font-weight: bold;\">When:</strong>&nbsp;Friday 3rd June, 19:00 &nbsp;(and the first Friday of each following month)<br /><strong style=\"font-weight: bold;\">Where:</strong>&nbsp;TrikeApps office, lvl 2, 55 Walsh St, West Melbourne 3003 (<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://trikeapps.com/contact\">http://trikeapps.com/contact</a>)</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><strong style=\"font-weight: bold;\">Directions:</strong><br />Enter the somewhat unfriendly&nbsp;building, climb the stairs to the top (2 floors), and turn left.<br />No wheelchair access (sorry - if you need help there and dignity and safety are not important to you I'm sure we can help you get to the top; if they are important then please speak up - we can at least move the next one to a more accessible venue).</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><strong style=\"font-weight: bold;\">Discussion:</strong></p>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; display: inline !important;\"><a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://groups.google.com/group/melbourne-less-wrong\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n</li>\n<li>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; display: inline !important;\"><a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://www.google.com/moderator/#16/e=6a317\">http://www.google.com/moderator/#16/e=6a317<br /></a></p>\n</li>\n</ul>\n<div><span style=\"color: #6a8a6b;\"><span style=\"text-decoration: underline;\"><br /></span></span></div>\n</div>\n</div>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gbMWi22wWkQuQar32", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.181762303393643e-07, "legacy": true, "legacyId": "7561", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T06:27:26.365Z", "modifiedAt": null, "url": null, "title": "What makes Less Wrong awesome?", "slug": "what-makes-less-wrong-awesome", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:24.407Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fffNr3nhmKq7CmbCu/what-makes-less-wrong-awesome", "pageUrlRelative": "/posts/fffNr3nhmKq7CmbCu/what-makes-less-wrong-awesome", "linkUrl": "https://www.lesswrong.com/posts/fffNr3nhmKq7CmbCu/what-makes-less-wrong-awesome", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20makes%20Less%20Wrong%20awesome%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20makes%20Less%20Wrong%20awesome%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfffNr3nhmKq7CmbCu%2Fwhat-makes-less-wrong-awesome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20makes%20Less%20Wrong%20awesome%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfffNr3nhmKq7CmbCu%2Fwhat-makes-less-wrong-awesome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfffNr3nhmKq7CmbCu%2Fwhat-makes-less-wrong-awesome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<p>Recently I asked <a href=\"/r/discussion/lw/5ro/what_bothers_you_about_less_wrong/\">\"What bothers you about Less Wrong?\"</a>. It might be worth going back and checking out what people had to say, to see if there's something you can do to make Less Wrong more fun for everyone. (A few people made cool posts in response to complaints about lack of technical discussion, for instance.)</p>\n<p>Let's hear the other side. What is cool about Less Wrong? What drew you in, what makes you stay, what makes you obsessively read every comment of every post? Is they're something we're doing right that we should be doing more? Bonus points for pointing out how we can make our awesome traits even more awesome, or how to make our awesomeness more obvious to outside folk who'd appreciate it. Whatever it is, add it to the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fffNr3nhmKq7CmbCu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 15, "extendedScore": null, "score": 7.18219228244912e-07, "legacy": true, "legacyId": "7564", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NLibKMPZvyhuDJrsR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T11:54:08.597Z", "modifiedAt": null, "url": null, "title": "Any LWers in UK West Midlands?", "slug": "any-lwers-in-uk-west-midlands", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:30.231Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dWte6vN4rRDheBphx/any-lwers-in-uk-west-midlands", "pageUrlRelative": "/posts/dWte6vN4rRDheBphx/any-lwers-in-uk-west-midlands", "linkUrl": "https://www.lesswrong.com/posts/dWte6vN4rRDheBphx/any-lwers-in-uk-west-midlands", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Any%20LWers%20in%20UK%20West%20Midlands%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAny%20LWers%20in%20UK%20West%20Midlands%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWte6vN4rRDheBphx%2Fany-lwers-in-uk-west-midlands%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Any%20LWers%20in%20UK%20West%20Midlands%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWte6vN4rRDheBphx%2Fany-lwers-in-uk-west-midlands", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWte6vN4rRDheBphx%2Fany-lwers-in-uk-west-midlands", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p>After seeing LW meetups pop up in London, Edinburgh, Oxford and Cambridge, I thought I'd see if there was any interest in kicking one off somewhere I might reasonably be able to get to.</p>\n<p>I know it's not something people like to readily admit to, but I've not seen anyone else mention being in a West-Midlands-y location. &nbsp;That said, I've not seen anyone say they were from Edinburgh, Oxford or Cambridge either, and Birmingham alone has a larger population than all those cities combined, so I figure the odds can't be that bad.</p>\n<p>Anyway, I'm in Birmingham. &nbsp;Is there anyone round the Brum/Worcester/Coventry/Warwick area on here, and if so would you be interested in attending a meetup?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dWte6vN4rRDheBphx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.183147931205277e-07, "legacy": true, "legacyId": "7566", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T13:53:38.400Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Majority Is Always Wrong", "slug": "seq-rerun-the-majority-is-always-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:32.787Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w2tgubpoyrAPckNAt/seq-rerun-the-majority-is-always-wrong", "pageUrlRelative": "/posts/w2tgubpoyrAPckNAt/seq-rerun-the-majority-is-always-wrong", "linkUrl": "https://www.lesswrong.com/posts/w2tgubpoyrAPckNAt/seq-rerun-the-majority-is-always-wrong", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Majority%20Is%20Always%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Majority%20Is%20Always%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw2tgubpoyrAPckNAt%2Fseq-rerun-the-majority-is-always-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Majority%20Is%20Always%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw2tgubpoyrAPckNAt%2Fseq-rerun-the-majority-is-always-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw2tgubpoyrAPckNAt%2Fseq-rerun-the-majority-is-always-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<p>Today's post, <a href=\"/lw/hd/the_majority_is_always_wrong/\">The Majority Is Always Wrong</a> was originally published on April 3, 2007. A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>Anything worse than the majority opinion should get selected out, so the majority opinion is rarely strictly superior to existing alternatives.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/5td/seq_rerun_the_error_of_crowds/\">The Error of Crowds</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w2tgubpoyrAPckNAt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 7.183497529928364e-07, "legacy": true, "legacyId": "7567", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yxFkuyPANtL6GSwiC", "sZqMn3yaPBfisiD4o", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T14:55:11.745Z", "modifiedAt": null, "url": null, "title": "Naive Decision Theory", "slug": "naive-decision-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:06.678Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "snarles", "createdAt": "2009-06-01T03:48:38.132Z", "isAdmin": false, "displayName": "snarles"}, "userId": "YsmFaM5MdsDW8GNop", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ojkr4uP7faMNTfF5G/naive-decision-theory", "pageUrlRelative": "/posts/ojkr4uP7faMNTfF5G/naive-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/ojkr4uP7faMNTfF5G/naive-decision-theory", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Naive%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANaive%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fojkr4uP7faMNTfF5G%2Fnaive-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Naive%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fojkr4uP7faMNTfF5G%2Fnaive-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fojkr4uP7faMNTfF5G%2Fnaive-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 341, "htmlBody": "<p>I am posting this is because I'm interested in self-modifying agent decision theory but I'm too lazy to read up on existing posts.&nbsp; I want to see a concise justification as to why a sophisticated decision theory would be needed for the implementation of an AGI.&nbsp; So I'll present a 'naive' decision theory, and I want to know why it is unsatisfactory.</p>\n<p>The one condition in the naive decision theory is that the decision-maker is the only agent in the universe who is capable of self-modification.&nbsp; This will probably suffice for production of the first Artificial General Intelligence (since humans aren't actually all that good at self-modification.)</p>\n<p>Suppose that our AGI has a probability model for predicting the 'state of the universe in time T (e.g. T= 10 billion years)' conditional on what it knows, and conditional on <em>one decision</em> it has to make.&nbsp; This one decision is how should it <em>rewrite its code</em> at time zero.&nbsp; We suppose it can rewrite its code instantly, and the code is limited to X bytes.&nbsp; So the AGI has to maximize utility at time T over all programs with X bytes.&nbsp; Supposing it can simulate its utility at the 'end state of the universe' conditional on which program it chooses, why can't it just choose the program with the highest utility? Implicit in our set-up is that the program it chooses may (and very likely) will have the capacity to self-modify again, but we're assuming that our AGI's probability model accounts for when and how it is likely to self-modify.&nbsp; Difficulties with infinite recursion loops should be avoidable if our AGI backtracks from the end of time.</p>\n<p>Of course our AGI will need a probability model for predicting what a program for its behavior will do without having to simulate or even completely specify the program.&nbsp; To me, that seems like the hard part.&nbsp; If this is possible, I don't see why it's necessary to develop a specific theory for dealing with convoluted Newcomb-like problems, since the above seems to take care of those issues automatically.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ojkr4uP7faMNTfF5G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "7569", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T15:34:33.696Z", "modifiedAt": null, "url": null, "title": "How to Dissolve the Problem of Free Will and Determinism (Video)", "slug": "how-to-dissolve-the-problem-of-free-will-and-determinism", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:07.554Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beriukay", "createdAt": "2010-02-16T16:20:00.989Z", "isAdmin": false, "displayName": "beriukay"}, "userId": "4fAd4zQLh2TnrzLmC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DMErzwPXf3LsJQmy9/how-to-dissolve-the-problem-of-free-will-and-determinism", "pageUrlRelative": "/posts/DMErzwPXf3LsJQmy9/how-to-dissolve-the-problem-of-free-will-and-determinism", "linkUrl": "https://www.lesswrong.com/posts/DMErzwPXf3LsJQmy9/how-to-dissolve-the-problem-of-free-will-and-determinism", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Dissolve%20the%20Problem%20of%20Free%20Will%20and%20Determinism%20(Video)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Dissolve%20the%20Problem%20of%20Free%20Will%20and%20Determinism%20(Video)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMErzwPXf3LsJQmy9%2Fhow-to-dissolve-the-problem-of-free-will-and-determinism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Dissolve%20the%20Problem%20of%20Free%20Will%20and%20Determinism%20(Video)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMErzwPXf3LsJQmy9%2Fhow-to-dissolve-the-problem-of-free-will-and-determinism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMErzwPXf3LsJQmy9%2Fhow-to-dissolve-the-problem-of-free-will-and-determinism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<p>Here is a <a href=\"http://www.youtube.com/watch?v=la31lOcbDHc\">pretty good lecture</a> posted on YouTube about dissolving the question of Free Will. It struck me how similar his thoughts were to some of the points that have been made on Less Wrong, like how some answers may seem like explanations without having any content. It may not have much in the way of new content, but it is stated pretty clearly and concisely, and they way he flat-out rejects Free Will as unscientific is bold and refreshing, especially coming from a Philosopher.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DMErzwPXf3LsJQmy9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 1, "extendedScore": null, "score": 7.183792807988567e-07, "legacy": true, "legacyId": "7570", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T18:59:39.436Z", "modifiedAt": null, "url": null, "title": "Algorithm-dependent problems with self-modification", "slug": "algorithm-dependent-problems-with-self-modification", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JcwhtxbNtofyJhK3B/algorithm-dependent-problems-with-self-modification", "pageUrlRelative": "/posts/JcwhtxbNtofyJhK3B/algorithm-dependent-problems-with-self-modification", "linkUrl": "https://www.lesswrong.com/posts/JcwhtxbNtofyJhK3B/algorithm-dependent-problems-with-self-modification", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Algorithm-dependent%20problems%20with%20self-modification&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlgorithm-dependent%20problems%20with%20self-modification%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJcwhtxbNtofyJhK3B%2Falgorithm-dependent-problems-with-self-modification%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Algorithm-dependent%20problems%20with%20self-modification%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJcwhtxbNtofyJhK3B%2Falgorithm-dependent-problems-with-self-modification", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJcwhtxbNtofyJhK3B%2Falgorithm-dependent-problems-with-self-modification", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 424, "htmlBody": "<p>Imagine you're a self-modifying intelligent agent, and your mother is taking you out to lunch. &nbsp;Over coffee, she offers you a deal: if you will self-modify to be repulsed by eating animals to the extent that you become a vegetarian, she will pay for your lunch. &nbsp;Don't try to cheat - she can tell when you're lying. &nbsp;So you compare the total utility of changing with the total utility of not-changing, and you decide that you would rather continue to eat meat than have your lunch paid for.</p>\n<p>This is an example of an algorithm-dependent problem, a more general type than causal problems or Newcomb-like problems. &nbsp;Someone can see inside your brain to some extent and rewards you based not just on your actions or your decisions, but on your values or your patterns of thought. &nbsp;It's sort of interesting, but they seem unlikely to happen because it's difficult for us to modify our own algorithms and it's difficult for some external observer to verify that we've really done so. &nbsp;AIs, on the other hand, can fulfill both of these requirements, and so might run into them on rare occasions.</p>\n<p>\n<ul>\n<li>More difficult modifications:</li>\n</ul>\n</p>\n<p>The vegetarian/omnivore tradeoff seems fairly comprehensible because of its nice properties. &nbsp;It's fairly time-symmetrical since being a vegetarian is pretty much the same day to day. &nbsp;It's familiar to us so that we don't have to guess too much about what it's like being a vegetarian. &nbsp;And since we don't feel like wanting to eat animals is an inherently valuable belief, we can just evaluate the utility of the consequences.</p>\n<p>What sort of modifications would be trickier? &nbsp;Well, going back to that last point, do we have inherently valuable beliefs? &nbsp;I'd argue that I do - I would not want to want to kill people even if God promised to keep an eye on me and stop me before I made any outward sign of trying. &nbsp;But it's fairly simple to extend our utility function over our own brains.</p>\n<p>Worse is when you have multiple modifications. &nbsp;If you modified to be a vegetarian, and then modified to enjoy skydiving, it wouldn't be a big deal - those are mostly independent. &nbsp;But what if your mom wanted you to modify something that impacted your self-modification system?</p>\n<p>\n<ul>\n<li>So:</li>\n</ul>\nI think it's not too hard to take an algorithm-determined problem, even with self-modification, and find the optimal classes of algorithms. &nbsp;This is analogous to the decisions made by a decision-theory-following agent. &nbsp;What would be nice is to have something analogous to a decision theory -&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JcwhtxbNtofyJhK3B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7572", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T19:25:58.934Z", "modifiedAt": null, "url": null, "title": "Las Vegas LW Meetup!", "slug": "las-vegas-lw-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Duke", "createdAt": "2010-07-08T00:48:30.542Z", "isAdmin": false, "displayName": "Duke"}, "userId": "67L9CtYdpqT79exBW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qni6Lwm78cjhju6TL/las-vegas-lw-meetup", "pageUrlRelative": "/posts/qni6Lwm78cjhju6TL/las-vegas-lw-meetup", "linkUrl": "https://www.lesswrong.com/posts/qni6Lwm78cjhju6TL/las-vegas-lw-meetup", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Las%20Vegas%20LW%20Meetup!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALas%20Vegas%20LW%20Meetup!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqni6Lwm78cjhju6TL%2Flas-vegas-lw-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Las%20Vegas%20LW%20Meetup!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqni6Lwm78cjhju6TL%2Flas-vegas-lw-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqni6Lwm78cjhju6TL%2Flas-vegas-lw-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<p><span style=\"font-family: verdana,arial,sans-serif;\"> </span></p>\n<p><strong>What</strong>: First Las Vegas/Henderson Less Wrong meetup</p>\n<p><strong>When</strong>: Saturday, May 28, 2011 7:00 PM<strong></strong></p>\n<p><strong>Where</strong>: Putters Bar &amp; Grill<br /> 6945 S. Rainbow Blvd<br /> Las Vegas, NV</p>\n<p>Look for a bitcoin sign and ask for Duke.</p>\n<p>For personal convenience I am piggybacking on this <a href=\"http://www.meetup.com/Las-Vegas-Bitcoin-Users/events/19392231/\">Bitcoin meetup</a>. A bitcoiner will be giving an informal presentation on cryptology and computer security. I don't have specific ideas for the LW aspect of the meetup so come prepared with your own.&nbsp;</p>\n<p>&nbsp;</p>\n<p>**Edit: Location changed**</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qni6Lwm78cjhju6TL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.18446998535605e-07, "legacy": true, "legacyId": "7573", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T22:44:53.138Z", "modifiedAt": null, "url": null, "title": "Analogies and learning", "slug": "analogies-and-learning", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:06.676Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lessdazed", "createdAt": "2011-02-02T05:06:52.010Z", "isAdmin": false, "displayName": "lessdazed"}, "userId": "ehZzKt5ByYBeyCLkz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gi9dwm4ps4WaTLv4m/analogies-and-learning", "pageUrlRelative": "/posts/Gi9dwm4ps4WaTLv4m/analogies-and-learning", "linkUrl": "https://www.lesswrong.com/posts/Gi9dwm4ps4WaTLv4m/analogies-and-learning", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Analogies%20and%20learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnalogies%20and%20learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGi9dwm4ps4WaTLv4m%2Fanalogies-and-learning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Analogies%20and%20learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGi9dwm4ps4WaTLv4m%2Fanalogies-and-learning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGi9dwm4ps4WaTLv4m%2Fanalogies-and-learning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<p>I told someone that I learn best by first hearing a general principle and only afterward being given examples and analogies. She replied that my explanations are hard to follow when my analogies are not from subjects already familiar to and well understood by her. She went further and said that sometimes she understood novel things I was trying to explain, only to be confounded by my subsequent analogies. I immediately replied that in my opinion, analogies to familiar topics are of course much better teaching tools than those to unfamiliar ones, but obscure analogies primarily function as tests to ensure understanding rather than tools to convey it. Someone fully understanding a concept ought to be able to use that understanding as a guide to understand analogous unfamiliar topics.</p>\n<p>I am very interested in what others have to say about my last point in particular and would appreciate comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gi9dwm4ps4WaTLv4m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 7.185052101786212e-07, "legacy": true, "legacyId": "7575", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T23:28:58.595Z", "modifiedAt": null, "url": null, "title": "Ace Attorney: pioneer Rationalism-didactic game?", "slug": "ace-attorney-pioneer-rationalism-didactic-game", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.412Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raw_Power", "createdAt": "2010-09-10T23:59:43.621Z", "isAdmin": false, "displayName": "Raw_Power"}, "userId": "kwSqcED9qTanFyNWG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jYTA6c6qNM4Fkqf48/ace-attorney-pioneer-rationalism-didactic-game", "pageUrlRelative": "/posts/jYTA6c6qNM4Fkqf48/ace-attorney-pioneer-rationalism-didactic-game", "linkUrl": "https://www.lesswrong.com/posts/jYTA6c6qNM4Fkqf48/ace-attorney-pioneer-rationalism-didactic-game", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ace%20Attorney%3A%20pioneer%20Rationalism-didactic%20game%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAce%20Attorney%3A%20pioneer%20Rationalism-didactic%20game%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjYTA6c6qNM4Fkqf48%2Face-attorney-pioneer-rationalism-didactic-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ace%20Attorney%3A%20pioneer%20Rationalism-didactic%20game%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjYTA6c6qNM4Fkqf48%2Face-attorney-pioneer-rationalism-didactic-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjYTA6c6qNM4Fkqf48%2Face-attorney-pioneer-rationalism-didactic-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1535, "htmlBody": "<p>This article aims to prove that Ace Attorney is <a href=\"/lw/55z/a_gameplay_exploration_of_yudkowskys_twelve/\">possibly</a> the first rationalist game in the lesswrongian sense, or at least a remarkable proto-example, and that it subliminally works to <a href=\"/lw/1e/raising_the_sanity_waterline/\">raise the sanity waterline</a> in the general population, and might provide a template on which to base future works that aim to achieve a similar effect.</p>\n<p>The Ace Attorney series of games for the Nintendo DS console puts you in the shoes of Phoenix Wright, an attorney who, in the vein of Perry Mason, takes on difficult cases to defend his clients from a judicial system that is heavily inspired by that of Japan, in which the odds are so stacked against the defense it's practically a Kangaroo Court where your clients are <em>guilty until proven innocent.</em></p>\n<p>For those unfamiliar with the game, and those who want to explore the \"social criticism\" aspect of the game, <a href=\"http://www.escapistmagazine.com/articles/view/issues/issue_253/7530-Phoenix-Wrights-Objection\">I wholeheartedly recommend this most excellent article from The Escapist</a>. Now that that's out of the way, we can move on to what makes this relevant for Less Wrong. What makes this game uniquely interesting from a Rationalist POV is that the entire game mechanics are based on</p>\n<ul>\n<li>gathering <strong>material evidence</strong> </li>\n</ul>\n<ul>\n<li>finding the <strong>factual contradictions</strong> in the witnesses' testimonies</li>\n</ul>\n<ul>\n<li>using the evidence to <strong>bust the lies open</strong> and <strong>force</strong> the truth out<a id=\"more\"></a></li>\n</ul>\n<p>That the judicial system is Japanese-inspired also means the legal system is <em>inquisitorial</em>: the court has an active role in the case (whereas the <em>adversarial</em> system in the West reduces the role of the court to a form of referee) and its (alleged) mission is to <em>dig out the truth</em>. That and the lack of <em><a href=\"http://en.wikipedia.org/wiki/In_dubio_pro_reo\">in dubio pro reo</a></em> mean you can't just be content with putting your client's guilt in <em>reasonable doubt</em>, you have to thoroughly prove their innocence <em>and</em> find the <em>true</em> culprit and get <em>them</em> imprisoned. That means you have to find out the entire story and you can't leave any threads hanging.</p>\n<p>Additionally, the fact that you are a lame attorney facing an unsympathetic judge and egomaniacal, dirty-playing, high-status prosecutors who *have led the police investigation and only prosecute when they think they have all the cards in their hand* means you will. not. catch. a break. Every single move you make will be scrutinized, you will face constant sarcasm, dismissal, condescending and ridicule, and sometimes a single mistake on your part (presenting the wrong piece of evidence) can cost you the entire case. This game forces you to take an unflinching stand for the truth in the face of every social sanction imaginable (including, obviously, attempts at your own life). Of course, the plot goes out of its way to make things difficult for you: everyone is as unhelpful as possible, and even your <em>clients</em> need to have the truth pried out of their mouths with the determination of a dentist. Other witnesses can cast remarkably subtle webs of lies that really force you to think out of the box in order to find their weak point. And, since the cases are Always Murder, your client's life is <em>always</em> on the line, and that's when you don't have <em>another</em> person in grave distress. This serves to motivate you and draw you into the story, but it also adds to the constant pressure you are in to <em>find the truth.</em></p>\n<p>But that's not all. In the latest sequel, Ace Attorney Investigation, you take the role of Miles Edgeworth, a prosecutor who Defected From Decadence and restricts himself to ethical methods in crime-solving, eschewing the questionable methods he used in the past, and which most of his colleagues still practice with abandon. The battle doesn't take place in court (which, unless Phoenix or his successor Apollo are defending, is but a formality) but during investigation, which is where the case is won for a prosecutor (if they aren't certain they have enough evidence to get a conviction, prosecutors just don't... er... prosecute).&nbsp; This means you have to investigate the crime scenes, interrogate the suspects, and <em>find the connections between the clues in order to reconstruct what happened</em>. This is represented in the game by an entire gameplay mechanic for <em>logical deductions</em> (and a fair bit of Will Mass Guessing) that are <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/ptitle8px80d2wm3pd\">hilariously over-the-top</a>, concluding with a literal \"<a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/EurekaMoment\">Eureka!</a>\". The interrogations are no piece of cake either: oftentimes, (and, surprisingly, realistically enough in a police investigation) you have to take your suspects through excruciating logical baby steps to break their lies, since they can rely on something as cheap as semantics. Actual Eureka Moments, that is, sudden piecing of mental puzzles in a moment where deductive thinking is stalled, thanks to someone saying something unrelated that just happens to trigger the right association, is also a common phenomenon during investigation: <a href=\"/lw/qj/einsteins_speed/\">composing a good hypothesis with nowhere near enough evidence is, of course, another rationalist skill, one that is underrated by modern Science as it is now</a>.<sup>2</sup></p>\n<p>So, to sum it up, what virtues does these games teach?</p>\n<ol>\n<li>Uncompromising <strong>curiosity</strong>. The truth must come out at all costs, or your client *dies*.</li>\n<li>The ability to quickly <strong>relinquish</strong> false leads and weak plans: getting attached to them will only harm you, in very immediate and very dire ways. </li>\n<li><strong>Lightness</strong> in the face of evidence: before the truth, <em>resistance is futile</em>. The witnesses themselves often lie, and often the lies are directed to themselves: the investigative process forces them to give the lies up, sometimes <em>traumatically</em>: in the case of the inocnet, it's almost always for their own good. In the case of the guilty, they are only delaying the inevitable. </li>\n<li><strong>Evenness</strong>: The lack of it in the opposition is portrayed as repulsive and reprehensible. Motivated Continuation and Motivated Stopping are egregiously featured and are the main difficulty you have to surpass in your battle against the Judge and the Prosecutor.</li>\n<li><strong>Argument</strong>: Those that refuse to plead are either guilty, and will be inescapably defeated by evidence, or innocent and are cutting themselves off from our help. Or just being uncooperative, callous witnesses, but they too will always find it eventually in their interest to talk. There's even an entire game mechanic built around this specific silence-breaking interrogation.</li>\n<li><strong>Empiricism</strong>: Sometimes your opponents will try to derail the discussion with semantics, ad-hominem, and similar fallacies, courtroom antics, and Chewbacca Prosecutions. It's your job to keep your feet on the ground and use your only weapon: <strong>hard fact</strong>. When <em>you</em> try a Chewbacca Defense, expect it to be in dire danger of breaking down at any moment, and only a way of stalling the trial until you can come up with something better. Failure to come up with something better once the judge loses patience will automatically lose you the case.</li>\n<li><strong>Simplicity</strong>: The best lies, those that are hardest to break, are those that rely on the least elements to function. The more lies a witness piles upon each other, the easier it is to expose them. On the other hand, disproving a lie doesn't require complicated dissertations, but often the presentation of <em>one</em> piece of evidence.</li>\n<li><strong>Humility</strong>: You are constantly made aware of your own fallibility. The game will penalize you for every mistake you make, and rub it in your face in humiliating and even tragic manners. Overconfidence and inaction before one's failings is not an option when lives are on the line.</li>\n<li><strong>Perfectionism</strong>: I could just quote that paragraph word for word, but I'll simply say this: the game teaches you to silence yourself and pay very close attention to what is being said. Anything short of perfect understanding of the testimonies and perfect thoroughness in investigating them can cost someone their life.</li>\n<li><strong>Precision</strong>: When you have to present a piece of evidence to highlight a contradiction, you must present the piece of evidence, in the most precise and direct manner. Fumbling about will only exhaust the judge's patience, and make your client that much closer to condemnation.</li>\n<li><strong>Scholarship</strong>: A very specialized version of this: talking to <em>everyone, </em>and asking<em> all of the questions you are allowed</em>, is extremely advisable: usually only a complete understanding of all the elements surrounding the case will allow you to find the right defense, and save your client.</li>\n<li><strong>Void</strong>: The game won't reward you for following a procedure. The game will reward you for saving your client, by any means necessary (including kleptomania). Admittedly, the fact that this is a videogame with very restrictive game mechanics kinda gets in the way of this message, but you still come out with the lesson that what matters is <em>getting the job done</em>.<sup>1</sup></li>\n</ol>\n<p><a href=\"http://doulifee.com//Storage/aceatt/EdgeworthHeroes/1-miles-bowb.gif\">I rest my case.</a></p>\n<p>&nbsp;</p>\n<hr />\n<p>1. Sorry, <a href=\"http://www.youtube.com/user/StarKidPotter#p/c/C6A915952D67F112\">couldn't resist the reference: I'm just that geeky.</a> <a href=\"http://www.youtube.com/watch?v=970yJGLpot0\">Sue</a> <a href=\"http://www.youtube.com/user/StarKidPotter#p/c/C6A915952D67F112\">me.</a></p>\n<p>2. That, and, honestly, who could resist a game that names one of it's tracks <a href=\"http://www.youtube.com/watch?v=H-a60ITJ2Ko\">\"Logic, The Way To The Truth\"</a> and, when winning a case, <a href=\"http://www.youtube.com/watch?v=fx1QItiBqEs&amp;feature=channel_video_title\">\"Solution! Splendid deduction.\"</a>&nbsp; <a href=\"http://www.youtube.com/watch?v=xG0e3Q8RpiY&amp;feature=related\">\"Cornered\"</a>, which plays when you are punching a hole in a witness's declaration that is so huge it could swallow galaxies, leaving them no room whatsoever to continue with their lies and often leading to <a href=\"http://www.youtube.com/watch?v=7x38Jxeyp-k&amp;feature=related\">spectacular villainous breakdowns</a>(MASSIVE SPOILER ALERT), remains an all-time <a href=\"http://www.youtube.com/watch?v=UYMDQRVuXtA&amp;feature=related\">classic.</a> (One clip is even peppered with quite interesting <a href=\"http://www.youtube.com/watch?v=t1qSe2GnHto&amp;feature=related\">quotes on Truth</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jYTA6c6qNM4Fkqf48", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 27, "extendedScore": null, "score": 7.185178232813848e-07, "legacy": true, "legacyId": "7576", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cFuc3v2iXfb7dJrDm", "XqmjdBKa4ZaXJtNmf", "mpaqTWGiLT7GA3w76"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-23T23:55:22.132Z", "modifiedAt": null, "url": null, "title": "Minneapolis Meetup: Saturday May 28, 3:00PM", "slug": "minneapolis-meetup-saturday-may-28-3-00pm", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JustinShovelain", "createdAt": "2009-06-10T00:56:47.112Z", "isAdmin": false, "displayName": "JustinShovelain"}, "userId": "LEeresErqn3BpWrwG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2QZoYyHfRoseBsN4c/minneapolis-meetup-saturday-may-28-3-00pm", "pageUrlRelative": "/posts/2QZoYyHfRoseBsN4c/minneapolis-meetup-saturday-may-28-3-00pm", "linkUrl": "https://www.lesswrong.com/posts/2QZoYyHfRoseBsN4c/minneapolis-meetup-saturday-may-28-3-00pm", "postedAtFormatted": "Monday, May 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Minneapolis%20Meetup%3A%20Saturday%20May%2028%2C%203%3A00PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMinneapolis%20Meetup%3A%20Saturday%20May%2028%2C%203%3A00PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2QZoYyHfRoseBsN4c%2Fminneapolis-meetup-saturday-may-28-3-00pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Minneapolis%20Meetup%3A%20Saturday%20May%2028%2C%203%3A00PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2QZoYyHfRoseBsN4c%2Fminneapolis-meetup-saturday-may-28-3-00pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2QZoYyHfRoseBsN4c%2Fminneapolis-meetup-saturday-may-28-3-00pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<address><a id=\"more\"></a><br /></address><address>Saturday May 28th, 3PM</address><address>Starbucks</address><address>Coffman Memorial Union</address><address>300 Washington Avenue SE,</address><address>University of Minnesota, Minneapolis, MN&nbsp;55455</address>\n<p>&nbsp;</p>\n<h3>Goals:</h3>\n<p><span style=\"font-size: 15px; font-weight: bold;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-weight: normal; font-size: small; line-height: 19px;\">To meet and network with likeminded Minnesotans.</span></span></p>\n<p><span style=\"font-size: 15px; font-weight: bold;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-weight: normal; font-size: small; line-height: 19px;\">I'll  do my best to answer any questions people have about SIAI, rationality,  x-risk strategy, and intelligence amplification (I spent about 2 years  as a fellow).</span></span></p>\n<p>To find ways we can help reduce existential risks. I'll design a list of projects, roles, and organizations.</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><br /></span></p>\n<p><span style=\"font-size: 15px; font-weight: bold;\">Directions:</span></p>\n<p><span style=\"font-family: arial, sans-serif;\"><span style=\"line-height: 15px;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><span style=\"line-height: normal;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"> </span></span></span></span></span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">The  Minneapolis meetup&nbsp;will take place this Saturday the 28th at 3:00 PM in  Coffman Memorial Union at the University of Minnesota. We'll be meeting  inside at Starbucks,&nbsp;next to the northwest entrance to the bookstore,  and&nbsp;underneath the northwest entrance to the Union. That entrance is a  separate glass-sided structure right next to Washington Avenue, not the  multistory brick building, but you can get to Starbucks from either  entrance.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">RSVP and I'll keep an eye out for you and add you to the Google group.&nbsp;You can contact me at jshovelainsiai@gmail.com.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2QZoYyHfRoseBsN4c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 7.18525840007853e-07, "legacy": true, "legacyId": "7577", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-24T02:25:04.801Z", "modifiedAt": null, "url": null, "title": "[REVIEW] Foundations of Neuroeconomic Analysis", "slug": "review-foundations-of-neuroeconomic-analysis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:03.714Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NebwNmKv3t7G3PHY8/review-foundations-of-neuroeconomic-analysis", "pageUrlRelative": "/posts/NebwNmKv3t7G3PHY8/review-foundations-of-neuroeconomic-analysis", "linkUrl": "https://www.lesswrong.com/posts/NebwNmKv3t7G3PHY8/review-foundations-of-neuroeconomic-analysis", "postedAtFormatted": "Tuesday, May 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BREVIEW%5D%20Foundations%20of%20Neuroeconomic%20Analysis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BREVIEW%5D%20Foundations%20of%20Neuroeconomic%20Analysis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNebwNmKv3t7G3PHY8%2Freview-foundations-of-neuroeconomic-analysis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BREVIEW%5D%20Foundations%20of%20Neuroeconomic%20Analysis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNebwNmKv3t7G3PHY8%2Freview-foundations-of-neuroeconomic-analysis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNebwNmKv3t7G3PHY8%2Freview-foundations-of-neuroeconomic-analysis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1486, "htmlBody": "<p>Neuroeconomics is the application of advances in neuroscience to the fundamentals of economics: choice and valuation. <em><a href=\"http://www.amazon.com/dp/0199744254/?tag=vglnk-c319-20\">Foundations of Neuroeconomic Analyis</a></em> by Paul Glimcher, an active researcher in this area, presents a summary of this relatively new field to psychologists and economists. Although written as a serious work, the presentation is made across disciplines, so it should be accessible to anyone interested without much background knowledge in either area. Although the writing is so-so, the book covers multiple Less Wrong-relevant themes, from reductionism to neuroscience to decision theory. If nothing else, the results discussed provide a wonderful example of how <a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">no one knows what science doesn't know</a>. I doubt many economists are aware researchers can point to something very similar to utility on a brain scanner and would scoff at the very notion.</p>\n<p>Because of the book's wide target audience, there is not enough detail for specialists, but possibly a little too much for non-specialists. If you are interested in this topic, the best reason to pick up the book would be to track down further references. I hope the following summary does the book justice for everyone else.</p>\n<p>Are book summaries of this sort useful? The recent review/summary of <a href=\"/r/discussion/lw/5lb/book_review_predictably_irrational_by_dan_ariely/\"><em>Predictably Irrational</em></a> appears to have gone over well. Any suggestions to improve possible future reviews?</p>\n<hr />\n<h3>Introduction</h3>\n<p>Many economists think economics is fundamentally separate from psychology and neuroscience; since they take choices as primitives, little if any knowledge would be gained from understanding the mechanisms underlying choice. However, science steadily brings reduction and linkages between previously unrelated disciplines. A striking amount has already been discovered about the exact processes in the brain governing choice and valuation. On the other side, neuroscientists and psychologist underestimate the ability of economists to say whether claims about the brain are logically coherent or not.</p>\n<h3 id=\"section-i-the-challenge-of-neuroeconomics\">Section I: The Challenge of Neuroeconomics</h3>\n<p>Consider a man and woman who have an affair with each other at a professional conference, which they later consider a mistake. An economist looking at this situation would treat their choice to sleep together as revealing a preference, regardless of their verbal claims. A psychologist would consider how mental states mediated this decision, and would be more willing to consider whether the decision was a mistake or not. Biologists would be more likely to point to ancestral benefits of extra-pair copulations, not considering the reflective judgements as directly relevant. These explanations largely speak past each other, hinting that a unified theory could do much better in predicting behavior.</p>\n<p>The key to this is establishing linkages between the logical primitives of each discipline. Behavior could be explained on the level of physics, biology, psychology, or economics, but whether low-level explanations are practical is a different matter. Realistically, linking disciplines will strengthen both fields by mutually constraining the theories available to them.</p>\n<p>With the neoclassical revolution, economics developed concepts of utility as reflecting ordinal relationships over revealed preferences. Choices that satisfied certain <a href=\"http://en.wikipedia.org/wiki/Revealed_preference#The_strong_axiom_of_revealed_preference\">consistency conditions</a> could be treated as if generated by a utility function. <a href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis#The_von_Neumann-Morgenstern_axioms\">Additional axioms</a> allowed consistent choice under uncertainty to be added to the theory. There are notable problems with this approach, but the core ideas of utility and maximization have surprisingly close neural analogues. Rather than operating \"as if\" individuals act on the basis of utility, a hard theory of \"because\" is being developed.</p>\n<p>A look at visual perception reveals our subjective experience of light intensity varies subtantially depending on the wavelength of the light. Brightness is concept that resides in the mind, and furthermore sensitivity to different wavelengths corresponds precisely to the absorption spectra of the chemical <em>rhodopsin</em> in our retinas. All perceptions are represented in the mind along a <a href=\"http://en.wikipedia.org/wiki/Stevens'_power_law\">power scale</a> with some variance. Because the distributions of perceptions overlap, subjects can report accurately that a dimmer light is perceptually brighter. This suggests random utility models developed for statisical purposes might be directly explain what happens in the brain. One interesting consequence about the power scaling law is that <a href=\"http://en.wikipedia.org/wiki/Risk_aversion\">risk aversion</a> would be embedded at the level of perception.</p>\n<h3 id=\"section-ii-the-choice-mechanism\">Section II: The Choice Mechanism</h3>\n<p>Due to its relative simplicity, eye movement serves as a model for motor control and perhaps decisions broadly. The <em>superior colliculus</em> represents possible eye movements topographically with \"hills\" of activity. Eventually, the tissue transitions to a bursting state where the most active hill becomes much more active and the rest are inhibited via a \"winner-take-all\" or \"argmax\" mechanism. All inputs have to eye motion have to pass through the <em>superior colliculus</em>, so this represents a common final pathway of processed sensory signals. By giving monkeys varying awards for eye-motion tasks, activity in the <em>lateral intraparietal area</em> (LIP) correlates strongly with the probability and size of reward in an area known to trigger action before the action is taken. In other words, this appears to be a direct neural representation of subjective expected valuation. If monkey subjects play a game with mixed strategies in equilibrium, neuron firing rates are all roughly equal, matching the conclusion that expected utilities of actions are equalized when an opponent is mixing.</p>\n<p>Cortial neurons fire almost like independent Poisson processes, resulting in neurons down the line being able to easily extract the mean firing rate of the inputs. Interneuronal correlation can vary according to the task at hand, resulting in greater or lesser variation of the final decision, so descriptive decision theories must incorporate randomness in choice. This also provides support for mixed strategies being represented directly in the brain.</p>\n<p>Subjective valuations are normalized, and are only considered relative to the other options at hand. This normalization maximizes the joint information of neurons, increasing the efficiency of value representation. One consequence is that as the choice set increases, valuations start overlapping, and choice becomes essentially random. Activity also varies according to the delay of rewards, matching previous findings of hyperbolic discounting. While these findings are largely based on eye-movements in monkeys, this provides a clear path of how choice can be reduced to neural mechanisms.</p>\n<h3 id=\"section-iii-valuation\">Section III: Valuation</h3>\n<p>Back to visual perception, our judgements are made relative to other elements in the environment. Color looks roughly the same indoors and outdoors, even though there can be six orders of magnitude more illumination outside. Drifting reference points make absolute values unrecoverable. Local irrationalies due to reliance on a reference point arise because evolution is trading off between accurate sensory encoding and the costs of these irrationalities.</p>\n<p>One promising way to specify the reference point is as the discounted sum of our future wealth. Learning depends on the difference between actual and expected rewards, so valuation compared to a reference point arises from the learning process. In the brain, reward prediction errors are encoded through dopamine. Dopamine firing rates are well-described by an exponentially weighted sum of previous awards subtracted from the most recent award. Hebb's law, which says \"cells that fire together, wire together\", describes how long-term predictions work.</p>\n<p>Valuation appears to be orginally constructed in the striatum and medial prefrontal cortex. The reference level encoded there can be directly observed with brain scanners. Various other regions provide inputs to construct value. For instance, the orbitofrontal cortex (OFC) provides an assessment of risk. Subjects with lesions in this area exhibit almost perfect risk neutrality. Values might also be stored in the OFC, again in a compressed and encoded way. Longer-term valuations might be stored in the amygdala.</p>\n<p>Because valuations are encoded relatively and don't work well over large choice sets, humans might edit out options by sequentially considering particular attributes until the choice set become manageable. Sorting by attributes can lead to irrational choices, unsurprisingly.</p>\n<p>Probabilistic valuations depend on whether the expectation was learned experientially or symbolically. Symbolically communicated probabilities, where the person is told a number, are overweighed near zero and underweighted near one. Experientially communicated probabilities, where the person samples the lotteries directly, exhibit the opposite pattern. This suggests at least two mechanisms at work, especially with the ability to deal with symbolic probabilities arising relatively late in our evolutionary history. Also, while experiential expected values incorporate probabilities implicitly, this information can't be extracted. When probabilities change, the only means to change valuations is to relearn them from scratch.</p>\n<h3 id=\"section-iv-summary-and-conclusions\">Section IV: Summary and Conclusions</h3>\n<p>Here the author presents formalized models of the descriptive theory. The normative uses of this theory are still unclear. Even if we can identify subjective valuations in the brain, does this have any relation to welfare?</p>\n<p>The four critical observations of neuroeconomics are reference-dependence, the lack of an absolute measure of anything in the brain, stochasticity in choice, and the influence of learning on choice. Along with the question of the welfare implications of these findings, six primary questions are currently unanswered:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Where is subjective value stored and how does it get to choice?</li>\n<li>What part of the brain governs when it is \"time to choose\"?</li>\n<li>What neural mechanism guides complementarity between goods?</li>\n<li>How does symbolic probability work?</li>\n<li>How does the state of the world and utility interact?</li>\n<li>How does the brain represent money?</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NebwNmKv3t7G3PHY8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 31, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "7585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Neuroeconomics is the application of advances in neuroscience to the fundamentals of economics: choice and valuation. <em><a href=\"http://www.amazon.com/dp/0199744254/?tag=vglnk-c319-20\">Foundations of Neuroeconomic Analyis</a></em> by Paul Glimcher, an active researcher in this area, presents a summary of this relatively new field to psychologists and economists. Although written as a serious work, the presentation is made across disciplines, so it should be accessible to anyone interested without much background knowledge in either area. Although the writing is so-so, the book covers multiple Less Wrong-relevant themes, from reductionism to neuroscience to decision theory. If nothing else, the results discussed provide a wonderful example of how <a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">no one knows what science doesn't know</a>. I doubt many economists are aware researchers can point to something very similar to utility on a brain scanner and would scoff at the very notion.</p>\n<p>Because of the book's wide target audience, there is not enough detail for specialists, but possibly a little too much for non-specialists. If you are interested in this topic, the best reason to pick up the book would be to track down further references. I hope the following summary does the book justice for everyone else.</p>\n<p>Are book summaries of this sort useful? The recent review/summary of <a href=\"/r/discussion/lw/5lb/book_review_predictably_irrational_by_dan_ariely/\"><em>Predictably Irrational</em></a> appears to have gone over well. Any suggestions to improve possible future reviews?</p>\n<hr>\n<h3 id=\"Introduction\">Introduction</h3>\n<p>Many economists think economics is fundamentally separate from psychology and neuroscience; since they take choices as primitives, little if any knowledge would be gained from understanding the mechanisms underlying choice. However, science steadily brings reduction and linkages between previously unrelated disciplines. A striking amount has already been discovered about the exact processes in the brain governing choice and valuation. On the other side, neuroscientists and psychologist underestimate the ability of economists to say whether claims about the brain are logically coherent or not.</p>\n<h3 id=\"Section_I__The_Challenge_of_Neuroeconomics\">Section I: The Challenge of Neuroeconomics</h3>\n<p>Consider a man and woman who have an affair with each other at a professional conference, which they later consider a mistake. An economist looking at this situation would treat their choice to sleep together as revealing a preference, regardless of their verbal claims. A psychologist would consider how mental states mediated this decision, and would be more willing to consider whether the decision was a mistake or not. Biologists would be more likely to point to ancestral benefits of extra-pair copulations, not considering the reflective judgements as directly relevant. These explanations largely speak past each other, hinting that a unified theory could do much better in predicting behavior.</p>\n<p>The key to this is establishing linkages between the logical primitives of each discipline. Behavior could be explained on the level of physics, biology, psychology, or economics, but whether low-level explanations are practical is a different matter. Realistically, linking disciplines will strengthen both fields by mutually constraining the theories available to them.</p>\n<p>With the neoclassical revolution, economics developed concepts of utility as reflecting ordinal relationships over revealed preferences. Choices that satisfied certain <a href=\"http://en.wikipedia.org/wiki/Revealed_preference#The_strong_axiom_of_revealed_preference\">consistency conditions</a> could be treated as if generated by a utility function. <a href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis#The_von_Neumann-Morgenstern_axioms\">Additional axioms</a> allowed consistent choice under uncertainty to be added to the theory. There are notable problems with this approach, but the core ideas of utility and maximization have surprisingly close neural analogues. Rather than operating \"as if\" individuals act on the basis of utility, a hard theory of \"because\" is being developed.</p>\n<p>A look at visual perception reveals our subjective experience of light intensity varies subtantially depending on the wavelength of the light. Brightness is concept that resides in the mind, and furthermore sensitivity to different wavelengths corresponds precisely to the absorption spectra of the chemical <em>rhodopsin</em> in our retinas. All perceptions are represented in the mind along a <a href=\"http://en.wikipedia.org/wiki/Stevens'_power_law\">power scale</a> with some variance. Because the distributions of perceptions overlap, subjects can report accurately that a dimmer light is perceptually brighter. This suggests random utility models developed for statisical purposes might be directly explain what happens in the brain. One interesting consequence about the power scaling law is that <a href=\"http://en.wikipedia.org/wiki/Risk_aversion\">risk aversion</a> would be embedded at the level of perception.</p>\n<h3 id=\"Section_II__The_Choice_Mechanism\">Section II: The Choice Mechanism</h3>\n<p>Due to its relative simplicity, eye movement serves as a model for motor control and perhaps decisions broadly. The <em>superior colliculus</em> represents possible eye movements topographically with \"hills\" of activity. Eventually, the tissue transitions to a bursting state where the most active hill becomes much more active and the rest are inhibited via a \"winner-take-all\" or \"argmax\" mechanism. All inputs have to eye motion have to pass through the <em>superior colliculus</em>, so this represents a common final pathway of processed sensory signals. By giving monkeys varying awards for eye-motion tasks, activity in the <em>lateral intraparietal area</em> (LIP) correlates strongly with the probability and size of reward in an area known to trigger action before the action is taken. In other words, this appears to be a direct neural representation of subjective expected valuation. If monkey subjects play a game with mixed strategies in equilibrium, neuron firing rates are all roughly equal, matching the conclusion that expected utilities of actions are equalized when an opponent is mixing.</p>\n<p>Cortial neurons fire almost like independent Poisson processes, resulting in neurons down the line being able to easily extract the mean firing rate of the inputs. Interneuronal correlation can vary according to the task at hand, resulting in greater or lesser variation of the final decision, so descriptive decision theories must incorporate randomness in choice. This also provides support for mixed strategies being represented directly in the brain.</p>\n<p>Subjective valuations are normalized, and are only considered relative to the other options at hand. This normalization maximizes the joint information of neurons, increasing the efficiency of value representation. One consequence is that as the choice set increases, valuations start overlapping, and choice becomes essentially random. Activity also varies according to the delay of rewards, matching previous findings of hyperbolic discounting. While these findings are largely based on eye-movements in monkeys, this provides a clear path of how choice can be reduced to neural mechanisms.</p>\n<h3 id=\"Section_III__Valuation\">Section III: Valuation</h3>\n<p>Back to visual perception, our judgements are made relative to other elements in the environment. Color looks roughly the same indoors and outdoors, even though there can be six orders of magnitude more illumination outside. Drifting reference points make absolute values unrecoverable. Local irrationalies due to reliance on a reference point arise because evolution is trading off between accurate sensory encoding and the costs of these irrationalities.</p>\n<p>One promising way to specify the reference point is as the discounted sum of our future wealth. Learning depends on the difference between actual and expected rewards, so valuation compared to a reference point arises from the learning process. In the brain, reward prediction errors are encoded through dopamine. Dopamine firing rates are well-described by an exponentially weighted sum of previous awards subtracted from the most recent award. Hebb's law, which says \"cells that fire together, wire together\", describes how long-term predictions work.</p>\n<p>Valuation appears to be orginally constructed in the striatum and medial prefrontal cortex. The reference level encoded there can be directly observed with brain scanners. Various other regions provide inputs to construct value. For instance, the orbitofrontal cortex (OFC) provides an assessment of risk. Subjects with lesions in this area exhibit almost perfect risk neutrality. Values might also be stored in the OFC, again in a compressed and encoded way. Longer-term valuations might be stored in the amygdala.</p>\n<p>Because valuations are encoded relatively and don't work well over large choice sets, humans might edit out options by sequentially considering particular attributes until the choice set become manageable. Sorting by attributes can lead to irrational choices, unsurprisingly.</p>\n<p>Probabilistic valuations depend on whether the expectation was learned experientially or symbolically. Symbolically communicated probabilities, where the person is told a number, are overweighed near zero and underweighted near one. Experientially communicated probabilities, where the person samples the lotteries directly, exhibit the opposite pattern. This suggests at least two mechanisms at work, especially with the ability to deal with symbolic probabilities arising relatively late in our evolutionary history. Also, while experiential expected values incorporate probabilities implicitly, this information can't be extracted. When probabilities change, the only means to change valuations is to relearn them from scratch.</p>\n<h3 id=\"Section_IV__Summary_and_Conclusions\">Section IV: Summary and Conclusions</h3>\n<p>Here the author presents formalized models of the descriptive theory. The normative uses of this theory are still unclear. Even if we can identify subjective valuations in the brain, does this have any relation to welfare?</p>\n<p>The four critical observations of neuroeconomics are reference-dependence, the lack of an absolute measure of anything in the brain, stochasticity in choice, and the influence of learning on choice. Along with the question of the welfare implications of these findings, six primary questions are currently unanswered:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Where is subjective value stored and how does it get to choice?</li>\n<li>What part of the brain governs when it is \"time to choose\"?</li>\n<li>What neural mechanism guides complementarity between goods?</li>\n<li>How does symbolic probability work?</li>\n<li>How does the state of the world and utility interact?</li>\n<li>How does the brain represent money?</li>\n</ol>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Section I: The Challenge of Neuroeconomics", "anchor": "Section_I__The_Challenge_of_Neuroeconomics", "level": 1}, {"title": "Section II: The Choice Mechanism", "anchor": "Section_II__The_Choice_Mechanism", "level": 1}, {"title": "Section III: Valuation", "anchor": "Section_III__Valuation", "level": 1}, {"title": "Section IV: Summary and Conclusions", "anchor": "Section_IV__Summary_and_Conclusions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vNBxmcHpnozjrJnJP", "fo5rNAQAJjnDQ5GAk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-24T05:33:22.595Z", "modifiedAt": null, "url": null, "title": "Inferring Our Desires", "slug": "inferring-our-desires", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:07.299Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2G7AH92pHyj3nC32T/inferring-our-desires", "pageUrlRelative": "/posts/2G7AH92pHyj3nC32T/inferring-our-desires", "linkUrl": "https://www.lesswrong.com/posts/2G7AH92pHyj3nC32T/inferring-our-desires", "postedAtFormatted": "Tuesday, May 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Inferring%20Our%20Desires&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInferring%20Our%20Desires%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2G7AH92pHyj3nC32T%2Finferring-our-desires%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Inferring%20Our%20Desires%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2G7AH92pHyj3nC32T%2Finferring-our-desires", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2G7AH92pHyj3nC32T%2Finferring-our-desires", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1793, "htmlBody": "<p><span style=\"font-size: 11px;\">Related: <a href=\"/lw/4e/cached_selves/\">Cached Selves</a>, <a href=\"/lw/4z7/the_neuroscience_of_desire/\">The Neuroscience of Desire</a></span></p>\n<blockquote style=\"padding-left: 30px;\">You don't know your own mind.<br />&nbsp;&nbsp;&nbsp;&nbsp;- Jonathan Swift, <em><a href=\"http://www.amazon.com/conversation-dialogues-Jonathan-introduction-Saintsbury/dp/1176297252/\">Polite Conversation</a></em></blockquote>\n<p>Researchers showed subjects two female faces for a few seconds and asked which face was more attractive. Researchers then placed the photos face down and handed subjects the face they had chosen, asking them to explain the motives behind their choice. But sometimes, researchers used a sleight-of-hand trick to switch the photos, showing viewers the face they had <em>not</em>&nbsp;chosen. Very few subjects noticed the face they were given was not the one they had chosen. Moreover, they happily explained why they preferred the face they had actually rejected, inventing reasons like \"I like her smile\" even though they had <em>actually</em>&nbsp;chosen the solemn-faced picture.<sup>1</sup></p>\n<p>The idea that we lack good introspective access to our own desires - that we often have no idea what we want<sup>2</sup> - is a key <a href=\"http://dictionary.reference.com/browse/lemma\">lemma</a> in <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">naturalistic metaethics</a>, so it seems worth a post to collect the science by which we know that.</p>\n<p>Early warnings came from split-brain research, which identified an 'interpreter' in the left hemisphere that invents reasons for beliefs and actions. When the command 'walk' was flashed to split-brain subjects' right hemispheres, they got up from their chairs and start walking away. When asked <em>why</em> they suddenly started walking away, they replied (for example) that they got up because they wanted a Coke.<sup>3</sup></p>\n<p><a id=\"more\"></a></p>\n<h4><br /></h4>\n<h4>The overjustification effect</h4>\n<p>Common sense suggests that we infer others' feelings from their appearance and actions, but we have a different, more direct route to our&nbsp;<em style=\"font-style: italic;\">own</em>&nbsp;feelings: direct perception or&nbsp;<em style=\"font-style: italic;\">introspection</em>.<sup>4</sup>&nbsp;In contrast,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Self-perception_theory\">self-perception theory</a><sup>5</sup>&nbsp;suggests that our knowledge of ourselves is exactly like&nbsp;our knowledge of others.<sup>6</sup>&nbsp;One famous result explained by self-perception theory is the overjustification effect.</p>\n<p>In a famous 1973 study,</p>\n<blockquote>\n<p>nursery school children drew pictures with a magic marker, a&nbsp;presumably intrinsically interesting activity, under one of three reward&nbsp;conditions. In the first condition the children expected to receive a reward&nbsp;(a fancy 'good player' award) for drawing, in the second they received&nbsp;the reward unexpectedly, and children in a third group received no reward.&nbsp;Only the expected reward produced a decrement in performance,&nbsp;during a later 'free play' period, as compared with the other two groups. [This] overjustification effect seemed to be due not to the reward itself but to&nbsp;the implication that the reward was the reason for the behavior. Only if the&nbsp;participants knew a reward was coming when they performed the behavior&nbsp;would it undermine their intrinsic interest in the task.<sup>7</sup></p>\n</blockquote>\n<p>It seems that&nbsp;subjects initially drew pictures because of intrinsic motivation in that activity,&nbsp;but the payment led them to unconsciously 'conclude' that their behavior did not represent their&nbsp;<em>actual&nbsp;</em>desires. Thus, they performed more poorly in the subsequent 'free play' period. This is known as the <a href=\"http://en.wikipedia.org/wiki/Overjustification_effect\">overjustification effect</a>.</p>\n<p>After dozens of similar studies, two meta-analyses confirmed that the overjustification effect occurs when&nbsp;(1) subjects are led to expect rewards before performing&nbsp;the behavior, (2) the rewards are tangible, and (3) the rewards are independent of the subjects&rsquo; level of performance.<sup>8</sup></p>\n<h4><br /></h4>\n<h4>Implicit motivation</h4>\n<p>If we can be wrong about our own desires, then presumably many of our desires are activated unconsciously and operate unconsciously. Such&nbsp;<em>implicit motivations</em>&nbsp;have been amply confirmed.<sup>9</sup></p>\n<p>In one study, subjects were primed with achievement-related words ('strive', 'win', 'attain') during a word-finding task. During a second word-finding task, subjects were interrupted by an intercom announcement asking them to stop working. Those who had been primed with achievement-related words kept working more often than those who had not been so primed. Subjects were unable to identify the effect of this priming on their own motivation.<sup>10</sup></p>\n<p>This demonstrates that priming unconsciously affects the accessibility or strength of existing goals.<sup>11</sup> Do we unconsciously <em>form </em>goals, too?</p>\n<p>We do, as shown by decades of research on <a href=\"http://en.wikipedia.org/wiki/Operant_conditioning\">operant conditioning</a>. When a neutral potential goal is associated with a stimulus of positive affect, we acquire new goals, and we can be unaware that this has happened:</p>\n<blockquote>\n<p>Watching&nbsp;someone smile while eating blueberry muffins may, for instance, link that&nbsp;activity to positive affect, which creates a goal representation. Indeed, such&nbsp;observational or social learning is thought to be a basic way in which&nbsp;infants learn which behavioral states are desired and which ones are not.<sup>12</sup></p>\n</blockquote>\n<h4><br /></h4>\n<h4>Implications</h4>\n<p>This research is how we know about the <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">hidden complexity of wishes</a>, a key lemma in the <a href=\"/lw/y3/value_is_fragile/\">fragility of human value</a>. We don't know what many of our desires are, we don't know where they come from, and we can be wrong about our own motivations.</p>\n<p>As such, we'd be unlikely to get what we <em>really </em>want if the world was&nbsp;<a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">re-engineered</a> in accordance with a description of what we want that came from verbal introspective access to our motivations.&nbsp;<a href=\"http://intelligence.org/upload/CEV.html\">Less naive proposals</a> would involve probing the <a href=\"/lw/4z7/the_neuroscience_of_desire/\">neuroscience of motivation</a> at the <a href=\"http://en.wikipedia.org/wiki/David_Marr_(neuroscientist)#Levels_of_analysis\">algorithmic level</a>.<sup>13</sup></p>\n<p>&nbsp;</p>\n<h4>Notes</h4>\n<p><small><sup>1</sup>&nbsp;Johansson et al. (2005).</small></p>\n<p><small><sup>2</sup>&nbsp;Several experiments have established that we infer rather than perceive the moment we decided to act:&nbsp;Rigoni et al (2010); Banks &amp; Isham (2009, 2010); Moore &amp; Haggard (2008); Sarrazin et al. (2008);&nbsp;Gomes (1998, 2002). But do not infer that conscious thoughts do not affect behavior. As on recent review put it: \"The evidence for conscious causation of behavior&nbsp;is profound, extensive, adaptive, multifaceted, and empirically strong.&nbsp;However, conscious causation is often indirect and delayed, and it depends&nbsp;on interplay with unconscious processes. Consciousness seems&nbsp;especially useful for enabling behavior to be shaped by nonpresent factors&nbsp;and by social and cultural information, as well as for dealing with&nbsp;multiple competing options or impulses\"&nbsp;(Baumeister et al. 2011).&nbsp;We can even be wrong about&nbsp;<em style=\"font-style: italic;\">whether</em>&nbsp;we intended&nbsp;to act at all:&nbsp;Lynn et al. (2010); Morsella et al (2010).&nbsp;If we don't have direct introspective access even to our decisions to act, why think we have introspective access to our&nbsp;<em style=\"font-style: italic;\">desires</em>?</small></p>\n<p><small><sup>3</sup> Gazzaniga (1992), pp. 124-126.</small></p>\n<p><small><sup>4</sup>&nbsp;But widespread findings of self-ignorance<span style=\"font-size: 11px;\">&nbsp;</span>challenge this view. See, for example,&nbsp;Wilson (2004).</small></p>\n<p><small><sup>5</sup> Zanna &amp; Cooper (1974) seemed to have disproved self-perception theory in favor of cognitive dissonance theory, but Fazio et al (1977) showed that the two co-exist. This remains the modern view.</small></p>\n<p><span style=\"font-size: 11px;\"><sup>6</sup> Laird (2007), p. 7.</span></p>\n<p><small><sup>7</sup> Laird (2007), p. 126. The study described is Lepper et al. (1973).</small></p>\n<p><small><sup>8</sup>&nbsp;Cameron &amp;&nbsp;Pierce (1994); Tang &amp; Hall (1995);&nbsp;Eisenberger &amp; Cameron (1996).</small></p>\n<p><small><sup>9</sup> Aarts &amp; Dijksterhuis (2000); Bargh (1990); Bargh &amp; Gollwitzer (1994); Chartrand &amp; Bargh (1996, 2002);&nbsp;Fishbach et al. (2003);&nbsp;Fitzsimons &amp; Bargh (2003); Glaser &amp;&nbsp;Kihlstrom (2005); Gollwitzer et al. (2005); Hassin (2005); Shah (2003). For reviews, see Ferguson et al. (2007); Kruglanski &amp; Kopetz (2008);&nbsp;Moskowitz et al. (2004).&nbsp;Unconscious motivations can even&nbsp;adapt to novel and changing circumstances:&nbsp;see Ferguson et al. (2007), pp. 155-157.</small></p>\n<p><small><sup>10</sup> Bargh et al. (2001).</small></p>\n<p><small><sup>11</sup> Shah (2003); Aarts et al. (2004).</small></p>\n<p><small><sup>12</sup> Custers (2009).</small></p>\n<p><small><sup>13</sup>&nbsp;Inferring desires from behavior alone probably won't work, either:&nbsp;Soraker &amp; Brey (2007). Also: My thanks to Eliezer Yudkowsky for his feedback on a draft of this post.</small></p>\n<p><small>&nbsp;</small></p>\n<h4>References</h4>\n<p><small>Aarts &amp; Dijksterhuis (2000).&nbsp;<a href=\"http://goallab.nl/publications/documents/Aarts,%20Dijksterhuis%20(2000)%20-%20habits%20as%20knowlegde%20structures.pdf\">Habits as knowledge structures: Automaticity in goal-directed behavior</a>.&nbsp;<em>Journal of Personality and Social Psychology, 78</em>:&nbsp;53&ndash;63.</small></p>\n<p><small>Aarts, Gollwitzer, &amp; Hassin (2004).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Aarts-Goal-contagion-Inferring-goals-from-others-actions.pdf\">Goal&nbsp;contagion: Perceiving is for pursuing</a>. <em>Journal of Personality&nbsp;and Social Psychology, 87</em>: 23&ndash;37.</small></p>\n<p><small>Bargh (1990).&nbsp;Auto-motives: Preconscious determinants&nbsp;of social interaction. In Higgins &amp; Sorrentino (eds.), <em>Handbook of motivation and&nbsp;cognition: Foundations of social behavior</em> (Vol. 2,&nbsp;pp. 93&ndash;130). Guilford.</small></p>\n<p><small>Bargh &amp; Gollwitzer (1994).&nbsp;Environmental&nbsp;control of goal-directed action: Automatic and strategic&nbsp;contingencies between situations and behavior.&nbsp;In Spaulding (ed.), <em>Nebraska Symposium on&nbsp;Motivation</em> (Vol. 41, pp. 71&ndash;124). University&nbsp;of Nebraska Press.</small></p>\n<p><small>Bargh, Gollwitzer, Lee-Chai, Barndollar, &amp; Troetschel&nbsp;(2001).&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3005626/pdf/nihms256255.pdf\">The automated will:&nbsp;Nonconscious activation and pursuit of behavioral&nbsp;goals</a>. <em>Journal of Personality and Social Psychology,&nbsp;81</em>: 1014&ndash;1027.</small></p>\n<p><small>Baumeister,&nbsp;Masicampo, &amp;&nbsp;Vohs (2011). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Baumeister-et-al-Do-conscious-thoughts-cause-behavior.pdf\">Do conscious thoughts cause behavior?</a>&nbsp;<em>Annual Review of Psychology, 62</em>: 331-361.</small></p>\n<p><small>Banks &amp; Isham (2009). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2010/12/Banks-Isham-We-infer-rather-than-perceive-the-moment-we-decided-to-act.pdf\">We infer rather than perceive the moment we decided to act</a>. <em>Psychological Science, 20</em>: 17&ndash;21.</small></p>\n<p><small>Banks &amp; Isham (2010).&nbsp;<a href=\"http://books.google.com/books?hl=en&amp;lr=&amp;id=FMPkA08Cjq8C&amp;oi=fnd&amp;pg=PA47&amp;ots=eRBTpd3GnZ&amp;sig=tm-serEd27xIK7xlkxxXnEl3AyM#v=onepage&amp;q&amp;f=false\">Do we really know what we are doing? Implications of reported time of decision for theories of volition</a>. In Sinnott-Armstrong &amp; Nadel (eds.), <em>Conscious Will and Responsibility: A Tribute to Benjamin Libet</em> (pp. 47-60).</small></p>\n<p><small>Cameron &amp;&nbsp;Pierce (1994).&nbsp;Reinforcement, reward and intrinsic motivation:&nbsp;A meta-analysis. <em>Review of Educational Research, 64</em>: 363&ndash;423.</small></p>\n<p><small>Chartrand &amp; Bargh (1996).&nbsp;<a href=\"http://www.yale.edu/acmelab/articles/chartrand_bargh_1996.pdf\">Automatic activation&nbsp;of impression formation and memorization&nbsp;goals: Nonconscious goal priming reproduces effects&nbsp;of explicit task instructions</a>. <em>Journal of Personality&nbsp;and Social Psychology, 71</em>: 464&ndash;478.</small></p>\n<p><small>Chartrand &amp; Bargh (2002).&nbsp;Nonconscious&nbsp;motivations: Their activation, operation, and consequences.&nbsp;In Tesser &amp; Stapel (eds.), <em>Self and&nbsp;motivation: Emerging psychological perspectives</em>&nbsp;(pp. 13&ndash;41). American Psychological&nbsp;Association.</small></p>\n<p><small>Custers (2009). How does our unconscious know what we want? The role of affect in goal representations. In Moskowitz &amp; Grant (eds.), The Psychology of Goals. Guilford.</small></p>\n<p><small>Eisenberger &amp; Cameron (1996).&nbsp;<a href=\"http://www.psychology.uh.edu/faculty/Eisenberger/files/16_Detrimental_Effects_of_Reward_Reality_or_Myth.pdf\">Detrimental effects of reward: Reality or&nbsp;myth?</a> <em>American Psychologist, 51</em>: 1153&ndash;1166.</small></p>\n<p><small>Fazio, Zanna, &amp; Cooper (1977).&nbsp;Dissonance and self-perception: An integrative view of each theory's proper domain of application. <em>Journal of Experimental Social Psychology, 13</em>: 464-479.</small></p>\n<p><small>Ferguson, Hassin, &amp; Bargh (2007). Implicit Motivation. In Shah &amp; Gardner (eds.), <em>Handbook of Motivation Science</em> (pp. 150-166). Guilford.</small></p>\n<p><small>Fishbach, Friedman, &amp; Kruglanski (2003).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Fishbach-Leading-us-not-unto-temptation-Momentary-allurements-elicit-automatic-goal-activation.pdf\">Leading us not unto temptation: Momentary&nbsp;allurements elicit automatic goal activation</a>. <em>Journal&nbsp;of Personality and Social Psychology, 84</em>: 296&ndash;309.</small></p>\n<p><small>Fitzsimons &amp; Bargh (2003).&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3011819/pdf/nihms256261.pdf\">Thinking of&nbsp;you: Nonconscious pursuit of interpersonal goals associated&nbsp;with relationship partners</a>. <em>Journal of Personality&nbsp;and Social Psychology, 84</em>: 148&ndash;163.</small></p>\n<p><small>Gazzaniga (1992).&nbsp;<em><a href=\"http://www.amazon.com/Natures-Mind-Biological-Sexuality-Intelligence/dp/0465048633/\">Nature's mind: The biological roots of thinking, emotion, sexuality,&nbsp;language, and intelligence</a></em>. Basic Books.</small></p>\n<p><small>Glaser &amp;&nbsp;Kihlstrom (2005).&nbsp;Compensatory&nbsp;automaticity: Unconscious volition is not an oxymoron.&nbsp;In Hassin, Uleman, &amp; Bargh (eds.),&nbsp;<em>The new unconscious</em> (pp. 171&ndash;195). Oxford University Press.</small></p>\n<p><small>Gollwitzer, Bayer, &amp; McCullouch (2005).&nbsp;The control of the unwanted. In Hassin, Uleman, &amp; Bargh (eds.), <em>The new unconscious</em>&nbsp;(pp. 485&ndash;515). Oxford University&nbsp;Press.</small></p>\n<p><small>Gomes (1998). The timing of conscious experience: a critical review and reinterpretation of Libet&rsquo;s research.&nbsp;<em>Consciousness and Cognition, 7:</em> 559&ndash;595.</small></p>\n<p><small>Gomes (2002). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Gomes-Problems-in-the-timing-of-conscious-experience.pdf\">Problems in the timing of conscious experience</a>. <em>Consciousness and Cognition, 11</em>: 191&ndash;97.</small></p>\n<p><small>Hassin (2005).&nbsp;Non-conscious control and implicit&nbsp;working memory. In Hassin, Uleman,&nbsp;&amp; Bargh (eds.), The new unconscious (pp. 196&ndash;224). Oxford University press.</small></p>\n<p><small>Johansson, Hall, Silkstrom, &amp; Olsson (2005). <a href=\"http://thebrowser.com/files/Mismatches-between-intention-and-outcome.pdf\">Failure to detect mismatches between intention and outcome in a simple decision task</a>. <em>Science, 310</em>: 116-119.</small></p>\n<p><small>Kruglanski &amp; Kopetz (2008). The role of goal systems in self-regulation. In<em>&nbsp;</em>Morsella, Bargh, &amp; Gollwitzer (eds.), <em>Oxford Handbook of Human Action</em>&nbsp;(pp. 350-369). Oxford University Press.</small></p>\n<p><small>Laird (2007). <em><a href=\"http://www.amazon.com/Feelings-Perception-Self-Affective-Science/dp/0195098897/\">Feelings: The Perception of Self</a></em>. Oxford University Press.</small></p>\n<p><small>Lepper, Green, &amp; Nisbett (1973).&nbsp;<a href=\"http://fitaba.com/page16/assets/Overjustification%20Study%20-%20Lepper.pdf\">Undermining children&rsquo;s intrinsic&nbsp;interest with extrinsic rewards: A test of the 'overjustification' hypothesis</a>.&nbsp;<em>Journal of Personality and Social Psychology, 28</em>: 129&ndash;137.</small></p>\n<p><small>Lynn, Berger, Riddle, &amp; Morsella (2010). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Lynn-et-al-Mind-control-Creating-illusory-intentions-through-a-phony-brain&ndash;computer-interface.pdf\">Mind control?&nbsp;Creating illusory intentions through a phony brain&ndash;computer interface</a>. <em>Consciousness and Cognition, 19</em>: 1007-1012.</small></p>\n<p><small>Moore &amp; Haggard (2008). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Moore-Haggard-Awareness-of-action-inference-and-prediction.pdf\">Awareness of action: inference and prediction</a>. <em>Consciousness and Cognition, 17</em>: 136&ndash;144.</small></p>\n<p><small>Morsella, Berger, &amp; Krieger (2010).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Morsella-et-al-Cognitive-and-neural-components-of-the-phenomenology-of-agency.pdf\">Cognitive and neural components of the phenomenology of agency</a>. <em>Neurocase.</em></small></p>\n<p><small><em><span style=\"font-style: normal;\">Moskowitz, Li, &amp; Kirk (2004).&nbsp;</span></em>The implicit volition model: On&nbsp;the preconscious regulation of temporarily adopted goals. In Zanna&nbsp;(ed.), <em>Advances in experimental social psychology</em> (Vol. 36, pp. 317&ndash;404). Academic Press.</small></p>\n<p><small>Rigoni, Brass, &amp; Sartori (2010).&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2876876/pdf/fnhum-04-00038.pdf\">Post-action determinants of the reported time of&nbsp;conscious intentions</a>. <em>Frontiers in Human Neuroscience, 4</em>: 38.</small></p>\n<p><small>Sarrazin, Cleeremans, &amp; Haggard (2008). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Sarrazin-et-al-How-do-we-know-what-we-are-doing-Time-intention-and-awareness-of-action.pdf\">How do we know what we are doing? Time, intention, and&nbsp;awareness of action</a>. <em>Consciousness and cognition, 17</em>: 602&ndash;615.</small></p>\n<p><small>Shah (2003).&nbsp;The motivational looking glass: How&nbsp;significant others implicitly affect goal appraisals.&nbsp;<em>Journal of Personality and Social Psychology, 85</em>:&nbsp;424&ndash;439.</small></p>\n<p><small>Soraker &amp; Brey (2007).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Soraker-Brey-Ambient-intelligence-and-problems-with-inferring-desires-from-behavior.pdf\">Ambient Intelligence and Problems with Inferring Desires from Behaviour</a>.&nbsp;<em>International Review of Information Ethics, 8</em>: 7-12.</small></p>\n<p><small>Tang &amp; Hall (1995).&nbsp;The overjustification effect: A meta-analysis. <em>Applied&nbsp;Cognitive Psychology, 9</em>: 365&ndash;404.</small></p>\n<p><small>Wilson (2004). <em><a href=\"http://www.amazon.com/Strangers-Ourselves-Discovering-Adaptive-Unconscious/dp/0674013824/\">Strangers to Ourselves: Discovering the Adaptive Unconscious</a></em>. Belknap.</small></p>\n<p><small>Zanna &amp; Cooper (1974).&nbsp;Dissonance and the pill: An attribution approach to studying the arousal properties of dissonance.&nbsp;<em>Journal of Personality and Social Psychology, 29: 703-709</em>.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwv9eHi7KGg5KA9oM": 2, "iP2X4jQNHMWHRNPne": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2G7AH92pHyj3nC32T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 58, "extendedScore": null, "score": 0.000118, "legacy": true, "legacyId": "7508", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 58, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"font-size: 11px;\">Related: <a href=\"/lw/4e/cached_selves/\">Cached Selves</a>, <a href=\"/lw/4z7/the_neuroscience_of_desire/\">The Neuroscience of Desire</a></span></p>\n<blockquote style=\"padding-left: 30px;\">You don't know your own mind.<br>&nbsp;&nbsp;&nbsp;&nbsp;- Jonathan Swift, <em><a href=\"http://www.amazon.com/conversation-dialogues-Jonathan-introduction-Saintsbury/dp/1176297252/\">Polite Conversation</a></em></blockquote>\n<p>Researchers showed subjects two female faces for a few seconds and asked which face was more attractive. Researchers then placed the photos face down and handed subjects the face they had chosen, asking them to explain the motives behind their choice. But sometimes, researchers used a sleight-of-hand trick to switch the photos, showing viewers the face they had <em>not</em>&nbsp;chosen. Very few subjects noticed the face they were given was not the one they had chosen. Moreover, they happily explained why they preferred the face they had actually rejected, inventing reasons like \"I like her smile\" even though they had <em>actually</em>&nbsp;chosen the solemn-faced picture.<sup>1</sup></p>\n<p>The idea that we lack good introspective access to our own desires - that we often have no idea what we want<sup>2</sup> - is a key <a href=\"http://dictionary.reference.com/browse/lemma\">lemma</a> in <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">naturalistic metaethics</a>, so it seems worth a post to collect the science by which we know that.</p>\n<p>Early warnings came from split-brain research, which identified an 'interpreter' in the left hemisphere that invents reasons for beliefs and actions. When the command 'walk' was flashed to split-brain subjects' right hemispheres, they got up from their chairs and start walking away. When asked <em>why</em> they suddenly started walking away, they replied (for example) that they got up because they wanted a Coke.<sup>3</sup></p>\n<p><a id=\"more\"></a></p>\n<h4><br></h4>\n<h4 id=\"The_overjustification_effect\">The overjustification effect</h4>\n<p>Common sense suggests that we infer others' feelings from their appearance and actions, but we have a different, more direct route to our&nbsp;<em style=\"font-style: italic;\">own</em>&nbsp;feelings: direct perception or&nbsp;<em style=\"font-style: italic;\">introspection</em>.<sup>4</sup>&nbsp;In contrast,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Self-perception_theory\">self-perception theory</a><sup>5</sup>&nbsp;suggests that our knowledge of ourselves is exactly like&nbsp;our knowledge of others.<sup>6</sup>&nbsp;One famous result explained by self-perception theory is the overjustification effect.</p>\n<p>In a famous 1973 study,</p>\n<blockquote>\n<p>nursery school children drew pictures with a magic marker, a&nbsp;presumably intrinsically interesting activity, under one of three reward&nbsp;conditions. In the first condition the children expected to receive a reward&nbsp;(a fancy 'good player' award) for drawing, in the second they received&nbsp;the reward unexpectedly, and children in a third group received no reward.&nbsp;Only the expected reward produced a decrement in performance,&nbsp;during a later 'free play' period, as compared with the other two groups. [This] overjustification effect seemed to be due not to the reward itself but to&nbsp;the implication that the reward was the reason for the behavior. Only if the&nbsp;participants knew a reward was coming when they performed the behavior&nbsp;would it undermine their intrinsic interest in the task.<sup>7</sup></p>\n</blockquote>\n<p>It seems that&nbsp;subjects initially drew pictures because of intrinsic motivation in that activity,&nbsp;but the payment led them to unconsciously 'conclude' that their behavior did not represent their&nbsp;<em>actual&nbsp;</em>desires. Thus, they performed more poorly in the subsequent 'free play' period. This is known as the <a href=\"http://en.wikipedia.org/wiki/Overjustification_effect\">overjustification effect</a>.</p>\n<p>After dozens of similar studies, two meta-analyses confirmed that the overjustification effect occurs when&nbsp;(1) subjects are led to expect rewards before performing&nbsp;the behavior, (2) the rewards are tangible, and (3) the rewards are independent of the subjects\u2019 level of performance.<sup>8</sup></p>\n<h4><br></h4>\n<h4 id=\"Implicit_motivation\">Implicit motivation</h4>\n<p>If we can be wrong about our own desires, then presumably many of our desires are activated unconsciously and operate unconsciously. Such&nbsp;<em>implicit motivations</em>&nbsp;have been amply confirmed.<sup>9</sup></p>\n<p>In one study, subjects were primed with achievement-related words ('strive', 'win', 'attain') during a word-finding task. During a second word-finding task, subjects were interrupted by an intercom announcement asking them to stop working. Those who had been primed with achievement-related words kept working more often than those who had not been so primed. Subjects were unable to identify the effect of this priming on their own motivation.<sup>10</sup></p>\n<p>This demonstrates that priming unconsciously affects the accessibility or strength of existing goals.<sup>11</sup> Do we unconsciously <em>form </em>goals, too?</p>\n<p>We do, as shown by decades of research on <a href=\"http://en.wikipedia.org/wiki/Operant_conditioning\">operant conditioning</a>. When a neutral potential goal is associated with a stimulus of positive affect, we acquire new goals, and we can be unaware that this has happened:</p>\n<blockquote>\n<p>Watching&nbsp;someone smile while eating blueberry muffins may, for instance, link that&nbsp;activity to positive affect, which creates a goal representation. Indeed, such&nbsp;observational or social learning is thought to be a basic way in which&nbsp;infants learn which behavioral states are desired and which ones are not.<sup>12</sup></p>\n</blockquote>\n<h4><br></h4>\n<h4 id=\"Implications\">Implications</h4>\n<p>This research is how we know about the <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">hidden complexity of wishes</a>, a key lemma in the <a href=\"/lw/y3/value_is_fragile/\">fragility of human value</a>. We don't know what many of our desires are, we don't know where they come from, and we can be wrong about our own motivations.</p>\n<p>As such, we'd be unlikely to get what we <em>really </em>want if the world was&nbsp;<a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">re-engineered</a> in accordance with a description of what we want that came from verbal introspective access to our motivations.&nbsp;<a href=\"http://intelligence.org/upload/CEV.html\">Less naive proposals</a> would involve probing the <a href=\"/lw/4z7/the_neuroscience_of_desire/\">neuroscience of motivation</a> at the <a href=\"http://en.wikipedia.org/wiki/David_Marr_(neuroscientist)#Levels_of_analysis\">algorithmic level</a>.<sup>13</sup></p>\n<p>&nbsp;</p>\n<h4 id=\"Notes\">Notes</h4>\n<p><small><sup>1</sup>&nbsp;Johansson et al. (2005).</small></p>\n<p><small><sup>2</sup>&nbsp;Several experiments have established that we infer rather than perceive the moment we decided to act:&nbsp;Rigoni et al (2010); Banks &amp; Isham (2009, 2010); Moore &amp; Haggard (2008); Sarrazin et al. (2008);&nbsp;Gomes (1998, 2002). But do not infer that conscious thoughts do not affect behavior. As on recent review put it: \"The evidence for conscious causation of behavior&nbsp;is profound, extensive, adaptive, multifaceted, and empirically strong.&nbsp;However, conscious causation is often indirect and delayed, and it depends&nbsp;on interplay with unconscious processes. Consciousness seems&nbsp;especially useful for enabling behavior to be shaped by nonpresent factors&nbsp;and by social and cultural information, as well as for dealing with&nbsp;multiple competing options or impulses\"&nbsp;(Baumeister et al. 2011).&nbsp;We can even be wrong about&nbsp;<em style=\"font-style: italic;\">whether</em>&nbsp;we intended&nbsp;to act at all:&nbsp;Lynn et al. (2010); Morsella et al (2010).&nbsp;If we don't have direct introspective access even to our decisions to act, why think we have introspective access to our&nbsp;<em style=\"font-style: italic;\">desires</em>?</small></p>\n<p><small><sup>3</sup> Gazzaniga (1992), pp. 124-126.</small></p>\n<p><small><sup>4</sup>&nbsp;But widespread findings of self-ignorance<span style=\"font-size: 11px;\">&nbsp;</span>challenge this view. See, for example,&nbsp;Wilson (2004).</small></p>\n<p><small><sup>5</sup> Zanna &amp; Cooper (1974) seemed to have disproved self-perception theory in favor of cognitive dissonance theory, but Fazio et al (1977) showed that the two co-exist. This remains the modern view.</small></p>\n<p><span style=\"font-size: 11px;\"><sup>6</sup> Laird (2007), p. 7.</span></p>\n<p><small><sup>7</sup> Laird (2007), p. 126. The study described is Lepper et al. (1973).</small></p>\n<p><small><sup>8</sup>&nbsp;Cameron &amp;&nbsp;Pierce (1994); Tang &amp; Hall (1995);&nbsp;Eisenberger &amp; Cameron (1996).</small></p>\n<p><small><sup>9</sup> Aarts &amp; Dijksterhuis (2000); Bargh (1990); Bargh &amp; Gollwitzer (1994); Chartrand &amp; Bargh (1996, 2002);&nbsp;Fishbach et al. (2003);&nbsp;Fitzsimons &amp; Bargh (2003); Glaser &amp;&nbsp;Kihlstrom (2005); Gollwitzer et al. (2005); Hassin (2005); Shah (2003). For reviews, see Ferguson et al. (2007); Kruglanski &amp; Kopetz (2008);&nbsp;Moskowitz et al. (2004).&nbsp;Unconscious motivations can even&nbsp;adapt to novel and changing circumstances:&nbsp;see Ferguson et al. (2007), pp. 155-157.</small></p>\n<p><small><sup>10</sup> Bargh et al. (2001).</small></p>\n<p><small><sup>11</sup> Shah (2003); Aarts et al. (2004).</small></p>\n<p><small><sup>12</sup> Custers (2009).</small></p>\n<p><small><sup>13</sup>&nbsp;Inferring desires from behavior alone probably won't work, either:&nbsp;Soraker &amp; Brey (2007). Also: My thanks to Eliezer Yudkowsky for his feedback on a draft of this post.</small></p>\n<p><small>&nbsp;</small></p>\n<h4 id=\"References\">References</h4>\n<p><small>Aarts &amp; Dijksterhuis (2000).&nbsp;<a href=\"http://goallab.nl/publications/documents/Aarts,%20Dijksterhuis%20(2000)%20-%20habits%20as%20knowlegde%20structures.pdf\">Habits as knowledge structures: Automaticity in goal-directed behavior</a>.&nbsp;<em>Journal of Personality and Social Psychology, 78</em>:&nbsp;53\u201363.</small></p>\n<p><small>Aarts, Gollwitzer, &amp; Hassin (2004).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Aarts-Goal-contagion-Inferring-goals-from-others-actions.pdf\">Goal&nbsp;contagion: Perceiving is for pursuing</a>. <em>Journal of Personality&nbsp;and Social Psychology, 87</em>: 23\u201337.</small></p>\n<p><small>Bargh (1990).&nbsp;Auto-motives: Preconscious determinants&nbsp;of social interaction. In Higgins &amp; Sorrentino (eds.), <em>Handbook of motivation and&nbsp;cognition: Foundations of social behavior</em> (Vol. 2,&nbsp;pp. 93\u2013130). Guilford.</small></p>\n<p><small>Bargh &amp; Gollwitzer (1994).&nbsp;Environmental&nbsp;control of goal-directed action: Automatic and strategic&nbsp;contingencies between situations and behavior.&nbsp;In Spaulding (ed.), <em>Nebraska Symposium on&nbsp;Motivation</em> (Vol. 41, pp. 71\u2013124). University&nbsp;of Nebraska Press.</small></p>\n<p><small>Bargh, Gollwitzer, Lee-Chai, Barndollar, &amp; Troetschel&nbsp;(2001).&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3005626/pdf/nihms256255.pdf\">The automated will:&nbsp;Nonconscious activation and pursuit of behavioral&nbsp;goals</a>. <em>Journal of Personality and Social Psychology,&nbsp;81</em>: 1014\u20131027.</small></p>\n<p><small>Baumeister,&nbsp;Masicampo, &amp;&nbsp;Vohs (2011). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Baumeister-et-al-Do-conscious-thoughts-cause-behavior.pdf\">Do conscious thoughts cause behavior?</a>&nbsp;<em>Annual Review of Psychology, 62</em>: 331-361.</small></p>\n<p><small>Banks &amp; Isham (2009). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2010/12/Banks-Isham-We-infer-rather-than-perceive-the-moment-we-decided-to-act.pdf\">We infer rather than perceive the moment we decided to act</a>. <em>Psychological Science, 20</em>: 17\u201321.</small></p>\n<p><small>Banks &amp; Isham (2010).&nbsp;<a href=\"http://books.google.com/books?hl=en&amp;lr=&amp;id=FMPkA08Cjq8C&amp;oi=fnd&amp;pg=PA47&amp;ots=eRBTpd3GnZ&amp;sig=tm-serEd27xIK7xlkxxXnEl3AyM#v=onepage&amp;q&amp;f=false\">Do we really know what we are doing? Implications of reported time of decision for theories of volition</a>. In Sinnott-Armstrong &amp; Nadel (eds.), <em>Conscious Will and Responsibility: A Tribute to Benjamin Libet</em> (pp. 47-60).</small></p>\n<p><small>Cameron &amp;&nbsp;Pierce (1994).&nbsp;Reinforcement, reward and intrinsic motivation:&nbsp;A meta-analysis. <em>Review of Educational Research, 64</em>: 363\u2013423.</small></p>\n<p><small>Chartrand &amp; Bargh (1996).&nbsp;<a href=\"http://www.yale.edu/acmelab/articles/chartrand_bargh_1996.pdf\">Automatic activation&nbsp;of impression formation and memorization&nbsp;goals: Nonconscious goal priming reproduces effects&nbsp;of explicit task instructions</a>. <em>Journal of Personality&nbsp;and Social Psychology, 71</em>: 464\u2013478.</small></p>\n<p><small>Chartrand &amp; Bargh (2002).&nbsp;Nonconscious&nbsp;motivations: Their activation, operation, and consequences.&nbsp;In Tesser &amp; Stapel (eds.), <em>Self and&nbsp;motivation: Emerging psychological perspectives</em>&nbsp;(pp. 13\u201341). American Psychological&nbsp;Association.</small></p>\n<p><small>Custers (2009). How does our unconscious know what we want? The role of affect in goal representations. In Moskowitz &amp; Grant (eds.), The Psychology of Goals. Guilford.</small></p>\n<p><small>Eisenberger &amp; Cameron (1996).&nbsp;<a href=\"http://www.psychology.uh.edu/faculty/Eisenberger/files/16_Detrimental_Effects_of_Reward_Reality_or_Myth.pdf\">Detrimental effects of reward: Reality or&nbsp;myth?</a> <em>American Psychologist, 51</em>: 1153\u20131166.</small></p>\n<p><small>Fazio, Zanna, &amp; Cooper (1977).&nbsp;Dissonance and self-perception: An integrative view of each theory's proper domain of application. <em>Journal of Experimental Social Psychology, 13</em>: 464-479.</small></p>\n<p><small>Ferguson, Hassin, &amp; Bargh (2007). Implicit Motivation. In Shah &amp; Gardner (eds.), <em>Handbook of Motivation Science</em> (pp. 150-166). Guilford.</small></p>\n<p><small>Fishbach, Friedman, &amp; Kruglanski (2003).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Fishbach-Leading-us-not-unto-temptation-Momentary-allurements-elicit-automatic-goal-activation.pdf\">Leading us not unto temptation: Momentary&nbsp;allurements elicit automatic goal activation</a>. <em>Journal&nbsp;of Personality and Social Psychology, 84</em>: 296\u2013309.</small></p>\n<p><small>Fitzsimons &amp; Bargh (2003).&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3011819/pdf/nihms256261.pdf\">Thinking of&nbsp;you: Nonconscious pursuit of interpersonal goals associated&nbsp;with relationship partners</a>. <em>Journal of Personality&nbsp;and Social Psychology, 84</em>: 148\u2013163.</small></p>\n<p><small>Gazzaniga (1992).&nbsp;<em><a href=\"http://www.amazon.com/Natures-Mind-Biological-Sexuality-Intelligence/dp/0465048633/\">Nature's mind: The biological roots of thinking, emotion, sexuality,&nbsp;language, and intelligence</a></em>. Basic Books.</small></p>\n<p><small>Glaser &amp;&nbsp;Kihlstrom (2005).&nbsp;Compensatory&nbsp;automaticity: Unconscious volition is not an oxymoron.&nbsp;In Hassin, Uleman, &amp; Bargh (eds.),&nbsp;<em>The new unconscious</em> (pp. 171\u2013195). Oxford University Press.</small></p>\n<p><small>Gollwitzer, Bayer, &amp; McCullouch (2005).&nbsp;The control of the unwanted. In Hassin, Uleman, &amp; Bargh (eds.), <em>The new unconscious</em>&nbsp;(pp. 485\u2013515). Oxford University&nbsp;Press.</small></p>\n<p><small>Gomes (1998). The timing of conscious experience: a critical review and reinterpretation of Libet\u2019s research.&nbsp;<em>Consciousness and Cognition, 7:</em> 559\u2013595.</small></p>\n<p><small>Gomes (2002). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Gomes-Problems-in-the-timing-of-conscious-experience.pdf\">Problems in the timing of conscious experience</a>. <em>Consciousness and Cognition, 11</em>: 191\u201397.</small></p>\n<p><small>Hassin (2005).&nbsp;Non-conscious control and implicit&nbsp;working memory. In Hassin, Uleman,&nbsp;&amp; Bargh (eds.), The new unconscious (pp. 196\u2013224). Oxford University press.</small></p>\n<p><small>Johansson, Hall, Silkstrom, &amp; Olsson (2005). <a href=\"http://thebrowser.com/files/Mismatches-between-intention-and-outcome.pdf\">Failure to detect mismatches between intention and outcome in a simple decision task</a>. <em>Science, 310</em>: 116-119.</small></p>\n<p><small>Kruglanski &amp; Kopetz (2008). The role of goal systems in self-regulation. In<em>&nbsp;</em>Morsella, Bargh, &amp; Gollwitzer (eds.), <em>Oxford Handbook of Human Action</em>&nbsp;(pp. 350-369). Oxford University Press.</small></p>\n<p><small>Laird (2007). <em><a href=\"http://www.amazon.com/Feelings-Perception-Self-Affective-Science/dp/0195098897/\">Feelings: The Perception of Self</a></em>. Oxford University Press.</small></p>\n<p><small>Lepper, Green, &amp; Nisbett (1973).&nbsp;<a href=\"http://fitaba.com/page16/assets/Overjustification%20Study%20-%20Lepper.pdf\">Undermining children\u2019s intrinsic&nbsp;interest with extrinsic rewards: A test of the 'overjustification' hypothesis</a>.&nbsp;<em>Journal of Personality and Social Psychology, 28</em>: 129\u2013137.</small></p>\n<p><small>Lynn, Berger, Riddle, &amp; Morsella (2010). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Lynn-et-al-Mind-control-Creating-illusory-intentions-through-a-phony-brain\u2013computer-interface.pdf\">Mind control?&nbsp;Creating illusory intentions through a phony brain\u2013computer interface</a>. <em>Consciousness and Cognition, 19</em>: 1007-1012.</small></p>\n<p><small>Moore &amp; Haggard (2008). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Moore-Haggard-Awareness-of-action-inference-and-prediction.pdf\">Awareness of action: inference and prediction</a>. <em>Consciousness and Cognition, 17</em>: 136\u2013144.</small></p>\n<p><small>Morsella, Berger, &amp; Krieger (2010).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Morsella-et-al-Cognitive-and-neural-components-of-the-phenomenology-of-agency.pdf\">Cognitive and neural components of the phenomenology of agency</a>. <em>Neurocase.</em></small></p>\n<p><small><em><span style=\"font-style: normal;\">Moskowitz, Li, &amp; Kirk (2004).&nbsp;</span></em>The implicit volition model: On&nbsp;the preconscious regulation of temporarily adopted goals. In Zanna&nbsp;(ed.), <em>Advances in experimental social psychology</em> (Vol. 36, pp. 317\u2013404). Academic Press.</small></p>\n<p><small>Rigoni, Brass, &amp; Sartori (2010).&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2876876/pdf/fnhum-04-00038.pdf\">Post-action determinants of the reported time of&nbsp;conscious intentions</a>. <em>Frontiers in Human Neuroscience, 4</em>: 38.</small></p>\n<p><small>Sarrazin, Cleeremans, &amp; Haggard (2008). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Sarrazin-et-al-How-do-we-know-what-we-are-doing-Time-intention-and-awareness-of-action.pdf\">How do we know what we are doing? Time, intention, and&nbsp;awareness of action</a>. <em>Consciousness and cognition, 17</em>: 602\u2013615.</small></p>\n<p><small>Shah (2003).&nbsp;The motivational looking glass: How&nbsp;significant others implicitly affect goal appraisals.&nbsp;<em>Journal of Personality and Social Psychology, 85</em>:&nbsp;424\u2013439.</small></p>\n<p><small>Soraker &amp; Brey (2007).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Soraker-Brey-Ambient-intelligence-and-problems-with-inferring-desires-from-behavior.pdf\">Ambient Intelligence and Problems with Inferring Desires from Behaviour</a>.&nbsp;<em>International Review of Information Ethics, 8</em>: 7-12.</small></p>\n<p><small>Tang &amp; Hall (1995).&nbsp;The overjustification effect: A meta-analysis. <em>Applied&nbsp;Cognitive Psychology, 9</em>: 365\u2013404.</small></p>\n<p><small>Wilson (2004). <em><a href=\"http://www.amazon.com/Strangers-Ourselves-Discovering-Adaptive-Unconscious/dp/0674013824/\">Strangers to Ourselves: Discovering the Adaptive Unconscious</a></em>. Belknap.</small></p>\n<p><small>Zanna &amp; Cooper (1974).&nbsp;Dissonance and the pill: An attribution approach to studying the arousal properties of dissonance.&nbsp;<em>Journal of Personality and Social Psychology, 29: 703-709</em>.</small></p>", "sections": [{"title": "The overjustification effect", "anchor": "The_overjustification_effect", "level": 1}, {"title": "Implicit motivation", "anchor": "Implicit_motivation", "level": 1}, {"title": "Implications", "anchor": "Implications", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "45 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BHYBdijDcAKQ6e45Z", "48DTJkBH58JbBNSFH", "4ARaTpNX62uaL86j6", "GNnHHmm8EzePmKzPk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-24T07:20:23.418Z", "modifiedAt": null, "url": null, "title": "Official Less Wrong Redesign: Nearly there", "slug": "official-less-wrong-redesign-nearly-there", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.328Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FXMQWrkNKcZNagZRS/official-less-wrong-redesign-nearly-there", "pageUrlRelative": "/posts/FXMQWrkNKcZNagZRS/official-less-wrong-redesign-nearly-there", "linkUrl": "https://www.lesswrong.com/posts/FXMQWrkNKcZNagZRS/official-less-wrong-redesign-nearly-there", "postedAtFormatted": "Tuesday, May 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Official%20Less%20Wrong%20Redesign%3A%20Nearly%20there&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOfficial%20Less%20Wrong%20Redesign%3A%20Nearly%20there%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFXMQWrkNKcZNagZRS%2Fofficial-less-wrong-redesign-nearly-there%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Official%20Less%20Wrong%20Redesign%3A%20Nearly%20there%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFXMQWrkNKcZNagZRS%2Fofficial-less-wrong-redesign-nearly-there", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFXMQWrkNKcZNagZRS%2Fofficial-less-wrong-redesign-nearly-there", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<h2>Design sketches</h2>\n<p><a id=\"more\"></a></p>\n<p>1. Notes on differences between these images and what we're planning on implementing:</p>\n<p><img src=\"http://images.lesswrong.com/t3_5ut_0.png?v=a36683d87cfb12f65548cd2428daddf6\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<p>2. Meetup widget in sidebar, listing the 5 nearest meetups occurring in the next 2 weeks, linking to:<br />(something like right-click &amp; \"Open Image in new tab\" will help here)</p>\n<p><img src=\"http://images.lesswrong.com/t3_5ut_2.png?v=b71ab37d8cd1eb6680719d3c14197d16\" alt=\"\" /></p>\n<p><img src=\"http://images.lesswrong.com/t3_5ut_1.png?v=ef3f41f155cf6e814df2052288fd41d0\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<p>3. New headers:</p>\n<p><img src=\"http://images.lesswrong.com/t3_5ut_3.png?v=78f25bee1b1abfa9b88576ec382ecf5a\" alt=\"\" /></p>\n<p><img src=\"http://images.lesswrong.com/t3_5ut_4.png?v=8c895929c29448f17bc414f9b81f8ea3\" alt=\"\" /></p>\n<p><img src=\"http://images.lesswrong.com/t3_5ut_5.png?v=ee98fd231ce5e2a2d9ee209887506e13\" alt=\"\" /></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FXMQWrkNKcZNagZRS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 39, "extendedScore": null, "score": 7.186561180338501e-07, "legacy": true, "legacyId": "7589", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 75, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-24T12:41:40.354Z", "modifiedAt": null, "url": null, "title": "Dissolution of free will as a call to action", "slug": "dissolution-of-free-will-as-a-call-to-action", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.626Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m4z9z8hGp5GQrTzf5/dissolution-of-free-will-as-a-call-to-action", "pageUrlRelative": "/posts/m4z9z8hGp5GQrTzf5/dissolution-of-free-will-as-a-call-to-action", "linkUrl": "https://www.lesswrong.com/posts/m4z9z8hGp5GQrTzf5/dissolution-of-free-will-as-a-call-to-action", "postedAtFormatted": "Tuesday, May 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dissolution%20of%20free%20will%20as%20a%20call%20to%20action&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADissolution%20of%20free%20will%20as%20a%20call%20to%20action%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4z9z8hGp5GQrTzf5%2Fdissolution-of-free-will-as-a-call-to-action%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dissolution%20of%20free%20will%20as%20a%20call%20to%20action%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4z9z8hGp5GQrTzf5%2Fdissolution-of-free-will-as-a-call-to-action", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4z9z8hGp5GQrTzf5%2Fdissolution-of-free-will-as-a-call-to-action", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>Accepting determinism and the insuing dissolution of free will is often feared as something that would lead to loss of will and fatalism. <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339\">Gary Drescher</a> and <a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution)\">Eliezer</a>&nbsp;spend considerable effort explaining this as a fallacy.&nbsp;</p>\n<p>The one thing I don't remember mentioned is the opposite effect (but maybe I missed it) - if you experienced a failure to accomplish something, the free will explanation is likely to make you stop investigating the root cause, leaving it as a mystery. Once you accept determinism you <strong>know </strong>that a failure is determined by your mental algorithms, and should be much more motivated to push the investigation further, <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">making yourself stronger</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1b8": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m4z9z8hGp5GQrTzf5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 7.187500552957286e-07, "legacy": true, "legacyId": "7592", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-24T12:54:31.986Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Knowing About Biases Can Hurt People", "slug": "seq-rerun-knowing-about-biases-can-hurt-people", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:07.964Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ubnLBvpedG7NwSpfs/seq-rerun-knowing-about-biases-can-hurt-people", "pageUrlRelative": "/posts/ubnLBvpedG7NwSpfs/seq-rerun-knowing-about-biases-can-hurt-people", "linkUrl": "https://www.lesswrong.com/posts/ubnLBvpedG7NwSpfs/seq-rerun-knowing-about-biases-can-hurt-people", "postedAtFormatted": "Tuesday, May 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Knowing%20About%20Biases%20Can%20Hurt%20People&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Knowing%20About%20Biases%20Can%20Hurt%20People%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FubnLBvpedG7NwSpfs%2Fseq-rerun-knowing-about-biases-can-hurt-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Knowing%20About%20Biases%20Can%20Hurt%20People%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FubnLBvpedG7NwSpfs%2Fseq-rerun-knowing-about-biases-can-hurt-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FubnLBvpedG7NwSpfs%2Fseq-rerun-knowing-about-biases-can-hurt-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>Today's post, <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">Knowing About Biases Can Hurt People</a> was originally published on April 4, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>Learning common biases won't help you obtain truth if you only use this knowledge to attack beliefs you don't like. Discussions about biases need to first do no harm by emphasizing motivated cognition, the sophistication effect, and dysrationalia, although even knowledge of these can backfire.</blockquote>\n<p>Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5u7/seq_rerun_the_majority_is_always_wrong/\">The Majority is Always Wrong</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ubnLBvpedG7NwSpfs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 7.187539643617094e-07, "legacy": true, "legacyId": "7593", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AdYdLP2sRqPMoe8fb", "w2tgubpoyrAPckNAt", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-24T19:00:21.968Z", "modifiedAt": null, "url": null, "title": "LessWrong Power Reader (Greasemonkey script, updated)", "slug": "lesswrong-power-reader-greasemonkey-script-updated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:37.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KFCHH2ZtuRvXo7EZZ/lesswrong-power-reader-greasemonkey-script-updated", "pageUrlRelative": "/posts/KFCHH2ZtuRvXo7EZZ/lesswrong-power-reader-greasemonkey-script-updated", "linkUrl": "https://www.lesswrong.com/posts/KFCHH2ZtuRvXo7EZZ/lesswrong-power-reader-greasemonkey-script-updated", "postedAtFormatted": "Tuesday, May 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20Power%20Reader%20(Greasemonkey%20script%2C%20updated)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20Power%20Reader%20(Greasemonkey%20script%2C%20updated)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKFCHH2ZtuRvXo7EZZ%2Flesswrong-power-reader-greasemonkey-script-updated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20Power%20Reader%20(Greasemonkey%20script%2C%20updated)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKFCHH2ZtuRvXo7EZZ%2Flesswrong-power-reader-greasemonkey-script-updated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKFCHH2ZtuRvXo7EZZ%2Flesswrong-power-reader-greasemonkey-script-updated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 451, "htmlBody": "<p>I posted <a href=\"http://www.ibiblio.org/weidai/lesswrong_comments_reade.user.js\">this script</a> previously to Open Thread, but it got broken by the discussion/main split-up and also didn't work in Firefox 4. It's now updated and fixed for Firefox 4. The original description follows. See the <a href=\"/lw/2nz/less_wrong_open_thread_september_2010/2l9m\">previous thread</a> for some additional questions and answers. (ETA: Firefox 4 seems to have made the script much faster, so try it again if you were previously put off by the slowness.)</p>\n<p>For those who may be having trouble keeping up with \"Recent Comments\" or finding the interface a bit plain, I've written a <a rel=\"nofollow\" href=\"https://addons.mozilla.org/en-US/firefox/addon/greasemonkey/\">Greasemonkey</a> script to make it easier/prettier. Here is a <a rel=\"nofollow\" href=\"http://www.ibiblio.org/weidai/lw_screenshot.png\">screenshot</a>.</p>\n<p>Explanation of features:</p>\n<ul>\n<li>loads and threads up to 400 most recent comments on one screen</li>\n<li>use [&uarr;] and [&darr;] to mark favored/disfavored authors</li>\n<li>comments are color coded based on author/points (pink) and recency (yellow)</li>\n<li>replies to you are outlined in red</li>\n<li>hover over [+] to view single collapsed comment</li>\n<li>hover over/click [^] to highlight/scroll to parent comment</li>\n<li>marks comments read (grey) based on scrolling</li>\n<li>shows only new/unread comments upon refresh</li>\n<li>date/time are converted to your local time zone</li>\n<li>click comment date/time for permalink</li>\n</ul>\n<p>To install, first get Greasemonkey, then click <a href=\"http://www.ibiblio.org/weidai/lesswrong_comments_reade.user.js\">here</a>. Once that's done, use <a href=\"/reader\">this link</a> to get to the reader interface.</p>\n<p>I've placed the script is in the public domain. EDIT: Chrome is supported as of version 1.0.5 of the script.</p>\n<p>On a related note, <a href=\"http://www.ibiblio.org/weidai/lesswrong_user.php\">here</a> is a way to view all posts and comments of a particular LW user as a single HTML page.</p>\n<p>EDIT - Version History:</p>\n<ul>\n<li>1.0.5 \n<ul>\n<li>loads comments directly from LW instead of through another server</li>\n<li>added Chrome support</li>\n<li>auto checking/notification of new versions</li>\n<li>can specify a starting point to load comments from (if you want, you can read all LW comments, 400 at a time, by starting at comment ID 1)</li>\n<li>can collapse all comments under a post</li>\n</ul>\n</li>\n<li>1.0.6 (10/4/2011)<br /> \n<ul>\n<li>misc bug fixes</li>\n<li>number of comments loaded changed to 800</li>\n<li>tested on Firefox 7.0 and Chrome 14.0</li>\n</ul>\n</li>\n<li>1.0.7 (11/10/2011)<br /> \n<ul>\n<li>bug fixes</li>\n<li>number of comments loaded changed to 800 on Chrome</li>\n</ul>\n</li>\n<li>1.0.8 (7/16/2012)<br /> \n<ul>\n<li>fixed broken parsing (pengvado)</li>\n<li>tested on Firefox 13.0.1 and Chrome 20.0</li>\n</ul>\n</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KFCHH2ZtuRvXo7EZZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 26, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "7595", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-24T19:45:16.079Z", "modifiedAt": null, "url": null, "title": "Free Stats Textbook: Principles of Uncertainty", "slug": "free-stats-textbook-principles-of-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:21.747Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sxQ2ELCDbKrSdTaW5/free-stats-textbook-principles-of-uncertainty", "pageUrlRelative": "/posts/sxQ2ELCDbKrSdTaW5/free-stats-textbook-principles-of-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/sxQ2ELCDbKrSdTaW5/free-stats-textbook-principles-of-uncertainty", "postedAtFormatted": "Tuesday, May 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Free%20Stats%20Textbook%3A%20Principles%20of%20Uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFree%20Stats%20Textbook%3A%20Principles%20of%20Uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsxQ2ELCDbKrSdTaW5%2Ffree-stats-textbook-principles-of-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Free%20Stats%20Textbook%3A%20Principles%20of%20Uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsxQ2ELCDbKrSdTaW5%2Ffree-stats-textbook-principles-of-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsxQ2ELCDbKrSdTaW5%2Ffree-stats-textbook-principles-of-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<p>Joseph Kadane, emeritus at Carnegie Mellon, released his new statistics textbook <em>Principles of Uncertainty</em>&nbsp;as a <a href=\"http://uncertainty.stat.cmu.edu/\">free pdf</a>. The book is written from a Bayesian perspective, covering basic probability, decision theory, conjugate distribution analysis, hierarchical modeling, MCMC simulation, and game theory. The focus is mathematical, but computation with R is touched on. A solid understanding of calculus seems sufficient to use the book. Curiously, the author devotes a fair number of pages to developing the <a href=\"http://en.wikipedia.org/wiki/Henstock%E2%80%93Kurzweil_integral#McShane_integral\">McShane integral</a>, which is equivalent to Lebesgue integration on the real line. There are lots of other unusual topics you don't normally see in an intermediate statistics textbook.</p>\n<p>Having came across this today, I can't say whether it is actually very good or not, but the range of topics seems perfectly suited to Less Wrong readers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sxQ2ELCDbKrSdTaW5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 29, "extendedScore": null, "score": 7.188742713150759e-07, "legacy": true, "legacyId": "7596", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-24T21:46:53.234Z", "modifiedAt": null, "url": null, "title": "Future of Humanity?", "slug": "future-of-humanity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:06.729Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RickJS", "createdAt": "2009-04-22T15:44:18.517Z", "isAdmin": false, "displayName": "RickJS"}, "userId": "PunGYJhYFqCdAfa8G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CE3nWRny9pCybiCX4/future-of-humanity", "pageUrlRelative": "/posts/CE3nWRny9pCybiCX4/future-of-humanity", "linkUrl": "https://www.lesswrong.com/posts/CE3nWRny9pCybiCX4/future-of-humanity", "postedAtFormatted": "Tuesday, May 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Future%20of%20Humanity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFuture%20of%20Humanity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCE3nWRny9pCybiCX4%2Ffuture-of-humanity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Future%20of%20Humanity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCE3nWRny9pCybiCX4%2Ffuture-of-humanity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCE3nWRny9pCybiCX4%2Ffuture-of-humanity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>I first attempted to post this in 2009, but bounced off the karma wall. &nbsp;Since then, MY forgetfulness and procrastination have been its nemesis.</p>\n<p>I invite you to listen (read) in an unusual way. \"Consider it\": think WITH this idea for a while. There will be plenty of time to refute it later. I find that, if I START with, \"That's so wrong!\", I <strong><span style=\"font-weight: bold;\">really</span></strong> weaken my ability to \"pan for the gold\".</p>\n<h1>Remember the Swamp!</h1>\n<p><a href=\"http://en.wiktionary.org/wiki/when_you're_up_to_your_neck_in_alligators,_it's_easy_to_forget_that_the_initial_objective_was_to_drain_the_swamp\"></a><a title=\"Use CTRL + click or middle-click to open in a new tab\" href=\"javascript:void(0);\">http://en.wiktionary.org/wiki/when_you</a>'re_up_to_your_neck_in_alligators,_it's_easy_to_forget_that_the_initial_objective_was_to_drain_the_swamp</p>\n<p>I looked over the tag cloud and didn't see:</p>\n<ul>\n<li>Existential Risk</li>\n<li>War</li>\n<li>Aggression</li>\n<li>Competitveness</li>\n<li>Territorialism</li>\n<li>Nuclear arsenals</li>\n</ul>\n<p class=\"NoSpacing\"><a id=\"more\"></a>Admittedly, searching on war did provide a few results. &nbsp;</p>\n<p class=\"NoSpacing\">I invite you all to let this concern guide your posts: humanity is on a path toward multiple futures, and way too many (IMHO) of them include our extinction or the destruction of our civilization. &nbsp;Not many people even think about it. &nbsp;We have become so <span style=\"font-weight: bold;\">numb </span>to the nuclear fear that was so palpable in the 1950s - early 1960s. &nbsp;When I talk to people about it, they remember, and most agree thet the huge nuclear arsenals are a really bad idea. &nbsp;But, it's not present in their daily thought.</p>\n<p class=\"NoSpacing\">Think big and think far.</p>\n<p class=\"NoSpacing\">Think about how we can have the great mass of people, not just a few rationalists, become Less Wrong about what is important.</p>\n<p class=\"NoSpacing\">Thanks for your <span style=\"font-weight: bold;\">Consideration</span>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CE3nWRny9pCybiCX4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -26, "extendedScore": null, "score": 7.189099012279506e-07, "legacy": true, "legacyId": "449", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-25T00:02:56.858Z", "modifiedAt": null, "url": null, "title": "Self-programming through spaced repetition", "slug": "self-programming-through-spaced-repetition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.073Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "YpTmfmnMjgakwFRQQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iocMeRcSKf3gAxsTh/self-programming-through-spaced-repetition", "pageUrlRelative": "/posts/iocMeRcSKf3gAxsTh/self-programming-through-spaced-repetition", "linkUrl": "https://www.lesswrong.com/posts/iocMeRcSKf3gAxsTh/self-programming-through-spaced-repetition", "postedAtFormatted": "Wednesday, May 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self-programming%20through%20spaced%20repetition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf-programming%20through%20spaced%20repetition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiocMeRcSKf3gAxsTh%2Fself-programming-through-spaced-repetition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self-programming%20through%20spaced%20repetition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiocMeRcSKf3gAxsTh%2Fself-programming-through-spaced-repetition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiocMeRcSKf3gAxsTh%2Fself-programming-through-spaced-repetition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 722, "htmlBody": "<p><span style=\"border-collapse: collapse; color: #444444; font-family: arial, sans-serif; font-size: 13px;\"> </span></p>\n<div><span style=\"font-family: arial, helvetica, sans-serif;\">Spaced repetition software is a flashcard memorization technology based on the&nbsp;<a style=\"color: #222222;\" href=\"http://en.wikipedia.org/wiki/Spacing_effect\" target=\"_blank\">spacing effect</a>. Personally, I think of it as a way of engineering dispositions, a form of self-programming. More concretely, I find that spaced repetition is helpful for</span></div>\n<div>\n<ul>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">internalizing knowledge&nbsp;</span></li>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">compressing recent experiences&nbsp;</span></li>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">conditioning specific future behaviors&nbsp;</span></li>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">making analogies&nbsp;</span></li>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">laying the groundwork for future insights&nbsp;</span></li>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">confusion identification&nbsp;</span></li>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">concept clarification&nbsp;</span></li>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">reconciling models&nbsp;</span></li>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">creating new representations&nbsp;</span></li>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">creating examples&nbsp;</span></li>\n</ul>\n</div>\n<div><span style=\"font-family: arial, helvetica, sans-serif;\">Below I give some principles, tips, and examples that aim at helping you get the most out of SRS. This post is compact, and I think it will be helpful to re-read it periodically as you use SRS more.</span></div>\n<div><a id=\"more\"></a></div>\n<div><br /></div>\n<div><span style=\"text-decoration: underline;\"><span style=\"font-family: arial, helvetica, sans-serif;\">Principles</span></span></div>\n<div>\n<ul>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">SR strengthens connections between mental representations.</span></li>\n</ul>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">There a variety of ways this can happen, seeing as there is both a variety of mental representations and connections between them. For example, the two mental representations could be of a context and a behavior, and strengthening the connection would mean making the behavior more likely in that context.&nbsp;</span></p>\n<ul>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">Mental representations precisely condition behavior.</span></li>\n</ul>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">The point of doing SR is to change behavior (whether mental or physical) and I think it helps to keep the chain of causality in mind. It gives me a framework to think about the different things SR does for me, and how those things are achieved.</span></p>\n<ul>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">SR establishes a personal language of thought.</span></li>\n</ul>\n</div>\n<div><span style=\"font-family: arial, helvetica, sans-serif;\">Cards about situations or concepts give you a canonical handle/representation for thinking about them (like Eliezer's clever post titles). This can have positive effects (consistency of thought and building potential) as well as negative effects (potential ridigity of thought).</span></div>\n<div><br /></div>\n<div><span style=\"text-decoration: underline;\"><span style=\"font-family: arial, helvetica, sans-serif;\">Tips</span></span></div>\n<div>\n<ul>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">Be very specific when conditioning behaviors.</span></li>\n</ul>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">Make cards that follow the form \"Do actionable_behavior_x in specific_situation_y under condition_z.\" This will limit confusion about whether you should execute the behavior and also make it more likely that you'll remember. For example, \"If you're eating when not hungry, reflect on what you're suffering from.\"</span></p>\n<ul>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">Use one unit of meaning per card.</span></li>\n</ul>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">This is similar to Piotr Wozniak's minimum information principle, which is one of his&nbsp;<a href=\"http://www.supermemo.com/articles/20rules.htm\">20 rules for formulating knowledge</a>&nbsp; (totally recommended). I almost entirely use cloze-deletion cards and I think it's easier to follow this advice with such cards. Cloze-deletion cards are made by deleting parts of a sentence (or an image). For example, starting with the sentence \"A lost purpose is a subgoal that no longer serves its supergoal.\" I made the cards</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">\"A [lost purpose] is a subgoal that no longer serves its supergoal.\"</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">\"A lost purpose is a [subgoal] that no longer serves its supergoal.\"</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">\"A lost purpose is a subgoal that [no longer serves] its supergoal.\"</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">\"A lost purpose is a subgoal that no longer serves its [supergoal].\"</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">where the part in brackets is blank on that card.</span></p>\n<ul>\n<li style=\"margin-left: 15px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">Discover your own SR style.</span></li>\n</ul>\n</div>\n<div><span style=\"font-family: arial, helvetica, sans-serif;\">I find that a small number of cards that are carefully phrased and highly compressed work well for me. Others use larger numbers of cards with less information per card, and then internally organize and compress the information. Try making different kinds of cards and using other people's decks to figure out what works best for you. You can also vary how long you spend recalling and reflecting on each card during your session. I sometimes spend up to a minute reflecting on a card, during which I do some of the activities I listed above.</span></div>\n<div><br /></div>\n<div><span style=\"text-decoration: underline;\"><span style=\"font-family: arial, helvetica, sans-serif;\">Examples - situational questions</span></span></div>\n<div>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">Some of my cards are aimed at conditioning myself to ask questions in specific situations. Here's one card inspired by&nbsp;<a href=\"/user/divia\">divia</a>:</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">Front:&nbsp;When you become aware that you are making a social judgment what should you ask yourself?</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">Back:&nbsp;What need of mine does this reflect?</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">This card has greatly helped me identify my unfulfilled social needs and outstanding concerns about my own social behavior. At the same time it has helped me increase my ability to empathize with others, and capacity to meet their social needs.&nbsp;</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">Here's another similar card:</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">Front:&nbsp;When you notice yourself pulling your hair what should you do?</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">Back: Reflect on what you were just thinking about.</span></p>\n<p><span style=\"font-family: arial, helvetica, sans-serif;\">Sometimes I run my fingers through my hair when I'm stressed out. I made this card in order to use this habit to become more aware of when I'm stressed, and what my sources of stress are. It has served that purpose fairly well.</span></p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iocMeRcSKf3gAxsTh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 35, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "7597", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-25T00:32:25.088Z", "modifiedAt": null, "url": null, "title": "LW Biology 101 Introduction: Constraining Anticipation", "slug": "lw-biology-101-introduction-constraining-anticipation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:56.673Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "virtualAdept", "createdAt": "2011-01-12T19:31:31.692Z", "isAdmin": false, "displayName": "virtualAdept"}, "userId": "cAggP7JZzrWwJCLEX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WJckvR7zgyx36396P/lw-biology-101-introduction-constraining-anticipation", "pageUrlRelative": "/posts/WJckvR7zgyx36396P/lw-biology-101-introduction-constraining-anticipation", "linkUrl": "https://www.lesswrong.com/posts/WJckvR7zgyx36396P/lw-biology-101-introduction-constraining-anticipation", "postedAtFormatted": "Wednesday, May 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20Biology%20101%20Introduction%3A%20Constraining%20Anticipation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20Biology%20101%20Introduction%3A%20Constraining%20Anticipation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWJckvR7zgyx36396P%2Flw-biology-101-introduction-constraining-anticipation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20Biology%20101%20Introduction%3A%20Constraining%20Anticipation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWJckvR7zgyx36396P%2Flw-biology-101-introduction-constraining-anticipation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWJckvR7zgyx36396P%2Flw-biology-101-introduction-constraining-anticipation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1733, "htmlBody": "<p><em>Since the responses to <a href=\"/r/discussion/lw/5sg/seeking_suggestions_less_wrong_biology_101/\" target=\"_self\">my recent inquiry</a>&nbsp;were positive, I've rolled up my sleeves and gotten started. &nbsp;Special thanks to badger for eir <a href=\"/r/discussion/lw/5sg/seeking_suggestions_less_wrong_biology_101/47xv\" target=\"_self\">comment</a>&nbsp;in that thread, as it inspired the framework used here. &nbsp;</em></p>\n<p class=\"MsoNormalCxSpFirst\" style=\"line-height:normal\">My intent in the upcoming posts is to offer a practical overview of biological topics of both broad-scale importance and particular interest to the Less Wrong community.<span style=\"mso-spacerun:yes\">&nbsp; </span>This will by no means be exhaustive (else I&rsquo;d be writing a textbook instead, or more likely, you&rsquo;d be reading one); instead I am going to attempt to sketch what amounts to a map of several parts of the discipline &ndash; where they stand in relation to other fields, where we are in the progress of their development, and their boundaries and frontiers.<span style=\"mso-spacerun:yes\">&nbsp; </span>I&rsquo;d like this to be a continually improving project as well, so I would very much welcome input on content relevance and clarity for any and all posts.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">I will list relevant/useful references for more in-depth reading at the end of each post.<span style=\"mso-spacerun:yes\">&nbsp; </span>The majority of in-text links will be used to provide a quick explanation of terms that may not be familiar or phenomena that may not be obvious.<span style=\"mso-spacerun:yes\">&nbsp; </span>If the terms are familiar to you, you probably do not need to worry about those links.<span style=\"mso-spacerun:yes\">&nbsp; </span>A significant minority of in-text links may or may not be purely for amusement.<span style=\"mso-spacerun:yes\"> <a id=\"more\"></a><br /></span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">It is a popular <a href=\"http://www.xkcd.com/435/\" target=\"_self\">half-joke</a> that biology is applied chemistry is applied physics is applied math.<span style=\"mso-spacerun:yes\">&nbsp; </span>While it&rsquo;s certainly necessary to apply all the usual considerations for a chemical system to a biological system or problem, there are some overall complications and themes that specifically (though not uniquely) apply to biological problems, and it is useful to keep them in mind.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\">1.<span style=\"mso-spacerun:yes\">&nbsp; </span>Biological processes are <a href=\"http://en.wikipedia.org/wiki/Stochastic_process\" target=\"_self\">stochastic</a>.</strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Cellular-scale chemistry is an event-dense environment, but the abundance of most reactants is generally quite low.<span style=\"mso-spacerun:yes\">&nbsp; </span>(Exceptions typically include oxygen, carbon dioxide, water, and small ions.)<span style=\"mso-spacerun:yes\">&nbsp; </span>Beyond the basic consideration of abundance, there are other <a href=\"http://en.wikipedia.org/wiki/Allosteric_regulation\" target=\"_self\">layers</a> of regulation that determine whether a given entity, usually a protein, can actually react at any given time, and complicated geometries involved that further decrease the frequency of a given reaction.<span style=\"mso-spacerun:yes\">&nbsp; </span>We therefore must consider the majority of reactions as discrete, and model them stochastically.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">If we take a step up to the scale of cells in culture &ndash; to give a ballpark idea, we&rsquo;re talking on the order of 10<sup>6</sup>-10<sup>9</sup> cells/mL for yeast or bacteria &ndash; the overall flux of nutrients, waste, and other metabolites becomes much more stable across samples under uniform conditions.<span style=\"mso-spacerun:yes\">&nbsp; </span>However, even with relatively well-behaved cells in liquid culture, sample variability is such that it is necessary to take replicate samples for each condition when using these in an experiment.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Taking another (large) step up the complexity hierarchy to consider multicellular organisms (and let&rsquo;s make them genetically identical, as in laboratory strains of fruit flies or worms), we now have aggregates of individual stochastic processes, which themselves exhibit stochasticity at the organism level.<span style=\"mso-spacerun:yes\">&nbsp; </span>And, as you might expect, the same holds for bigger, more complicated, and genetically non-identical organism.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><span style=\"text-decoration: underline;\">The functional implications of this behavior are:</span></p>\n<ul>\n<li>If we wish to model biochemical processes on a molecular scale, we must account for stochastic behavior.</li>\n<li>Careful statistical analysis is intrinsically necessary to studying biological systems at all levels.<span style=\"mso-spacerun:yes\">&nbsp;</span></li>\n</ul>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\">2. Biological systems are complex networks, and variable deconvolution is nontrivial.<span style=\"mso-spacerun:yes\">&nbsp;</span></strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Stochastic processes aren&rsquo;t especially difficult to model if you have access to accurate probability predictions for the set of possible events&hellip;<span style=\"mso-spacerun:yes\">&nbsp; </span>but getting those accurate predictions is a process that in biological systems is painstaking at best.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">If you&rsquo;re trying to model a single node in a biochemical pathway &ndash; say, an enzyme catalyzing the cyclization of a linear <a href=\"http://en.wikipedia.org/wiki/2,3-Oxidosqualene\" target=\"_self\">cholesterol precursor</a> - you have to consider the upstream and downstream reactions&rsquo; effect on that node, the interaction of your whole pathway with other biochemical pathways in the cell, and then the chemical environment of that cell, which could be as simple as a uniform flask of culture medium or as complex as a human tissue, which is within a human body, which&hellip; you get the idea.<span style=\"mso-spacerun:yes\">&nbsp; </span>(This is somewhat hyperbolic for purposes of illustration.<span style=\"mso-spacerun:yes\">&nbsp; </span>In reality, you can probably get away with assuming either a near-constant culture medium or some known dynamic cycle of states in a multicellular tissue, depending on the application you&rsquo;re looking at.)<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">In other words, there are a <em style=\"mso-bidi-font-style:normal\">lot</em> of variables, and almost none of them are fully independent.<span style=\"mso-spacerun:yes\">&nbsp; </span>It is therefore necessary to expend a great deal of time and resources to characterize these variables, relative to non-biological systems.<span style=\"mso-spacerun:yes\">&nbsp; </span>Even for our most-studied, favorite single-celled organisms, whose genomes we&rsquo;ve sequenced and whose <a href=\"http://en.wikipedia.org/wiki/Metabolism\" target=\"_self\">metabolisms</a> we&rsquo;ve begun to model, there are still huge blank areas in our lists of variable definitions.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">I&rsquo;ll be discussing experimental paradigms and difficulties with specific systems in later posts, as well as the methods used to deal with them.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\">3. Due to (1) and (2), modeling efforts are heavily limited by computing power.<span style=\"mso-spacerun:yes\">&nbsp;</span></strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">This is worth mentioning here, but fairly self-explanatory, although it&rsquo;s also worth noting that a specific consequence of the interconnectedness of system branches is a vast range of time scales on which events occur.<span style=\"mso-spacerun:yes\">&nbsp; </span>This time scale diversity makes system-level modeling a very <a href=\"http://en.wikipedia.org/wiki/Stiff_equation\" target=\"_self\">stiff</a> endeavor.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">I&rsquo;ll go into some of the more successful and potentially successful modeling strategies for various systems in later posts.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\">4. Information transfer is error-prone in biological systems.<span style=\"mso-spacerun:yes\">&nbsp;</span></strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Genes are replicated by a sequential polymerization reaction that constructs a new strand of DNA using the old one as a template.<span style=\"mso-spacerun:yes\">&nbsp; </span>Each time a monomer is added to the new strand, there is a small* chance that the incorrect type of monomer will be added, and a smaller chance that the error will not be recognized and corrected by proofreading mechanisms.<span style=\"mso-spacerun:yes\">&nbsp; </span>Since genes are on the order of thousands of monomer units (&lsquo;<a href=\"http://en.wikipedia.org/wiki/Dna#Base_pairing\" target=\"_self\">bases</a>&rsquo;) long, in aggregate these mutations have a large chance of at least some occurrence over the course of the cell&rsquo;s lifetime.<span style=\"mso-spacerun:yes\">&nbsp; </span>(Mutation probabilities differ by organism, by gene, and in accordance to a host of other factors.)<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Aside from this familiar form of error in the genetic code itself, abnormalities can also occur in how DNA is partitioned between new units in a dividing cell, and on a less permanent level, there are many phenomena that amount to &lsquo;miscommunication&rsquo; between parts of a cell, or whole cells or organs.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><span style=\"text-decoration: underline;\">Functionally speaking,</span> this rather patchy scheme of information fidelity gives rise to the phenomenon of evolution, a great deal of useful experimental methods oriented around inducing mutations, and the occasional thorn in the side of a researcher who has suddenly found eir cell line has lost a trait that it needed to have or gained one it didn&rsquo;t.<span style=\"mso-spacerun:yes\">&nbsp; </span>On the broad scale, it also makes the study of biological systems something of a moving target, particularly in mutation-prone systems such as human pathogens.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">* This chance has actually been estimated for various situations, and tables of these estimates are used heavily in <a href=\"http://en.wikipedia.org/wiki/Phylogeny\" target=\"_self\">mapping evolution</a>.<span style=\"mso-spacerun:yes\">&nbsp; </span>(NB: This inference is descriptive only; it is NOT predictive.)<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\">5. Biological processes are limited to life-sustaining conditions.<span style=\"mso-spacerun:yes\">&nbsp;</span></strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">&hellip;Or, in chemical engineering terms, it is not advisable to blow up your reactor.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">The kinds of chemistry we can convince cells to do for us are those that are not toxic to the cell, and that do not completely overwhelm the cell&rsquo;s ability to handle its own biochemical needs.<span style=\"mso-spacerun:yes\">&nbsp; </span>There are ways to partially circumvent this &ndash; you can sometimes get away with <em style=\"mso-bidi-font-style:normal\">slightly</em> toxic products in an engineered metabolic pathway, or if you have to completely hijack some part of the cell&rsquo;s essential machinery you can sometimes provide it with whatever it&rsquo;s missing externally &ndash; but it&rsquo;s a rule that can only be bent so far before you&rsquo;ve got dead cells on your hands.<span style=\"mso-spacerun:yes\">&nbsp; </span>(And chances are, unless they were cancer cells, that&rsquo;s not what you wanted.)<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Biological systems also exhibit a high degree of organization, allowing the partitioning of microenvironments necessary to support the full spectrum of biochemical reactions.<span style=\"mso-spacerun:yes\">&nbsp; </span>The maintenance of this organization is just as vital as temperature and pH homeostasis, and the avoidance of toxin buildup.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\">6. Biological processes are transport-constrained.<span style=\"mso-spacerun:yes\">&nbsp;</span></strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">For similar, but more complex reasons as (1), chemical transport is a Big Deal in biology from the cellular level all the way to a clinical setting.<span style=\"mso-spacerun:yes\">&nbsp; </span>Due to (5), we can&rsquo;t just put everything in a blender and assume perfect mixing (though I&rsquo;m sure <a href=\"http://www.willitblend.com/\" target=\"_self\">some people</a> would be happy to try), so to a certain extent on a cellular level (depending on the complexity of your cells and what you&rsquo;re trying to make them do), and to an all-consuming extent on an organism level, biological problems contain transport problems.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">The easiest illustrative example of this is to consider cancer, and conventional chemotherapy treatment.<span style=\"mso-spacerun:yes\">&nbsp; </span>You&rsquo;ve got a patient with a tumor, and they&rsquo;re receiving chemotherapy.<span style=\"mso-spacerun:yes\">&nbsp; </span>The chemotherapeutic chemicals are injected into the bloodstream, which they then ride through the body to the tumor, and get to work.<span style=\"mso-spacerun:yes\">&nbsp; </span>Except&hellip; since they took the scenic route getting there, they&rsquo;ve also come into contact with a lot of erstwhile healthy tissue, which they have also attacked, producing the host of nasty side effects that comes with chemo.<span style=\"mso-spacerun:yes\">&nbsp; </span>You could inject the drugs directly into the tumor or the tissue surrounding it, but then you&rsquo;ve got to hope the drugs manage to diffuse far enough into the tumor to do some good despite the fact that they aren&rsquo;t riding any blood vessels.<span style=\"mso-spacerun:yes\">&nbsp; </span>This is the sort of transport-focused engineering problem that is necessary to solve in some capacity for nearly all clinical applications.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\">Given these considerations</strong>, much of our most productive, ground-breaking research** in biology and bioengineering today is focused on:</p>\n<ul>\n<li>Finding new ways to model systems of interest</li>\n<li>Finding more efficient ways to update our existing models and understanding (more efficient variable characterization)</li>\n<li>Designing streamlined, well-behaved systems based on our emerging understanding of how all these processes work individually and link together (&lsquo;plug and play&rsquo; biology)</li>\n</ul>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">**Reflects my engineering-slanted opinion on the future of biology, as well as that of most people who are doing bioinformatics and are excited about it, but could be open to contrary opinion.<span style=\"mso-spacerun:yes\">&nbsp; </span>&lsquo;Fastest-developing&rsquo; would perhaps be a better description.<span style=\"mso-spacerun:yes\">&nbsp; </span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">&nbsp;</p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\">Useful/interesting references consulted for this section:</strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><a href=\"http://www.amazon.com/Lehninger-Principles-Biochemistry-Fourth-Nelson/dp/0716743396/ref=sr_1_1?ie=UTF8&amp;qid=1306282641&amp;sr=8-1\" target=\"_self\">Lehninger Principles of Biochemistry</a>, 4th ed., by David L. Nelson and Michael M. Cox</p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><a href=\"http://www.amazon.com/Molecular-Biology-Fourth-Bruce-Alberts/dp/0815332181/ref=sr_1_2?ie=UTF8&amp;qid=1306282751&amp;sr=8-2\" target=\"_self\">Molecular Biology of the Cell</a>, 4th ed., by Alberts, Johnson, Lewis, Raff, Roberts, and Walter</p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><a href=\"http://www.amazon.com/Receptors-Models-Binding-Trafficking-Signaling/dp/0195106636/ref=sr_1_1?ie=UTF8&amp;qid=1306282827&amp;sr=8-1\" target=\"_self\">Receptors: Models for Binding, Trafficking, and Signaling</a>, by Douglas A. Lauffenburger and Jennifer J. Linderman</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jaf5zfcGgCB2REXGw": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WJckvR7zgyx36396P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 69, "extendedScore": null, "score": 0.000136, "legacy": true, "legacyId": "7599", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 69, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Since the responses to <a href=\"/r/discussion/lw/5sg/seeking_suggestions_less_wrong_biology_101/\" target=\"_self\">my recent inquiry</a>&nbsp;were positive, I've rolled up my sleeves and gotten started. &nbsp;Special thanks to badger for eir <a href=\"/r/discussion/lw/5sg/seeking_suggestions_less_wrong_biology_101/47xv\" target=\"_self\">comment</a>&nbsp;in that thread, as it inspired the framework used here. &nbsp;</em></p>\n<p class=\"MsoNormalCxSpFirst\" style=\"line-height:normal\">My intent in the upcoming posts is to offer a practical overview of biological topics of both broad-scale importance and particular interest to the Less Wrong community.<span style=\"mso-spacerun:yes\">&nbsp; </span>This will by no means be exhaustive (else I\u2019d be writing a textbook instead, or more likely, you\u2019d be reading one); instead I am going to attempt to sketch what amounts to a map of several parts of the discipline \u2013 where they stand in relation to other fields, where we are in the progress of their development, and their boundaries and frontiers.<span style=\"mso-spacerun:yes\">&nbsp; </span>I\u2019d like this to be a continually improving project as well, so I would very much welcome input on content relevance and clarity for any and all posts.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">I will list relevant/useful references for more in-depth reading at the end of each post.<span style=\"mso-spacerun:yes\">&nbsp; </span>The majority of in-text links will be used to provide a quick explanation of terms that may not be familiar or phenomena that may not be obvious.<span style=\"mso-spacerun:yes\">&nbsp; </span>If the terms are familiar to you, you probably do not need to worry about those links.<span style=\"mso-spacerun:yes\">&nbsp; </span>A significant minority of in-text links may or may not be purely for amusement.<span style=\"mso-spacerun:yes\"> <a id=\"more\"></a><br></span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">It is a popular <a href=\"http://www.xkcd.com/435/\" target=\"_self\">half-joke</a> that biology is applied chemistry is applied physics is applied math.<span style=\"mso-spacerun:yes\">&nbsp; </span>While it\u2019s certainly necessary to apply all the usual considerations for a chemical system to a biological system or problem, there are some overall complications and themes that specifically (though not uniquely) apply to biological problems, and it is useful to keep them in mind.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\" id=\"1___Biological_processes_are_stochastic_\">1.<span style=\"mso-spacerun:yes\">&nbsp; </span>Biological processes are <a href=\"http://en.wikipedia.org/wiki/Stochastic_process\" target=\"_self\">stochastic</a>.</strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Cellular-scale chemistry is an event-dense environment, but the abundance of most reactants is generally quite low.<span style=\"mso-spacerun:yes\">&nbsp; </span>(Exceptions typically include oxygen, carbon dioxide, water, and small ions.)<span style=\"mso-spacerun:yes\">&nbsp; </span>Beyond the basic consideration of abundance, there are other <a href=\"http://en.wikipedia.org/wiki/Allosteric_regulation\" target=\"_self\">layers</a> of regulation that determine whether a given entity, usually a protein, can actually react at any given time, and complicated geometries involved that further decrease the frequency of a given reaction.<span style=\"mso-spacerun:yes\">&nbsp; </span>We therefore must consider the majority of reactions as discrete, and model them stochastically.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">If we take a step up to the scale of cells in culture \u2013 to give a ballpark idea, we\u2019re talking on the order of 10<sup>6</sup>-10<sup>9</sup> cells/mL for yeast or bacteria \u2013 the overall flux of nutrients, waste, and other metabolites becomes much more stable across samples under uniform conditions.<span style=\"mso-spacerun:yes\">&nbsp; </span>However, even with relatively well-behaved cells in liquid culture, sample variability is such that it is necessary to take replicate samples for each condition when using these in an experiment.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Taking another (large) step up the complexity hierarchy to consider multicellular organisms (and let\u2019s make them genetically identical, as in laboratory strains of fruit flies or worms), we now have aggregates of individual stochastic processes, which themselves exhibit stochasticity at the organism level.<span style=\"mso-spacerun:yes\">&nbsp; </span>And, as you might expect, the same holds for bigger, more complicated, and genetically non-identical organism.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><span style=\"text-decoration: underline;\">The functional implications of this behavior are:</span></p>\n<ul>\n<li>If we wish to model biochemical processes on a molecular scale, we must account for stochastic behavior.</li>\n<li>Careful statistical analysis is intrinsically necessary to studying biological systems at all levels.<span style=\"mso-spacerun:yes\">&nbsp;</span></li>\n</ul>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\" id=\"2__Biological_systems_are_complex_networks__and_variable_deconvolution_is_nontrivial__\">2. Biological systems are complex networks, and variable deconvolution is nontrivial.<span style=\"mso-spacerun:yes\">&nbsp;</span></strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Stochastic processes aren\u2019t especially difficult to model if you have access to accurate probability predictions for the set of possible events\u2026<span style=\"mso-spacerun:yes\">&nbsp; </span>but getting those accurate predictions is a process that in biological systems is painstaking at best.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">If you\u2019re trying to model a single node in a biochemical pathway \u2013 say, an enzyme catalyzing the cyclization of a linear <a href=\"http://en.wikipedia.org/wiki/2,3-Oxidosqualene\" target=\"_self\">cholesterol precursor</a> - you have to consider the upstream and downstream reactions\u2019 effect on that node, the interaction of your whole pathway with other biochemical pathways in the cell, and then the chemical environment of that cell, which could be as simple as a uniform flask of culture medium or as complex as a human tissue, which is within a human body, which\u2026 you get the idea.<span style=\"mso-spacerun:yes\">&nbsp; </span>(This is somewhat hyperbolic for purposes of illustration.<span style=\"mso-spacerun:yes\">&nbsp; </span>In reality, you can probably get away with assuming either a near-constant culture medium or some known dynamic cycle of states in a multicellular tissue, depending on the application you\u2019re looking at.)<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">In other words, there are a <em style=\"mso-bidi-font-style:normal\">lot</em> of variables, and almost none of them are fully independent.<span style=\"mso-spacerun:yes\">&nbsp; </span>It is therefore necessary to expend a great deal of time and resources to characterize these variables, relative to non-biological systems.<span style=\"mso-spacerun:yes\">&nbsp; </span>Even for our most-studied, favorite single-celled organisms, whose genomes we\u2019ve sequenced and whose <a href=\"http://en.wikipedia.org/wiki/Metabolism\" target=\"_self\">metabolisms</a> we\u2019ve begun to model, there are still huge blank areas in our lists of variable definitions.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">I\u2019ll be discussing experimental paradigms and difficulties with specific systems in later posts, as well as the methods used to deal with them.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\" id=\"3__Due_to__1__and__2___modeling_efforts_are_heavily_limited_by_computing_power__\">3. Due to (1) and (2), modeling efforts are heavily limited by computing power.<span style=\"mso-spacerun:yes\">&nbsp;</span></strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">This is worth mentioning here, but fairly self-explanatory, although it\u2019s also worth noting that a specific consequence of the interconnectedness of system branches is a vast range of time scales on which events occur.<span style=\"mso-spacerun:yes\">&nbsp; </span>This time scale diversity makes system-level modeling a very <a href=\"http://en.wikipedia.org/wiki/Stiff_equation\" target=\"_self\">stiff</a> endeavor.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">I\u2019ll go into some of the more successful and potentially successful modeling strategies for various systems in later posts.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\" id=\"4__Information_transfer_is_error_prone_in_biological_systems__\">4. Information transfer is error-prone in biological systems.<span style=\"mso-spacerun:yes\">&nbsp;</span></strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Genes are replicated by a sequential polymerization reaction that constructs a new strand of DNA using the old one as a template.<span style=\"mso-spacerun:yes\">&nbsp; </span>Each time a monomer is added to the new strand, there is a small* chance that the incorrect type of monomer will be added, and a smaller chance that the error will not be recognized and corrected by proofreading mechanisms.<span style=\"mso-spacerun:yes\">&nbsp; </span>Since genes are on the order of thousands of monomer units (\u2018<a href=\"http://en.wikipedia.org/wiki/Dna#Base_pairing\" target=\"_self\">bases</a>\u2019) long, in aggregate these mutations have a large chance of at least some occurrence over the course of the cell\u2019s lifetime.<span style=\"mso-spacerun:yes\">&nbsp; </span>(Mutation probabilities differ by organism, by gene, and in accordance to a host of other factors.)<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Aside from this familiar form of error in the genetic code itself, abnormalities can also occur in how DNA is partitioned between new units in a dividing cell, and on a less permanent level, there are many phenomena that amount to \u2018miscommunication\u2019 between parts of a cell, or whole cells or organs.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><span style=\"text-decoration: underline;\">Functionally speaking,</span> this rather patchy scheme of information fidelity gives rise to the phenomenon of evolution, a great deal of useful experimental methods oriented around inducing mutations, and the occasional thorn in the side of a researcher who has suddenly found eir cell line has lost a trait that it needed to have or gained one it didn\u2019t.<span style=\"mso-spacerun:yes\">&nbsp; </span>On the broad scale, it also makes the study of biological systems something of a moving target, particularly in mutation-prone systems such as human pathogens.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">* This chance has actually been estimated for various situations, and tables of these estimates are used heavily in <a href=\"http://en.wikipedia.org/wiki/Phylogeny\" target=\"_self\">mapping evolution</a>.<span style=\"mso-spacerun:yes\">&nbsp; </span>(NB: This inference is descriptive only; it is NOT predictive.)<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\" id=\"5__Biological_processes_are_limited_to_life_sustaining_conditions__\">5. Biological processes are limited to life-sustaining conditions.<span style=\"mso-spacerun:yes\">&nbsp;</span></strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">\u2026Or, in chemical engineering terms, it is not advisable to blow up your reactor.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">The kinds of chemistry we can convince cells to do for us are those that are not toxic to the cell, and that do not completely overwhelm the cell\u2019s ability to handle its own biochemical needs.<span style=\"mso-spacerun:yes\">&nbsp; </span>There are ways to partially circumvent this \u2013 you can sometimes get away with <em style=\"mso-bidi-font-style:normal\">slightly</em> toxic products in an engineered metabolic pathway, or if you have to completely hijack some part of the cell\u2019s essential machinery you can sometimes provide it with whatever it\u2019s missing externally \u2013 but it\u2019s a rule that can only be bent so far before you\u2019ve got dead cells on your hands.<span style=\"mso-spacerun:yes\">&nbsp; </span>(And chances are, unless they were cancer cells, that\u2019s not what you wanted.)<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">Biological systems also exhibit a high degree of organization, allowing the partitioning of microenvironments necessary to support the full spectrum of biochemical reactions.<span style=\"mso-spacerun:yes\">&nbsp; </span>The maintenance of this organization is just as vital as temperature and pH homeostasis, and the avoidance of toxin buildup.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\" id=\"6__Biological_processes_are_transport_constrained__\">6. Biological processes are transport-constrained.<span style=\"mso-spacerun:yes\">&nbsp;</span></strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">For similar, but more complex reasons as (1), chemical transport is a Big Deal in biology from the cellular level all the way to a clinical setting.<span style=\"mso-spacerun:yes\">&nbsp; </span>Due to (5), we can\u2019t just put everything in a blender and assume perfect mixing (though I\u2019m sure <a href=\"http://www.willitblend.com/\" target=\"_self\">some people</a> would be happy to try), so to a certain extent on a cellular level (depending on the complexity of your cells and what you\u2019re trying to make them do), and to an all-consuming extent on an organism level, biological problems contain transport problems.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">The easiest illustrative example of this is to consider cancer, and conventional chemotherapy treatment.<span style=\"mso-spacerun:yes\">&nbsp; </span>You\u2019ve got a patient with a tumor, and they\u2019re receiving chemotherapy.<span style=\"mso-spacerun:yes\">&nbsp; </span>The chemotherapeutic chemicals are injected into the bloodstream, which they then ride through the body to the tumor, and get to work.<span style=\"mso-spacerun:yes\">&nbsp; </span>Except\u2026 since they took the scenic route getting there, they\u2019ve also come into contact with a lot of erstwhile healthy tissue, which they have also attacked, producing the host of nasty side effects that comes with chemo.<span style=\"mso-spacerun:yes\">&nbsp; </span>You could inject the drugs directly into the tumor or the tissue surrounding it, but then you\u2019ve got to hope the drugs manage to diffuse far enough into the tumor to do some good despite the fact that they aren\u2019t riding any blood vessels.<span style=\"mso-spacerun:yes\">&nbsp; </span>This is the sort of transport-focused engineering problem that is necessary to solve in some capacity for nearly all clinical applications.<span style=\"mso-spacerun:yes\">&nbsp;</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\">Given these considerations</strong>, much of our most productive, ground-breaking research** in biology and bioengineering today is focused on:</p>\n<ul>\n<li>Finding new ways to model systems of interest</li>\n<li>Finding more efficient ways to update our existing models and understanding (more efficient variable characterization)</li>\n<li>Designing streamlined, well-behaved systems based on our emerging understanding of how all these processes work individually and link together (\u2018plug and play\u2019 biology)</li>\n</ul>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">**Reflects my engineering-slanted opinion on the future of biology, as well as that of most people who are doing bioinformatics and are excited about it, but could be open to contrary opinion.<span style=\"mso-spacerun:yes\">&nbsp; </span>\u2018Fastest-developing\u2019 would perhaps be a better description.<span style=\"mso-spacerun:yes\">&nbsp; </span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\">&nbsp;</p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><strong style=\"mso-bidi-font-weight: normal\" id=\"Useful_interesting_references_consulted_for_this_section_\">Useful/interesting references consulted for this section:</strong></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><a href=\"http://www.amazon.com/Lehninger-Principles-Biochemistry-Fourth-Nelson/dp/0716743396/ref=sr_1_1?ie=UTF8&amp;qid=1306282641&amp;sr=8-1\" target=\"_self\">Lehninger Principles of Biochemistry</a>, 4th ed., by David L. Nelson and Michael M. Cox</p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><a href=\"http://www.amazon.com/Molecular-Biology-Fourth-Bruce-Alberts/dp/0815332181/ref=sr_1_2?ie=UTF8&amp;qid=1306282751&amp;sr=8-2\" target=\"_self\">Molecular Biology of the Cell</a>, 4th ed., by Alberts, Johnson, Lewis, Raff, Roberts, and Walter</p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"line-height:normal\"><a href=\"http://www.amazon.com/Receptors-Models-Binding-Trafficking-Signaling/dp/0195106636/ref=sr_1_1?ie=UTF8&amp;qid=1306282827&amp;sr=8-1\" target=\"_self\">Receptors: Models for Binding, Trafficking, and Signaling</a>, by Douglas A. Lauffenburger and Jennifer J. Linderman</p>", "sections": [{"title": "1.\u00a0 Biological processes are stochastic.", "anchor": "1___Biological_processes_are_stochastic_", "level": 1}, {"title": "2. Biological systems are complex networks, and variable deconvolution is nontrivial.\u00a0", "anchor": "2__Biological_systems_are_complex_networks__and_variable_deconvolution_is_nontrivial__", "level": 1}, {"title": "3. Due to (1) and (2), modeling efforts are heavily limited by computing power.\u00a0", "anchor": "3__Due_to__1__and__2___modeling_efforts_are_heavily_limited_by_computing_power__", "level": 1}, {"title": "4. Information transfer is error-prone in biological systems.\u00a0", "anchor": "4__Information_transfer_is_error_prone_in_biological_systems__", "level": 1}, {"title": "5. Biological processes are limited to life-sustaining conditions.\u00a0", "anchor": "5__Biological_processes_are_limited_to_life_sustaining_conditions__", "level": 1}, {"title": "6. Biological processes are transport-constrained.\u00a0", "anchor": "6__Biological_processes_are_transport_constrained__", "level": 1}, {"title": "Useful/interesting references consulted for this section:", "anchor": "Useful_interesting_references_consulted_for_this_section_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["g2CNSsjW4rEqY3yBp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-25T04:43:27.988Z", "modifiedAt": null, "url": null, "title": "Lightswitches", "slug": "lightswitches", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:06.819Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NNB4qkSKXSfGwgQtG/lightswitches", "pageUrlRelative": "/posts/NNB4qkSKXSfGwgQtG/lightswitches", "linkUrl": "https://www.lesswrong.com/posts/NNB4qkSKXSfGwgQtG/lightswitches", "postedAtFormatted": "Wednesday, May 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lightswitches&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALightswitches%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNNB4qkSKXSfGwgQtG%2Flightswitches%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lightswitches%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNNB4qkSKXSfGwgQtG%2Flightswitches", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNNB4qkSKXSfGwgQtG%2Flightswitches", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>There is probably some obvious solution to this puzzle, but it eludes me.&nbsp; I'm not sure how to plug it into the equation for Bayes' Theorem.&nbsp; And the situation described happened last August, so I'm probably not going to figure it out on my own.</p>\n<p>There are two lightswitches next to each other, and they control two lights (which have no other switches connected to them).&nbsp; I have used the switches a few times before, but don't occurrently recall which switch goes to which light, or whether the up or down position is the one that signifies off-ness.&nbsp; One light is on, one light is off, and the switches are in different positions.&nbsp; I want both lights off.&nbsp; So I guess a switch, and I'm right.&nbsp; What should be my credence be that my previous experience with this set of lightswitches helped me guess correctly, given that I felt like I was guessing at random (and would have had a 50% shot at being right were that the case)?&nbsp; How much would this be different if I'd guessed wrong the first time?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NNB4qkSKXSfGwgQtG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "7612", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-25T12:06:11.700Z", "modifiedAt": null, "url": null, "title": "Pancritical Rationalism Can Apply to Preferences and Behavior", "slug": "pancritical-rationalism-can-apply-to-preferences-and", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:07.389Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TimFreeman", "createdAt": "2011-04-12T22:58:16.873Z", "isAdmin": false, "displayName": "TimFreeman"}, "userId": "AAP7Amn8h8BhWCjjC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fh5XT3fpznjfdHksb/pancritical-rationalism-can-apply-to-preferences-and", "pageUrlRelative": "/posts/Fh5XT3fpznjfdHksb/pancritical-rationalism-can-apply-to-preferences-and", "linkUrl": "https://www.lesswrong.com/posts/Fh5XT3fpznjfdHksb/pancritical-rationalism-can-apply-to-preferences-and", "postedAtFormatted": "Wednesday, May 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pancritical%20Rationalism%20Can%20Apply%20to%20Preferences%20and%20Behavior&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APancritical%20Rationalism%20Can%20Apply%20to%20Preferences%20and%20Behavior%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFh5XT3fpznjfdHksb%2Fpancritical-rationalism-can-apply-to-preferences-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pancritical%20Rationalism%20Can%20Apply%20to%20Preferences%20and%20Behavior%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFh5XT3fpznjfdHksb%2Fpancritical-rationalism-can-apply-to-preferences-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFh5XT3fpznjfdHksb%2Fpancritical-rationalism-can-apply-to-preferences-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1725, "htmlBody": "<p>ETA: As stated below, criticizing beliefs is trivial in principle, either they were arrived at with an approximation to Bayes' rule starting with a reasonable prior and then updated with actual observations, or they weren't.&nbsp; Subsequent conversation made it clear that criticizing behavior is also trivial in principle, since someone is either taking the action that they believe will best suit their preferences, or not.&nbsp; Finally, criticizing preferences <a href=\"/lw/5vm/pancritical_rationalism_can_apply_to_preferences/494a\">became</a> <a href=\"/lw/5vm/pancritical_rationalism_can_apply_to_preferences/494v\">trivial</a> too -- the relevant question is \"Does/will agent X behave as though they have preferences Y\", and that's a belief, so go back to Bayes' rule and a reasonable prior. So the entire issue that this post was meant to solve has evaporated, in my opinion. Here's the original article, in case anyone is still interested:</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Pancritical_rationalism\">Pancritical rationalism</a> is a <a href=\"http://www.maxmore.com/pcr.htm\">fundamental value</a> in Extropianism that has only been <a href=\"/lw/jv/recommended_rationalist_reading/fcf\">mentioned</a> <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">in</a> <a href=\"/lw/551/popperian_decision_making/3yl4\">passing</a> <a href=\"/lw/s2/my_kind_of_reflection/\">on</a> <a href=\"/lw/vh/complexity_and_intelligence/t7g\">LessWrong</a>. I think it deserves more attention here. It's an approach to epistemology, that is, the question of \"How do we know what we know?\", that avoids the contradictions inherent in some of the alternative approaches.</p>\n<p>The fundamental source document for it is William Bartley's <a href=\"http://www.amazon.com/Retreat-Commitment-W-Bartley/dp/081269127X\">Retreat to Commitment</a>. He describes three approaches to epistemology, along with the dissatisfying aspects of the other two:</p>\n<ul>\n<li>Nihilism. Nothing matters, so it doesn't matter what you believe. This path is self-consistent, but it gives no guidance. </li>\n<li>Justificationlism. Your belief is justified because it is a consequence of other beliefs. This path is self-contradictory. Eventually you'll go in circles trying to justify the other beliefs, or you'll find beliefs you can't jutify. Justificationalism itself cannot be justified. </li>\n<li>Pancritical rationalism. You have taken the available criticisms for the belief into account and still feel comfortable with the belief. This path gives guidance about what to believe, although it does not uniquely determine one's beliefs. Pancritical rationalism can be criticized, so it is self-consistent in that sense. </li>\n</ul>\n<p>Read on for a discussion about emotional consequences and extending this to include preferences and behaviors as well as beliefs.</p>\n<p><a id=\"more\"></a></p>\n<p>\"Criticism\" here basically means philosophical discussion. Keep in mind that \"criticism\" as a hostile verbal interaction is a typical cause of failed relationships. If you do nothing but criticize a person, the other person will eventually find it emotionally impossible to spend much time with you. If you want to keep your relationships and do pancritical rationalism, be sure that the criticism that's part of pancriticial rationalism is understood to be offered in a helpful way, not a hostile way, and that you're doing it with a consenting adult. In particular, it has to be clear to all participants that there every available option will, in practice, have at least one valid criticism, so the goal is to choose something with criticisms you can accept, not to find something perfect.</p>\n<p>We'll start by listing some typical criticisms of beliefs, and then move on to criticizing preferences and behaviors.</p>\n<p>Criticizing beliefs is a special case in several ways. First, you can't judge the criticisms as true or false, since you haven't decided what to believe yet. Second, the process of criticizing beliefs is almost trivial in principle: apply Bayes' rule, starting with some reasonable prior. Neither of these special cases apply to criticizing preferences or behaviors, so pancriticial rationalism provides an especially useful framework for discussing them.</p>\n<p>Criticizing beliefs is not trivial in practice, since there are nonrational criticisms of belief, there is more than one reasonable prior, Bayes' rule can be computationally intractable, and in practice people have preexisting non-Bayesian belief strategies that they follow.</p>\n<p>With that said, a number of possible criticisms of a belief come to mind:</p>\n<ul>\n<li>Perhaps it contains self-contradictions. </li>\n<li>Perhaps it cannot arrived at by starting with a reasonably unbiased prior and doing updates according to Bayes' rule. (As a special case, perhaps it is contradicted by available evidence.) </li>\n<li>Perhaps it is so structured that it is invulnerable to being changed after it is adopted, regardless of the evidence observed. </li>\n<li>Perhaps it does not make predictions about the world. </li>\n<li>Perhaps it is really a preference or a behavior. (\"I believe in free speech\" or \"I believe I'll have another drink.\") </li>\n<li>Perhaps it is unpopular. </li>\n<li>Perhaps it is inconsistent with some ancient religious book or another. </li>\n</ul>\n<p>The last two of these illustrate that the weight one gives to a criticism is subjectively determined. Those last two criticisms are true for many beliefs discussed here, and the last one is true for essentially every belief if you pick the right religious book.</p>\n<p>Once you accept the idea that beliefs can be criticized, it's a small step from there to adopting a similar approach to preferences and behavior. Here are some plausible criticisms of a preference:</p>\n<ul>\n<li>Perhaps it is not consistent with your beliefs about cause-and-effect. That is, the preference prefers X over Y and also prefers the expected consequences of Y over the expected consequences of X. </li>\n<li>Perhaps it cannot be used to actually decide what to do. There are several subcases here:  \n<ul>\n<li>Perhaps it has mathematical properties that break some decision theories, such as an unbounded utility. Concerns about actual known breakage or conjectured breakage are two different criticisms. </li>\n<li>Perhaps it is defined in such a way that what you prefer depends on things you cannot know. </li>\n<li>Perhaps it gives little guidance, that its, it considers many pairs of outcomes that you expect to actually encounter as equally preferable. </li>\n</ul>\n</li>\n<li>Perhaps the stated preference is ineffective or counterproductive as a social signal. There are several subcases here:  \n<ul>\n<li>Perhaps it is psychologically implausible. That is, perhaps it is so unlikely that a human would hold such a preference that stating the preference to others will lead the others to reasonably conclude that you're a liar or confused, rather than leading them to conclude that you have the given preference. </li>\n<li>Perhaps it does not help others to predict your behavior. For example, it may require complicated decisions based on debatable guesses about the remote consequences of one's actions. </li>\n<li>Perhaps it is not something that anybody else would want to cooperate with. </li>\n<li>Perhaps it is at cross-purposes with the specific people you want to signal to. </li>\n</ul>\n</li>\n<li>Perhaps the preference does not include preferring that you want to stay alive enough, so one would expect the preference to select itself out if there's enough time and selection pressure. (\"Selection\" here might mean biological evolution or some sort of technological process, take your pick based on your beliefs.) </li>\n<li>Perhaps the preference does not include preferring that you accumulate enough power to actually do anything important. </li>\n<li>If you believe in objective morality, perhaps the preference is inconsistent with objective morality. Someone who does believe in objective morality should fill in the details here. </li>\n<li>Perhaps a preference is likely to have problems because it is held by only a non-controlling minority of the persons mind. This can happen in several ways:  \n<ul>\n<li>Perhaps a preference is likely to be self-deception because it is being claimed only because of a philosopical position, and not as a consequence of introspection or generalization from observed behavior. </li>\n<li>Perhaps a preference is likely to be self-decpetion because it is being claimed only because of introspection, and we expect introspection to yield socially convenient lies. </li>\n<li>Perhaps a claimed preference is likely to be poorly thought out because it arose nonverbally and has not been reflected upon. </li>\n</ul>\n</li>\n<li>Perhaps a preference is an overt deception, that is, the person claiming it knows they do not hold it. This criticism can be used by a person against themselves if they know they are lying and want clarity, or used by others against a person if the person is a poor liar. </li>\n<li>Perhaps a preference has short-term terminal values that aren't also instrumental values. </li>\n</ul>\n<p>We can also criticize behavior in at least the following ways:</p>\n<ul>\n<li>Perhaps the behavior is not consistent with any reasonable guess about your preferences. </li>\n<li>Perhaps the behavior is not consistent with your actual statements about your preferences. </li>\n<li>Perhaps the behavior does not promote personal survival. </li>\n<li>Perhaps the behavior is undesired by others, that is, others would prefer that you not do it. </li>\n<li>Perhaps you did not take into account your own preferences about the outcome for others at the time you did the behavior. </li>\n<li>Perhaps the behavior leads to active conflict with others, that is, in addition to it being against the preferences of others, it motivates them to act against you. </li>\n<li>Perhaps the behavior will lead others to exploit you. </li>\n<li>Perhaps you didn't take into account some of the important consequences of the behavior when you chose it. </li>\n</ul>\n<p>In all cases, if you're doing or preferring or believing something that has a valid criticism, the response does not necessarily have to be \"don't do/prefer/believe that\". The response might be \"In light of the alternatives I know about and the criticisms of all available alternatives, I accept that\".</p>\n<p>Of course, another response might be \"I don't have time to consider any of that right now\", but in that case you are at a level of urgency where this article won't be directly useful to you. You'll have to get yourself straightened out when things are less urgent and make use of that preparation when things are urgent.</p>\n<p>Assuming this post doesn't quickly get negative karma, a reasonable next step would be to put a list of criticisms of beliefs, preferences, and behaviors on a not-yet-created LessWrong pancritical rationalism Wiki page. Posting them in comments might also be worthwhile. If someone else could take the initiative to update the Wiki, it would be great. Otherwise I would like to get to it eventually, but that probably won't happen soon.</p>\n<p>Question for the readers: Is criticising a decision theory a useful separate category from the three listed above (beliefs, preferences, and behaviors)? If so, what criticisms are relevant?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fh5XT3fpznjfdHksb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 0, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "7618", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["C8nEXTcjZb9oauTCW", "TynBiYt6zg42StRbb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-25T12:56:16.897Z", "modifiedAt": null, "url": null, "title": "So how much utility is it?", "slug": "so-how-much-utility-is-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:34.384Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "zRwEwWNXmoTPueowb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uXKjX2QjcPQMcZzpA/so-how-much-utility-is-it", "pageUrlRelative": "/posts/uXKjX2QjcPQMcZzpA/so-how-much-utility-is-it", "linkUrl": "https://www.lesswrong.com/posts/uXKjX2QjcPQMcZzpA/so-how-much-utility-is-it", "postedAtFormatted": "Wednesday, May 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20So%20how%20much%20utility%20is%20it%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASo%20how%20much%20utility%20is%20it%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXKjX2QjcPQMcZzpA%2Fso-how-much-utility-is-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=So%20how%20much%20utility%20is%20it%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXKjX2QjcPQMcZzpA%2Fso-how-much-utility-is-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXKjX2QjcPQMcZzpA%2Fso-how-much-utility-is-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<p>I would like to know what value of utility you would give to certain kinds of pleasure in order to see how much the perceived ratios are differing between people. Of course, you can object that the real amount of pleasure someone experiences may be different from the pleasure she will recall; furthermore, pleasure is not a scalar, and it is a question of definition of someones' utility function how much she would want to have different kinds of pleasure; furthermore, there are effects of diminishing returns. However, you probably can get some orders of magnitude out of this.</p>\n<p>Let's define your favorite meal, one time, when you are hungry but not \"starving to death\" as one hundred utilium (You see this is pretty heuristical).</p>\n<p>You can include painful experiences, too.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uXKjX2QjcPQMcZzpA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 7.191764187182047e-07, "legacy": true, "legacyId": "7620", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-25T13:13:09.250Z", "modifiedAt": null, "url": null, "title": "Wiki: Standard Reference or Original Research?", "slug": "wiki-standard-reference-or-original-research", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:08.313Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wedrifid", "createdAt": "2009-07-04T22:18:20.822Z", "isAdmin": false, "displayName": "wedrifid"}, "userId": "FqKohKFRCZnbfbbcS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k2jxH5KhcJkiKuBw6/wiki-standard-reference-or-original-research", "pageUrlRelative": "/posts/k2jxH5KhcJkiKuBw6/wiki-standard-reference-or-original-research", "linkUrl": "https://www.lesswrong.com/posts/k2jxH5KhcJkiKuBw6/wiki-standard-reference-or-original-research", "postedAtFormatted": "Wednesday, May 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wiki%3A%20Standard%20Reference%20or%20Original%20Research%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWiki%3A%20Standard%20Reference%20or%20Original%20Research%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk2jxH5KhcJkiKuBw6%2Fwiki-standard-reference-or-original-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wiki%3A%20Standard%20Reference%20or%20Original%20Research%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk2jxH5KhcJkiKuBw6%2Fwiki-standard-reference-or-original-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk2jxH5KhcJkiKuBw6%2Fwiki-standard-reference-or-original-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 652, "htmlBody": "<p>My understanding of the purpose of the <a href=\"http://wiki.lesswrong.com/wiki/LessWrong_Wiki\">lesswrong wiki</a> has been that it is a collection of well established concepts and local jargon that we can use as a reference and an easy way to communicate across <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distance</a>. The material on the wiki (I assumed) was to be summarised from prominent and uncontroversial blog posts that are already referenced to from time to time. Yet on several occasions I have seen pages edited with new content straight from the author's creativity.</p>\n<p>A stark example was brought to my attention recently by User: bogus.</p>\n<blockquote>\n<p>Please read the Less Wrong wiki page on <a href=\"http://wiki.lesswrong.com/wiki/Mind-killer\">Mind-killer</a>, which summarizes the arguments for <em>not doing politics at LessWrong</em> better than any 'sequence' or blog post could.</p>\n</blockquote>\n<p>What? I certainly hope not. If it the content isn't straight from a post then get it off the wiki and make it a post! And if the meaning of a concept differs in emphasis from that used in a sequence then so much the worse for your wiki comment.</p>\n<p>Looking at the aforementioned mind-killer <a href=\"http://wiki.lesswrong.com/wiki/Mind-killer\">page</a> the kind of thing I do not expect to see on the wiki is this:</p>\n<blockquote>\n<p>many of these <em>political virtues</em> were identified by <a class=\"extiw\" title=\"wikipedia:Bernard Crick\" href=\"http://en.wikipedia.org/wiki/Bernard_Crick\">Bernard Crick</a> in his work <em>In Defense of Politics</em>.</p>\n</blockquote>\n<p>Huh? Bernard Crick? Since when was Bernard Crick part of an uncontroversial well established concept of 'mind killing' on lesswrong? The only reference to that author is in <a href=\"/lw/4gf/social_ethics_vs_decision_theory/3kzx\">one comment</a> by bogus in a post that is itself obscure. I've got nothing against Bernard Crick but I think the way to go about sharing the good news about his work is by making a post on him not injecting references into the wiki. Because then the new content has a chance to be vetted, commented on and voted on by the users.</p>\n<p>Less obvious but to my mind more important is the distorted emphasis the article places on the subject, such as in the opening \"politics is a mind killer\" paragraph:</p>\n<blockquote>\n<p>Political disputes are not limited to standard disagreements about factual matters, nor to disputes of personality or perspective or even <a class=\"mw-redirect\" title=\"Faction\" href=\"http://wiki.lesswrong.com/wiki/Faction\">faction</a>: they involve matters that people physically fight over in the real world&mdash;or at least, matters that are to be enforced by the government's monopoly of violence.</p>\n</blockquote>\n<p>That is kind of true. At least it isn't quite misleading enough that I would outright downvote it if it were a comment in a thread. But it certainly distracts from the core of the issue. On the other hand the related <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">Politics is the Mind-Killer</a> page nails it with a paragraph from an actual blog post:</p>\n<blockquote>\n<p>People go funny in the head when talking about politics. The evolutionary reasons for this are so obvious as to be worth belaboring: In the ancestral environment, politics was a matter of life and death. And sex, and wealth, and allies, and reputation... When, today, you get into an argument about whether \"we\" ought to raise the minimum wage, you're executing adaptations for an ancestral environment where being on the wrong side of the argument could get you killed... Politics is an extension of war by other means. Arguments are soldiers. Once you know which side you're on, you must support all arguments of that side, and attack all arguments that appear to favor the enemy side; otherwise it's like stabbing your soldiers in the back - providing aid and comfort to the enemy.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>What the mind killer page does have in its favour is links. Apart from links to the PITMK posts and the <a href=\"http://wiki.lesswrong.com/wiki/Color_politics\">color politics</a> page it links to the related Paul Graham <a href=\"http://paulgraham.com/say.html\">post</a> which is also commonly referred to here. So basically if I was a wiki editor I would probably just nuke the content and leave the links and do the same thing whenever I found wiki pages that are original content. This is perhaps one good reason why I don't spend my time editing the wiki. ;)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k2jxH5KhcJkiKuBw6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 21, "extendedScore": null, "score": 7.191813652228235e-07, "legacy": true, "legacyId": "7621", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-25T14:14:03.878Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Debiasing as Non-Self-Destruction", "slug": "seq-rerun-debiasing-as-non-self-destruction", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:06.561Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tbYGugR36hcMKeTrh/seq-rerun-debiasing-as-non-self-destruction", "pageUrlRelative": "/posts/tbYGugR36hcMKeTrh/seq-rerun-debiasing-as-non-self-destruction", "linkUrl": "https://www.lesswrong.com/posts/tbYGugR36hcMKeTrh/seq-rerun-debiasing-as-non-self-destruction", "postedAtFormatted": "Wednesday, May 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Debiasing%20as%20Non-Self-Destruction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Debiasing%20as%20Non-Self-Destruction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbYGugR36hcMKeTrh%2Fseq-rerun-debiasing-as-non-self-destruction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Debiasing%20as%20Non-Self-Destruction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbYGugR36hcMKeTrh%2Fseq-rerun-debiasing-as-non-self-destruction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbYGugR36hcMKeTrh%2Fseq-rerun-debiasing-as-non-self-destruction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>Today's post, <a href=\"/lw/hf/debiasing_as_nonselfdestruction/\">Debiasing as Non-Self-Destruction</a> was originally published on April 7, 2007. A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>Not being stupid seems like a more easily generalizable skill than breakthrough success. If debiasing is mostly about not being stupid, its benefits are hidden: lottery tickets not bought, blind alleys not followed, cults not joined. Hence, checking whether debiasing works is difficult, especially in the absence of organizations or systematized training.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/5ux/seq_rerun_knowing_about_biases_can_hurt_people/\">Knowing About Biases Can Hurt People</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tbYGugR36hcMKeTrh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 7.191992228081626e-07, "legacy": true, "legacyId": "7622", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XZWMeeqKmfMSPTLha", "ubnLBvpedG7NwSpfs", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-25T17:33:03.814Z", "modifiedAt": null, "url": null, "title": "Strategic Reliablism", "slug": "strategic-reliablism", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j58HZLfswdbNMM6c9/strategic-reliablism", "pageUrlRelative": "/posts/j58HZLfswdbNMM6c9/strategic-reliablism", "linkUrl": "https://www.lesswrong.com/posts/j58HZLfswdbNMM6c9/strategic-reliablism", "postedAtFormatted": "Wednesday, May 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Strategic%20Reliablism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStrategic%20Reliablism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj58HZLfswdbNMM6c9%2Fstrategic-reliablism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Strategic%20Reliablism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj58HZLfswdbNMM6c9%2Fstrategic-reliablism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj58HZLfswdbNMM6c9%2Fstrategic-reliablism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 957, "htmlBody": "<p>&nbsp;</p>\n<p><a href=\"http://www.amazon.com/dp/0195162307/?tag=vglnk-c319-20\"><em>Epistemology and the Psychology of Human Judgment</em></a> by <a href=\"http://philosophy.fsu.edu/People/Faculty/Michael-Bishop\">Michael Bishop</a> and <a href=\"http://www.jdtrout.com/\">J.D. Trout</a></p>\n<hr />\n<h2 id=\"ch-1.-laying-our-cards-on-the-table\">Ch 1. Laying Our Cards on the Table</h2>\n<p>Epistemology as a discipline needs to start offering practical advice for living. Defective epistemologies can compromise one's ability to act in all areas, but there is little social condemnation of weak reasoning. Prescriptive epistemology might be called \"critical thinking\", but this field is divorced from contemporary epistemology. This book is driven by a vision of what epistemology could be, but is, of course, only a modest first step in that direction.</p>\n<p>Standard Analytic Epistemology (SAE) is primarily concerned with an account of knowledge and epistemic justification. This program assumes any account of justification must not radically alter our existing judgments, even if this commitment to stasis is not explicitly stated. SAE tends to unproductively proceed via counterexamples. SAE might provide some useful advice, but it's goals and methods are beyond repair.</p>\n<p>In the authors' view, epistemology is a branch of philosophy of science and should start from Ameliorative Psychology, which encompasses parts of cognitive science, the heuristics and biases program, statistics, and artificial intelligence. This field makes recommendations about how to reason based on empirical findings. For example, statistical prediction rules (SPRs) were found to be at least as reliable as human experts and frequently more so. As another example, Bayesian reasoning is more successful when information is presented in frequencies rather than probabilities. Rather than being concerned with an account of knowledge or warrant, the authors' approach is to provide an account of <em>reasoning excellence</em>.</p>\n<p>A healthy epistemological tradition will have theoretical, practical, and social components. Theory and practice should mutually inform one another. Communication of practical results to the wider public is an important aspect of the social component. Since the authors argue epistemology is a science, but nevertheless normative, one might worry the approach is circular. This concern assumes the normative must come all at once, though. Instead, one can rely on the Aristotelian Principle as an empirical hook to accumulate justification. The Aristotelian Principle says that <em>in the long run, good reasoning tends to lead to better outcomes than poor reasoning</em>. Why accept the Principle? Without it, epistemology wouldn't be useful. If bad reasoning leads to better outcomes and if there are many types of bad reasoning, how could we figure out which bad reasoning to use, except by good reasoning? We have reason to think useful epistemology is possible since we live in a stable enough environment where quality has a chance to make a difference.</p>\n<h2 id=\"the-amazing-success-of-statistical-prediction-rules\">The Amazing Success of Statistical Prediction Rules</h2>\n<p>Judgments are an essential part of life. Choices of whether to release a prisoner on parole, admit someone to medical school, or offer a loan are too important to be \"close enough\". Only the best reasoning strategies available to us are satisfactory. Statistical prediction rules are robustly successful in these and many other high-stakes areas. In 136 studies comparing proper linear models to expert judgment, 64 clearly favored the SPR, 64 showed statistically equivalent accuracy, and 8 favored the expert. <sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup> SPRs built explicitly to mimic experts' judgments are more reliable than the expert, suggesting at least some errors are due to inconsistency and making exceptions to one's own rules.</p>\n<p>Improper linear models with unit or even random weights on standardized variables do surprisingly well. Qualitative human judgment can always be used as an inuput to an SPR or used to select variables and the direction of their effect. The flat maximum principle says that as long as the sign on coefficients is correct, all linear models do approximately the same. This principle applies when the problem is difficult and the inputs are reasonable predictive and redundant. Summing together inputs can be viewed as exploiting <a href=\"http://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem\">Condorcet's jury theorem</a>. Linear models tend to work when inputs interact monotonely, which appears to be the case in most social situations.</p>\n<p>All this is not to say SPRs are especially good, but that humans are very bad predictors. We pick up false patterns and are unable to consider even medium amounts of information at once. Resistance to SPR findings typically comes from a belief in <em>epistemic exceptionalism</em>. There is an impulse to tweak the conclusions of an SPR, which leads to worse results. It is surprising to find out we do so badly that random linear models can do better, yet another manifestation of overconfidence. Ironically, experts are best suited to deviate from SPRs grounded in theory because they have a better understanding of when the SPR will apply.</p>\n<h2 id=\"extracting-epistemic-lessons-from-ameliorative-psychology\">Extracting Epistemic Lessons from Ameliorative Psychology</h2>\n<p>Ameliorative Psychology offers a number of useful recommendations, but its normative assumptions are rarely stated explicitly. The authors identify three factors underlying the quality of a reasoning strategy: reliablity across a wide range of problems, tractability, and applicability to significant problems. Strategies need to be robustly reliable to survive changes in environments. Cheaper and easier strategies allow one to \"purchase\" more truths. Simple strategies like SPRs have tended to be more successful as well, possibly by avoiding overfitting, but a easy, low-quality rule is better than a high-quality one that is never used. Finally, the world is full of useless correlations, so the trick is to find important ones.</p>\n<p>Cost-benefit relations have <a href=\"http://en.wikipedia.org/wiki/Diminishing_marginal_returns\">diminishing marginal returns</a>. By considering possible cost-benefit curves, startup costs, and marginal expected reliability, the possible ways to improve reasoning fall into exactly four categories. Three possible ways can be seem in the following matrix:</p>\n<table border=\"0\">\n<colgroup><col width=\"27%\"></col><col width=\"39%\"></col><col width=\"33%\"></col></colgroup> \n<tbody>\n<tr class=\"odd\">\n<td align=\"left\">&nbsp;</td>\n<td align=\"center\"><strong>Same (or lower) cost</strong></td>\n<td align=\"center\"><strong>Higher Cost</strong></td>\n</tr>\n<tr class=\"even\">\n<td align=\"left\"><strong>Greater</strong> <strong>Benefit</strong></td>\n<td align=\"center\">(1) Adopt more reliable, cheaper strategy.</td>\n<td align=\"center\">(2) Adopt more reliable, expensive strategy.</td>\n</tr>\n<tr class=\"odd\">\n<td align=\"left\"><strong>Same (or less)</strong> <strong>Benefit</strong></td>\n<td align=\"center\">(3) Adopt less reliable, but cheaper strategy.</td>\n<td align=\"center\">&nbsp;</td>\n</tr>\n</tbody>\n</table>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\">\n<p>Grove and Meehl (1996), \"Comparative Efficiency of Informal (Subjective, Impressionistic) and Formal (Mechanical, Algorithmic) Prediction Procedures: The Clinical Statistical Controvery\", <em>Psychology, Public Policy, and Law</em> 2: 293--323 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 1\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j58HZLfswdbNMM6c9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7623", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p>&nbsp;</p>\n<p><a href=\"http://www.amazon.com/dp/0195162307/?tag=vglnk-c319-20\"><em>Epistemology and the Psychology of Human Judgment</em></a> by <a href=\"http://philosophy.fsu.edu/People/Faculty/Michael-Bishop\">Michael Bishop</a> and <a href=\"http://www.jdtrout.com/\">J.D. Trout</a></p>\n<hr>\n<h2 id=\"Ch_1__Laying_Our_Cards_on_the_Table\">Ch 1. Laying Our Cards on the Table</h2>\n<p>Epistemology as a discipline needs to start offering practical advice for living. Defective epistemologies can compromise one's ability to act in all areas, but there is little social condemnation of weak reasoning. Prescriptive epistemology might be called \"critical thinking\", but this field is divorced from contemporary epistemology. This book is driven by a vision of what epistemology could be, but is, of course, only a modest first step in that direction.</p>\n<p>Standard Analytic Epistemology (SAE) is primarily concerned with an account of knowledge and epistemic justification. This program assumes any account of justification must not radically alter our existing judgments, even if this commitment to stasis is not explicitly stated. SAE tends to unproductively proceed via counterexamples. SAE might provide some useful advice, but it's goals and methods are beyond repair.</p>\n<p>In the authors' view, epistemology is a branch of philosophy of science and should start from Ameliorative Psychology, which encompasses parts of cognitive science, the heuristics and biases program, statistics, and artificial intelligence. This field makes recommendations about how to reason based on empirical findings. For example, statistical prediction rules (SPRs) were found to be at least as reliable as human experts and frequently more so. As another example, Bayesian reasoning is more successful when information is presented in frequencies rather than probabilities. Rather than being concerned with an account of knowledge or warrant, the authors' approach is to provide an account of <em>reasoning excellence</em>.</p>\n<p>A healthy epistemological tradition will have theoretical, practical, and social components. Theory and practice should mutually inform one another. Communication of practical results to the wider public is an important aspect of the social component. Since the authors argue epistemology is a science, but nevertheless normative, one might worry the approach is circular. This concern assumes the normative must come all at once, though. Instead, one can rely on the Aristotelian Principle as an empirical hook to accumulate justification. The Aristotelian Principle says that <em>in the long run, good reasoning tends to lead to better outcomes than poor reasoning</em>. Why accept the Principle? Without it, epistemology wouldn't be useful. If bad reasoning leads to better outcomes and if there are many types of bad reasoning, how could we figure out which bad reasoning to use, except by good reasoning? We have reason to think useful epistemology is possible since we live in a stable enough environment where quality has a chance to make a difference.</p>\n<h2 id=\"The_Amazing_Success_of_Statistical_Prediction_Rules\">The Amazing Success of Statistical Prediction Rules</h2>\n<p>Judgments are an essential part of life. Choices of whether to release a prisoner on parole, admit someone to medical school, or offer a loan are too important to be \"close enough\". Only the best reasoning strategies available to us are satisfactory. Statistical prediction rules are robustly successful in these and many other high-stakes areas. In 136 studies comparing proper linear models to expert judgment, 64 clearly favored the SPR, 64 showed statistically equivalent accuracy, and 8 favored the expert. <sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup> SPRs built explicitly to mimic experts' judgments are more reliable than the expert, suggesting at least some errors are due to inconsistency and making exceptions to one's own rules.</p>\n<p>Improper linear models with unit or even random weights on standardized variables do surprisingly well. Qualitative human judgment can always be used as an inuput to an SPR or used to select variables and the direction of their effect. The flat maximum principle says that as long as the sign on coefficients is correct, all linear models do approximately the same. This principle applies when the problem is difficult and the inputs are reasonable predictive and redundant. Summing together inputs can be viewed as exploiting <a href=\"http://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem\">Condorcet's jury theorem</a>. Linear models tend to work when inputs interact monotonely, which appears to be the case in most social situations.</p>\n<p>All this is not to say SPRs are especially good, but that humans are very bad predictors. We pick up false patterns and are unable to consider even medium amounts of information at once. Resistance to SPR findings typically comes from a belief in <em>epistemic exceptionalism</em>. There is an impulse to tweak the conclusions of an SPR, which leads to worse results. It is surprising to find out we do so badly that random linear models can do better, yet another manifestation of overconfidence. Ironically, experts are best suited to deviate from SPRs grounded in theory because they have a better understanding of when the SPR will apply.</p>\n<h2 id=\"Extracting_Epistemic_Lessons_from_Ameliorative_Psychology\">Extracting Epistemic Lessons from Ameliorative Psychology</h2>\n<p>Ameliorative Psychology offers a number of useful recommendations, but its normative assumptions are rarely stated explicitly. The authors identify three factors underlying the quality of a reasoning strategy: reliablity across a wide range of problems, tractability, and applicability to significant problems. Strategies need to be robustly reliable to survive changes in environments. Cheaper and easier strategies allow one to \"purchase\" more truths. Simple strategies like SPRs have tended to be more successful as well, possibly by avoiding overfitting, but a easy, low-quality rule is better than a high-quality one that is never used. Finally, the world is full of useless correlations, so the trick is to find important ones.</p>\n<p>Cost-benefit relations have <a href=\"http://en.wikipedia.org/wiki/Diminishing_marginal_returns\">diminishing marginal returns</a>. By considering possible cost-benefit curves, startup costs, and marginal expected reliability, the possible ways to improve reasoning fall into exactly four categories. Three possible ways can be seem in the following matrix:</p>\n<table border=\"0\">\n<colgroup><col width=\"27%\"><col width=\"39%\"><col width=\"33%\"></colgroup> \n<tbody>\n<tr class=\"odd\">\n<td align=\"left\">&nbsp;</td>\n<td align=\"center\"><strong>Same (or lower) cost</strong></td>\n<td align=\"center\"><strong>Higher Cost</strong></td>\n</tr>\n<tr class=\"even\">\n<td align=\"left\"><strong>Greater</strong> <strong>Benefit</strong></td>\n<td align=\"center\">(1) Adopt more reliable, cheaper strategy.</td>\n<td align=\"center\">(2) Adopt more reliable, expensive strategy.</td>\n</tr>\n<tr class=\"odd\">\n<td align=\"left\"><strong>Same (or less)</strong> <strong>Benefit</strong></td>\n<td align=\"center\">(3) Adopt less reliable, but cheaper strategy.</td>\n<td align=\"center\">&nbsp;</td>\n</tr>\n</tbody>\n</table>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn1\">\n<p>Grove and Meehl (1996), \"Comparative Efficiency of Informal (Subjective, Impressionistic) and Formal (Mechanical, Algorithmic) Prediction Procedures: The Clinical Statistical Controvery\", <em>Psychology, Public Policy, and Law</em> 2: 293--323 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 1\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n</ol></div>", "sections": [{"title": "Ch 1. Laying Our Cards on the Table", "anchor": "Ch_1__Laying_Our_Cards_on_the_Table", "level": 1}, {"title": "The Amazing Success of Statistical Prediction Rules", "anchor": "The_Amazing_Success_of_Statistical_Prediction_Rules", "level": 1}, {"title": "Extracting Epistemic Lessons from Ameliorative Psychology", "anchor": "Extracting_Epistemic_Lessons_from_Ameliorative_Psychology", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-25T20:08:55.418Z", "modifiedAt": null, "url": null, "title": "Peter Thiel announces the 20 talented people he will pay to drop out of college to pursue innovative scientific and technical projects", "slug": "peter-thiel-announces-the-20-talented-people-he-will-pay-to", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:08.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3KuXLZTnMsEeQpSFp/peter-thiel-announces-the-20-talented-people-he-will-pay-to", "pageUrlRelative": "/posts/3KuXLZTnMsEeQpSFp/peter-thiel-announces-the-20-talented-people-he-will-pay-to", "linkUrl": "https://www.lesswrong.com/posts/3KuXLZTnMsEeQpSFp/peter-thiel-announces-the-20-talented-people-he-will-pay-to", "postedAtFormatted": "Wednesday, May 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Peter%20Thiel%20announces%20the%2020%20talented%20people%20he%20will%20pay%20to%20drop%20out%20of%20college%20to%20pursue%20innovative%20scientific%20and%20technical%20projects&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APeter%20Thiel%20announces%20the%2020%20talented%20people%20he%20will%20pay%20to%20drop%20out%20of%20college%20to%20pursue%20innovative%20scientific%20and%20technical%20projects%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3KuXLZTnMsEeQpSFp%2Fpeter-thiel-announces-the-20-talented-people-he-will-pay-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Peter%20Thiel%20announces%20the%2020%20talented%20people%20he%20will%20pay%20to%20drop%20out%20of%20college%20to%20pursue%20innovative%20scientific%20and%20technical%20projects%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3KuXLZTnMsEeQpSFp%2Fpeter-thiel-announces-the-20-talented-people-he-will-pay-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3KuXLZTnMsEeQpSFp%2Fpeter-thiel-announces-the-20-talented-people-he-will-pay-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p><a href=\"http://thielfoundation.org/index.php?option=com_content&amp;view=article&amp;id=15&amp;Itemid=19\">http://thielfoundation.org/index.php?option=com_content&amp;view=article&amp;id=15&amp;Itemid=19</a></p>\n<p>Thoughts?</p>\n<p>A lot of Thiel's beliefs are in line with those of a significant portion of the LessWrong community, so I trust his judgment more than I trust that of most.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3KuXLZTnMsEeQpSFp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "7626", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-25T22:28:59.388Z", "modifiedAt": null, "url": null, "title": "Link: If you don't already read Bad Science", "slug": "link-if-you-don-t-already-read-bad-science", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.362Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ENNsP2Bu3cX7F2swi/link-if-you-don-t-already-read-bad-science", "pageUrlRelative": "/posts/ENNsP2Bu3cX7F2swi/link-if-you-don-t-already-read-bad-science", "linkUrl": "https://www.lesswrong.com/posts/ENNsP2Bu3cX7F2swi/link-if-you-don-t-already-read-bad-science", "postedAtFormatted": "Wednesday, May 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20If%20you%20don't%20already%20read%20Bad%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20If%20you%20don't%20already%20read%20Bad%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FENNsP2Bu3cX7F2swi%2Flink-if-you-don-t-already-read-bad-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20If%20you%20don't%20already%20read%20Bad%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FENNsP2Bu3cX7F2swi%2Flink-if-you-don-t-already-read-bad-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FENNsP2Bu3cX7F2swi%2Flink-if-you-don-t-already-read-bad-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<p>So there's this blog called Bad Science, consisting mostly of the articles that medical doctor Ben Goldacre writes for the Guardian. &nbsp;It's about pseudoscience, medicine and medical research. &nbsp;And also <em>awesome</em>.</p>\n<p>The recent article was a wonderful bit of emotional whiplash, and is about as subject I think is useful to keep in mind when contemplating research. &nbsp;But really, I recommend reading everything. &nbsp;</p>\n<p><a href=\"http://www.badscience.net/2011/05/existential-angst-about-the-bigger-picture/\">http://www.badscience.net/2011/05/existential-angst-about-the-bigger-picture/</a></p>\n<p>&nbsp;</p>\n<p>Also, the second-most-recent article, which should appeal to LW-types: &nbsp;</p>\n<p><a href=\"http://www.badscience.net/2011/05/we-should-so-blatantly-do-more-randomised-trials-on-policy/\">http://www.badscience.net/2011/05/we-should-so-blatantly-do-more-randomised-trials-on-policy/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ENNsP2Bu3cX7F2swi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 17, "extendedScore": null, "score": 7.193443529478912e-07, "legacy": true, "legacyId": "7627", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T00:08:50.702Z", "modifiedAt": null, "url": null, "title": "Houston Hackerspace Meetup: Sunday May 29, 5:00PM", "slug": "houston-hackerspace-meetup-sunday-may-29-5-00pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:27.743Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cog", "createdAt": "2011-04-25T04:58:53.803Z", "isAdmin": false, "displayName": "Cog"}, "userId": "xkp87vCZ56dp2tWnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fEaaK8ybYnjaZJ5ob/houston-hackerspace-meetup-sunday-may-29-5-00pm", "pageUrlRelative": "/posts/fEaaK8ybYnjaZJ5ob/houston-hackerspace-meetup-sunday-may-29-5-00pm", "linkUrl": "https://www.lesswrong.com/posts/fEaaK8ybYnjaZJ5ob/houston-hackerspace-meetup-sunday-may-29-5-00pm", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Houston%20Hackerspace%20Meetup%3A%20Sunday%20May%2029%2C%205%3A00PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHouston%20Hackerspace%20Meetup%3A%20Sunday%20May%2029%2C%205%3A00PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfEaaK8ybYnjaZJ5ob%2Fhouston-hackerspace-meetup-sunday-may-29-5-00pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Houston%20Hackerspace%20Meetup%3A%20Sunday%20May%2029%2C%205%3A00PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfEaaK8ybYnjaZJ5ob%2Fhouston-hackerspace-meetup-sunday-may-29-5-00pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfEaaK8ybYnjaZJ5ob%2Fhouston-hackerspace-meetup-sunday-may-29-5-00pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p>&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><em>Sunday May 29, 5:00PM</em></p>\n<p style=\"margin-bottom: 0in;\"><em>TX/RX Hackerspace</em></p>\n<p style=\"margin-bottom: 0in;\"><em>2010 Commerce St</em></p>\n<p style=\"margin-bottom: 0in;\"><em>Houston, TX 77002</em></p>\n<p>&nbsp;</p>\n<p>Another meeting at the TX/RX hackerspace will happen on Sunday, May 29th at 5PM. Since we had no attendance last week,it will again be a meet and greet and planning for future activities. Pizza can and will be ordered if people show up.</p>\n<p>&nbsp;</p>\n<p>I understand many of the people who have expressed interest in a Houston meet up find this to be an inconvenient time. Unfortunately, my Saturday and early Sunday have been consumed by other plans this week. If this is a bad time for you, I plan to have a meetup the Saturday after next in the early afternoon. If that is usually more convenient for you, let me know, and I can decide how to structure future scheduling.</p>\n<p>&nbsp;</p>\n<p><strong>Directions</strong></p>\n<p><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">A pictoral view</p>\n<p><img src=\"http://images.lesswrong.com/t3_5pp_1.png\" alt=\"Front\" width=\"645\" height=\"470\" /></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This is the set of buildings that the  hackerspace is in. It's difficult to see our front from this angle -  unfortunately google maps decided to map everything but our little  section of commerce street. It's near where the white truck and red  motorcycle are. Currently, there is an old military vehicle and generator in front.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><img src=\"http://images.lesswrong.com/t3_5pp_0.png\" alt=\"Empy Lot\" width=\"644\" height=\"465\" /></p>\n<p style=\"margin-bottom: 0in;\">And this is the empty lot that you can park in .</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For more reference:</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><a href=\"http://maps.google.com/maps?client=ubuntu&amp;channel=fs&amp;q=2010\">http://maps.google.com/maps?client=ubuntu&amp;channel=fs&amp;q=2010</a>+Commerce+St.+Houston,+Tx+77002&amp;oe=utf-8&amp;um=1&amp;ie=UTF-8&amp;hq=&amp;hnear=0x8640bed8ed95625d:0x4c9af214d2032035,2010+Commerce+St,+Houston,+TX+77002&amp;gl=us&amp;ei=C9LRTYHvE8fL0QGu8OjlCw&amp;sa=X&amp;oi=geocode_result&amp;ct=title&amp;resnum=1&amp;ved=0CBkQ8gEwAA</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fEaaK8ybYnjaZJ5ob", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 7.193736404369409e-07, "legacy": true, "legacyId": "7628", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T01:00:12.649Z", "modifiedAt": null, "url": null, "title": "Torture Simulated with Flipbooks", "slug": "torture-simulated-with-flipbooks", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.546Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Amanojack", "createdAt": "2010-03-11T16:58:05.275Z", "isAdmin": false, "displayName": "Amanojack"}, "userId": "k8hFoMRRgGWgXPwjt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dNWpFRvmihC7qN7z8/torture-simulated-with-flipbooks", "pageUrlRelative": "/posts/dNWpFRvmihC7qN7z8/torture-simulated-with-flipbooks", "linkUrl": "https://www.lesswrong.com/posts/dNWpFRvmihC7qN7z8/torture-simulated-with-flipbooks", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Torture%20Simulated%20with%20Flipbooks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATorture%20Simulated%20with%20Flipbooks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdNWpFRvmihC7qN7z8%2Ftorture-simulated-with-flipbooks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Torture%20Simulated%20with%20Flipbooks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdNWpFRvmihC7qN7z8%2Ftorture-simulated-with-flipbooks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdNWpFRvmihC7qN7z8%2Ftorture-simulated-with-flipbooks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<p>What if the brain of the person you most care about were scanned and the entirety of that person's mind and utility function at this moment were printed out on paper, and then several more \"clock ticks\" of their mind as its states changed <em>exactly as they would if the person were being horribly tortured</em> were printed out as well, into a gigantic book? And then the book were flipped through, over and over again. Fl-l-l-l-liiiiip! Fl-l-l-l-liiiiip!</p>\n<p>Would this count as simulated torture? If so, would you care about stopping it, or is it different from computer-simulated torture?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb187": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dNWpFRvmihC7qN7z8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 9, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "7629", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T01:05:49.558Z", "modifiedAt": null, "url": null, "title": "Dominus' Razor", "slug": "dominus-razor", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:02.986Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/okXN8kZSDQwb6ywSu/dominus-razor", "pageUrlRelative": "/posts/okXN8kZSDQwb6ywSu/dominus-razor", "linkUrl": "https://www.lesswrong.com/posts/okXN8kZSDQwb6ywSu/dominus-razor", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dominus'%20Razor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADominus'%20Razor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokXN8kZSDQwb6ywSu%2Fdominus-razor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dominus'%20Razor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokXN8kZSDQwb6ywSu%2Fdominus-razor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokXN8kZSDQwb6ywSu%2Fdominus-razor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<p>You are probably familiar with <a href=\"http://en.wikipedia.org/wiki/Hanlon's_razor\">Hanlon&rsquo;s Razor</a>, the adage that you should never attribute to malice what can adequately be explained by stupidity. In Bayesian terms, stupidity is sufficiently abundant that even fairly strong evidence of harmful intent can&rsquo;t overcome the base rate. However, there is something of a converse, which to my knowledge doesn&rsquo;t have an eponymous name. In honor of a <a href=\"http://blog.plover.com/tech/stadiometer.html\">recent post</a> by Mark Dominus, I propose Dominus&rsquo; Razor: <em>Never attribute to complete stupidity what can adequately be explained by ordinary stupidity and a good reason</em>.</p>\n<p>Dominus, well-known as a Perl programmer, found that astonishingly bad code looks better (if still bad) after hearing the reasons for its development. For instance, one program passed data between functions by writing it to a temporary file, only to read it back again. It turns out the programmer did this for debugging purposes, an admirable goal, even if done in non-standard ways.</p>\n<p>The Razor is one more explanation for the frequent failure of <a href=\"http://wiki.lesswrong.com/wiki/Other-optimizing\">other-optimization</a>. People and institutions usually have some reason for doing what they do, even if they&rsquo;ve since forgotten or never knew in the first place. &ldquo;Evolution is cleverer than you are&rdquo; (<a href=\"http://en.wikipedia.org/wiki/Orgel's_rule\">Orgel&rsquo;s Second Rule</a>) and &ldquo;Free markets are cleverer than you are&rdquo; are two related rules of thumb. Something that looks obviously stupid was probably implemented to meet some non-obvious need or constraint.</p>\n<p>In the end, this is another way of saying to not expect short <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distances</a>. Based on personal observation, this community does a good job anticipating inferential jumps when playing the role of the sender, but not quite as well when acting as the receiver. Even if someone is wrong, be careful not to <a href=\"http://wiki.lesswrong.com/wiki/Least_convenient_possible_world\">dismiss them entirely</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YQW2DxpZFTrqrxHBJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "okXN8kZSDQwb6ywSu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 60, "extendedScore": null, "score": 0.000163, "legacy": true, "legacyId": "7630", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T01:09:08.717Z", "modifiedAt": null, "url": null, "title": "NC Triangle LW meetup: Wed June 1, 7:00PM", "slug": "nc-triangle-lw-meetup-wed-june-1-7-00pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:09.577Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mutterc", "createdAt": "2011-01-03T03:21:37.188Z", "isAdmin": false, "displayName": "mutterc"}, "userId": "iCw35Tj27hbg5LQFh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CaL2P6tzSM7sTaaQv/nc-triangle-lw-meetup-wed-june-1-7-00pm", "pageUrlRelative": "/posts/CaL2P6tzSM7sTaaQv/nc-triangle-lw-meetup-wed-june-1-7-00pm", "linkUrl": "https://www.lesswrong.com/posts/CaL2P6tzSM7sTaaQv/nc-triangle-lw-meetup-wed-june-1-7-00pm", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20NC%20Triangle%20LW%20meetup%3A%20Wed%20June%201%2C%207%3A00PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANC%20Triangle%20LW%20meetup%3A%20Wed%20June%201%2C%207%3A00PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaL2P6tzSM7sTaaQv%2Fnc-triangle-lw-meetup-wed-june-1-7-00pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=NC%20Triangle%20LW%20meetup%3A%20Wed%20June%201%2C%207%3A00PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaL2P6tzSM7sTaaQv%2Fnc-triangle-lw-meetup-wed-june-1-7-00pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaL2P6tzSM7sTaaQv%2Fnc-triangle-lw-meetup-wed-june-1-7-00pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 35, "htmlBody": "<p>The venue: <a href=\"http://www.noodles.com\">Noodles &amp; Co</a>, at <a href=\"http://maps.google.com/maps?f=d&amp;source=s_d&amp;saddr=&amp;daddr=2608%20Erwin%20Rd%20Durham,%20NC%2027705&amp;hl=en&amp;geocode=&amp;mra=ls&amp;sll=&amp;sspn=&amp;ie=UTF8&amp;z=6\">this Durham location</a></p>\n<p>Agenda:</p>\n<ul>\n<li>A round of <a href=\"http://en.wikipedia.org/wiki/Zendo_%28game%29\">Zendo</a> (we didn't get around to it last time) </li>\n<li>Socialize, by discussing socialization </li>\n</ul>\n<p>C'mon... all your friends are doing it...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CaL2P6tzSM7sTaaQv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 7.193913274648224e-07, "legacy": true, "legacyId": "7631", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T02:29:07.334Z", "modifiedAt": null, "url": null, "title": "Test article", "slug": "test-article-8", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:07.114Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "handoflixue", "createdAt": "2010-11-24T17:23:14.338Z", "isAdmin": false, "displayName": "handoflixue"}, "userId": "vGzwXwmR2qERvoGvb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fJWvpTzkYPeCA8iDN/test-article-8", "pageUrlRelative": "/posts/fJWvpTzkYPeCA8iDN/test-article-8", "linkUrl": "https://www.lesswrong.com/posts/fJWvpTzkYPeCA8iDN/test-article-8", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Test%20article&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATest%20article%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJWvpTzkYPeCA8iDN%2Ftest-article-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Test%20article%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJWvpTzkYPeCA8iDN%2Ftest-article-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJWvpTzkYPeCA8iDN%2Ftest-article-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>Testing</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fJWvpTzkYPeCA8iDN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7634", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T02:33:58.215Z", "modifiedAt": null, "url": null, "title": "The cost of universal cryonics", "slug": "the-cost-of-universal-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "handoflixue", "createdAt": "2010-11-24T17:23:14.338Z", "isAdmin": false, "displayName": "handoflixue"}, "userId": "vGzwXwmR2qERvoGvb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zJfDgGkWN4unGSrix/the-cost-of-universal-cryonics", "pageUrlRelative": "/posts/zJfDgGkWN4unGSrix/the-cost-of-universal-cryonics", "linkUrl": "https://www.lesswrong.com/posts/zJfDgGkWN4unGSrix/the-cost-of-universal-cryonics", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20cost%20of%20universal%20cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20cost%20of%20universal%20cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzJfDgGkWN4unGSrix%2Fthe-cost-of-universal-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20cost%20of%20universal%20cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzJfDgGkWN4unGSrix%2Fthe-cost-of-universal-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzJfDgGkWN4unGSrix%2Fthe-cost-of-universal-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1445, "htmlBody": "<p><strong>INTRODUCTION</strong><br /><br />I recently got sparked by both Eliezer's post on Cryonics(http://lesswrong.com/lw/qx/timeless_identity/) and lsparrish's post on the economies of scale(http://lesswrong.com/lw/2f5/cryonics_wants_to_be_big/) that go in to cryonics, to do some actual research. Unfortunately, while both authors are happy to assert that there are \"economies of scale\" at work, there doesn't seem to actually be any published research on the matter. If I happen to be wrong, and someone else has more accurate numbers, I'll be pleasantly surprised to see myself corrected :)</p>\n<p>Alcor Costs as of 1990 (http://www.alcor.org/Library/html/CostOfCryonicsTables.txt) seems like a reasonably reliable source of information. I'll be using them primarily because they were the only institute I could find that actually provides a break-down of their costs. The accompany article(http://www.alcor.org/Library/html/CostOfCryonics.html) suggests that the labor rates and equipment markups are actually excessively optimistic, but it gives a simple cost of $18,908.76 for neurosuspension (not whole body). Maintenance costs are given as $66.08 annually, which would require a $6600 investment to yield suitable interest. Call it $25K total.<br /><br />Now, figuring out how economies of scale will affect this is tricky. I'll go ahead and run two estimates, but they're both reasonably crude. I'm trying to be optimistic in my math, because my starting premise is \"cryonics is <em>not</em> financially viable, even with economies of scale\", and I don't want my numbers to favour my starting hypothesis. It's also worth noting that I am assuming that the major cryonics facilities are already taking advantage of some economies of scale: it is quite true that one can get a 90% discount on liquid nitrogen, if you start at the price that someone would pay for a liter for personal usage; it is far less likely that a business that already dropped it's prices from $0.50/L to $0.13/L [1] can still claim a 90% savings by sufficient economies of scale.<br /><a id=\"more\"></a><br />----<br /><br /><strong>METHOD 1</strong><br /><br />The actual cost of the chemicals and equipment won't scale dramatically - you can't get a 90% discount just because you're huge, unless the company is making a 10x profit off the item normally. A 50% reduction due to bulk savings is therefor a reasonably optimistic assumption.<br /><br />Transportation is going to remain an issue, although certainly as this becomes more \"main stream\", you could imagine hospitals having a cryogenics ward and thus only having transportation when someone dies outside a hospital. A quick peruse of Google says ~50% of deaths occur in a hospital, so we can cut transportation costs in half right there. Obviously, transportation costs will also drop as there are more facilities, because distances decrease. However, it's also worth noting that, should we want this to be a truly universal option, transportation costs will rise to include transportation of people who do not live near major urban centers. We should also be able to claim economies of scale on the financial cost of vehicles and equipment. That probably works out about the same as equipment, so another 50% off; we gain a 75% economy of scale on transportation.<br /><br />Labor is an interesting point: Alcor points out that their labor rates are generally 2-3 times <em>less</em> than you'd expect mainstream, and they use a lot of volunteers. It appears that the Cryonics Institute also has a large volunteer staff. Unless society radically changes, a reliance on volunteer labor is probably not a fair assumption as things scale up. However, we actually get the most powerful economies of scale here, because we're no longer looking at 48-80 hours of standby per person. You'll need a sufficient staff to handle catastrophes quickly (plane crashes, etc.), and thus some degree of standby is still essential for prompt responses. If we reduce standby from 48 hours down to 2 hours, then multiplying costs by 3 to bring pay up to market rates, we get a total savings of 87.5%!<br /><br />Based on the summarized charges, it looks like the charges break down as approximately:<br />Transportation: $9,000<br />Equipment: $4,000<br />Labor: $15,000<br /><br />Which, with these new adjustments:<br />Equipment (50% of original cost): $2,000<br />Transportation (25% of original cost): $2,250<br />Labor (12.5% of original cost): $1,875<br /><br />That gives us a net total of $6,125, before maintenance fees are taken in to account.<br /><br />---<br /><br /><strong>METHOD 2</strong><br /><br />Alternately, we could extrapolate economies of scale based on observed data. The UK has centralized health care and spends $3,000 per capita on health care. The US is decentralized, and spends $7,500 per capita. So we have reason to assume that medical costs specifically can be cut down to 40% simply based on economies of scale.<br /><br />Another approximation often used for hospitals is that there is a 10% increase in hospital productivity per doubling in size. Alcor currently has 100 patients. Scaling up to 150,000 is ~17 doublings or, being generous, a tripling of productivity, so cutting costs to ~33%. The two figures are reasonably close, so we'll go with the more favourable 33%.<br /><br />Given an adjusted grand total of $18,908.76 (this excludes the remote charges and nursing fees), and taking only 33% of the cost yields about $6K. Once again, this figure ignores maintenance fees.<br /><br />---<br /><br /><strong>FINAL COSTS</strong><br /><br />Either way you do the math (and I'm quite welcome to being told I've been vastly pessimistic, if there's some supporting evidence I've missed in my searches), the final cost per person for cryonics is probably around $6K for the suspension. <br /><br />Storage costs are another matter, and we will simply assume that storage is magically free, as I am attempting to be optimistic, and storage is probably going to realize the greatest economies of scale. It is worth noting that storage is only approximately 25% of the current expense! Alcor requires an additional $6600 fund and uses the interest from that to pay maintenance costs. CI cites maintenance costs that are 50% higher ($100 vs $66), and thus would presumably require a $10,000 fund. This is against an expense of $18K and $28K for each respective organization.<br /><br />We thus have a final figure of $6K per person.<br /><br />If you still think this is an overly pessimistic figure, keep in mind that the current market rate is $80,000 via Alcor, and Alcor's discussion of costs(http://www.alcor.org/Library/html/CostOfCryonics.html) explains a lot of why this is a really quite expensive service. The Cryonics Institute(<a href=\"http://www.cryonics.org/comparisons.html#Prices\">http://www.cryonics.org/comparisons.html#Prices</a>) charges $88,000 for a complete package (suspension, standby, and transport). Our $6,000 per person figure is a 90% savings due to economies of scale - which, except for the noted quirk of labor charges, is an exceedingly optimistic economy of scale for any enterprise to aim for!<br /><br />---<br /><br /><strong>CONCLUSIONS</strong><br /><br />The fundamental point here is that economies of scale only take us so far: We still need to pay professionals to do their job, we still need a vast amount amount of equipment and supplies to actually perform the operation, and we still want to attempt vitrification as soon after death as possible. While the actual storage of a human body might come cheaply (I have seen figures of $100/year for CI's whole-body option, and $66/year for Alcor's neuro-only option), even if we discount this to <em>free</em>, we are only managing a 25% savings; as Alcor's numbers demonstrate, storage is a relatively trivial if you assume an investment fund with a mere 2% return. Even the space requirements are modest; maybe 4 buildings the size of the empire state building each year.<br /><br />The true expense of cryonics is getting someone vitrified, and doing it in a timely manner.<br /><br />At present, approximately 150,000 people die per day, or 54,750,000 per year. At our optimistic rates of $6K per person, we are looking at a sum expense of $328,500,000,000 (<strong>$328 billion</strong>). This is approximately half of the US defense budget ($663.8 billion) and approximately 0.56% of the world GDP (58.26 Trillion)<br /><br />In an ideal, rationalist world, is this a viable figure? Certainly.<br /><br />In our actual world, with our actual politics, does this even vaguely approach a rational goal to shoot for today? It seems unlikely.<br /><br />---<br /><br />[1] http://cryonics.org/cryostats.html - About halfway down the page; the search term \"Prior to getting the bulk liquid nitrogen\" will locate the relevant paragraph.<br /><br />All other sources are marked via in-line links</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1, "PDJ6KqJBRzvKPfuS3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zJfDgGkWN4unGSrix", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 52, "extendedScore": null, "score": 0.000131, "legacy": true, "legacyId": "7633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T11:11:37.524Z", "modifiedAt": null, "url": null, "title": "Edinburgh LW meetup, Saturday May 28, 2pm", "slug": "edinburgh-lw-meetup-saturday-may-28-2pm", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sark", "createdAt": "2010-01-20T20:18:10.889Z", "isAdmin": false, "displayName": "sark"}, "userId": "cJYNhyCitpdzsZqeP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zPoHJujoJei5QocSd/edinburgh-lw-meetup-saturday-may-28-2pm", "pageUrlRelative": "/posts/zPoHJujoJei5QocSd/edinburgh-lw-meetup-saturday-may-28-2pm", "linkUrl": "https://www.lesswrong.com/posts/zPoHJujoJei5QocSd/edinburgh-lw-meetup-saturday-may-28-2pm", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Edinburgh%20LW%20meetup%2C%20Saturday%20May%2028%2C%202pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEdinburgh%20LW%20meetup%2C%20Saturday%20May%2028%2C%202pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPoHJujoJei5QocSd%2Fedinburgh-lw-meetup-saturday-may-28-2pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Edinburgh%20LW%20meetup%2C%20Saturday%20May%2028%2C%202pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPoHJujoJei5QocSd%2Fedinburgh-lw-meetup-saturday-may-28-2pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPoHJujoJei5QocSd%2Fedinburgh-lw-meetup-saturday-may-28-2pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>Location: Delhi Cafe, 67 Nicolson Street</p>\n<p>Map:&nbsp;<a href=\"http://maps.google.co.uk/maps?ie=UTF8&amp;t=h&amp;cid=1874860554950886070\">http://maps.google.co.uk/maps?ie=UTF8&amp;t=h&amp;cid=1874860554950886070</a></p>\n<p>I will be there with the <a href=\"http://www.amazon.com/Oxford-Book-Aphorisms-John-Gross/dp/0192804561/ref=dp_ob_title_bk\">Oxford Book of Aphorisms</a> (let me know if you object to this)</p>\n<p><img src=\"http://ecx.images-amazon.com/images/I/417GgkDjTrL._BO2,204,203,200_PIsitb-sticker-arrow-click,TopRight,35,-76_AA300_SH20_OU01_.jpg\" alt=\"\" width=\"300\" height=\"300\" /></p>\n<p>Apologies for the fact that there was no meetup last week, some coordination failure on our part.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zPoHJujoJei5QocSd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.195680869261403e-07, "legacy": true, "legacyId": "7647", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T12:11:52.796Z", "modifiedAt": null, "url": null, "title": "Freedom From Choice: Should we surrender our freedom to an external agent? How much?", "slug": "freedom-from-choice-should-we-surrender-our-freedom-to-an", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:07.602Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raw_Power", "createdAt": "2010-09-10T23:59:43.621Z", "isAdmin": false, "displayName": "Raw_Power"}, "userId": "kwSqcED9qTanFyNWG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tpGCGjzd52QhNWkjd/freedom-from-choice-should-we-surrender-our-freedom-to-an", "pageUrlRelative": "/posts/tpGCGjzd52QhNWkjd/freedom-from-choice-should-we-surrender-our-freedom-to-an", "linkUrl": "https://www.lesswrong.com/posts/tpGCGjzd52QhNWkjd/freedom-from-choice-should-we-surrender-our-freedom-to-an", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Freedom%20From%20Choice%3A%20Should%20we%20surrender%20our%20freedom%20to%20an%20external%20agent%3F%20How%20much%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFreedom%20From%20Choice%3A%20Should%20we%20surrender%20our%20freedom%20to%20an%20external%20agent%3F%20How%20much%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtpGCGjzd52QhNWkjd%2Ffreedom-from-choice-should-we-surrender-our-freedom-to-an%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Freedom%20From%20Choice%3A%20Should%20we%20surrender%20our%20freedom%20to%20an%20external%20agent%3F%20How%20much%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtpGCGjzd52QhNWkjd%2Ffreedom-from-choice-should-we-surrender-our-freedom-to-an", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtpGCGjzd52QhNWkjd%2Ffreedom-from-choice-should-we-surrender-our-freedom-to-an", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1352, "htmlBody": "<p>This article explores the following topic: \"When we are presented with too many choices, we can get paralized, and do nothing at all, or follow harmful heuristics, such as the path of least difficulty, or the path of least risk. Should we surrender that choice to external agents, so that among the choices that remain it is easier to determine a \"best\" choice? But which agents should we choose, and how much of our freedom should we surrender to them? Would a general AI be able to play this role for all of humanity? Given the inevitablity of the Singularity, can this even be avoided? What possibilities does this open? Is it a desirable outcome? We might end up becoming eternal minors. Literally, if immortality is reached.\"</p>\n<p>&nbsp;</p>\n<p>Sometimes life can feel like a wide open <em>quick</em>sand box: you have so many choices before you, calculating the optimal choice is nigh-impossible. The more options you have, the harder it is to make a decision. <a href=\"http://1.bp.blogspot.com/-u4-lthdxsuM/TbqkaNi7hHI/AAAAAAAAA0k/Q26S59OOo-U/s1600/Ep26_screenshot_sketch_shinji.jpg\">To employ a visual metaphor, there is no greater freedom of movement than floating in an empty void&nbsp; </a>Yet there's nowhere to go from there, and all choices are meaningless. Drawing a floor, a horizon, allows you to move along it... but you have sacrificed a degree of freedom.</p>\n<p>Life choices present you with a bit of a traveling salesman's dilemma. You may use some heuristic or another, but since heuristics by definition don't guarantee the optimum result, you still have to choose between heuristics, and consistently use the same heuristic. However, the more restrictions you place on your journey, the easier it is to discriminate between routes, and come out of it with the impression of having made the right choice, rather than lingering doubt that plagues you ever time your path becomes dangerously steep, or crowded to a crawl, where you tell yourself \"I really shouldn't have taken that<em></em> right turn at Albuquerque. Or should I have? Either way, there's no way for me to have known. But there's no way I can climb this road. I have ruined my life. But right now there's nowhere to go but on.&nbsp; There is no hope. There is no respite. There is only car.\"</p>\n<p>Hence, to make your choice in the crossroads of life, which can look less like the intersection of two curves and more like a point connecting an infinity of hyperplanes, you might be tempted to let other people, or other... things, outside of yourself, make the choices for you, just the same way when you aren't sure of the fastest way from point A to point B you just ask your GPS (or Google). You could:</p>\n<ul>\n<li>Find a leader you'd like to follow. </li>\n<li>Obey a dogmatic religious or philosophical creed</li>\n<li>Get a romantic partner in a \"property of love\" type of relationship</li>\n<li>Get into a marriage/have (a) kid(s) and allow the ties and responsibilities to force your life into a specific direction, and allow all of your free time to be allocated to childrearing.</li>\n<li>Or even write yourself a WorldOfDarkness character sheet and roll the dice everytime you have to choose. </li>\n<li>You could even get some smartphone app that lets you state options which you weight with a preference coefficient and randomly gives you one. </li>\n<li>Y<a href=\"http://actuallyusefulhoroscopes.tumblr.com/\">ou could use this very site's rational horoscope</a>. (Yesterday's advice was pretty damn useful, too!). </li>\n<li>Worst case scenario, get yourself a good old-fashione enemy, and you can center your lives into a feud against each other! (I wouldn't recommend it, but it's a fairly popular option). </li>\n<li>Or you could simply do whatever society expects you to do, like most people. Either as a follower or as a leader: don't forget being a leader often means showing a very generic personality and being a slave to PR.</li>\n<li>Once you're stuck in a career, you could devote yourself to advance through it given pre-established chains of command/promotion. One can live through a lifetime like this&nbsp;</li>\n<li>Or you could just wander aimelssly, get bit jobs you quit as soon as they get boring, or easy jobs where you have to do little, or even live off benefits, fall in love with your couch, and sink into the depths of the internet or some MMORPG where the choices and plots have already been written for you.</li>\n</ul>\n<p>So, the subordinates (for example: children, citizens, employees, intellectuals) support Freedom of Choice so that they can follow the strong desires they have every now and then, that go against the norm set by authority (sometimes this is an end in itself, especially in the arts). The superiors (for example: parents, politicians and civil servants, bosses, censors and editors) might want to give their wards more leeway in order to escape responsibility for making hard choices for the others, because they know they will be blamed if the choice leads to a failure, and because they don't want to have to deal with accusations of being oppressive, tyrannical, or heavy-handed in their use of authority.</p>\n<p>But such a climate can lead to a paralysis and a listlessness that is as bad and destructive and unhealthy as the worst dictatorship. But where to strike a balance? Which methods are the most questionable, which are the least? Surrendering your freedom to a foreign agent is a dangerous gamble! And this is where the biggest difficulty arises: the general self-modifying tranhuman AI.&nbsp;</p>\n<p>The actual traveling salesman can be brute-forced by enough processing power. Can something similar be said of every human's life? How are we going to deal with that? Will we allow it to turn our lives into scripted events optimized to every player's personalities? Ones with actual, life-threatening danger in them, even? (As immortals, will we become reckless with our lives, or more cowardly? Or will it simply be a matter of age?) Do we give the machine an Omniscient Morality License to make us live lives of excitement, drama, love, deception, hard, productive, rewarding work, and fun^4, with just the right balance of exaltation and relaxation for every individual? Will we start bitching like whiny spoiled brats if the processes aren't <em>exactly</em> optimal? There's a limit to how good a scripted event you can get in Real Life, with Real People. Or will we free-</p>\n<p>Ohmygosh. I have just found a Wild Mass Guessing for The Matrix: humans have <em>freely</em> abandoned Real Life, which they leave the literal Deux Ex Machina (should we call the general AI D.E.M., or are the doomsday-cult connotations just too massive?) to run for the continued existence of the material support of their minds. The Matrix itself, including it's blue-filtered \"Real World\" is the game the machine created for those individuals that showed that they would enjoy their lives best as cyberpunk anti-heroes. Everything that happened in the movie was staged for their sake, and nothing is real. There are other massive multiplayer games, each catering to a specific type of individual, if not an entire universe for each individual, some of them having recursive levels of reality (\"We must go deeper\"). Each of them tailor-made to entertain them the most. If DEM decides a certain individual born into the games is not fit to be told the truth (perhaps they might try something stupid like trying to \"free\" those who are aware and perfectly content), they can live their whole lives without knowing the machine<a href=\"http://cdn2.knowyourmeme.com/i/000/096/153/original/dicaprio.png\"> put a dream in their dream so they could dream while they dream.</a></p>\n<p>So, fiction aside, this seems like a fairly probable hypothetical, an attractor of futures. Should we try to avoid it? Can we? Giving up a paralizing freeedom in exchange for an exciting but pre-plotted existence? We'd be stuck as children forever, we could never grow into responsible, self reliant adults (in fact it would be strongly unadvisable: you'd utterly lose to those DEM-advised overgrown kids, and that's if the DEM isn't constantly protecting and covering your skin against your own wishes).</p>\n<p>And all of the people of the world were told they could remain children for ever. As in, for eternity.</p>\n<p><a href=\"http://www.youtube.com/watch?v=KvcsasyuP8w\">CONGRATULATIONS</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tpGCGjzd52QhNWkjd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -2, "extendedScore": null, "score": -9e-06, "legacy": true, "legacyId": "7648", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T13:20:46.506Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Inductive Bias", "slug": "seq-rerun-inductive-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:08.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XXConPwLhtfvpJGxx/seq-rerun-inductive-bias", "pageUrlRelative": "/posts/XXConPwLhtfvpJGxx/seq-rerun-inductive-bias", "linkUrl": "https://www.lesswrong.com/posts/XXConPwLhtfvpJGxx/seq-rerun-inductive-bias", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Inductive%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Inductive%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXConPwLhtfvpJGxx%2Fseq-rerun-inductive-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Inductive%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXConPwLhtfvpJGxx%2Fseq-rerun-inductive-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXConPwLhtfvpJGxx%2Fseq-rerun-inductive-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>Today's post, <a href=\"/lw/hg/inductive_bias/\">Inductive Bias</a> was originally published on April 8, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>Inductive bias is a systematic direction in belief revisions. The same observations could be evidence for or against a belief, depending on your prior. Inductive biases are more or less correct depending on how well they correspond with reality, so \"bias\" might not be the best description.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5vq/seq_rerun_debiasing_as_nonselfdestruction/\">Debiasing as Non-Self-Destruction</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XXConPwLhtfvpJGxx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.19605987569564e-07, "legacy": true, "legacyId": "7649", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["H59YqogX94z5jb8xx", "tbYGugR36hcMKeTrh", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T17:47:55.751Z", "modifiedAt": null, "url": null, "title": "Seeing Red: Dissolving Mary's Room and Qualia", "slug": "seeing-red-dissolving-mary-s-room-and-qualia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:37.705Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3wYjyQ839MDsZ6E3L/seeing-red-dissolving-mary-s-room-and-qualia", "pageUrlRelative": "/posts/3wYjyQ839MDsZ6E3L/seeing-red-dissolving-mary-s-room-and-qualia", "linkUrl": "https://www.lesswrong.com/posts/3wYjyQ839MDsZ6E3L/seeing-red-dissolving-mary-s-room-and-qualia", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeing%20Red%3A%20Dissolving%20Mary's%20Room%20and%20Qualia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeing%20Red%3A%20Dissolving%20Mary's%20Room%20and%20Qualia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3wYjyQ839MDsZ6E3L%2Fseeing-red-dissolving-mary-s-room-and-qualia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeing%20Red%3A%20Dissolving%20Mary's%20Room%20and%20Qualia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3wYjyQ839MDsZ6E3L%2Fseeing-red-dissolving-mary-s-room-and-qualia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3wYjyQ839MDsZ6E3L%2Fseeing-red-dissolving-mary-s-room-and-qualia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1660, "htmlBody": "<p><strong>Essential Background:</strong> <a href=\"/lw/of/dissolving_the_question/\">Dissolving the Question</a><a href=\"http://wiki.lesswrong.com/wiki/Zombies_%28sequence%29\"><br /></a></p>\n<p>How could we fully explain the difference between red and green to a colorblind person?</p>\n<p>Well, we could of course draw the analogy between colors of the spectrum and tones of sound; have them learn <a href=\"/lw/nh/extensions_and_intensions\">which objects are typically green and which are typically red</a> (or better yet, give them a video camera with a red filter to look through); explain many of the political, cultural and emotional associations of red and green, and so forth... but it seems that the actual difference between our experience of redness and our experience of greenness is something much harder to convey. If we focus in on that aspect of experience, we end up with the classic philosophical concept of <a href=\"http://en.wikipedia.org/wiki/Qualia\">qualia</a>, and the famous thought experiment known as <a href=\"http://en.wikipedia.org/wiki/Mary%27s_room\">Mary&rsquo;s Room</a><sup>1</sup>.<br /><br />Mary is a brilliant neuroscientist who has been colorblind from birth (due to a retina problem; her visual cortex would work normally if it were given the color input). She&rsquo;s an expert on the electromagnetic spectrum, optics, and the science of color vision. We can postulate, since this is a thought experiment, that she knows and fully understands <em>every</em> physical fact involved in color vision; she knows precisely what happens, on various levels, when the human eye sees red (and the optic nerve transmits particular types of signals, and the visual cortex processes these signals, etc).<br /><br />One day, Mary gets an operation that fixes her retinas, so that she finally sees in color for the first time. And when she wakes up, she looks at an apple and exclaims, \"Oh! So <span style=\"color: #ff0000;\"><strong>that's</strong></span> what red actually looks like.\"<sup>2</sup></p>\n<p>Now, this exclamation poses a challenge to <em>any</em> physical reductionist account of subjective experience. For if the qualia of seeing red could be reduced to a collection of basic facts about the physical world, then Mary would have learned those facts earlier and wouldn't learn anything extra now&ndash; but of course it seems that she really does learn something when she sees red for the first time. This is <em>not</em> merely the <a href=\"http://en.wikipedia.org/wiki/God_of_the_gaps\">god-of-the-gaps argument</a> that we haven't yet found a full reductionist explanation of subjective experience, but an intuitive proof that no such explanation would be complete.</p>\n<p>The argument in academic philosophy over Mary's Room <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?tid=10261&amp;ttype=2\">remains unsettled</a> to this day (though it has an interesting history, including a <a href=\"http://en.wikipedia.org/wiki/Mary%27s_room#Frank_Jackson\">change of mind</a> on the part of its originator). If we ignore the topic of subjective experience, the arguments for <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_%28sequence%29\">reductionism</a> appear to be quite overwhelming; so why does this objection, in a <a href=\"http://en.wikipedia.org/wiki/Cognitive_science\">domain</a> in which our ignorance is so vast<sup>3</sup>, seem so difficult for reductionists to convincingly reject?</p>\n<p>Veterans of this blog will know where I'm going: a question like this needs to be <a href=\"/lw/of/dissolving_the_question/\">dissolved</a>, not merely answered.</p>\n<p><a id=\"more\"></a>That is, rather than just rehashing the philosophical arguments about whether and in what sense qualia exist<sup>4</sup>, as plenty of philosophers have done without reaching consensus, we might instead ask where our <em>thoughts</em> about qualia come from, and search for a simplified version of the <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">cognitive algorithm</a> behind (our expectation of) Mary's reaction. The great thing about this alternative query is that <a href=\"/lw/oh/righting_a_wrong_question/\">it's likely to actually have an answer</a>, and that this answer can help us in our thinking about the original question.</p>\n<p>Eliezer introduced this approach in his discussion of <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">classical definitional disputes</a> and later on in the sequence on <a href=\"http://wiki.lesswrong.com/wiki/Free_will_%28solution%29\">free will</a>, and (independently, it seems) Gary Drescher relied on it in his excellent book <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=10902\">Good and Real</a> to account for a number of apparent paradoxes, but it seems that academic philosophers haven't yet taken to the idea. Essentially, it brings to the philosophy of mind an approach that is standard in the mathematical sciences: if there's a phenomenon we don't understand, it usually helps to find a simpler model that exhibits the same phenomenon, and figure out how exactly it arises in that model.</p>\n<h3>Modeling Qualia<br /></h3>\n<p>Our goal, then, is to build a model of a mind that would have an analogous reaction for a genuine reason<sup>5</sup> when placed in a scenario like Mary's Room<a href=\"/lw/pa/gazp_vs_glut/\"></a>. We don't need this model to encapsulate the full structure of human subjective experience, just enough to see where the Mary's Room argument pulls a sleight of hand.</p>\n<p>What kinds of features might our model require in order to qualify? Since the argument relies on the notions of learning and direct experience, we will certainly need to incorporate these. Another factor which is not immediately relevant, but which I argue is vital, is that our model must designate some smaller part of itself as the \"conscious\" mind, and have much of its activity take place outside of that part.</p>\n<p>Now, why should the conscious/unconscious divide matter to the experience of qualia? Firstly, we note that our qualia feel <a href=\"http://en.wikipedia.org/wiki/Ineffable\">ineffable</a> to us: that is, it seems like we know their nature very well but could never adequately communicate or articulate it. If we're thinking like a cognitive scientist, we might hypothesize that an unconscious part of the mind knows something more fully while the conscious mind, better suited to using language, lacks access to the full knowledge<sup>6</sup>.</p>\n<p>Secondly, there's an interesting pattern to our intuitions about qualia: we only get this feeling of ineffability about mental events that we're conscious of, but which are mostly processed subconsciously. For example, we don't experience the feeling of ineffability for something like counting, which happens consciously (above a threshold of five or six). If Mary had never counted more than 100 objects before, and today she counted 113 sheep in a field, we wouldn't expect her to exclaim \"Oh, so <em>that's</em> what 113 looks like!\"</p>\n<p>In the other direction, there's a <em>lot</em> of unconscious processing that goes into the process of digestion, but unless we get sick, the intermediate steps don't generally rise to conscious awareness. If Mary had never had pineapple before, she might well extol the qualia of its taste, but not that of its properties as it navigates her small intestine. You could think of these as hidden qualia, perhaps, but it doesn't intuitively feel like there's something extra to be explained the way there is with redness.</p>\n<p>Of course, there are plenty of other features we might nominate for inclusion in our model, but as it turns out, we can get a long way with just these two. In the next post, I'll introduce Martha, a simple model of a learning mind with a conscious/unconscious distinction, and in the third post I'll show how Martha reacts in the situation of Mary's Room, and how this reaction arises in a non-mysterious way. Even without claiming that Martha is a good analogue of the human mind, this will suffice to show why Mary's Room is not a logically valid argument against reductionism, since if it were then it would equally apply to Martha. And if we start to see a bit of ourselves in Martha after all, so much the better for our understanding of qualia...</p>\n<h3 style=\"padding-left: 120px;\"><a href=\"/lw/5op/qualia_strike_back/\">TO BE CONTINUED</a><br /></h3>\n<p>&nbsp;</p>\n<h3>Disclaimer</h3>\n<p>One could reasonably ask <a href=\"/lw/2qy/what_makes_my_attempt_special/\">what makes my attempt special</a> on such a well-argued topic, given that I&rsquo;m not credentialed as a philosopher. First, I'd reiterate that academic philosophers really haven&rsquo;t started to use the concept of dissolving a question- I don&rsquo;t think Daniel Dennett, for instance, ever explored this train of thought. And secondly, of those who do try and map cognitive algorithms within philosophy of mind, Eliezer hasn't tackled qualia in this way, while Gary Drescher gives them short shrift in <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=10902\">Good and Real</a>. (The latter essentially makes Dennett's argument that with enough self-knowledge qualia wouldn&rsquo;t be ineffable. But in my mind this fails to really dissolve the question- see my footnote 4.)</p>\n<h3>Footnotes:</h3>\n<p><strong>1.</strong> The argument is called \"Mary&rsquo;s Room\" because the original version (due to Frank Jackson) posited that Mary had perfectly normal vision but happened to be raised and educated in a perfectly grayscale environment, and one day stepped out into the colorful world like Dorothy in The Wizard of Oz. I prefer the more plausible and philosophically equivalent variant discussed above, although it drifts away from the etymology of the argument&rsquo;s name.</p>\n<p><strong>2.</strong> Ironically, it was a <span style=\"color: #ff0000;\"><strong>green</strong></span> apple rather than a <span style=\"color: #339966;\"><strong>red</strong></span> one, but Mary soon realized and rectified her error. The point stands.</p>\n<p><strong>3.</strong> In general, an important rationalist heuristic is to not draw far-reaching conclusions from an intuitively plausible argument about a subject (like subjective experience) which you find extremely confusing.</p>\n<p><strong>4.</strong> Before we move on, though, one key reductionist reply to Mary&rsquo;s Room is that either qualia have physical effects (like causing Mary to say \"Oh!\") or they don't. <a href=\"http://en.wikipedia.org/wiki/Substance_dualism#Substance_dualism\">If they do</a>, then either they reduce to ordinary physics or you could expect to find violations of physical law in the human brain, which few modern philosophers would dare to bet on. And <a href=\"http://en.wikipedia.org/wiki/Property_dualism\">if they don't have any physical effects</a>, then somehow whatever causes her to say \"Oh!\" has nothing to do with her actual experience of redness, which is an exceptionally weird stance if you ponder it for a moment; read the <a href=\"http://wiki.lesswrong.com/wiki/Zombies_%28sequence%29\">zombie sequence</a> if you're curious.</p>\n<p>Furthermore, one could object (as Dennett does) that Mary&rsquo;s Room, like <a href=\"http://en.wikipedia.org/wiki/Chinese_room\">Searle&rsquo;s Chinese Room</a>, is playing sleight of hand with impossible levels of knowledge for a human, and that an agent who could really handle such massive quantities of information really wouldn't learn anything new when finally having the experience. But to me this is an unsatisfying objection, because we don&rsquo;t expect to see the effect of the experience diminish significantly as we increase her level of understanding within human bounds&ndash; and at most, this objection provides a plausible escape from the argument rather than a refutation.</p>\n<p><strong>5.</strong> (and not, for instance, <a href=\"/lw/pa/gazp_vs_glut\">because we programmed in that specific reaction on its own</a>)</p>\n<p><strong>6.</strong> Indeed, the vast majority of visual processing- estimating distances, distinguishing objects, even identifying colors- is done subconsciously; that's why knowing that something is an optical illusion doesn't make you stop seeing the illusion. Steven Pinker's <a href=\"http://en.wikipedia.org/wiki/How_the_Mind_Works\">How the Mind Works</a> contains a treasure trove of examples on this subject.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8e9e8fzXuW5gGBS3F": 3, "XSryTypw5Hszpa4TS": 2, "RMtdp6eGNjTZcmwJ6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3wYjyQ839MDsZ6E3L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 52, "extendedScore": null, "score": 9.9e-05, "legacy": true, "legacyId": "7317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Essential Background:</strong> <a href=\"/lw/of/dissolving_the_question/\">Dissolving the Question</a><a href=\"http://wiki.lesswrong.com/wiki/Zombies_%28sequence%29\"><br></a></p>\n<p>How could we fully explain the difference between red and green to a colorblind person?</p>\n<p>Well, we could of course draw the analogy between colors of the spectrum and tones of sound; have them learn <a href=\"/lw/nh/extensions_and_intensions\">which objects are typically green and which are typically red</a> (or better yet, give them a video camera with a red filter to look through); explain many of the political, cultural and emotional associations of red and green, and so forth... but it seems that the actual difference between our experience of redness and our experience of greenness is something much harder to convey. If we focus in on that aspect of experience, we end up with the classic philosophical concept of <a href=\"http://en.wikipedia.org/wiki/Qualia\">qualia</a>, and the famous thought experiment known as <a href=\"http://en.wikipedia.org/wiki/Mary%27s_room\">Mary\u2019s Room</a><sup>1</sup>.<br><br>Mary is a brilliant neuroscientist who has been colorblind from birth (due to a retina problem; her visual cortex would work normally if it were given the color input). She\u2019s an expert on the electromagnetic spectrum, optics, and the science of color vision. We can postulate, since this is a thought experiment, that she knows and fully understands <em>every</em> physical fact involved in color vision; she knows precisely what happens, on various levels, when the human eye sees red (and the optic nerve transmits particular types of signals, and the visual cortex processes these signals, etc).<br><br>One day, Mary gets an operation that fixes her retinas, so that she finally sees in color for the first time. And when she wakes up, she looks at an apple and exclaims, \"Oh! So <span style=\"color: #ff0000;\"><strong>that's</strong></span> what red actually looks like.\"<sup>2</sup></p>\n<p>Now, this exclamation poses a challenge to <em>any</em> physical reductionist account of subjective experience. For if the qualia of seeing red could be reduced to a collection of basic facts about the physical world, then Mary would have learned those facts earlier and wouldn't learn anything extra now\u2013 but of course it seems that she really does learn something when she sees red for the first time. This is <em>not</em> merely the <a href=\"http://en.wikipedia.org/wiki/God_of_the_gaps\">god-of-the-gaps argument</a> that we haven't yet found a full reductionist explanation of subjective experience, but an intuitive proof that no such explanation would be complete.</p>\n<p>The argument in academic philosophy over Mary's Room <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?tid=10261&amp;ttype=2\">remains unsettled</a> to this day (though it has an interesting history, including a <a href=\"http://en.wikipedia.org/wiki/Mary%27s_room#Frank_Jackson\">change of mind</a> on the part of its originator). If we ignore the topic of subjective experience, the arguments for <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_%28sequence%29\">reductionism</a> appear to be quite overwhelming; so why does this objection, in a <a href=\"http://en.wikipedia.org/wiki/Cognitive_science\">domain</a> in which our ignorance is so vast<sup>3</sup>, seem so difficult for reductionists to convincingly reject?</p>\n<p>Veterans of this blog will know where I'm going: a question like this needs to be <a href=\"/lw/of/dissolving_the_question/\">dissolved</a>, not merely answered.</p>\n<p><a id=\"more\"></a>That is, rather than just rehashing the philosophical arguments about whether and in what sense qualia exist<sup>4</sup>, as plenty of philosophers have done without reaching consensus, we might instead ask where our <em>thoughts</em> about qualia come from, and search for a simplified version of the <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">cognitive algorithm</a> behind (our expectation of) Mary's reaction. The great thing about this alternative query is that <a href=\"/lw/oh/righting_a_wrong_question/\">it's likely to actually have an answer</a>, and that this answer can help us in our thinking about the original question.</p>\n<p>Eliezer introduced this approach in his discussion of <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">classical definitional disputes</a> and later on in the sequence on <a href=\"http://wiki.lesswrong.com/wiki/Free_will_%28solution%29\">free will</a>, and (independently, it seems) Gary Drescher relied on it in his excellent book <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=10902\">Good and Real</a> to account for a number of apparent paradoxes, but it seems that academic philosophers haven't yet taken to the idea. Essentially, it brings to the philosophy of mind an approach that is standard in the mathematical sciences: if there's a phenomenon we don't understand, it usually helps to find a simpler model that exhibits the same phenomenon, and figure out how exactly it arises in that model.</p>\n<h3 id=\"Modeling_Qualia\">Modeling Qualia<br></h3>\n<p>Our goal, then, is to build a model of a mind that would have an analogous reaction for a genuine reason<sup>5</sup> when placed in a scenario like Mary's Room<a href=\"/lw/pa/gazp_vs_glut/\"></a>. We don't need this model to encapsulate the full structure of human subjective experience, just enough to see where the Mary's Room argument pulls a sleight of hand.</p>\n<p>What kinds of features might our model require in order to qualify? Since the argument relies on the notions of learning and direct experience, we will certainly need to incorporate these. Another factor which is not immediately relevant, but which I argue is vital, is that our model must designate some smaller part of itself as the \"conscious\" mind, and have much of its activity take place outside of that part.</p>\n<p>Now, why should the conscious/unconscious divide matter to the experience of qualia? Firstly, we note that our qualia feel <a href=\"http://en.wikipedia.org/wiki/Ineffable\">ineffable</a> to us: that is, it seems like we know their nature very well but could never adequately communicate or articulate it. If we're thinking like a cognitive scientist, we might hypothesize that an unconscious part of the mind knows something more fully while the conscious mind, better suited to using language, lacks access to the full knowledge<sup>6</sup>.</p>\n<p>Secondly, there's an interesting pattern to our intuitions about qualia: we only get this feeling of ineffability about mental events that we're conscious of, but which are mostly processed subconsciously. For example, we don't experience the feeling of ineffability for something like counting, which happens consciously (above a threshold of five or six). If Mary had never counted more than 100 objects before, and today she counted 113 sheep in a field, we wouldn't expect her to exclaim \"Oh, so <em>that's</em> what 113 looks like!\"</p>\n<p>In the other direction, there's a <em>lot</em> of unconscious processing that goes into the process of digestion, but unless we get sick, the intermediate steps don't generally rise to conscious awareness. If Mary had never had pineapple before, she might well extol the qualia of its taste, but not that of its properties as it navigates her small intestine. You could think of these as hidden qualia, perhaps, but it doesn't intuitively feel like there's something extra to be explained the way there is with redness.</p>\n<p>Of course, there are plenty of other features we might nominate for inclusion in our model, but as it turns out, we can get a long way with just these two. In the next post, I'll introduce Martha, a simple model of a learning mind with a conscious/unconscious distinction, and in the third post I'll show how Martha reacts in the situation of Mary's Room, and how this reaction arises in a non-mysterious way. Even without claiming that Martha is a good analogue of the human mind, this will suffice to show why Mary's Room is not a logically valid argument against reductionism, since if it were then it would equally apply to Martha. And if we start to see a bit of ourselves in Martha after all, so much the better for our understanding of qualia...</p>\n<h3 style=\"padding-left: 120px;\" id=\"TO_BE_CONTINUED\"><a href=\"/lw/5op/qualia_strike_back/\">TO BE CONTINUED</a><br></h3>\n<p>&nbsp;</p>\n<h3 id=\"Disclaimer\">Disclaimer</h3>\n<p>One could reasonably ask <a href=\"/lw/2qy/what_makes_my_attempt_special/\">what makes my attempt special</a> on such a well-argued topic, given that I\u2019m not credentialed as a philosopher. First, I'd reiterate that academic philosophers really haven\u2019t started to use the concept of dissolving a question- I don\u2019t think Daniel Dennett, for instance, ever explored this train of thought. And secondly, of those who do try and map cognitive algorithms within philosophy of mind, Eliezer hasn't tackled qualia in this way, while Gary Drescher gives them short shrift in <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=10902\">Good and Real</a>. (The latter essentially makes Dennett's argument that with enough self-knowledge qualia wouldn\u2019t be ineffable. But in my mind this fails to really dissolve the question- see my footnote 4.)</p>\n<h3 id=\"Footnotes_\">Footnotes:</h3>\n<p><strong>1.</strong> The argument is called \"Mary\u2019s Room\" because the original version (due to Frank Jackson) posited that Mary had perfectly normal vision but happened to be raised and educated in a perfectly grayscale environment, and one day stepped out into the colorful world like Dorothy in The Wizard of Oz. I prefer the more plausible and philosophically equivalent variant discussed above, although it drifts away from the etymology of the argument\u2019s name.</p>\n<p><strong>2.</strong> Ironically, it was a <span style=\"color: #ff0000;\"><strong>green</strong></span> apple rather than a <span style=\"color: #339966;\"><strong>red</strong></span> one, but Mary soon realized and rectified her error. The point stands.</p>\n<p><strong>3.</strong> In general, an important rationalist heuristic is to not draw far-reaching conclusions from an intuitively plausible argument about a subject (like subjective experience) which you find extremely confusing.</p>\n<p><strong>4.</strong> Before we move on, though, one key reductionist reply to Mary\u2019s Room is that either qualia have physical effects (like causing Mary to say \"Oh!\") or they don't. <a href=\"http://en.wikipedia.org/wiki/Substance_dualism#Substance_dualism\">If they do</a>, then either they reduce to ordinary physics or you could expect to find violations of physical law in the human brain, which few modern philosophers would dare to bet on. And <a href=\"http://en.wikipedia.org/wiki/Property_dualism\">if they don't have any physical effects</a>, then somehow whatever causes her to say \"Oh!\" has nothing to do with her actual experience of redness, which is an exceptionally weird stance if you ponder it for a moment; read the <a href=\"http://wiki.lesswrong.com/wiki/Zombies_%28sequence%29\">zombie sequence</a> if you're curious.</p>\n<p>Furthermore, one could object (as Dennett does) that Mary\u2019s Room, like <a href=\"http://en.wikipedia.org/wiki/Chinese_room\">Searle\u2019s Chinese Room</a>, is playing sleight of hand with impossible levels of knowledge for a human, and that an agent who could really handle such massive quantities of information really wouldn't learn anything new when finally having the experience. But to me this is an unsatisfying objection, because we don\u2019t expect to see the effect of the experience diminish significantly as we increase her level of understanding within human bounds\u2013 and at most, this objection provides a plausible escape from the argument rather than a refutation.</p>\n<p><strong>5.</strong> (and not, for instance, <a href=\"/lw/pa/gazp_vs_glut\">because we programmed in that specific reaction on its own</a>)</p>\n<p><strong>6.</strong> Indeed, the vast majority of visual processing- estimating distances, distinguishing objects, even identifying colors- is done subconsciously; that's why knowing that something is an optical illusion doesn't make you stop seeing the illusion. Steven Pinker's <a href=\"http://en.wikipedia.org/wiki/How_the_Mind_Works\">How the Mind Works</a> contains a treasure trove of examples on this subject.</p>", "sections": [{"title": "Modeling Qualia", "anchor": "Modeling_Qualia", "level": 1}, {"title": "TO BE CONTINUED", "anchor": "TO_BE_CONTINUED", "level": 1}, {"title": "Disclaimer", "anchor": "Disclaimer", "level": 1}, {"title": "Footnotes:", "anchor": "Footnotes_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "154 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 154, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mc6QcrsbH5NRXbCRX", "HsznWM9A7NiuGsp28", "yA4gF5KrboK2m2Xu7", "rQEwySCcLtdKHkrHp", "k6EPphHiBH4WWYFCj", "pi5DAEZWJK3c9NAhW", "DbJ7tEhtxNWpPBxo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T18:06:52.947Z", "modifiedAt": null, "url": null, "title": "Bias in capital project decision making", "slug": "bias-in-capital-project-decision-making", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:10.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xWCiPNkzMvkdWNgb2/bias-in-capital-project-decision-making", "pageUrlRelative": "/posts/xWCiPNkzMvkdWNgb2/bias-in-capital-project-decision-making", "linkUrl": "https://www.lesswrong.com/posts/xWCiPNkzMvkdWNgb2/bias-in-capital-project-decision-making", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bias%20in%20capital%20project%20decision%20making&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABias%20in%20capital%20project%20decision%20making%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWCiPNkzMvkdWNgb2%2Fbias-in-capital-project-decision-making%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bias%20in%20capital%20project%20decision%20making%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWCiPNkzMvkdWNgb2%2Fbias-in-capital-project-decision-making", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWCiPNkzMvkdWNgb2%2Fbias-in-capital-project-decision-making", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1094, "htmlBody": "<p><span style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px; \">This is a story about an odd fact about capital project decision making in engineering I noticed and how it might be related to cognitive biases</span></p>\n<p style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \"><span style=\"font-family: arial; font-size: small; border-collapse: separate; \"> </span></p>\n<h1 style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">Background</h1>\n<div style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">Although I don't work in the field, I was trained as a&nbsp;<a style=\"color: #0065cc; \" href=\"http://en.wikipedia.org/wiki/Chemical_engineering\" target=\"_blank\">chemical engineer</a>. A chemical engineer's job is a little different than you might imagine. A chemical engineers primary job isn't to design chemical processes, they actually do relatively little chemistry, but to build, optimize and maintain industrial plants that produce chemicals (petrol products, cleaners, paint etc.) and materials that are produced similarly to chemicals (wood pulp,&nbsp;composite&nbsp;materials etc.). Questions similar to 'how fast should we mix the fluid in this reactor to make it most efficient?' or 'how can we reuse the waste heat from this process?' are much more common than questions similar to 'how can we create compound A from compound B?'. &nbsp;</div>\n<div style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">\n<p>Chemical engineers&nbsp;often have to make decisions about what capital improvement projects the firm will undertake, so they must answer questions such as 'install cheap pumps that wear out quickly or the expensive ones that don't?', &nbsp;'what ethanol producing bacteria is most efficient for producing ethanol?' and 'is it worth it to install a heat exchanger to recover the waste head from this process or not?'. The standard technical way of judging the profitability of an option or project is to calculate the&nbsp;<a style=\"color: #0065cc; \" href=\"http://en.wikipedia.org/wiki/Net_present_value\" target=\"_blank\">Net Present Value</a>&nbsp;(NPV) of the expected cash flows to and from the firm for each different option (installing pump type A or B, using bacteria A, B or C, installing or not installing a heat exchanger). The option with the highest NPV is the most profitable. Calculating the NPV discounts future expected cash flows for the fact that they occur in the future and you have other productive things you could do with money, such as earning interest with it.&nbsp;</p>\n</div>\n<h1 style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">Oddly high discount rates</h1>\n<div class=\"im\" style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px; \">\n<div style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">\n<p>When I was in school, I noticed an odd thing: the interest rates that people used to evaluate projects on this basis, called the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Minimum_acceptable_rate_of_return\" target=\"_blank\">Minimum Acceptable Rate of Return</a>&nbsp;(MARR), were often rather high, 15-50%/year. I saw this in textbook discussions and had it confirmed by several working engineers and engineering managers. My engineering economics teacher mentioned that firms often require two year \"pay back periods\" for projects; that&nbsp;annualizes&nbsp;to a 50% interest rate! I was very confused about this because a bank will loan a small business at ~8% interest (<a href=\"http://www.becu.org/small-business/loans.aspx\" target=\"_blank\">source</a>) and mortgage rates are around 5% (<a href=\"https://www.google.com/advisor/mortgages\" target=\"_blank\">source</a>). This implied that many many industrial projects would be profitable if only outsiders could fund them, because investors should jump at the chance to get 15% returns. I know I would! The profit opportunity seemed so great that I started to work out business models around alternative sources of investment for industrial projects.&nbsp;</p>\n</div>\n</div>\n<h1 style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">Overestimating benefits, underestimating costs</h1>\n<p style=\"border-collapse: separate; font-family: arial; \"><span style=\"font-family: arial, sans-serif;\"><span style=\"border-collapse: collapse; \">To understand why MARRs are so high in chemical engineering, I tried to find research on the question and talked to experienced engineers and managers. Unfortunately, I never did find research on this topic (let me know if you know of relevant research). I talked to several managers and engineers and the most common answer I got was that investors are short sighted and primarily interested in short run profits. I didn't find this answer very plausible. Later, I met an engineer in charge of re</span></span>viewing project evaluations made by other engineers in order to decide which projects would be approved. His explanation was that engineers usually overestimate the benefits of a project under consideration and underestimate the costs and that they gave engineers high MARRs in order to counterbalance this. I asked him why they didn't just apply a scaling factor to the costs and benefits and he explained that they did this a little bit, but engineers respond to this by inflating benefits and deflating costs even more! I later met another engineer who talked about doing exactly that; adjusting estimated costs down and estimated benefits up because the process evaluating projects did the reverse.&nbsp;</p>\n<p style=\"border-collapse: separate; font-family: arial; \">One thing to note is that if engineers overestimate benefits, underestimate costs uniformly over time, then a high MARR will make projects which pay off in the short term artificially attractive (which is why I asked about using a scaling factor instead of a large interest rate). On the other hand, if engineers make more biased predictions about costs and benefits the further out they are in time (for example, if they tend to overestimate the productive life of equipment), then a high MARR is a more appropriate remedy.</p>\n<h1 style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">Possible explanations</h1>\n<p style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">There are a couple of reasons why engineers might&nbsp;end to overestimate the benefits and underestimate the costs of projects. Any number of these may contribute. I suspect cognitive bias is a significant contributor.&nbsp;</p>\n<ol style=\"border-collapse: separate; font-family: arial; \">\n<li><a style=\"color: #0065cc; border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \" href=\"http://en.wikipedia.org/wiki/Confirmation_bias\" target=\"_blank\">Confirmation bias</a><span style=\"font-family: arial, sans-serif;\"><span style=\"border-collapse: collapse; \">&nbsp;suggests engineers will tend to overestimate the benefits and underestimate the costs of projects they initially think are good ideas. The head project engineer I spoke with described a common mindset thus,&nbsp;<br /></span></span>'And this is why we tend to focus on the goodness and diminish the badness of projects.&nbsp; We&nbsp;know&nbsp;they are good so all we need to do is prove it to get the approvers bought in.&nbsp;Then the project approvers over time notice that these projects returns are lower than expected so they say, &ldquo;Let&rsquo;s raise the bar.&rdquo;&nbsp; But guess what?&nbsp; The bar never rises.&nbsp; Why?&nbsp; Because we still \"know\" what the good projects are and all we need to do is prove they are good.'</li>\n<li>The&nbsp;<a style=\"color: #0065cc; \" href=\"http://wiki.lesswrong.com/wiki/Planning_fallacy\" target=\"_blank\">planning fallacy</a>&nbsp;suggests engineers will underestimate completion times and costs.&nbsp;</li>\n<li><a style=\"color: #0065cc; \" href=\"http://wiki.lesswrong.com/wiki/Overconfidence\" target=\"_blank\">Overconfidence</a>&nbsp;suggests engineers will underestimate costs even when explicitly accounting for uncertainty.</li>\n<li>Bad incentives: engineers may often be rewarded for spearheading projects and not punished commensurately if the project is not beneficial so that they often expect to be rewarded for spearheading a project even if they don't expect it to be a success.</li>\n</ol>\n<div style=\"border-collapse: separate; font-family: arial; \"><strong>Addendum: </strong>The same head project engineer suggests that one way to get better predictions, at least with respect to project duration, is to have non-technical observers make the predictions (related to taking an <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">outside view</a>).</div>\n<blockquote style=\"border-collapse: separate; font-family: arial; \">\n<p>Anyway I started asking our cost coordinator about predicted schedule and she is by far more accurate than the engineers with how long it takes to do a project. That has led me to think that an independent review would be a good step in project returns. Unfortunately, I have not noticed her to be any better on predicting project performance than the engineers.&nbsp;</p>\n</blockquote>\n<p style=\"border-collapse: separate; font-family: arial; \">On adjusting predictions based on a track record</p>\n<blockquote style=\"border-collapse: separate; font-family: arial; \">\n<p>The problem with predicting a project will take longer than expected based on experience does not help because managers (usually engineers) want to know \"why\" so the can \"fix it.\"</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1, "PDJ6KqJBRzvKPfuS3": 1, "KoXbd2HmbdRfqLngk": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xWCiPNkzMvkdWNgb2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 56, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "7650", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px; \">This is a story about an odd fact about capital project decision making in engineering I noticed and how it might be related to cognitive biases</span></p>\n<p style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \"><span style=\"font-family: arial; font-size: small; border-collapse: separate; \"> </span></p>\n<h1 style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \" id=\"Background\">Background</h1>\n<div style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">Although I don't work in the field, I was trained as a&nbsp;<a style=\"color: #0065cc; \" href=\"http://en.wikipedia.org/wiki/Chemical_engineering\" target=\"_blank\">chemical engineer</a>. A chemical engineer's job is a little different than you might imagine. A chemical engineers primary job isn't to design chemical processes, they actually do relatively little chemistry, but to build, optimize and maintain industrial plants that produce chemicals (petrol products, cleaners, paint etc.) and materials that are produced similarly to chemicals (wood pulp,&nbsp;composite&nbsp;materials etc.). Questions similar to 'how fast should we mix the fluid in this reactor to make it most efficient?' or 'how can we reuse the waste heat from this process?' are much more common than questions similar to 'how can we create compound A from compound B?'. &nbsp;</div>\n<div style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">\n<p>Chemical engineers&nbsp;often have to make decisions about what capital improvement projects the firm will undertake, so they must answer questions such as 'install cheap pumps that wear out quickly or the expensive ones that don't?', &nbsp;'what ethanol producing bacteria is most efficient for producing ethanol?' and 'is it worth it to install a heat exchanger to recover the waste head from this process or not?'. The standard technical way of judging the profitability of an option or project is to calculate the&nbsp;<a style=\"color: #0065cc; \" href=\"http://en.wikipedia.org/wiki/Net_present_value\" target=\"_blank\">Net Present Value</a>&nbsp;(NPV) of the expected cash flows to and from the firm for each different option (installing pump type A or B, using bacteria A, B or C, installing or not installing a heat exchanger). The option with the highest NPV is the most profitable. Calculating the NPV discounts future expected cash flows for the fact that they occur in the future and you have other productive things you could do with money, such as earning interest with it.&nbsp;</p>\n</div>\n<h1 style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \" id=\"Oddly_high_discount_rates\">Oddly high discount rates</h1>\n<div class=\"im\" style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px; \">\n<div style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">\n<p>When I was in school, I noticed an odd thing: the interest rates that people used to evaluate projects on this basis, called the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Minimum_acceptable_rate_of_return\" target=\"_blank\">Minimum Acceptable Rate of Return</a>&nbsp;(MARR), were often rather high, 15-50%/year. I saw this in textbook discussions and had it confirmed by several working engineers and engineering managers. My engineering economics teacher mentioned that firms often require two year \"pay back periods\" for projects; that&nbsp;annualizes&nbsp;to a 50% interest rate! I was very confused about this because a bank will loan a small business at ~8% interest (<a href=\"http://www.becu.org/small-business/loans.aspx\" target=\"_blank\">source</a>) and mortgage rates are around 5% (<a href=\"https://www.google.com/advisor/mortgages\" target=\"_blank\">source</a>). This implied that many many industrial projects would be profitable if only outsiders could fund them, because investors should jump at the chance to get 15% returns. I know I would! The profit opportunity seemed so great that I started to work out business models around alternative sources of investment for industrial projects.&nbsp;</p>\n</div>\n</div>\n<h1 style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \" id=\"Overestimating_benefits__underestimating_costs\">Overestimating benefits, underestimating costs</h1>\n<p style=\"border-collapse: separate; font-family: arial; \"><span style=\"font-family: arial, sans-serif;\"><span style=\"border-collapse: collapse; \">To understand why MARRs are so high in chemical engineering, I tried to find research on the question and talked to experienced engineers and managers. Unfortunately, I never did find research on this topic (let me know if you know of relevant research). I talked to several managers and engineers and the most common answer I got was that investors are short sighted and primarily interested in short run profits. I didn't find this answer very plausible. Later, I met an engineer in charge of re</span></span>viewing project evaluations made by other engineers in order to decide which projects would be approved. His explanation was that engineers usually overestimate the benefits of a project under consideration and underestimate the costs and that they gave engineers high MARRs in order to counterbalance this. I asked him why they didn't just apply a scaling factor to the costs and benefits and he explained that they did this a little bit, but engineers respond to this by inflating benefits and deflating costs even more! I later met another engineer who talked about doing exactly that; adjusting estimated costs down and estimated benefits up because the process evaluating projects did the reverse.&nbsp;</p>\n<p style=\"border-collapse: separate; font-family: arial; \">One thing to note is that if engineers overestimate benefits, underestimate costs uniformly over time, then a high MARR will make projects which pay off in the short term artificially attractive (which is why I asked about using a scaling factor instead of a large interest rate). On the other hand, if engineers make more biased predictions about costs and benefits the further out they are in time (for example, if they tend to overestimate the productive life of equipment), then a high MARR is a more appropriate remedy.</p>\n<h1 style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \" id=\"Possible_explanations\">Possible explanations</h1>\n<p style=\"border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \">There are a couple of reasons why engineers might&nbsp;end to overestimate the benefits and underestimate the costs of projects. Any number of these may contribute. I suspect cognitive bias is a significant contributor.&nbsp;</p>\n<ol style=\"border-collapse: separate; font-family: arial; \">\n<li><a style=\"color: #0065cc; border-collapse: collapse; font-size: 13px; font-family: arial, sans-serif; \" href=\"http://en.wikipedia.org/wiki/Confirmation_bias\" target=\"_blank\">Confirmation bias</a><span style=\"font-family: arial, sans-serif;\"><span style=\"border-collapse: collapse; \">&nbsp;suggests engineers will tend to overestimate the benefits and underestimate the costs of projects they initially think are good ideas. The head project engineer I spoke with described a common mindset thus,&nbsp;<br></span></span>'And this is why we tend to focus on the goodness and diminish the badness of projects.&nbsp; We&nbsp;know&nbsp;they are good so all we need to do is prove it to get the approvers bought in.&nbsp;Then the project approvers over time notice that these projects returns are lower than expected so they say, \u201cLet\u2019s raise the bar.\u201d&nbsp; But guess what?&nbsp; The bar never rises.&nbsp; Why?&nbsp; Because we still \"know\" what the good projects are and all we need to do is prove they are good.'</li>\n<li>The&nbsp;<a style=\"color: #0065cc; \" href=\"http://wiki.lesswrong.com/wiki/Planning_fallacy\" target=\"_blank\">planning fallacy</a>&nbsp;suggests engineers will underestimate completion times and costs.&nbsp;</li>\n<li><a style=\"color: #0065cc; \" href=\"http://wiki.lesswrong.com/wiki/Overconfidence\" target=\"_blank\">Overconfidence</a>&nbsp;suggests engineers will underestimate costs even when explicitly accounting for uncertainty.</li>\n<li>Bad incentives: engineers may often be rewarded for spearheading projects and not punished commensurately if the project is not beneficial so that they often expect to be rewarded for spearheading a project even if they don't expect it to be a success.</li>\n</ol>\n<div style=\"border-collapse: separate; font-family: arial; \"><strong>Addendum: </strong>The same head project engineer suggests that one way to get better predictions, at least with respect to project duration, is to have non-technical observers make the predictions (related to taking an <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">outside view</a>).</div>\n<blockquote style=\"border-collapse: separate; font-family: arial; \">\n<p>Anyway I started asking our cost coordinator about predicted schedule and she is by far more accurate than the engineers with how long it takes to do a project. That has led me to think that an independent review would be a good step in project returns. Unfortunately, I have not noticed her to be any better on predicting project performance than the engineers.&nbsp;</p>\n</blockquote>\n<p style=\"border-collapse: separate; font-family: arial; \">On adjusting predictions based on a track record</p>\n<blockquote style=\"border-collapse: separate; font-family: arial; \">\n<p>The problem with predicting a project will take longer than expected based on experience does not help because managers (usually engineers) want to know \"why\" so the can \"fix it.\"</p>\n</blockquote>", "sections": [{"title": "Background", "anchor": "Background", "level": 1}, {"title": "Oddly high discount rates", "anchor": "Oddly_high_discount_rates", "level": 1}, {"title": "Overestimating benefits, underestimating costs", "anchor": "Overestimating_benefits__underestimating_costs", "level": 1}, {"title": "Possible explanations", "anchor": "Possible_explanations", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T20:39:08.216Z", "modifiedAt": null, "url": null, "title": "lessannoying.org", "slug": "lessannoying-org-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bongo", "createdAt": "2009-02-27T12:08:06.258Z", "isAdmin": false, "displayName": "Bongo"}, "userId": "mLnNK3xEMczLs8ind", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DMQ9PDzAsADNkGzed/lessannoying-org-0", "pageUrlRelative": "/posts/DMQ9PDzAsADNkGzed/lessannoying-org-0", "linkUrl": "https://www.lesswrong.com/posts/DMQ9PDzAsADNkGzed/lessannoying-org-0", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20lessannoying.org&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Alessannoying.org%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMQ9PDzAsADNkGzed%2Flessannoying-org-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=lessannoying.org%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMQ9PDzAsADNkGzed%2Flessannoying-org-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMQ9PDzAsADNkGzed%2Flessannoying-org-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2, "htmlBody": "<p>&lt;a href=\"http://<a href=\"http://www.lessannoying.org/\">http://www.lessannoying.org/</a>\"&gt;http://imgur.com/hNC03.jpg&lt;/a&gt;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DMQ9PDzAsADNkGzed", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7651", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-26T20:40:46.865Z", "modifiedAt": null, "url": null, "title": "lessannoying.org", "slug": "lessannoying-org", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:08.291Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bongo", "createdAt": "2009-02-27T12:08:06.258Z", "isAdmin": false, "displayName": "Bongo"}, "userId": "mLnNK3xEMczLs8ind", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7N6ZwsgwJ9BAAyBGb/lessannoying-org", "pageUrlRelative": "/posts/7N6ZwsgwJ9BAAyBGb/lessannoying-org", "linkUrl": "https://www.lesswrong.com/posts/7N6ZwsgwJ9BAAyBGb/lessannoying-org", "postedAtFormatted": "Thursday, May 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20lessannoying.org&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Alessannoying.org%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7N6ZwsgwJ9BAAyBGb%2Flessannoying-org%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=lessannoying.org%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7N6ZwsgwJ9BAAyBGb%2Flessannoying-org", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7N6ZwsgwJ9BAAyBGb%2Flessannoying-org", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4, "htmlBody": "<p><a href=\"http://www.lessannoying.org/\">What does it mean?</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7N6ZwsgwJ9BAAyBGb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 7.197351399999166e-07, "legacy": true, "legacyId": "7652", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-27T01:05:25.387Z", "modifiedAt": null, "url": null, "title": "Measuring aversion and habit strength", "slug": "measuring-aversion-and-habit-strength", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:52.920Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fxv4o3LGEkgR2Qsz7/measuring-aversion-and-habit-strength", "pageUrlRelative": "/posts/Fxv4o3LGEkgR2Qsz7/measuring-aversion-and-habit-strength", "linkUrl": "https://www.lesswrong.com/posts/Fxv4o3LGEkgR2Qsz7/measuring-aversion-and-habit-strength", "postedAtFormatted": "Friday, May 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Measuring%20aversion%20and%20habit%20strength&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeasuring%20aversion%20and%20habit%20strength%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFxv4o3LGEkgR2Qsz7%2Fmeasuring-aversion-and-habit-strength%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Measuring%20aversion%20and%20habit%20strength%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFxv4o3LGEkgR2Qsz7%2Fmeasuring-aversion-and-habit-strength", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFxv4o3LGEkgR2Qsz7%2Fmeasuring-aversion-and-habit-strength", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 872, "htmlBody": "<!-- Measuring aversion and habit strength -->\n<p><img src=\"http://i51.tinypic.com/mcq23t.jpg\" alt=\"Not me!\" width=\"30%\" align=\"right\" /> tl;dr: <em>Strong aversions don't always originate from strong feelings (see <a href=\"/lw/21b/ugh_fields/\">Ugh fields</a>).  It's useful to measure the strength of an aversion by <strong>how effectively</strong> it averts your thoughts/behavior instead of <strong>how saliently</strong> you can feel it, or even remember feeling it.  If there's a low cost behaviour that you somehow always \"end up not doing\", there's evidence for a mechanism steering you away from it.  Try to find it, and defy it.</em></p>\n<p><strong>Story</strong></p>\n<p>Right after writing <a href=\"/lw/2sh/break_your_habits_be_more_empirical/\">Break your habits: be more empirical</a>, someone asked me to a live music show, and I  declined, with some explanation about being busy.  This <a href=\"/lw/if/your_strength_as_a_rationalist/\">felt a little forced</a>, and I realized: <em>I always decline live music shows.  This counts as a habit.</em> The interesting thing was that I declined them for many different, unrelated reasons.  This was evidence for something more systemic, because it would be a coincidence if random, unrelated reasons always came up to prevent me from attending live music.</p>\n<p>So I asked myself if I really disliked live music.  Emotions returned: \"Not really.  It's not awesome, but it's not terrible.\"  Now, there was a time when I would have stopped thinking there.  My time is valuable, and mediocrity is enough to stop me from doing anything, right?</p>\n<p>But wait... is it?  Is it enough to <em>always</em> stop me?  If it was only mediocre, and not terrible, than surely on <em>one</em> of the many occasions I could have seen live music, there would have been sufficient justification to go... a particularly good composer, a particularly interesting group of people go with, a particular need to get out and do something different...  but no, somehow I <em>always</em> didn't go.</p>\n<p>And that's when I realized I probably had an <em>aversion</em> to live music: some brain mechanism that consistently and effectively averted me from seeing it, and in this case, <em>not</em> something I could feel.  In particular, it wasn't accompanied by any sense of <a href=\"/lw/21b/ugh_fields/\">\"Ugh\"</a>.  So since I couldn't feel the aversion, I took an <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">outside view</a> to ask what could have caused it, if it indeed exists...  <a id=\"more\"></a></p>\n<p>My first guess was the fact that I used to perform live music a lot, until I suffered a hand injury.  Huh!  That could totally have been emotional at some point.</p>\n<p>Could that be the cause of my current aversion?  I instrospected on how I might have felt at live music show shortly after my injury, and the answer was \"excluded\".  I felt disappointed that I wasn't, and couldn't be, one of the performers.  I began to suspect my aversion <em>was</em> conditioned from this \"ugh\" response that had lost salience many years ago.</p>\n<p>Solution: shortly afterward I went to the symphony with a very dear friend, and sat in the front row behind the brass section where I could read their music and feel like I was involved.  It worked!  The experience was very cathartic.  I cried a little bit, thanked my friend, and have been to many more live music performances since.</p>\n<p><strong>Some lessons to learn here:</strong></p>\n<p>Look at the picture at the top of the post... this man's face doesn't show any strong emotion averting him from the many feminine hands that reach for him, yet he avoids them.  If he does this a lot, and he's not gay or already taken, he should be curious about what aversion mechanism might be causing this behavior... because however it works, it's working!  In general,</p>\n<ol>\n<li><strong>Alarm bells</strong> should go off when there's a small-cost option you <em>always</em> avoid (or a small-gain  option you <em>always</em> pursue). </li>\n<li><strong>Measure</strong> the strength of the aversion (or propsensity) by how <em>effectively</em> it averts (or attracts) your thoughts/behavior, not how <em>saliently</em> you feel it. </li>\n<li><strong>Search</strong> for the underlying aversion or propensity mechanism. </li>\n<li><strong>Combine</strong> introspection with an outside view in your search, especially when you can't easily feel the mechanism, and don't forget, </li>\n<li><strong><a href=\"/lw/2sh/break_your_habits_be_more_empirical/\">Break</a></strong> the habit when it's not costly to do so! </li>\n</ol>\n<p><strong>Operationalizing aversion and propensity</strong></p>\n<p>I've been using 1-4 a lot more ever since I realized them explicitly back in October, and I must say it has been a helpful and eye-opening experience.</p>\n<p>Aversions &mdash; habit mechanisms that steer you away from a thought or behavior &mdash; are more general than <a href=\"/lw/21b/ugh_fields/\">ugh fields</a> in that they don't have to originate from bad feelings, even though my example did.  They can also originate from <a href=\"http://en.wikipedia.org/wiki/Anchoring\">anchoring</a>, or other biases.  That's why I want to promote an attitude of measuring aversion by effect instead of salience.  Whatever the mechanism &mdash; an ugh field, an anchor, or pink elephants &mdash; we need an <a href=\"http://en.wikipedia.org/wiki/Operationalization\">operationalized</a> notion of aversion to search for and notice in our daily lives.</p>\n<p>The same goes for <em>propensities</em> (or <em>apetites</em>): habit mechanisms that steer you <em>toward</em> a behavior.  If you <em>always</em> take the bus, even though it's only slightly shorter than walking, there's probably a very effective mechanism in place for it.  Are you curious how it works?</p>\n<p>Now, I'm no behaviorist.  The outside view should complement, not replace, the inside one.  My introspection at the end of the story helped me finish the search for my aversion, but it was an outside view that made me notice it in the first place.</p>\n<p>So, let us go forth to notice aversions to small costs, and propensities for small gains.   And then defy them :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5Whwix4cZ3p5otshm": 1, "r7qAjcbfhj2256EHH": 1, "Zwv9eHi7KGg5KA9oM": 1, "3ee9k6NJfcGzL6kMS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fxv4o3LGEkgR2Qsz7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 83, "baseScore": 109, "extendedScore": null, "score": 0.000214, "legacy": true, "legacyId": "7654", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 109, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<!-- Measuring aversion and habit strength -->\n<p><img src=\"http://i51.tinypic.com/mcq23t.jpg\" alt=\"Not me!\" width=\"30%\" align=\"right\"> tl;dr: <em>Strong aversions don't always originate from strong feelings (see <a href=\"/lw/21b/ugh_fields/\">Ugh fields</a>).  It's useful to measure the strength of an aversion by <strong>how effectively</strong> it averts your thoughts/behavior instead of <strong>how saliently</strong> you can feel it, or even remember feeling it.  If there's a low cost behaviour that you somehow always \"end up not doing\", there's evidence for a mechanism steering you away from it.  Try to find it, and defy it.</em></p>\n<p><strong id=\"Story\">Story</strong></p>\n<p>Right after writing <a href=\"/lw/2sh/break_your_habits_be_more_empirical/\">Break your habits: be more empirical</a>, someone asked me to a live music show, and I  declined, with some explanation about being busy.  This <a href=\"/lw/if/your_strength_as_a_rationalist/\">felt a little forced</a>, and I realized: <em>I always decline live music shows.  This counts as a habit.</em> The interesting thing was that I declined them for many different, unrelated reasons.  This was evidence for something more systemic, because it would be a coincidence if random, unrelated reasons always came up to prevent me from attending live music.</p>\n<p>So I asked myself if I really disliked live music.  Emotions returned: \"Not really.  It's not awesome, but it's not terrible.\"  Now, there was a time when I would have stopped thinking there.  My time is valuable, and mediocrity is enough to stop me from doing anything, right?</p>\n<p>But wait... is it?  Is it enough to <em>always</em> stop me?  If it was only mediocre, and not terrible, than surely on <em>one</em> of the many occasions I could have seen live music, there would have been sufficient justification to go... a particularly good composer, a particularly interesting group of people go with, a particular need to get out and do something different...  but no, somehow I <em>always</em> didn't go.</p>\n<p>And that's when I realized I probably had an <em>aversion</em> to live music: some brain mechanism that consistently and effectively averted me from seeing it, and in this case, <em>not</em> something I could feel.  In particular, it wasn't accompanied by any sense of <a href=\"/lw/21b/ugh_fields/\">\"Ugh\"</a>.  So since I couldn't feel the aversion, I took an <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">outside view</a> to ask what could have caused it, if it indeed exists...  <a id=\"more\"></a></p>\n<p>My first guess was the fact that I used to perform live music a lot, until I suffered a hand injury.  Huh!  That could totally have been emotional at some point.</p>\n<p>Could that be the cause of my current aversion?  I instrospected on how I might have felt at live music show shortly after my injury, and the answer was \"excluded\".  I felt disappointed that I wasn't, and couldn't be, one of the performers.  I began to suspect my aversion <em>was</em> conditioned from this \"ugh\" response that had lost salience many years ago.</p>\n<p>Solution: shortly afterward I went to the symphony with a very dear friend, and sat in the front row behind the brass section where I could read their music and feel like I was involved.  It worked!  The experience was very cathartic.  I cried a little bit, thanked my friend, and have been to many more live music performances since.</p>\n<p><strong id=\"Some_lessons_to_learn_here_\">Some lessons to learn here:</strong></p>\n<p>Look at the picture at the top of the post... this man's face doesn't show any strong emotion averting him from the many feminine hands that reach for him, yet he avoids them.  If he does this a lot, and he's not gay or already taken, he should be curious about what aversion mechanism might be causing this behavior... because however it works, it's working!  In general,</p>\n<ol>\n<li><strong>Alarm bells</strong> should go off when there's a small-cost option you <em>always</em> avoid (or a small-gain  option you <em>always</em> pursue). </li>\n<li><strong>Measure</strong> the strength of the aversion (or propsensity) by how <em>effectively</em> it averts (or attracts) your thoughts/behavior, not how <em>saliently</em> you feel it. </li>\n<li><strong>Search</strong> for the underlying aversion or propensity mechanism. </li>\n<li><strong>Combine</strong> introspection with an outside view in your search, especially when you can't easily feel the mechanism, and don't forget, </li>\n<li><strong><a href=\"/lw/2sh/break_your_habits_be_more_empirical/\">Break</a></strong> the habit when it's not costly to do so! </li>\n</ol>\n<p><strong id=\"Operationalizing_aversion_and_propensity\">Operationalizing aversion and propensity</strong></p>\n<p>I've been using 1-4 a lot more ever since I realized them explicitly back in October, and I must say it has been a helpful and eye-opening experience.</p>\n<p>Aversions \u2014 habit mechanisms that steer you away from a thought or behavior \u2014 are more general than <a href=\"/lw/21b/ugh_fields/\">ugh fields</a> in that they don't have to originate from bad feelings, even though my example did.  They can also originate from <a href=\"http://en.wikipedia.org/wiki/Anchoring\">anchoring</a>, or other biases.  That's why I want to promote an attitude of measuring aversion by effect instead of salience.  Whatever the mechanism \u2014 an ugh field, an anchor, or pink elephants \u2014 we need an <a href=\"http://en.wikipedia.org/wiki/Operationalization\">operationalized</a> notion of aversion to search for and notice in our daily lives.</p>\n<p>The same goes for <em>propensities</em> (or <em>apetites</em>): habit mechanisms that steer you <em>toward</em> a behavior.  If you <em>always</em> take the bus, even though it's only slightly shorter than walking, there's probably a very effective mechanism in place for it.  Are you curious how it works?</p>\n<p>Now, I'm no behaviorist.  The outside view should complement, not replace, the inside one.  My introspection at the end of the story helped me finish the search for my aversion, but it was an outside view that made me notice it in the first place.</p>\n<p>So, let us go forth to notice aversions to small costs, and propensities for small gains.   And then defy them :)</p>", "sections": [{"title": "Story", "anchor": "Story", "level": 1}, {"title": "Some lessons to learn here:", "anchor": "Some_lessons_to_learn_here_", "level": 1}, {"title": "Operationalizing aversion and propensity", "anchor": "Operationalizing_aversion_and_propensity", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "14 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EFQ3F6kmt4WHXRqik", "iA25AvZqAr6G8mAXR", "5JDkW4MYXit2CquLs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-27T01:36:48.891Z", "modifiedAt": null, "url": null, "title": "The 48 Rules of Power; Viable?", "slug": "the-48-rules-of-power-viable", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:39.333Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raw_Power", "createdAt": "2010-09-10T23:59:43.621Z", "isAdmin": false, "displayName": "Raw_Power"}, "userId": "kwSqcED9qTanFyNWG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kK8hE3xCDmfqC5pjM/the-48-rules-of-power-viable", "pageUrlRelative": "/posts/kK8hE3xCDmfqC5pjM/the-48-rules-of-power-viable", "linkUrl": "https://www.lesswrong.com/posts/kK8hE3xCDmfqC5pjM/the-48-rules-of-power-viable", "postedAtFormatted": "Friday, May 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%2048%20Rules%20of%20Power%3B%20Viable%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%2048%20Rules%20of%20Power%3B%20Viable%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkK8hE3xCDmfqC5pjM%2Fthe-48-rules-of-power-viable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%2048%20Rules%20of%20Power%3B%20Viable%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkK8hE3xCDmfqC5pjM%2Fthe-48-rules-of-power-viable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkK8hE3xCDmfqC5pjM%2Fthe-48-rules-of-power-viable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 427, "htmlBody": "<p>This is not a thesis post, it's an open-question, discussion-provoking post. That's why I'm posting it as Discussion, since this is what appears to serve the function of forum on this site. I am not looking for ratings, but for answers. With everyone's collaboration, they should present themselves, at least in outline. Please don't hesitate to point it out if you think i have completely misunderstood the purpose of the Discussion section and if I should refrain from this sort of posting in the future.</p>\n<p>So, here is <a href=\"http://en.wikipedia.org/wiki/The_48_Laws_of_Power\">a</a> summary of the rules the book proposes. <a href=\"http://www2.tech.purdue.edu/cg/courses/cgt411/covey/48_laws_of_power.htm\">Here is a little more expanded text</a>.</p>\n<p>To be honest, my first reaction to reading this was visceral rejection (\"Preposterous! Try to act by those rules and you'll be labeled a psychopath, people will know not to trust you or deal with you.\"). The second was consternated acceptance (\"But people do seem to behave in the way this book suggests... wouldn't it be better to adapt to a reality we have no power to change?\"). This is the result of the third approximation: confused questioning.</p>\n<p>The question I'd like to ask is this: are they rational? As in, would everyone's lives improve or worsen from following this? Unlike riches and actual achievements, competition for power does seem to be a Zero Sum Game, at least in a society that isn't expanding (demographically or by conquest or otherwise). Not only that, it appears to be a resource-intensive game, one that even gets in the way of doing actual work.</p>\n<p>What is remarkable is that, when I think of my experiences in hindsight, Real Life does appear to work this way, and these would explain many behaviors people demonstrate that are out of synch with what they profess. This is especially egregious if you compare it with fiction, in which such behavior isn't used except by the most <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/MagnificentBastard\">magnificent bastards,</a> and even then it is portrayed as extremely questionable, and common moral philosophy, that seems to preach the opposite.</p>\n<p>However, everything seems to indicate that this is definitely not the optimum way for things to work, in a utilitarian sense. If everyone followed the rules of this book, would we ever get anything done?</p>\n<p>So should these social, anti-productive tendencies, be fought with education, or should they be embraced? Is there a way to harness them into a motivation for productive work, the way Capitalism advocates harnessing human greed?&sup1;</p>\n<p>&nbsp;</p>\n<hr />\n<p>1.Remarkably enough, lust for power can and does get in the way of greed for riches and even welfare. As does pride in scrupulous, principled, but materialistically impoverishing behavior.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kK8hE3xCDmfqC5pjM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "7655", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-27T01:53:56.834Z", "modifiedAt": null, "url": null, "title": "Quote help", "slug": "quote-help", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:07.329Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Goobahman", "createdAt": "2011-01-13T05:09:28.962Z", "isAdmin": false, "displayName": "Goobahman"}, "userId": "cidN68rGuy4wwnvFp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D8rQKYt4b2NRDR4Ws/quote-help", "pageUrlRelative": "/posts/D8rQKYt4b2NRDR4Ws/quote-help", "linkUrl": "https://www.lesswrong.com/posts/D8rQKYt4b2NRDR4Ws/quote-help", "postedAtFormatted": "Friday, May 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quote%20help&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuote%20help%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD8rQKYt4b2NRDR4Ws%2Fquote-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quote%20help%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD8rQKYt4b2NRDR4Ws%2Fquote-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD8rQKYt4b2NRDR4Ws%2Fquote-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p>There's a famous I'm struggling to find but it's along the lines of:</p>\r\n<p>'Those who are wise seek clarity. Those who wish to merely appear wise seek obscurity'</p>\r\n<p>It's great for getting a read on people and their intention.</p>\r\n<p>Does anyone know who said it and what the original quote is?</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D8rQKYt4b2NRDR4Ws", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 7.198270868227476e-07, "legacy": true, "legacyId": "7657", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-27T10:12:36.305Z", "modifiedAt": null, "url": null, "title": "Requesting advice", "slug": "requesting-advice-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:17.290Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FGkJYhn57fAfJE3Ga/requesting-advice-0", "pageUrlRelative": "/posts/FGkJYhn57fAfJE3Ga/requesting-advice-0", "linkUrl": "https://www.lesswrong.com/posts/FGkJYhn57fAfJE3Ga/requesting-advice-0", "postedAtFormatted": "Friday, May 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Requesting%20advice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequesting%20advice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGkJYhn57fAfJE3Ga%2Frequesting-advice-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Requesting%20advice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGkJYhn57fAfJE3Ga%2Frequesting-advice-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGkJYhn57fAfJE3Ga%2Frequesting-advice-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<p>I've noticed during my thoughts on the issue that I seem to be biased against Christianity- although raised in a Christian household, I have noticed that I become more tense when reading effective arguments for Christianity and more relaxed when reading good arguments against it- I also feel strongly tempted to pull out books which I know give good arguments against Christianity.</p>\r\n<p>I thought the issue of whether Christianity was actually true concluded- but given that I am now aware I'm biased, it's difficult to be sure. On the one hand, there is a lot of evidence against it (biblical contradictions etc...). On the other, there are some pieces of evidence that appear false 'on the surface' but which seem plausible when I take my bias into account.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FGkJYhn57fAfJE3Ga", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 4, "extendedScore": null, "score": 7.19973537276666e-07, "legacy": true, "legacyId": "7671", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-27T11:44:04.222Z", "modifiedAt": null, "url": null, "title": "Advice request: Homeownership", "slug": "advice-request-homeownership", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:09.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AdeleneDawner", "createdAt": "2009-04-28T14:40:00.131Z", "isAdmin": false, "displayName": "AdeleneDawner"}, "userId": "MeSREm4SMRGxeQ8X3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7YG4LyK9FCjmBJhaf/advice-request-homeownership", "pageUrlRelative": "/posts/7YG4LyK9FCjmBJhaf/advice-request-homeownership", "linkUrl": "https://www.lesswrong.com/posts/7YG4LyK9FCjmBJhaf/advice-request-homeownership", "postedAtFormatted": "Friday, May 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20request%3A%20Homeownership&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20request%3A%20Homeownership%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YG4LyK9FCjmBJhaf%2Fadvice-request-homeownership%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20request%3A%20Homeownership%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YG4LyK9FCjmBJhaf%2Fadvice-request-homeownership", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YG4LyK9FCjmBJhaf%2Fadvice-request-homeownership", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>So I'm probably about two months from owning a home. (Realtor says we might close within a month; experienced friend says 3-6 weeks; I'll be vaguely surprised if it's not done by August.)</p>\n<p>This is exciting, and also more than a little daunting. My near-mode brainbits don't know quite what to make of it; this is my first time owning anything on nearly this scope (I don't drive, so I've never owned a car), and also my first time taking on any large amount of debt. It's pretty obviously a good thing overall - my mortgage payment should be not much more than half of what I've been paying for my apartment, and I'll be in an area that's better by several relevant measurements, and I'll have more space and more freedom - but it's still a rather large change.</p>\n<p>So, on behalf of those near-mode brainbits, which are mostly going <em>aaaaaaaaah what have you gotten us into</em>, I'd like to request any advice that you might have for a soon-to-be new homeowner.</p>\n<p><sub>(More information is available, but I'm not even sure what's important enough about the situation to mention.)</sub></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7YG4LyK9FCjmBJhaf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "7672", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-27T15:52:26.823Z", "modifiedAt": null, "url": null, "title": "A less wrong way to discuss the arts?", "slug": "a-less-wrong-way-to-discuss-the-arts", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j69r8Z7Wh2JuK3dJH/a-less-wrong-way-to-discuss-the-arts", "pageUrlRelative": "/posts/j69r8Z7Wh2JuK3dJH/a-less-wrong-way-to-discuss-the-arts", "linkUrl": "https://www.lesswrong.com/posts/j69r8Z7Wh2JuK3dJH/a-less-wrong-way-to-discuss-the-arts", "postedAtFormatted": "Friday, May 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20less%20wrong%20way%20to%20discuss%20the%20arts%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20less%20wrong%20way%20to%20discuss%20the%20arts%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj69r8Z7Wh2JuK3dJH%2Fa-less-wrong-way-to-discuss-the-arts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20less%20wrong%20way%20to%20discuss%20the%20arts%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj69r8Z7Wh2JuK3dJH%2Fa-less-wrong-way-to-discuss-the-arts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj69r8Z7Wh2JuK3dJH%2Fa-less-wrong-way-to-discuss-the-arts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 478, "htmlBody": "<p>A <a href=\"/lw/5ro/what_bothers_you_about_less_wrong/47l3\">comment</a> by komponisto got me thinking. &nbsp;Why are favorite movie/book/tv show/etc exchanges typically low on content? &nbsp;Do they need to be? &nbsp;Here's&nbsp;a sample exchange, exaggerated for comic effect:</p>\n<p>&nbsp;</p>\n<p>X: \"Did you see The Hangover 2?\"</p>\n<p>Y: \"No. &nbsp;I don't feel compelled to.\"</p>\n<p>X: \"Why not? &nbsp;I think it looks really funny! &nbsp;Don't you think it looks really funny? &nbsp;Plus, the first one was funny.\"</p>\n<p>Y: \"I didn't see the first one. &nbsp;Like everyone, I enjoy the physical experience of laughter, but I expect that if I laugh at all during this particular movie, it will be 'uncomfortable situation' or 'shocking moment' tension-releasing laughter, or at best 'aren't we all having a nice time together in front of this huge image and amidst these loud noises.' &nbsp;I seriously doubt that I will experience the transportive joy of 'in spite of myself,' 'my stomach hurts,' 'I can't breathe,' 'this is better than an orgasm' laughter. &nbsp;In fact, the incidence of that kind of laughter post-adolescence is disappointingly low. &nbsp;Do you ever think about that? &nbsp;Do you worry that the best kind of laughter vanishes as our lives contain fewer and fewer novel experiences?\"</p>\n<p>X: \"Uh... \"</p>\n<p>Y: \"Exactly. &nbsp;So why would I want to pay $12 to hollowly go through the motions?\"</p>\n<p>X: \"How can you be sure you won't laugh?\"</p>\n<p>Y: \"I guess I can't be 100% sure. &nbsp;Nothing in the advertising speaks to my cached expectations of what constitutes true funniness. &nbsp;Are <em>you </em>sure you've made a conscious decision to see The Hangover 2 <em>specifically because </em>you <em>expect </em>to laugh really hard? &nbsp;And that's why you're willing to invest your hard-earned money?\"</p>\n<p>X: \"I don't know. &nbsp;It just seems like a fun thing to do. &nbsp;I like Zach Galifianakis. &nbsp;He's so weird.\"</p>\n<p>&nbsp;</p>\n<p>...and so forth. &nbsp;I sometimes get the sense that LWers advocate using their bird's-eye-view of interpersonal communication to blend in rather than exclude, but I personally prefer to challenge others because I wish to be challenged. &nbsp;That way, I believe I'm more likely to form relationships that rest on equal footing.</p>\n<p>This isn't a comprehensive thesis or anything, but rather an attempt to start a conversation. &nbsp;The general idea is that most people talk about the arts in order to get to know each other--as well as decide who they wish to know. &nbsp;Why not improve the signal/noise ratio? &nbsp;</p>\n<p>I propose an exercise: pick a work of art. &nbsp;It can a book, a movie, whatever. &nbsp;Try to find the line between mostly subjective (your mood at the time, your anticipated experience, your ingroup(s) and your relative need to signal membership) and mostly objective (relationship to established forms). &nbsp;Unless you are an artist in that particular medium, your responses will fall mostly in the subjective column. &nbsp;The objective is to go from \"______ sucked/was awesome\" to \"______ caused me to have internal experiences A, B, C, H, and R. &nbsp;Did you have different experiences? &nbsp;If so, why?\"</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j69r8Z7Wh2JuK3dJH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7673", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-27T15:55:50.745Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.042Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ABxioKq2ysByHdJXR/", "pageUrlRelative": "/posts/ABxioKq2ysByHdJXR/", "linkUrl": "https://www.lesswrong.com/posts/ABxioKq2ysByHdJXR/", "postedAtFormatted": "Friday, May 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FABxioKq2ysByHdJXR%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FABxioKq2ysByHdJXR%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FABxioKq2ysByHdJXR%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ABxioKq2ysByHdJXR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 7.200743742878805e-07, "legacy": true, "legacyId": "7674", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-27T15:57:47.356Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Futuristic Predictions as Consumable Goods", "slug": "seq-rerun-futuristic-predictions-as-consumable-goods", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:07.978Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4oN3hLPfCTffDQ4bs/seq-rerun-futuristic-predictions-as-consumable-goods", "pageUrlRelative": "/posts/4oN3hLPfCTffDQ4bs/seq-rerun-futuristic-predictions-as-consumable-goods", "linkUrl": "https://www.lesswrong.com/posts/4oN3hLPfCTffDQ4bs/seq-rerun-futuristic-predictions-as-consumable-goods", "postedAtFormatted": "Friday, May 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Futuristic%20Predictions%20as%20Consumable%20Goods&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Futuristic%20Predictions%20as%20Consumable%20Goods%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4oN3hLPfCTffDQ4bs%2Fseq-rerun-futuristic-predictions-as-consumable-goods%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Futuristic%20Predictions%20as%20Consumable%20Goods%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4oN3hLPfCTffDQ4bs%2Fseq-rerun-futuristic-predictions-as-consumable-goods", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4oN3hLPfCTffDQ4bs%2Fseq-rerun-futuristic-predictions-as-consumable-goods", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Today's post, <a href=\"/lw/hi/futuristic_predictions_as_consumable_goods/\">Futuristic Predictions as Consumable Goods</a> was originally published on April 10, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>The Friedman Unit is named after Thomas Friedman who called \"the next six months\" the critical period in Iraq eight times between 2003 and 2007.  This is because future predictions are created and consumed in the now; they are used to create feelings of delicious goodness or delicious horror now, not provide useful future advice.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5wh/seq_rerun_inductive_bias/\">Inductive Bias</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4oN3hLPfCTffDQ4bs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 7.200749453192483e-07, "legacy": true, "legacyId": "7675", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mZJs7FxxmhMvFxuse", "XXConPwLhtfvpJGxx", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-27T20:13:26.876Z", "modifiedAt": null, "url": null, "title": "A Study of Scarlet: The Conscious Mental Graph", "slug": "a-study-of-scarlet-the-conscious-mental-graph", "viewCount": null, "lastCommentedAt": "2022-01-30T11:22:55.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pi5DAEZWJK3c9NAhW/a-study-of-scarlet-the-conscious-mental-graph", "pageUrlRelative": "/posts/pi5DAEZWJK3c9NAhW/a-study-of-scarlet-the-conscious-mental-graph", "linkUrl": "https://www.lesswrong.com/posts/pi5DAEZWJK3c9NAhW/a-study-of-scarlet-the-conscious-mental-graph", "postedAtFormatted": "Friday, May 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Study%20of%20Scarlet%3A%20The%20Conscious%20Mental%20Graph&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Study%20of%20Scarlet%3A%20The%20Conscious%20Mental%20Graph%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpi5DAEZWJK3c9NAhW%2Fa-study-of-scarlet-the-conscious-mental-graph%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Study%20of%20Scarlet%3A%20The%20Conscious%20Mental%20Graph%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpi5DAEZWJK3c9NAhW%2Fa-study-of-scarlet-the-conscious-mental-graph", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpi5DAEZWJK3c9NAhW%2Fa-study-of-scarlet-the-conscious-mental-graph", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1703, "htmlBody": "<p><strong>Sequel to:</strong> <a href=\"/lw/5n9/qualia_a_new_hope/\">Seeing Red: Dissolving Mary's Room and Qualia</a></p>\n<p><strong>Seriously, you should read first:</strong> <a href=\"/lw/of/dissolving_the_question/\">Dissolving the Question</a>, <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">How an Algorithm Feels From Inside</a></p>\n<p>In the previous post, we introduced the concept of qualia and the thought experiment of Mary's Room, set out to dissolve the question, and decided that we were seeking a simple model of a mind which includes both learning and a conscious/subconscious distinction. Since for now we're just trying to prove a philosophical point, we don't need to worry whether our model corresponds well to the human mind (though it would certainly be convenient if it did); we'll therefore pick an abstract mathematical structure that we can analyze more easily.</p>\n<h3><a id=\"more\"></a>The Mobile Graph<br /></h3>\n<p>Let's consider a graph, or a network of simple agents<sup>1</sup>; nodes can correspond to concepts, representations of objects and people, emotions, memories, actions, etc. These nodes are connected to one another, and the connections have varying strengths. At any given moment, some of the nodes are active (changing their connections and affecting nearby nodes), and others are not. This skein of nodes and connections will serve to direct the actions of some organism; let's call her Martha, and let's call this graph Martha's mind.</p>\n<p>It's important to note that the graph is <em>all</em> there is to Martha's mind; when the mental agent for \"hunger\" is activated, that doesn't mean that some <a href=\"http://en.wikipedia.org/wiki/Homunculus_argument\">homunculus</a> within her mind becomes hungry, but rather that the agents corresponding to eating nearby food are strongly activated (unless otherwise inhibited by nodes pertaining to social constraints, concerns of sanitation, etc), that other nodes which visualize and evaluate plans of action to obtain food are activated, that other mental processes are somewhat inhibited to save energy and prevent distraction, and so on. (The evolutionary benefits of such an admittedly complicated system directing an organism are <a href=\"/lw/l2/protein_reinforcement_and_dna_consequentialism/\">relatively significant</a>.)</p>\n<p>Since we'll be discussing experience and learning, the graph will need to change over time, and perhaps change structure in large ways. As different agents are activated, connections can form, strengthen, weaken, or sever, and the graph as a whole can rearrange itself in response<sup>2</sup>. Imagine, as a geometric analogy, that the nodes repel each other and the connections pull them together. Like a protein folding itself as it is assembled or a rope assuming surprising new shapes as we continually twist one end, the graph seeks the lowest-energy state and occasionally makes cascades of rearrangements in response to one new connection being added. This is going to be important, so let's consider an example.</p>\n<p>Let's say that Martha is listening to a story of long ago and far, far away. The LONE STALWART has trained himself in the mystical ways of his teacher, the OLD WARRIOR. The OLD WARRIOR, a generation before, had taught both the LONE STALWART's father (the AMAZING STARFIGHTER) and another pupil, who turned to the ways of darkness and became the DEADLY VILLAIN. This DEADLY VILLAIN killed the AMAZING STARFIGHTER and, soon after the LONE STALWART began to train, struck down the OLD WARRIOR himself.</p>\n<p>Within Martha's mind might be the following subgraph, where different arrows represent these various relationships<sup>3</sup>:</p>\n<p style=\"padding-left: 60px;\"><img src=\"http://images.lesswrong.com/t3_5op_3.png?v=2ef97a44ff2d20619ecb9cb8719e215a\" alt=\"\" width=\"256\" height=\"266\" /></p>\n<p>Of course, there are various other connections to ideas: Martha sees the OLD WARRIOR as wise and virtuous, and expects the story to end with the LONE STALWART killing the DEADLY VILLAIN to avenge both his father and his mentor.</p>\n<p>But then, when the LONE STALWART finally duels the DEADLY VILLAIN, the latter reveals a shocking secret: he <em>is</em> the LONE STALWART's father! As this piece of information is processed, the graph undergoes a revolution: two distinct nodes abruptly merge.</p>\n<p style=\"padding-left: 60px;\"><img src=\"http://images.lesswrong.com/t3_5op_4.png?v=c34bc22ad12e1b2f5fb9617cffef8c93\" alt=\"\" width=\"246\" height=\"166\" /></p>\n<p>Part of the reason that this revelation is so significant, compared to other facts in the story (such as when the DEADLY VILLAIN hires bounty hunters to track the LONE STALWART's companions, or when he unilaterally alters the deal he has cut with a fearful LOCAL COMMISSAR) is that it leads to a larger cascade of changes within Martha's mind: the OLD WARRIOR must have lied to the LONE STALWART, who might be in real danger of corruption after all (hence the symbolic darkening of those two nodes in the picture above); it would no longer be an unambiguously happy ending if the LONE STALWART killed the DEADLY VILLAIN, et cetera.</p>\n<p>On the other hand, just because something has a large effect on Martha's mind doesn't automatically imply that Martha 'notices' the effect in the way we might think, any more than a vacuum-tube computer 'notices' <a href=\"http://en.wikipedia.org/wiki/Software_bug#Etymology\">when a moth dies inside a relay</a>; it's only by dint of Herculean engineering labors that our modern computers can tell us when and how they've gone awry (most of the time). What we can say, however, is that this bit of information changes the structure of Martha's mind more than did the other pieces of information in the story.</p>\n<p>So far, so good. It's now time to introduce the second key element of our model: the conscious/subconscious distinction.</p>\n<h3>The Conscious Subgraph<br /></h3>\n<p>What exactly do we mean by conscious and unconscious parts of Martha's mind? Is it just a <a href=\"/lw/la/truly_part_of_you/\">label we affix</a> to some nodes and connections, but not others? Why should such a thing matter to Martha or to us?</p>\n<p>To create a plausible role for such a distinction, it may help to think of the evolutionary dynamics of communication. Let's say that Martha's species has gradually evolved the ability to communicate in great detail through language. So there is a significant part of her mind devoted to language, and any agent that connects to this part of the graph can cause Martha to speak (or write, etc.) in particular ways.</p>\n<p>But of course, not everything is useful to communicate. It may be very helpful to tell each other that a certain food tastes good, but (unless one starts to get sick) not worth the trouble of communicating the details of digestion. The raw data of vision might take forever to explain, when all that's really relevant is that the speaker just spotted a tiger or a mammoth. And there can be <a href=\"/lw/2jt/conflicts_between_mental_subagents_expanding_wei/\">social reasons</a> why it's best to keep some of one's mental processes away from the means of explicit communication.</p>\n<p>So within Martha's graph, there's a relatively small subgraph that's hooked up to the language areas; we'll call this her <em>conscious subgraph</em>. Again, there's no homunculus located here; every agent simply performs its simple function when activated. The conscious subgraph isn't kept separate from the rest of the graph, nor did it evolve to be otherwise particularly special. The only difference between it and the remainder is that the agents in the conscious subgraph can articulate their activation, causing Martha to speak it, write it, point to an object, think it<sup>4</sup>, or otherwise render it effable.</p>\n<p>When something like color vision is activated, most of Martha's processing happens subconsciously and only the reportable details about objects appear on her conscious graph. For example, Martha's mind might contain the following pattern:</p>\n<p style=\"padding-left: 60px;\"><img src=\"http://images.lesswrong.com/t3_5op_2.png?v=e3b491f6635958e2a69be3c1163c6879\" alt=\"\" width=\"342\" height=\"311\" /></p>\n<p>Here, the vast visual-processing agent \"Detect Red Object\" is not part of the conscious subgraph, but it can affect many nodes that are, and it thereby binds them all closer together than they would be on the strength of conscious connections alone<sup>5</sup>. In the middle of this cluster is the <a href=\"/lw/nn/neural_categories/\">extra node</a> representing the color red, which turns out to be an especially effective shorthand for communication. And since most of Martha's processing of color is done subconsciously, I've shaded the diagram accordingly; she can talk about particularities of color by giving pointers (red like a firetruck) or analogies (an angry red) shaped by unconscious processes, but the level of detail pales by comparison to what she can do by (for example) painting the object she's thinking of, using the same subconscious agents to determine whether the canvas matches the visualization.</p>\n<p>Now, it's pretty clear where we ought to look for qualia in Martha's mind: we should focus on those subconscious agents whose activation has significant effects on the conscious graph, like the \"Detect Red Object\" agent above. In the next post we'll consider one crucial agent that plays a key role in the Mary's Room experiment, and then we'll be ready to observe what goes though Martha's mind when she first sees in color.</p>\n<h3 style=\"padding-left: 60px;\"><a href=\"/lw/5ot/return_of_the_beisutsukai/\"><strong>TO BE CONCLUDED...</strong></a></h3>\n<h3><strong>Footnotes:</strong></h3>\n<p><strong>1.</strong> More complicated nodes are actually networks of simpler ones; the node for \"catch a ball\" is actually a connected tangle combining hundreds of particular muscle movements with visual perception of the moving ball, which itself breaks down into smaller nodes of visual processing. But as a first approximation, we can treat even large nodes as basic objects. (Also, I've cribbed the network of mental agents idea from Minsky's <a href=\"http://en.wikipedia.org/wiki/Society_of_Mind\">Society of Mind</a>, though it's also a pretty common meme round here by now.)</p>\n<p><strong>2.</strong> Also, the graph will form new nodes, at least by taking new recurring patterns of simple nodes to be new complicated nodes. Martha's mind may not have started out with a node for a concept like coconuts, but upon seeing them, a collection of visual perceptions get bound together as a new object, and then a kind of object; the texture and taste are added to this node when they are discovered, along with ideas like the coconut tree. For our discussion, though, we'll try and elide this complication, and stick to the analysis of changing connections between nodes, or combining two existing nodes, etc.</p>\n<p><strong>3.</strong> We could have different types of connections, or we could just mean that the connections between LS and AS are connected to nodes representing the concept of fatherhood, as well. It's of course much easier to represent the former in simple images.</p>\n<p><strong>4.</strong> That is, those agents could activate the agents in charge of articulating speech and suppress those in charge of actually speaking it, and the agents that articulate speech could pass on information to the agents in charge of hearing, who would activate in imitation of how it would sound if spoken. This too is a useful feature for Martha's mind to have, so that she can try out different articulations of a concept and imagine their effect on listeners before selecting one.</p>\n<p><strong>5.</strong> Similarly, Martha's self-serving subconscious desires can <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">knit together</a> a collection of conscious moral notions much more tightly than is warranted by her noble-sounding conscious arguments for them. But I digress.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8e9e8fzXuW5gGBS3F": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pi5DAEZWJK3c9NAhW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 44, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "7369", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Sequel to:</strong> <a href=\"/lw/5n9/qualia_a_new_hope/\">Seeing Red: Dissolving Mary's Room and Qualia</a></p>\n<p><strong>Seriously, you should read first:</strong> <a href=\"/lw/of/dissolving_the_question/\">Dissolving the Question</a>, <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">How an Algorithm Feels From Inside</a></p>\n<p>In the previous post, we introduced the concept of qualia and the thought experiment of Mary's Room, set out to dissolve the question, and decided that we were seeking a simple model of a mind which includes both learning and a conscious/subconscious distinction. Since for now we're just trying to prove a philosophical point, we don't need to worry whether our model corresponds well to the human mind (though it would certainly be convenient if it did); we'll therefore pick an abstract mathematical structure that we can analyze more easily.</p>\n<h3 id=\"The_Mobile_Graph\"><a id=\"more\"></a>The Mobile Graph<br></h3>\n<p>Let's consider a graph, or a network of simple agents<sup>1</sup>; nodes can correspond to concepts, representations of objects and people, emotions, memories, actions, etc. These nodes are connected to one another, and the connections have varying strengths. At any given moment, some of the nodes are active (changing their connections and affecting nearby nodes), and others are not. This skein of nodes and connections will serve to direct the actions of some organism; let's call her Martha, and let's call this graph Martha's mind.</p>\n<p>It's important to note that the graph is <em>all</em> there is to Martha's mind; when the mental agent for \"hunger\" is activated, that doesn't mean that some <a href=\"http://en.wikipedia.org/wiki/Homunculus_argument\">homunculus</a> within her mind becomes hungry, but rather that the agents corresponding to eating nearby food are strongly activated (unless otherwise inhibited by nodes pertaining to social constraints, concerns of sanitation, etc), that other nodes which visualize and evaluate plans of action to obtain food are activated, that other mental processes are somewhat inhibited to save energy and prevent distraction, and so on. (The evolutionary benefits of such an admittedly complicated system directing an organism are <a href=\"/lw/l2/protein_reinforcement_and_dna_consequentialism/\">relatively significant</a>.)</p>\n<p>Since we'll be discussing experience and learning, the graph will need to change over time, and perhaps change structure in large ways. As different agents are activated, connections can form, strengthen, weaken, or sever, and the graph as a whole can rearrange itself in response<sup>2</sup>. Imagine, as a geometric analogy, that the nodes repel each other and the connections pull them together. Like a protein folding itself as it is assembled or a rope assuming surprising new shapes as we continually twist one end, the graph seeks the lowest-energy state and occasionally makes cascades of rearrangements in response to one new connection being added. This is going to be important, so let's consider an example.</p>\n<p>Let's say that Martha is listening to a story of long ago and far, far away. The LONE STALWART has trained himself in the mystical ways of his teacher, the OLD WARRIOR. The OLD WARRIOR, a generation before, had taught both the LONE STALWART's father (the AMAZING STARFIGHTER) and another pupil, who turned to the ways of darkness and became the DEADLY VILLAIN. This DEADLY VILLAIN killed the AMAZING STARFIGHTER and, soon after the LONE STALWART began to train, struck down the OLD WARRIOR himself.</p>\n<p>Within Martha's mind might be the following subgraph, where different arrows represent these various relationships<sup>3</sup>:</p>\n<p style=\"padding-left: 60px;\"><img src=\"http://images.lesswrong.com/t3_5op_3.png?v=2ef97a44ff2d20619ecb9cb8719e215a\" alt=\"\" width=\"256\" height=\"266\"></p>\n<p>Of course, there are various other connections to ideas: Martha sees the OLD WARRIOR as wise and virtuous, and expects the story to end with the LONE STALWART killing the DEADLY VILLAIN to avenge both his father and his mentor.</p>\n<p>But then, when the LONE STALWART finally duels the DEADLY VILLAIN, the latter reveals a shocking secret: he <em>is</em> the LONE STALWART's father! As this piece of information is processed, the graph undergoes a revolution: two distinct nodes abruptly merge.</p>\n<p style=\"padding-left: 60px;\"><img src=\"http://images.lesswrong.com/t3_5op_4.png?v=c34bc22ad12e1b2f5fb9617cffef8c93\" alt=\"\" width=\"246\" height=\"166\"></p>\n<p>Part of the reason that this revelation is so significant, compared to other facts in the story (such as when the DEADLY VILLAIN hires bounty hunters to track the LONE STALWART's companions, or when he unilaterally alters the deal he has cut with a fearful LOCAL COMMISSAR) is that it leads to a larger cascade of changes within Martha's mind: the OLD WARRIOR must have lied to the LONE STALWART, who might be in real danger of corruption after all (hence the symbolic darkening of those two nodes in the picture above); it would no longer be an unambiguously happy ending if the LONE STALWART killed the DEADLY VILLAIN, et cetera.</p>\n<p>On the other hand, just because something has a large effect on Martha's mind doesn't automatically imply that Martha 'notices' the effect in the way we might think, any more than a vacuum-tube computer 'notices' <a href=\"http://en.wikipedia.org/wiki/Software_bug#Etymology\">when a moth dies inside a relay</a>; it's only by dint of Herculean engineering labors that our modern computers can tell us when and how they've gone awry (most of the time). What we can say, however, is that this bit of information changes the structure of Martha's mind more than did the other pieces of information in the story.</p>\n<p>So far, so good. It's now time to introduce the second key element of our model: the conscious/subconscious distinction.</p>\n<h3 id=\"The_Conscious_Subgraph\">The Conscious Subgraph<br></h3>\n<p>What exactly do we mean by conscious and unconscious parts of Martha's mind? Is it just a <a href=\"/lw/la/truly_part_of_you/\">label we affix</a> to some nodes and connections, but not others? Why should such a thing matter to Martha or to us?</p>\n<p>To create a plausible role for such a distinction, it may help to think of the evolutionary dynamics of communication. Let's say that Martha's species has gradually evolved the ability to communicate in great detail through language. So there is a significant part of her mind devoted to language, and any agent that connects to this part of the graph can cause Martha to speak (or write, etc.) in particular ways.</p>\n<p>But of course, not everything is useful to communicate. It may be very helpful to tell each other that a certain food tastes good, but (unless one starts to get sick) not worth the trouble of communicating the details of digestion. The raw data of vision might take forever to explain, when all that's really relevant is that the speaker just spotted a tiger or a mammoth. And there can be <a href=\"/lw/2jt/conflicts_between_mental_subagents_expanding_wei/\">social reasons</a> why it's best to keep some of one's mental processes away from the means of explicit communication.</p>\n<p>So within Martha's graph, there's a relatively small subgraph that's hooked up to the language areas; we'll call this her <em>conscious subgraph</em>. Again, there's no homunculus located here; every agent simply performs its simple function when activated. The conscious subgraph isn't kept separate from the rest of the graph, nor did it evolve to be otherwise particularly special. The only difference between it and the remainder is that the agents in the conscious subgraph can articulate their activation, causing Martha to speak it, write it, point to an object, think it<sup>4</sup>, or otherwise render it effable.</p>\n<p>When something like color vision is activated, most of Martha's processing happens subconsciously and only the reportable details about objects appear on her conscious graph. For example, Martha's mind might contain the following pattern:</p>\n<p style=\"padding-left: 60px;\"><img src=\"http://images.lesswrong.com/t3_5op_2.png?v=e3b491f6635958e2a69be3c1163c6879\" alt=\"\" width=\"342\" height=\"311\"></p>\n<p>Here, the vast visual-processing agent \"Detect Red Object\" is not part of the conscious subgraph, but it can affect many nodes that are, and it thereby binds them all closer together than they would be on the strength of conscious connections alone<sup>5</sup>. In the middle of this cluster is the <a href=\"/lw/nn/neural_categories/\">extra node</a> representing the color red, which turns out to be an especially effective shorthand for communication. And since most of Martha's processing of color is done subconsciously, I've shaded the diagram accordingly; she can talk about particularities of color by giving pointers (red like a firetruck) or analogies (an angry red) shaped by unconscious processes, but the level of detail pales by comparison to what she can do by (for example) painting the object she's thinking of, using the same subconscious agents to determine whether the canvas matches the visualization.</p>\n<p>Now, it's pretty clear where we ought to look for qualia in Martha's mind: we should focus on those subconscious agents whose activation has significant effects on the conscious graph, like the \"Detect Red Object\" agent above. In the next post we'll consider one crucial agent that plays a key role in the Mary's Room experiment, and then we'll be ready to observe what goes though Martha's mind when she first sees in color.</p>\n<h3 style=\"padding-left: 60px;\" id=\"TO_BE_CONCLUDED___\"><a href=\"/lw/5ot/return_of_the_beisutsukai/\"><strong>TO BE CONCLUDED...</strong></a></h3>\n<h3 id=\"Footnotes_\"><strong>Footnotes:</strong></h3>\n<p><strong>1.</strong> More complicated nodes are actually networks of simpler ones; the node for \"catch a ball\" is actually a connected tangle combining hundreds of particular muscle movements with visual perception of the moving ball, which itself breaks down into smaller nodes of visual processing. But as a first approximation, we can treat even large nodes as basic objects. (Also, I've cribbed the network of mental agents idea from Minsky's <a href=\"http://en.wikipedia.org/wiki/Society_of_Mind\">Society of Mind</a>, though it's also a pretty common meme round here by now.)</p>\n<p><strong>2.</strong> Also, the graph will form new nodes, at least by taking new recurring patterns of simple nodes to be new complicated nodes. Martha's mind may not have started out with a node for a concept like coconuts, but upon seeing them, a collection of visual perceptions get bound together as a new object, and then a kind of object; the texture and taste are added to this node when they are discovered, along with ideas like the coconut tree. For our discussion, though, we'll try and elide this complication, and stick to the analysis of changing connections between nodes, or combining two existing nodes, etc.</p>\n<p><strong>3.</strong> We could have different types of connections, or we could just mean that the connections between LS and AS are connected to nodes representing the concept of fatherhood, as well. It's of course much easier to represent the former in simple images.</p>\n<p><strong>4.</strong> That is, those agents could activate the agents in charge of articulating speech and suppress those in charge of actually speaking it, and the agents that articulate speech could pass on information to the agents in charge of hearing, who would activate in imitation of how it would sound if spoken. This too is a useful feature for Martha's mind to have, so that she can try out different articulations of a concept and imagine their effect on listeners before selecting one.</p>\n<p><strong>5.</strong> Similarly, Martha's self-serving subconscious desires can <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">knit together</a> a collection of conscious moral notions much more tightly than is warranted by her noble-sounding conscious arguments for them. But I digress.</p>", "sections": [{"title": "The Mobile Graph", "anchor": "The_Mobile_Graph", "level": 1}, {"title": "The Conscious Subgraph", "anchor": "The_Conscious_Subgraph", "level": 1}, {"title": "TO BE CONCLUDED...", "anchor": "TO_BE_CONCLUDED___", "level": 1}, {"title": "Footnotes:", "anchor": "Footnotes_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYjyQ839MDsZ6E3L", "Mc6QcrsbH5NRXbCRX", "yA4gF5KrboK2m2Xu7", "gTNB9CQd5hnbkMxAG", "fg9fXrHpeaDD6pEPL", "eaSJtg8Kvc56bFBdt", "yFDKvfN6D87Tf5J9f", "gyz2MsHM9GKxX62hj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-27T21:57:39.619Z", "modifiedAt": null, "url": null, "title": "Teachable Rationality Skills", "slug": "teachable-rationality-skills", "viewCount": null, "lastCommentedAt": "2020-05-13T12:59:08.460Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f4CZNEHirweN3XEjs/teachable-rationality-skills", "pageUrlRelative": "/posts/f4CZNEHirweN3XEjs/teachable-rationality-skills", "linkUrl": "https://www.lesswrong.com/posts/f4CZNEHirweN3XEjs/teachable-rationality-skills", "postedAtFormatted": "Friday, May 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Teachable%20Rationality%20Skills&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATeachable%20Rationality%20Skills%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff4CZNEHirweN3XEjs%2Fteachable-rationality-skills%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Teachable%20Rationality%20Skills%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff4CZNEHirweN3XEjs%2Fteachable-rationality-skills", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff4CZNEHirweN3XEjs%2Fteachable-rationality-skills", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 250, "htmlBody": "<p>Recent brainstorming sessions at SIAI (with participants including Anna, Carl, Jasen, Divia, Will, Amy Willey, and Andrew Critch) have started to produce lists of rationality skills that we could potentially try to teach (at Rationality Boot Camp, at Less Wrong meetups, or similar venues).&nbsp; We've also been trying to break those skills down to the <a href=\"/lw/5kz/the_5second_level/\">5-second level</a> (step 2) and come up with ideas for exercises that might teach them (step 3) although we haven't actually composed those exercises yet (step 4, where the actual work takes place).</p>\n<p>The bulk of this post will mainly go into the comments, which I'll try to keep to the following format:&nbsp; A top-level comment is a major or minor skill to teach; upvote this comment if you think this skill should get priority in teaching.&nbsp; Sub-level comments describe 5-second subskills that go into this skill, and then third-level comments are ideas for exercises which could potentially train that 5-second skill.&nbsp; If anyone actually went to the work of composing a specific exercise people could run through, that would go to the fourth-level of commenting, I guess.&nbsp; For some major practicable arts with a known standard learning format like \"Improv\" or \"Acting\", I'll put the exercise at the top and guesses at <em>which skills it might teach </em>below.&nbsp; (And any plain old replies can go at any level.)</p>\n<p>I probably won't be able to get to all of what we brainstormed today, so here's <a href=\"http://images.lesswrong.com/t3_5x8_0.png?v=768eb3d2d6c3e55d569bf94b7865090d\">a PNG of the Freemind map</a> that I generated during our session.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"EdDGrAxYcrXnKkDca": 1, "X7v7Fyp9cgBYaMe2e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f4CZNEHirweN3XEjs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 72, "extendedScore": null, "score": 0.000122, "legacy": true, "legacyId": "7676", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 263, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JcpzFpPBSmzuksmWM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-28T01:52:32.732Z", "modifiedAt": null, "url": null, "title": "Upcoming meet-ups: Bangalore, Minneapolis, Edinburgh, Melbourne, Houston, Dublin", "slug": "upcoming-meet-ups-bangalore-minneapolis-edinburgh-melbourne", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:31.485Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LrP7hCfGsgj74PARE/upcoming-meet-ups-bangalore-minneapolis-edinburgh-melbourne", "pageUrlRelative": "/posts/LrP7hCfGsgj74PARE/upcoming-meet-ups-bangalore-minneapolis-edinburgh-melbourne", "linkUrl": "https://www.lesswrong.com/posts/LrP7hCfGsgj74PARE/upcoming-meet-ups-bangalore-minneapolis-edinburgh-melbourne", "postedAtFormatted": "Saturday, May 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Upcoming%20meet-ups%3A%20Bangalore%2C%20Minneapolis%2C%20Edinburgh%2C%20Melbourne%2C%20Houston%2C%20Dublin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpcoming%20meet-ups%3A%20Bangalore%2C%20Minneapolis%2C%20Edinburgh%2C%20Melbourne%2C%20Houston%2C%20Dublin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrP7hCfGsgj74PARE%2Fupcoming-meet-ups-bangalore-minneapolis-edinburgh-melbourne%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Upcoming%20meet-ups%3A%20Bangalore%2C%20Minneapolis%2C%20Edinburgh%2C%20Melbourne%2C%20Houston%2C%20Dublin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrP7hCfGsgj74PARE%2Fupcoming-meet-ups-bangalore-minneapolis-edinburgh-melbourne", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrP7hCfGsgj74PARE%2Fupcoming-meet-ups-bangalore-minneapolis-edinburgh-melbourne", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 306, "htmlBody": "<div id=\"entry_t3_5sp\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/r/discussion/lw/5ps/bangalore_meetup_28th_may\">Bangalore: Saturday May 28, 4 pm</a></li>\n<li><a href=\"/lw/5uh/minneapolis_meetup_saturday_may_28_300pm\">Minneapolis: Saturday May 28, 3 pm</a></li>\n<li><a href=\"/r/discussion/lw/5wf/edinburgh_lw_meetup_saturday_may_28_2pm\">Edinburgh LW meetup, Saturday May 28, 2pm</a></li>\n<li><a href=\"/r/discussion/lw/5u1/melbourne_meetup_friday_3rd_june_7pm\">Melbourne Meetup: Friday 3rd June, 7pm</a></li>\n<li><a href=\"/r/discussion/lw/5vw/houston_hackerspace_meetup_sunday_may_29_500pm\">Houston Hackerspace Meetup: Sunday May 29, 5:00P</a></li>\n<li><a href=\"/r/discussion/lw/5t0/irish_less_wrong_meetup_sunday_29_may\">Dublin Less Wrong meetup Sunday 29 May</a></li>\n<li><a href=\"/r/discussion/lw/5vz/nc_triangle_lw_meetup_wed_june_1_700pm/\">Triangle/Durham, NC: Wednesday June 1, 7:00PM</a></li>\n<li><a href=\"/r/discussion/lw/5x9/ottawa_lw_meetup_june_2_7pm_two_bayesian\">Ottawa LW meetup, June 2, 7pm; two Bayesian Conspiracy sessions</a><a href=\"/r/discussion/lw/5x9/ottawa_lw_meetup_june_2_7pm_two_bayesian\" target=\"_blank\"></a></li>\n<li><a href=\"/lw/5xa/upcoming_meetups_bangalore_minneapolis_edinburgh/49li\">London: Sunday June 5, 14:00</a></li>\n</ul>\nAnd while not quite a meet-up, there will be a <a href=\"http://www.singularitysummitslc.com/\">one-day Singularity Summit in Salt Lake City</a>.\n<p>Cities with regularly scheduled meetups:&nbsp;<strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>, <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a>.<a id=\"more\"></a></strong></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>To reduce front page clutter, the new plan&nbsp;is for meetups to be initially posted in the Discussion section, and for Anna Salamon to make a promoted post \"upcoming meetups\" post every Friday that links to every meet-up that has been planned for the next two weeks. [HT: <a href=\"/r/discussion/lw/5iy/proposal_consolidate_meetup_announcements_before\">Carl Shulman</a>.] Please let her know if your meetup is omitted. (I'm filling in for Anna this week, as she's directing her powers at the minicamp.)</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post about your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening:&nbsp;<strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>.</strong></p>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LrP7hCfGsgj74PARE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.202497318436772e-07, "legacy": true, "legacyId": "7678", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["77pFWACkmGSsBAzeb", "2QZoYyHfRoseBsN4c", "zPoHJujoJei5QocSd", "gbMWi22wWkQuQar32", "fEaaK8ybYnjaZJ5ob", "Adq7KbNpXDZJjmhE9", "CaL2P6tzSM7sTaaQv", "9a86BD3uro46vSkxe", "pAHo9zSFXygp5A5dL", "d28mWBMrFt8nwpXLp", "eaKHojBdtsf35937k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-28T03:22:53.260Z", "modifiedAt": null, "url": null, "title": "Ottawa LW meetup, June 2, 7pm; two Bayesian Conspiracy sessions", "slug": "ottawa-lw-meetup-june-2-7pm-two-bayesian-conspiracy-sessions", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9a86BD3uro46vSkxe/ottawa-lw-meetup-june-2-7pm-two-bayesian-conspiracy-sessions", "pageUrlRelative": "/posts/9a86BD3uro46vSkxe/ottawa-lw-meetup-june-2-7pm-two-bayesian-conspiracy-sessions", "linkUrl": "https://www.lesswrong.com/posts/9a86BD3uro46vSkxe/ottawa-lw-meetup-june-2-7pm-two-bayesian-conspiracy-sessions", "postedAtFormatted": "Saturday, May 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ottawa%20LW%20meetup%2C%20June%202%2C%207pm%3B%20two%20Bayesian%20Conspiracy%20sessions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOttawa%20LW%20meetup%2C%20June%202%2C%207pm%3B%20two%20Bayesian%20Conspiracy%20sessions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9a86BD3uro46vSkxe%2Fottawa-lw-meetup-june-2-7pm-two-bayesian-conspiracy-sessions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ottawa%20LW%20meetup%2C%20June%202%2C%207pm%3B%20two%20Bayesian%20Conspiracy%20sessions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9a86BD3uro46vSkxe%2Fottawa-lw-meetup-june-2-7pm-two-bayesian-conspiracy-sessions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9a86BD3uro46vSkxe%2Fottawa-lw-meetup-june-2-7pm-two-bayesian-conspiracy-sessions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<p><a id=\"more\"></a><strong>Less Wrong meeting:</strong></p>\n<p>Date: Thursday June 2, 7:00pm 'til whenever.</p>\n<p>Venue: Pub Italia, 434.5 Preston St.</p>\n<p><strong>Bayes study group:&nbsp;</strong>Anyone in the region interested in learning how to do Bayesian statistics is welcome to join us. We'll be using the statistical package&nbsp;<strong>R</strong>&nbsp;(<a href=\"http://cran.r-project.org/\">http://cran.r-project.org/</a>)&nbsp;as a platform, so bring your laptop if you have one.</p>\n<p>Date: Thursday June 2, 9:00am to 10:30am. NB: 9 in the morning.</p>\n<p>Venue:&nbsp;<a href=\"http://maps.google.ca/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=health+canada+jeanne+mance+building+ottawa&amp;aq=&amp;sll=45.408936,-75.737514&amp;sspn=0.003984,0.006899&amp;ie=UTF8&amp;hq=health+canada+jeanne+mance+building&amp;hnear=Ottawa,+Ottawa+Division,+Ontario&amp;ll=45.408936,-75.737514&amp;spn=0.007592,0.013797&amp;t=h&amp;z=16\">Jeanne Mance Building</a>, Tunney's Pasture. Meet me in the lobby.</p>\n<p><strong>Bayes study group:</strong>&nbsp;This session will run in parallel (but slightly delayed relative) to the sessions at Tunney's Pasture. This session will cover the introductory material. I'm not sure yet if I'll cover material on R in this series -- I'll see what the attendees want to do.</p>\n<p>Date: Tuesday May 31, 9:30pm</p>\n<p>Venue: wmiles's house; join the <a href=\"http://groups.google.com/group/less-wrong-ottawa\">Google group</a> for the address.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9a86BD3uro46vSkxe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 7.202762879546292e-07, "legacy": true, "legacyId": "7677", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-28T05:15:50.017Z", "modifiedAt": null, "url": null, "title": "Epistemology and the Psychology of Human Judgment", "slug": "epistemology-and-the-psychology-of-human-judgment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:29.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dKtetFsoY3f85vtJi/epistemology-and-the-psychology-of-human-judgment", "pageUrlRelative": "/posts/dKtetFsoY3f85vtJi/epistemology-and-the-psychology-of-human-judgment", "linkUrl": "https://www.lesswrong.com/posts/dKtetFsoY3f85vtJi/epistemology-and-the-psychology-of-human-judgment", "postedAtFormatted": "Saturday, May 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Epistemology%20and%20the%20Psychology%20of%20Human%20Judgment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEpistemology%20and%20the%20Psychology%20of%20Human%20Judgment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdKtetFsoY3f85vtJi%2Fepistemology-and-the-psychology-of-human-judgment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Epistemology%20and%20the%20Psychology%20of%20Human%20Judgment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdKtetFsoY3f85vtJi%2Fepistemology-and-the-psychology-of-human-judgment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdKtetFsoY3f85vtJi%2Fepistemology-and-the-psychology-of-human-judgment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3238, "htmlBody": "<p><img style=\"float:right; margin:0 0 10px 10px\" src=\"http://images.lesswrong.com/t3_5vs_0.png\" alt=\"Cover of Epistemology and Human Judgment\" /></p>\n<p>Strategic Reliabilism is an epistemological framework that, unlike other contemporary academic theories, is grounded in psychology and seeks to give genuine advice on how to form beliefs. The framework was first laid out by <a href=\"http://philosophy.fsu.edu/People/Faculty/Michael-Bishop\">Michael Bishop</a> and <a href=\"http://www.jdtrout.com/\">J.D. Trout</a> in their book <a href=\"http://www.amazon.com/dp/0195162307/?tag=vglnk-c319-20\"><em>Epistemology and the Psychology of Human Judgment</em></a>. Although regular readers here won&rsquo;t necessarily find a lot of new material here, Bishop and Trout provide a clear description of many of the working assumptions and goals of this community. In contrast to standard epistemology, which seeks to explain what constitutes a justified belief, Strategic Reliabilism is meant to explain excellent reasoning. In particular, reasoning is excellent to the extent it reliably and efficiently produces truths about significant matters. When combined with the Aristotelian principle that good reasoning tends to produce good outcomes in the long run (i.e. <a href=\"http://wiki.lesswrong.com/wiki/Rationalists_should_win\">rationalists should win</a>), empirical findings about good reasoning gain prescriptive power. Rather than getting bogged down in definitional debates, epistemology really is about being less wrong.</p>\n<p>The book is an easily read 150 pages, and I highly recommend you find a copy, but a chapter-by-chapter summary is below. As I said, you might not find a lot of new ideas in this book, but it went a long ways in clarifying how I think about this topic. For instance, even though it can seem trivial to be told to focus on significant problems, these basic issues deserve a little extra thought.</p>\n<p>If you enjoy podcasts, check out <a href=\"http://commonsenseatheism.com/?p=10553\">lukeprog&rsquo;s interview</a> with Michael Bishop. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2010/08/Strategic-Reliabilism.pdf\">This article</a> provides another overview of Strategic Reliabilism, addressing objections raised since the publication of the book. <a id=\"more\"></a></p>\n<hr />\n<h2 id=\"ch-1.-laying-our-cards-on-the-table\"><br /></h2>\n<h2>Ch 1. Laying Our Cards on the Table</h2>\n<p>Epistemology as a discipline needs to start offering practical advice. Defective epistemologies can compromise one&rsquo;s ability to act in all areas, but there is little social condemnation of weak reasoning. Prescriptive epistemology might be called &ldquo;critical thinking&rdquo;, but this field is divorced from contemporary epistemology. This book is driven by a vision of what epistemology could be, although, of course, is only a modest first step in that direction.</p>\n<p>Standard Analytic Epistemology (SAE) is primarily concerned with an account of knowledge and epistemic justification. This program assumes any account of justification must not radically alter our existing judgments, though this commitment to stasis is often not explicitly stated. SAE tends to proceed unproductively via tests against intuition and might provide some useful advice, but it&rsquo;s goals and methods are beyond repair.</p>\n<p>In the authors&rsquo; view, epistemology is a branch of philosophy of science and should start from Ameliorative Psychology, which encompasses parts of cognitive science, the heuristics and biases program, statistics, and artificial intelligence. This field makes recommendations about how to reason based on empirical findings. Rather than being concerned with an account of knowledge or warrant, the authors&rsquo; approach is to provide an account of <em>reasoning excellence</em>.</p>\n<p>A healthy epistemological tradition will have theoretical, practical, and social components. Theory and practice should mutually inform one another. Communication of practical results to the wider public would be part of its social role. Since the authors argue epistemology is a science, but nevertheless normative, one might worry the approach is circular. This concern assumes the normative must come all at once, though. Instead, one can rely on the Aristotelian Principle as an empirical hook to accumulate justification. The Aristotelian Principle says that <em>in the long run, good reasoning tends to lead to better outcomes than poor reasoning</em>. Why accept the Principle? Without it, epistemology wouldn&rsquo;t be useful. If bad reasoning leads to better outcomes and if there are many types of bad reasoning, how could we figure out which bad reasoning to use, except by good reasoning? We have reason to think useful epistemology is possible since we live in a stable enough environment where quality has a chance to make a difference.</p>\n<h2 id=\"ch-2.-the-amazing-success-of-statistical-prediction-rules\">Ch 2. The Amazing Success of Statistical Prediction Rules</h2>\n<p>Judgments are an essential part of life. Choices of whether to release a prisoner on parole, admit someone to medical school, or offer a loan are too important to be &ldquo;close enough&rdquo;. Only the best reasoning strategies available to us are satisfactory. Statistical prediction rules are robustly successful in these and many other high-stakes areas. In 136 studies comparing proper linear models to expert judgment, 64 clearly favored the SPR, 64 showed statistically equivalent accuracy, and 8 favored the expert <sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>. SPRs built explicitly to mimic experts&rsquo; judgments are more reliable than the expert, suggesting some errors are due to making exceptions to one&rsquo;s own rules.</p>\n<p>Improper linear models with unit or even random weights on standardized variables do surprisingly well. Qualitative human judgment can always be used as an input to an SPR or used to select variables and the direction of their effect. The flat maximum principle says that as long as the sign on coefficients is correct, all linear models do approximately the same. This principle applies when the problem is difficult and the inputs are reasonable predictive and redundant. Summing together inputs can be viewed as exploiting <a href=\"http://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem\">Condorcet&rsquo;s jury theorem</a>. Linear models tend to work when inputs interact monotonely, which appears to be the case in most social situations.</p>\n<p>All this is not to say SPRs are especially good, but that humans are especially bad predictors. We pick up false patterns and are unable to consider even medium amounts of information at once. Resistance to SPR findings typically comes from a belief in <em>epistemic exceptionalism</em>. There is an impulse to tweak the conclusions of an SPR, which leads to worse results. It is surprising to find out we do so badly that random linear models can do better &mdash; yet another manifestation of overconfidence. Counterintuitively, experts are best suited to deviate from SPRs grounded in theory because they have a better understanding of when the SPR will apply<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup>.</p>\n<h2 id=\"ch-3.-extracting-epistemic-lessons-from-ameliorative-psychology\">Ch 3. Extracting Epistemic Lessons from Ameliorative Psychology</h2>\n<p>Ameliorative Psychology offers a number of useful recommendations, but its normative assumptions are rarely stated explicitly. The authors identify three factors underlying the quality of a reasoning strategy: reliability across a wide range of problems, tractability, and applicability to significant problems. Strategies need to be robustly reliable to survive changes in environments. Cheaper and easier strategies allow one to &ldquo;purchase&rdquo; more truths. Simple strategies like SPRs have tended to be more successful as well, possibly by avoiding overfitting, but a easy, low-quality rule is better than a high-quality one that is never used. Finally, the world is full of useless correlations, so the trick is to find important ones.</p>\n<p>Cost-benefit relations have <a href=\"http://en.wikipedia.org/wiki/Diminishing_marginal_returns\">diminishing marginal returns</a>. By considering possible cost-benefit curves, startup costs, and marginal expected reliability, the possible ways to improve reasoning fall into exactly four categories. Three ways consist of changing strategies, and can be seen in the following matrix:</p>\n<table border=\"0\">\n<colgroup><col width=\"27%\"></col><col width=\"39%\"></col><col width=\"33%\"></col></colgroup> \n<tbody>\n<tr class=\"odd\">\n<td align=\"left\">&nbsp;</td>\n<td align=\"center\"><strong>Same (or lower) cost</strong></td>\n<td align=\"center\"><strong>Higher Cost</strong></td>\n</tr>\n<tr class=\"even\">\n<td align=\"left\"><strong>Greater</strong> <strong>Benefit</strong></td>\n<td align=\"center\">(1) Adopt more reliable, cheaper strategy.</td>\n<td align=\"center\">(2) Adopt more reliable, expensive strategy.</td>\n</tr>\n<tr class=\"odd\">\n<td align=\"left\"><strong>Same (or less)</strong> <strong>Benefit</strong></td>\n<td align=\"center\">(3) Adopt less reliable, but cheaper strategy.</td>\n<td align=\"center\">&nbsp;</td>\n</tr>\n</tbody>\n</table>\n<p><br /><br />The first is always worth adopting. The second, where more expensive strategies are adopted, is worth it if opportunity costs are not too high. The third is worth it for fairly insignificant matters. The fourth way is to reallocate resources among existing strategies, especially towards more significant issues.</p>\n<h2 id=\"ch-4.-strategic-reliabilism-robust-reliability\">Ch 4. Strategic Reliabilism: Robust Reliability</h2>\n<p>Strategic Reliabilism is the epistemological theory underlying Ameliorative Psychology which says, &ldquo;epistemic excellence involves the efficient allocation of cognitive resources to robustly reliable reasoning strategies applied to significant problems&rdquo;.</p>\n<p>Reasoning strategies can be characterized by four elements: the range of objects the rule makes judgments about, the inputs used to make the judgment, the formula combining the inputs, and the target or goal of the prediction. Rules would be perfectly reliable if their ranges could be defined as precisely the cases where they are accurate, but a rule isn&rsquo;t feasible if detecting the conditions of applicability isn&rsquo;t feasible. Rules are more or less accurate in different environments, so to evaluate a strategy&rsquo;s reliability, one must consider its expected range by a subject in an environment.</p>\n<p>Rules are robustly reliable if they make consistent, accurate predictions over a wide range. Being consistent involves being reliable on all natural subsets of a range, not just a few. Valuing robustness is important because low-reliability rules will be filtered out quicker and robust rules are easier to implement (since their wide scope means you need fewer of them) and safer for general use.</p>\n<h2 id=\"ch-5.-strategic-reliabilism-the-cost-and-benefits-of-excellent-judgment\">Ch 5. Strategic Reliabilism: The Cost and Benefits of Excellent Judgment</h2>\n<p>Since we are limited creatures, any practical epistemology must consider resource allocation. Cost-benefit analysis has been criticized for attempting to compare the incomparable. However, even a flawed cost-benefit analysis forces us to slow down and reflect on what we really value.</p>\n<p>Epistemic benefits are reflected in a problem&rsquo;s significance for a person. If outcomes are mostly the same (good or bad), errors aren&rsquo;t costly. For tractability, the authors propose to measure epistemic benefits in terms of reliability.</p>\n<p>Cognitive resources may not be easily transferred across tasks, so cost accounting must acknowledge how scarce time, memory, and attention are as well as how they interact. Time is one easily measurable proxy for overall costs.</p>\n<p>When thinking about the ways people reason badly, it may be easier to cultivate new habits rather revise how we reason. Better to stick with simple, automatic actions if possible, rather than rely on discipline.</p>\n<h2 id=\"ch-6.-strategic-reliablism-epistemic-significance\">Ch 6. Strategic Reliablism: Epistemic Significance</h2>\n<p>If all you want are true beliefs, life is easy. Spend all your time sitting outside counting blimps, and you will be perfectly accurate almost all the time. Excellent reasoners must reason well about significant matters, not just arbitrary ones. Significance in general will be difficult to judge, since significance varies depending on the particular situation. Perhaps we can pick out features significant matters tend to share. For instance, not all reasoning about causality is significant, but most significant matters involve causal reasoning, so this is a skill worth improving.</p>\n<p>The difficulty in creating an account of significance is that it can&rsquo;t be too strong or too weak. We shouldn&rsquo;t be able to say almost anything is significant, but we need room for substantial interpersonal differences. The authors&rsquo; view is that significance of a problem for a person is the strength of the objective reasons that person has for devoting resources to solving that problem. Epistemology must acknowledge other normative domains, and the authors assume there are objective reasons for action, i.e.&nbsp;the reasons hold whether or not the person in quesition recognizes them or thinks them legitimate. At a minimum, individuals have moral and prudential reasons for action. Not all reasons are tied to consequences; some reasons might be tied to duties. Knowing certain basic truths might be intrinsically valuable, so there could be purely epistemic reasons.</p>\n<p>It&rsquo;s not hard to find <em>some</em> reason to solve a problem, so the main question is the strength of those reasons. Even &ldquo;lost causes&rdquo; might be significant, especially if one accepts duty-based reasons. Some problems might be negatively significant, where one has reasons <em>not</em> to spend time reasoning about it. For instance, &ldquo;philosophy grad student disease&rdquo; (constant monitoring of how smart you are relative to your peers) is negatively significant. Since our reasons are ultimately tied to human well-being, so is epistemic significance.</p>\n<p>One problem with this account is that people may not know the relevant reasons. Any theory incorporating signifance must deal with people lacking a good sense of it, so this is a fact of life. Part of the problem of allocating resources involves spending time determining significance to guide further allocation.</p>\n<h2 id=\"ch-7.-the-troubles-with-standard-analytic-epistemology\">Ch 7. The Troubles with Standard Analytic Epistemology</h2>\n<p>Modern versions of Standard Analytic Epistemology include foundationalism, coherentism, reliabilism, and contextualism. Most proponents of SAE agree that naturalized epistemology can&rsquo;t work. The authors&rsquo; approach is naturalistic because it begins with a descriptive core and works from there. The standard objection is a descriptive theory can&rsquo;t yield prescriptions. However, SAE has a descriptive theory at its core and is less likely to overcome the is-ought gap, so Strategic Reliabilism is superior to any existing theory of SAE.</p>\n<p>Since SAE theories of justification are tested against philosophers&rsquo; considered judgments, there is an implicit stasis criterion. If we were magically granted the best SAE theory, it would essentially be a descriptive theory of these opinions. After all, epistemic judgments vary considerably across and between cultures, so it is slightly odd to focus on the intuitions of high-SES Westerners.</p>\n<p>If SAE works from a descriptive core, how are normative consequences extracted? The authors do not contend this is impossible for theories of SAE, but the prospects aren&rsquo;t good. Many criticisms of naturalism by SAE proponents apply to their own theories. In the end, everyone has to bridge the is-ought gap. Philosophers are essentially experts in their own opinions, while Ameliorative Psychologists have documented success at helping people and institutions reason better. By the Aristotelian Principle, this success is what gives Strategic Reliabilism a chance at normativity.</p>\n<p>Strategic Reliabilism is not a theory of justification, but if it were cast in that light, it would be more worthy of belief than any available theory. If it recommends justified beliefs, no other theories are necessary. If it occasionally recommends unjustified beliefs, what would that mean? The belief is a result of excellent reasoning produces true beliefs and hence better outcomes about significant matters on average, but isn&rsquo;t deemed justified by a bunch of philosophers. Would proponents of SAE have the holders of this belief adopt less reliable strategies or think about less significant problems? What would justification really buy us?</p>\n<h2 id=\"ch-8.-putting-epistemology-into-practice-normative-disputes-in-psychology\">Ch 8. Putting Epistemology into Practice: Normative Disputes in Psychology</h2>\n<p>The Heuristics and Biases program revealed many systematic flaws in human reasoning. Unlike Ameliorative Psychology, philosophers have paid attention to the HB program. Some have been critical of the interpretation of the findings, arguing subjects&rsquo; performances on tasks are justified under different norms than applied by the experimenter. These reject-the-norm arguments can be made on empirical grounds, for instance claiming that subjects understand the problem differently than the experimenter intended. Reject-the-norm arguments can also be made on conceptual grounds.</p>\n<p>One such conceptual argument is given by Gerd Gigerenzer. He begins by noting that from a frequentist point of view, it doesn&rsquo;t make sense to assign probabilities to one-time events, as subjects are often asked to do. Hence subjects&rsquo; answers can&rsquo;t be judged as errors since they are valid under a possible interpretation. Kahneman and Tversky argued with Gigerenzer in this narrow normative framework over whether subjects violated the laws of probability, but from a Strategic Reliabilist perspective, the fundamental issue is that people can make serious mistakes, whether or not these count as &ldquo;errors&rdquo;. Ironically, Gigerenzer understands perfectly well that subjects reason poorly, even if he won&rsquo;t call it an error.</p>\n<p>Another argument comes from L.J. Cohen, who argues normal adults cannot be empirically shown to be irrational. Ordinary human reasoning sets its own standards, so any flaws must be performance errors, not flaws in reasoning competence. This performance-competence distinction could be analogous to linguistics, where we might think everyone is competent at their native language, even if that isn&rsquo;t perfectly reflected in performance. Then, nothing could be an error unless its author, under ideal conditions, would agree it was an error. Cohen is surely right that there is distinction between performance and competence, but language is the wrong comparison. We wouldn&rsquo;t treat everyone&rsquo;s baseline capabilities at painting, swimming, or math as the measure of excellence, so we should be able to recognize differences in reasoning competences. This is supported by the considerable correlation between scores on typical reasoning tasks, which are also correlated with SAT scores, although somewhat curiously, not math education <sup><a id=\"fnref3\" class=\"footnoteRef\" href=\"#fn3\">3</a></sup>. Even though Cohen seems to be arguing for epistemic relativism, he must actually be arguing psychologists and others are wrong.</p>\n<p>From the perspective of Strategic Reliabilism, the quality of a reasoning strategy depends on its expected costs and benefits relative to its competitors. Hence, a demonstration of reasoning excellence involves evidence that a person chose the best strategy available, which conceptual reject-the-norm arguments ignore, so these arguments can only succeed by changing the subject, redefining &ldquo;rationality&rdquo; and &ldquo;error&rdquo; in non-useful ways.</p>\n<h2 id=\"ch-9.-putting-epistemology-into-practice-positive-advice\">Ch 9. Putting Epistemology into Practice: Positive Advice</h2>\n<p>Practical advice can only be made so far as empirical data allows, but the Strategic Reliabilist theory that reasoning strategies are better to the extent they are cheaper, more robustly reliable, and address more significant issues can tell us what evidence is missing if we want to offer guidance.</p>\n<p>In diagnostic problems, subjects have a difficult time directly employing Bayes&rsquo; Rule. However, if probabilities are recast as natural frequencies, subjects perform much better. Even though both rely on Bayes&rsquo; Rule as a mathematical identity, as reasoning strategies Bayes&rsquo; Rule and frequency formats are very different. The start-up costs to reliably use the former are too high for many.</p>\n<p>Overconfidence is a pervasive feature of reasoning. Monetary incentives and simple declarations to reduce bias have no effect. In controlled environments, calibration exercises can eliminate overconfidence. For individuals, the most feasible method is to <em>consider the opposite</em>. An effective form of this strategy involves the simple rule &ldquo;Stop and consider why your judgment might be wrong&rdquo;. Applying this to every facet of our lives might be too expensive, either because it requires too much discipline or makes us neurotic. Both are valid concerns from a Strategic Reliabilist view, but is likely to be worth employing for significant problems.</p>\n<p>Compelling narratives are often accepted as causal explanations. Though they can go awry, controlled experiments provide the best way of understanding causal relationships. Considering what might happen for a control even if there isn&rsquo;t one is a first step in addressing these biases. Acknowledging that a control might be impossible would lead us to accept fewer causal claims. Narratives come too easily, especially for rare or unique events.</p>\n<p>It is not clear how well good reasoning can be taught, but there is hope. One group of researchers surprised themselves when they found formal discipline has an effect, though admitting we know very little about reasoning, how to teach it, or how much of an improvement is possible by instruction <sup><a id=\"fnref4\" class=\"footnoteRef\" href=\"#fn4\">4</a></sup>.</p>\n<h2 id=\"ch-10.-conclusion\">Ch 10. Conclusion</h2>\n<p>Psychology profitable divorced itself from philosophy in the mid&ndash;19th century. Philosophers have been particularly neglectful of developments in the other field, but both disciplines could benefit from increased interaction.</p>\n<p>The authors propose three projects that would aid the development of a strong, mature epistemology. The first is to acquire a wide-range of new heuristics people can feasibly employ. Second, to guide the first project, an stronger account of human well-being is needed to highlight significant areas. Third, social institutions should be developed keeping in mind that much of our reasoning is ecological.</p>\n<p>Philosophy might be about self-knowledge, but that knowledge is unlikely to come from introspection. Epistemologists might become theoreticians describing an applied science, but the overall discipline will be stronger for it.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\">\n<p><a href=\"http://www.tc.umn.edu/~pemeehl/167GroveMeehlClinstix.pdf\">Grove and Meehl (1996)</a>, &ldquo;Comparative Efficiency of Informal (Subjective, Impressionistic) and Formal (Mechanical, Algorithmic) Prediction Procedures: The Clinical Statistical Controvery&rdquo;, <em>Psychology, Public Policy, and Law</em> 2: 293&mdash;323 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 1\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p><a href=\"http://www.psychologicalscience.org/journals/pspi/pdf/pspi001.pdf\">Swets, Dawes, and Monihan (2000)</a>. &ldquo;Psychological science can improve diagnostic decisions&rdquo;. <em>Psychological Science in the Public Interest</em> 1:1&mdash;26 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 2\" href=\"#fnref2\">\u21a9</a></p>\n</li>\n<li id=\"fn3\">\n<p><a href=\"http://educ.jmu.edu/~westrf/papers/Stanovich-Ind-Dif-JEPG98.pdf\">Stanovich and West (1998)</a>, &ldquo;Individual Differences in Rational Thought&rdquo;. <em>Journal of Experimental Psychology: General</em> 127: 161&mdash;188 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 3\" href=\"#fnref3\">\u21a9</a></p>\n</li>\n<li id=\"fn4\">\n<p>Lehman, Lempert, and Nisbett (1988). &ldquo;The effects of graduate training on reasoning: Formal discipline and thinking about everyday-life events&rdquo;. <em>American Psychologist</em> 43:6, 431&mdash;442. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 4\" href=\"#fnref4\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 1, "xgpBASEThXPuKRhbS": 1, "3uE2pXvbcnS9nnZRE": 1, "dBPou4ihoQNY4cquv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dKtetFsoY3f85vtJi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 44, "extendedScore": null, "score": 8.8e-05, "legacy": true, "legacyId": "7624", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><img style=\"float:right; margin:0 0 10px 10px\" src=\"http://images.lesswrong.com/t3_5vs_0.png\" alt=\"Cover of Epistemology and Human Judgment\"></p>\n<p>Strategic Reliabilism is an epistemological framework that, unlike other contemporary academic theories, is grounded in psychology and seeks to give genuine advice on how to form beliefs. The framework was first laid out by <a href=\"http://philosophy.fsu.edu/People/Faculty/Michael-Bishop\">Michael Bishop</a> and <a href=\"http://www.jdtrout.com/\">J.D. Trout</a> in their book <a href=\"http://www.amazon.com/dp/0195162307/?tag=vglnk-c319-20\"><em>Epistemology and the Psychology of Human Judgment</em></a>. Although regular readers here won\u2019t necessarily find a lot of new material here, Bishop and Trout provide a clear description of many of the working assumptions and goals of this community. In contrast to standard epistemology, which seeks to explain what constitutes a justified belief, Strategic Reliabilism is meant to explain excellent reasoning. In particular, reasoning is excellent to the extent it reliably and efficiently produces truths about significant matters. When combined with the Aristotelian principle that good reasoning tends to produce good outcomes in the long run (i.e. <a href=\"http://wiki.lesswrong.com/wiki/Rationalists_should_win\">rationalists should win</a>), empirical findings about good reasoning gain prescriptive power. Rather than getting bogged down in definitional debates, epistemology really is about being less wrong.</p>\n<p>The book is an easily read 150 pages, and I highly recommend you find a copy, but a chapter-by-chapter summary is below. As I said, you might not find a lot of new ideas in this book, but it went a long ways in clarifying how I think about this topic. For instance, even though it can seem trivial to be told to focus on significant problems, these basic issues deserve a little extra thought.</p>\n<p>If you enjoy podcasts, check out <a href=\"http://commonsenseatheism.com/?p=10553\">lukeprog\u2019s interview</a> with Michael Bishop. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2010/08/Strategic-Reliabilism.pdf\">This article</a> provides another overview of Strategic Reliabilism, addressing objections raised since the publication of the book. <a id=\"more\"></a></p>\n<hr>\n<h2 id=\"ch-1.-laying-our-cards-on-the-table\"><br></h2>\n<h2 id=\"Ch_1__Laying_Our_Cards_on_the_Table\">Ch 1. Laying Our Cards on the Table</h2>\n<p>Epistemology as a discipline needs to start offering practical advice. Defective epistemologies can compromise one\u2019s ability to act in all areas, but there is little social condemnation of weak reasoning. Prescriptive epistemology might be called \u201ccritical thinking\u201d, but this field is divorced from contemporary epistemology. This book is driven by a vision of what epistemology could be, although, of course, is only a modest first step in that direction.</p>\n<p>Standard Analytic Epistemology (SAE) is primarily concerned with an account of knowledge and epistemic justification. This program assumes any account of justification must not radically alter our existing judgments, though this commitment to stasis is often not explicitly stated. SAE tends to proceed unproductively via tests against intuition and might provide some useful advice, but it\u2019s goals and methods are beyond repair.</p>\n<p>In the authors\u2019 view, epistemology is a branch of philosophy of science and should start from Ameliorative Psychology, which encompasses parts of cognitive science, the heuristics and biases program, statistics, and artificial intelligence. This field makes recommendations about how to reason based on empirical findings. Rather than being concerned with an account of knowledge or warrant, the authors\u2019 approach is to provide an account of <em>reasoning excellence</em>.</p>\n<p>A healthy epistemological tradition will have theoretical, practical, and social components. Theory and practice should mutually inform one another. Communication of practical results to the wider public would be part of its social role. Since the authors argue epistemology is a science, but nevertheless normative, one might worry the approach is circular. This concern assumes the normative must come all at once, though. Instead, one can rely on the Aristotelian Principle as an empirical hook to accumulate justification. The Aristotelian Principle says that <em>in the long run, good reasoning tends to lead to better outcomes than poor reasoning</em>. Why accept the Principle? Without it, epistemology wouldn\u2019t be useful. If bad reasoning leads to better outcomes and if there are many types of bad reasoning, how could we figure out which bad reasoning to use, except by good reasoning? We have reason to think useful epistemology is possible since we live in a stable enough environment where quality has a chance to make a difference.</p>\n<h2 id=\"Ch_2__The_Amazing_Success_of_Statistical_Prediction_Rules\">Ch 2. The Amazing Success of Statistical Prediction Rules</h2>\n<p>Judgments are an essential part of life. Choices of whether to release a prisoner on parole, admit someone to medical school, or offer a loan are too important to be \u201cclose enough\u201d. Only the best reasoning strategies available to us are satisfactory. Statistical prediction rules are robustly successful in these and many other high-stakes areas. In 136 studies comparing proper linear models to expert judgment, 64 clearly favored the SPR, 64 showed statistically equivalent accuracy, and 8 favored the expert <sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>. SPRs built explicitly to mimic experts\u2019 judgments are more reliable than the expert, suggesting some errors are due to making exceptions to one\u2019s own rules.</p>\n<p>Improper linear models with unit or even random weights on standardized variables do surprisingly well. Qualitative human judgment can always be used as an input to an SPR or used to select variables and the direction of their effect. The flat maximum principle says that as long as the sign on coefficients is correct, all linear models do approximately the same. This principle applies when the problem is difficult and the inputs are reasonable predictive and redundant. Summing together inputs can be viewed as exploiting <a href=\"http://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem\">Condorcet\u2019s jury theorem</a>. Linear models tend to work when inputs interact monotonely, which appears to be the case in most social situations.</p>\n<p>All this is not to say SPRs are especially good, but that humans are especially bad predictors. We pick up false patterns and are unable to consider even medium amounts of information at once. Resistance to SPR findings typically comes from a belief in <em>epistemic exceptionalism</em>. There is an impulse to tweak the conclusions of an SPR, which leads to worse results. It is surprising to find out we do so badly that random linear models can do better \u2014 yet another manifestation of overconfidence. Counterintuitively, experts are best suited to deviate from SPRs grounded in theory because they have a better understanding of when the SPR will apply<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup>.</p>\n<h2 id=\"Ch_3__Extracting_Epistemic_Lessons_from_Ameliorative_Psychology\">Ch 3. Extracting Epistemic Lessons from Ameliorative Psychology</h2>\n<p>Ameliorative Psychology offers a number of useful recommendations, but its normative assumptions are rarely stated explicitly. The authors identify three factors underlying the quality of a reasoning strategy: reliability across a wide range of problems, tractability, and applicability to significant problems. Strategies need to be robustly reliable to survive changes in environments. Cheaper and easier strategies allow one to \u201cpurchase\u201d more truths. Simple strategies like SPRs have tended to be more successful as well, possibly by avoiding overfitting, but a easy, low-quality rule is better than a high-quality one that is never used. Finally, the world is full of useless correlations, so the trick is to find important ones.</p>\n<p>Cost-benefit relations have <a href=\"http://en.wikipedia.org/wiki/Diminishing_marginal_returns\">diminishing marginal returns</a>. By considering possible cost-benefit curves, startup costs, and marginal expected reliability, the possible ways to improve reasoning fall into exactly four categories. Three ways consist of changing strategies, and can be seen in the following matrix:</p>\n<table border=\"0\">\n<colgroup><col width=\"27%\"><col width=\"39%\"><col width=\"33%\"></colgroup> \n<tbody>\n<tr class=\"odd\">\n<td align=\"left\">&nbsp;</td>\n<td align=\"center\"><strong>Same (or lower) cost</strong></td>\n<td align=\"center\"><strong>Higher Cost</strong></td>\n</tr>\n<tr class=\"even\">\n<td align=\"left\"><strong>Greater</strong> <strong>Benefit</strong></td>\n<td align=\"center\">(1) Adopt more reliable, cheaper strategy.</td>\n<td align=\"center\">(2) Adopt more reliable, expensive strategy.</td>\n</tr>\n<tr class=\"odd\">\n<td align=\"left\"><strong>Same (or less)</strong> <strong>Benefit</strong></td>\n<td align=\"center\">(3) Adopt less reliable, but cheaper strategy.</td>\n<td align=\"center\">&nbsp;</td>\n</tr>\n</tbody>\n</table>\n<p><br><br>The first is always worth adopting. The second, where more expensive strategies are adopted, is worth it if opportunity costs are not too high. The third is worth it for fairly insignificant matters. The fourth way is to reallocate resources among existing strategies, especially towards more significant issues.</p>\n<h2 id=\"Ch_4__Strategic_Reliabilism__Robust_Reliability\">Ch 4. Strategic Reliabilism: Robust Reliability</h2>\n<p>Strategic Reliabilism is the epistemological theory underlying Ameliorative Psychology which says, \u201cepistemic excellence involves the efficient allocation of cognitive resources to robustly reliable reasoning strategies applied to significant problems\u201d.</p>\n<p>Reasoning strategies can be characterized by four elements: the range of objects the rule makes judgments about, the inputs used to make the judgment, the formula combining the inputs, and the target or goal of the prediction. Rules would be perfectly reliable if their ranges could be defined as precisely the cases where they are accurate, but a rule isn\u2019t feasible if detecting the conditions of applicability isn\u2019t feasible. Rules are more or less accurate in different environments, so to evaluate a strategy\u2019s reliability, one must consider its expected range by a subject in an environment.</p>\n<p>Rules are robustly reliable if they make consistent, accurate predictions over a wide range. Being consistent involves being reliable on all natural subsets of a range, not just a few. Valuing robustness is important because low-reliability rules will be filtered out quicker and robust rules are easier to implement (since their wide scope means you need fewer of them) and safer for general use.</p>\n<h2 id=\"Ch_5__Strategic_Reliabilism__The_Cost_and_Benefits_of_Excellent_Judgment\">Ch 5. Strategic Reliabilism: The Cost and Benefits of Excellent Judgment</h2>\n<p>Since we are limited creatures, any practical epistemology must consider resource allocation. Cost-benefit analysis has been criticized for attempting to compare the incomparable. However, even a flawed cost-benefit analysis forces us to slow down and reflect on what we really value.</p>\n<p>Epistemic benefits are reflected in a problem\u2019s significance for a person. If outcomes are mostly the same (good or bad), errors aren\u2019t costly. For tractability, the authors propose to measure epistemic benefits in terms of reliability.</p>\n<p>Cognitive resources may not be easily transferred across tasks, so cost accounting must acknowledge how scarce time, memory, and attention are as well as how they interact. Time is one easily measurable proxy for overall costs.</p>\n<p>When thinking about the ways people reason badly, it may be easier to cultivate new habits rather revise how we reason. Better to stick with simple, automatic actions if possible, rather than rely on discipline.</p>\n<h2 id=\"Ch_6__Strategic_Reliablism__Epistemic_Significance\">Ch 6. Strategic Reliablism: Epistemic Significance</h2>\n<p>If all you want are true beliefs, life is easy. Spend all your time sitting outside counting blimps, and you will be perfectly accurate almost all the time. Excellent reasoners must reason well about significant matters, not just arbitrary ones. Significance in general will be difficult to judge, since significance varies depending on the particular situation. Perhaps we can pick out features significant matters tend to share. For instance, not all reasoning about causality is significant, but most significant matters involve causal reasoning, so this is a skill worth improving.</p>\n<p>The difficulty in creating an account of significance is that it can\u2019t be too strong or too weak. We shouldn\u2019t be able to say almost anything is significant, but we need room for substantial interpersonal differences. The authors\u2019 view is that significance of a problem for a person is the strength of the objective reasons that person has for devoting resources to solving that problem. Epistemology must acknowledge other normative domains, and the authors assume there are objective reasons for action, i.e.&nbsp;the reasons hold whether or not the person in quesition recognizes them or thinks them legitimate. At a minimum, individuals have moral and prudential reasons for action. Not all reasons are tied to consequences; some reasons might be tied to duties. Knowing certain basic truths might be intrinsically valuable, so there could be purely epistemic reasons.</p>\n<p>It\u2019s not hard to find <em>some</em> reason to solve a problem, so the main question is the strength of those reasons. Even \u201clost causes\u201d might be significant, especially if one accepts duty-based reasons. Some problems might be negatively significant, where one has reasons <em>not</em> to spend time reasoning about it. For instance, \u201cphilosophy grad student disease\u201d (constant monitoring of how smart you are relative to your peers) is negatively significant. Since our reasons are ultimately tied to human well-being, so is epistemic significance.</p>\n<p>One problem with this account is that people may not know the relevant reasons. Any theory incorporating signifance must deal with people lacking a good sense of it, so this is a fact of life. Part of the problem of allocating resources involves spending time determining significance to guide further allocation.</p>\n<h2 id=\"Ch_7__The_Troubles_with_Standard_Analytic_Epistemology\">Ch 7. The Troubles with Standard Analytic Epistemology</h2>\n<p>Modern versions of Standard Analytic Epistemology include foundationalism, coherentism, reliabilism, and contextualism. Most proponents of SAE agree that naturalized epistemology can\u2019t work. The authors\u2019 approach is naturalistic because it begins with a descriptive core and works from there. The standard objection is a descriptive theory can\u2019t yield prescriptions. However, SAE has a descriptive theory at its core and is less likely to overcome the is-ought gap, so Strategic Reliabilism is superior to any existing theory of SAE.</p>\n<p>Since SAE theories of justification are tested against philosophers\u2019 considered judgments, there is an implicit stasis criterion. If we were magically granted the best SAE theory, it would essentially be a descriptive theory of these opinions. After all, epistemic judgments vary considerably across and between cultures, so it is slightly odd to focus on the intuitions of high-SES Westerners.</p>\n<p>If SAE works from a descriptive core, how are normative consequences extracted? The authors do not contend this is impossible for theories of SAE, but the prospects aren\u2019t good. Many criticisms of naturalism by SAE proponents apply to their own theories. In the end, everyone has to bridge the is-ought gap. Philosophers are essentially experts in their own opinions, while Ameliorative Psychologists have documented success at helping people and institutions reason better. By the Aristotelian Principle, this success is what gives Strategic Reliabilism a chance at normativity.</p>\n<p>Strategic Reliabilism is not a theory of justification, but if it were cast in that light, it would be more worthy of belief than any available theory. If it recommends justified beliefs, no other theories are necessary. If it occasionally recommends unjustified beliefs, what would that mean? The belief is a result of excellent reasoning produces true beliefs and hence better outcomes about significant matters on average, but isn\u2019t deemed justified by a bunch of philosophers. Would proponents of SAE have the holders of this belief adopt less reliable strategies or think about less significant problems? What would justification really buy us?</p>\n<h2 id=\"Ch_8__Putting_Epistemology_into_Practice__Normative_Disputes_in_Psychology\">Ch 8. Putting Epistemology into Practice: Normative Disputes in Psychology</h2>\n<p>The Heuristics and Biases program revealed many systematic flaws in human reasoning. Unlike Ameliorative Psychology, philosophers have paid attention to the HB program. Some have been critical of the interpretation of the findings, arguing subjects\u2019 performances on tasks are justified under different norms than applied by the experimenter. These reject-the-norm arguments can be made on empirical grounds, for instance claiming that subjects understand the problem differently than the experimenter intended. Reject-the-norm arguments can also be made on conceptual grounds.</p>\n<p>One such conceptual argument is given by Gerd Gigerenzer. He begins by noting that from a frequentist point of view, it doesn\u2019t make sense to assign probabilities to one-time events, as subjects are often asked to do. Hence subjects\u2019 answers can\u2019t be judged as errors since they are valid under a possible interpretation. Kahneman and Tversky argued with Gigerenzer in this narrow normative framework over whether subjects violated the laws of probability, but from a Strategic Reliabilist perspective, the fundamental issue is that people can make serious mistakes, whether or not these count as \u201cerrors\u201d. Ironically, Gigerenzer understands perfectly well that subjects reason poorly, even if he won\u2019t call it an error.</p>\n<p>Another argument comes from L.J. Cohen, who argues normal adults cannot be empirically shown to be irrational. Ordinary human reasoning sets its own standards, so any flaws must be performance errors, not flaws in reasoning competence. This performance-competence distinction could be analogous to linguistics, where we might think everyone is competent at their native language, even if that isn\u2019t perfectly reflected in performance. Then, nothing could be an error unless its author, under ideal conditions, would agree it was an error. Cohen is surely right that there is distinction between performance and competence, but language is the wrong comparison. We wouldn\u2019t treat everyone\u2019s baseline capabilities at painting, swimming, or math as the measure of excellence, so we should be able to recognize differences in reasoning competences. This is supported by the considerable correlation between scores on typical reasoning tasks, which are also correlated with SAT scores, although somewhat curiously, not math education <sup><a id=\"fnref3\" class=\"footnoteRef\" href=\"#fn3\">3</a></sup>. Even though Cohen seems to be arguing for epistemic relativism, he must actually be arguing psychologists and others are wrong.</p>\n<p>From the perspective of Strategic Reliabilism, the quality of a reasoning strategy depends on its expected costs and benefits relative to its competitors. Hence, a demonstration of reasoning excellence involves evidence that a person chose the best strategy available, which conceptual reject-the-norm arguments ignore, so these arguments can only succeed by changing the subject, redefining \u201crationality\u201d and \u201cerror\u201d in non-useful ways.</p>\n<h2 id=\"Ch_9__Putting_Epistemology_into_Practice__Positive_Advice\">Ch 9. Putting Epistemology into Practice: Positive Advice</h2>\n<p>Practical advice can only be made so far as empirical data allows, but the Strategic Reliabilist theory that reasoning strategies are better to the extent they are cheaper, more robustly reliable, and address more significant issues can tell us what evidence is missing if we want to offer guidance.</p>\n<p>In diagnostic problems, subjects have a difficult time directly employing Bayes\u2019 Rule. However, if probabilities are recast as natural frequencies, subjects perform much better. Even though both rely on Bayes\u2019 Rule as a mathematical identity, as reasoning strategies Bayes\u2019 Rule and frequency formats are very different. The start-up costs to reliably use the former are too high for many.</p>\n<p>Overconfidence is a pervasive feature of reasoning. Monetary incentives and simple declarations to reduce bias have no effect. In controlled environments, calibration exercises can eliminate overconfidence. For individuals, the most feasible method is to <em>consider the opposite</em>. An effective form of this strategy involves the simple rule \u201cStop and consider why your judgment might be wrong\u201d. Applying this to every facet of our lives might be too expensive, either because it requires too much discipline or makes us neurotic. Both are valid concerns from a Strategic Reliabilist view, but is likely to be worth employing for significant problems.</p>\n<p>Compelling narratives are often accepted as causal explanations. Though they can go awry, controlled experiments provide the best way of understanding causal relationships. Considering what might happen for a control even if there isn\u2019t one is a first step in addressing these biases. Acknowledging that a control might be impossible would lead us to accept fewer causal claims. Narratives come too easily, especially for rare or unique events.</p>\n<p>It is not clear how well good reasoning can be taught, but there is hope. One group of researchers surprised themselves when they found formal discipline has an effect, though admitting we know very little about reasoning, how to teach it, or how much of an improvement is possible by instruction <sup><a id=\"fnref4\" class=\"footnoteRef\" href=\"#fn4\">4</a></sup>.</p>\n<h2 id=\"Ch_10__Conclusion\">Ch 10. Conclusion</h2>\n<p>Psychology profitable divorced itself from philosophy in the mid\u201319th century. Philosophers have been particularly neglectful of developments in the other field, but both disciplines could benefit from increased interaction.</p>\n<p>The authors propose three projects that would aid the development of a strong, mature epistemology. The first is to acquire a wide-range of new heuristics people can feasibly employ. Second, to guide the first project, an stronger account of human well-being is needed to highlight significant areas. Third, social institutions should be developed keeping in mind that much of our reasoning is ecological.</p>\n<p>Philosophy might be about self-knowledge, but that knowledge is unlikely to come from introspection. Epistemologists might become theoreticians describing an applied science, but the overall discipline will be stronger for it.</p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn1\">\n<p><a href=\"http://www.tc.umn.edu/~pemeehl/167GroveMeehlClinstix.pdf\">Grove and Meehl (1996)</a>, \u201cComparative Efficiency of Informal (Subjective, Impressionistic) and Formal (Mechanical, Algorithmic) Prediction Procedures: The Clinical Statistical Controvery\u201d, <em>Psychology, Public Policy, and Law</em> 2: 293\u2014323 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 1\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p><a href=\"http://www.psychologicalscience.org/journals/pspi/pdf/pspi001.pdf\">Swets, Dawes, and Monihan (2000)</a>. \u201cPsychological science can improve diagnostic decisions\u201d. <em>Psychological Science in the Public Interest</em> 1:1\u201426 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 2\" href=\"#fnref2\">\u21a9</a></p>\n</li>\n<li id=\"fn3\">\n<p><a href=\"http://educ.jmu.edu/~westrf/papers/Stanovich-Ind-Dif-JEPG98.pdf\">Stanovich and West (1998)</a>, \u201cIndividual Differences in Rational Thought\u201d. <em>Journal of Experimental Psychology: General</em> 127: 161\u2014188 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 3\" href=\"#fnref3\">\u21a9</a></p>\n</li>\n<li id=\"fn4\">\n<p>Lehman, Lempert, and Nisbett (1988). \u201cThe effects of graduate training on reasoning: Formal discipline and thinking about everyday-life events\u201d. <em>American Psychologist</em> 43:6, 431\u2014442. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 4\" href=\"#fnref4\">\u21a9</a></p>\n</li>\n</ol></div>", "sections": [{"title": "Ch 1. Laying Our Cards on the Table", "anchor": "Ch_1__Laying_Our_Cards_on_the_Table", "level": 1}, {"title": "Ch 2. The Amazing Success of Statistical Prediction Rules", "anchor": "Ch_2__The_Amazing_Success_of_Statistical_Prediction_Rules", "level": 1}, {"title": "Ch 3. Extracting Epistemic Lessons from Ameliorative Psychology", "anchor": "Ch_3__Extracting_Epistemic_Lessons_from_Ameliorative_Psychology", "level": 1}, {"title": "Ch 4. Strategic Reliabilism: Robust Reliability", "anchor": "Ch_4__Strategic_Reliabilism__Robust_Reliability", "level": 1}, {"title": "Ch 5. Strategic Reliabilism: The Cost and Benefits of Excellent Judgment", "anchor": "Ch_5__Strategic_Reliabilism__The_Cost_and_Benefits_of_Excellent_Judgment", "level": 1}, {"title": "Ch 6. Strategic Reliablism: Epistemic Significance", "anchor": "Ch_6__Strategic_Reliablism__Epistemic_Significance", "level": 1}, {"title": "Ch 7. The Troubles with Standard Analytic Epistemology", "anchor": "Ch_7__The_Troubles_with_Standard_Analytic_Epistemology", "level": 1}, {"title": "Ch 8. Putting Epistemology into Practice: Normative Disputes in Psychology", "anchor": "Ch_8__Putting_Epistemology_into_Practice__Normative_Disputes_in_Psychology", "level": 1}, {"title": "Ch 9. Putting Epistemology into Practice: Positive Advice", "anchor": "Ch_9__Putting_Epistemology_into_Practice__Positive_Advice", "level": 1}, {"title": "Ch 10. Conclusion", "anchor": "Ch_10__Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-28T07:22:20.911Z", "modifiedAt": null, "url": null, "title": "Skepticalcon", "slug": "skepticalcon", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "zntneo", "createdAt": "2011-02-25T06:06:34.571Z", "isAdmin": false, "displayName": "zntneo"}, "userId": "AgBNRryLw7RWBT7uh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DG8oStFwKj33zNoZT/skepticalcon", "pageUrlRelative": "/posts/DG8oStFwKj33zNoZT/skepticalcon", "linkUrl": "https://www.lesswrong.com/posts/DG8oStFwKj33zNoZT/skepticalcon", "postedAtFormatted": "Saturday, May 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Skepticalcon&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASkepticalcon%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDG8oStFwKj33zNoZT%2Fskepticalcon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Skepticalcon%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDG8oStFwKj33zNoZT%2Fskepticalcon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDG8oStFwKj33zNoZT%2Fskepticalcon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<p>I know there is some overlap between the lesswrong community and the skeptics community. So my question is if anyone on lesswrong is going to skepticalcon this week and if so if you'd like to meet up there.</p>\n<p>&nbsp;</p>\n<p>Oh and in case anyone is curious its in berkley,ca</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DG8oStFwKj33zNoZT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 7.203466862763803e-07, "legacy": true, "legacyId": "7690", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-28T09:57:10.801Z", "modifiedAt": null, "url": null, "title": "What can you teach us?", "slug": "what-can-you-teach-us", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:17.351Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QzDfep4hhn2LhjrTG/what-can-you-teach-us", "pageUrlRelative": "/posts/QzDfep4hhn2LhjrTG/what-can-you-teach-us", "linkUrl": "https://www.lesswrong.com/posts/QzDfep4hhn2LhjrTG/what-can-you-teach-us", "postedAtFormatted": "Saturday, May 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20can%20you%20teach%20us%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20can%20you%20teach%20us%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQzDfep4hhn2LhjrTG%2Fwhat-can-you-teach-us%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20can%20you%20teach%20us%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQzDfep4hhn2LhjrTG%2Fwhat-can-you-teach-us", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQzDfep4hhn2LhjrTG%2Fwhat-can-you-teach-us", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<p>In a <a href=\"/r/discussion/lw/5ro/what_bothers_you_about_less_wrong/47iv\">recent thread</a>, SarahC said:</p>\n<blockquote>\n<p>I'd prefer more posts that aim to teach something the author knows a lot about, as opposed to an insight somebody just thought of. Even something less immediately related to rationality -- I'd love, say, posts on science, or how-to posts, at the epistemic standard of LessWrong.</p>\n</blockquote>\n<p>... so here's the place to float ideas around: is there an area you know a lot about? A topic you've been considering writing about? Here's the place to mention it!</p>\n<p>From <a href=\"/lw/3ag/what_topics_would_you_like_to_see_more_of_on/\">a poll on what people want to see more of</a>, the most votes went to:</p>\n<ul>\n<li>Statistics</li>\n<li>Game Theory</li>\n<li>Direct advice for young people</li>\n<li>General cognitive enhancing tools (such as Adderall and N-Back)</li>\n<li>Information Theory</li>\n<li>Economics</li>\n</ul>\n<p>Some that got less votes:</p>\n<ul>\n<li>Data visualization</li>\n<li> (Defence against the) Dark Arts</li>\n<li>Moral Philosophy (looks like that's being done already)</li>\n<li>Postmodernism</li>\n<li>Getting along in an irrational world</li>\n<li>Existential risks</li>\n<li>Medicine, Applied Human Biology</li>\n</ul>\n<p>... but there are certainly many more things that would be interesting and useful to the community. So what can <em>you</em> teach us?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QzDfep4hhn2LhjrTG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 7.203922111814922e-07, "legacy": true, "legacyId": "7692", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bEEMXkZEkYRBEMXdr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-28T11:17:17.349Z", "modifiedAt": null, "url": null, "title": "Rationality case study: How to evaluate untested medical procedures?", "slug": "rationality-case-study-how-to-evaluate-untested-medical", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.505Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4z2SRmvs7jiggfiK9/rationality-case-study-how-to-evaluate-untested-medical", "pageUrlRelative": "/posts/4z2SRmvs7jiggfiK9/rationality-case-study-how-to-evaluate-untested-medical", "linkUrl": "https://www.lesswrong.com/posts/4z2SRmvs7jiggfiK9/rationality-case-study-how-to-evaluate-untested-medical", "postedAtFormatted": "Saturday, May 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20case%20study%3A%20How%20to%20evaluate%20untested%20medical%20procedures%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20case%20study%3A%20How%20to%20evaluate%20untested%20medical%20procedures%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4z2SRmvs7jiggfiK9%2Frationality-case-study-how-to-evaluate-untested-medical%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20case%20study%3A%20How%20to%20evaluate%20untested%20medical%20procedures%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4z2SRmvs7jiggfiK9%2Frationality-case-study-how-to-evaluate-untested-medical", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4z2SRmvs7jiggfiK9%2Frationality-case-study-how-to-evaluate-untested-medical", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>I'm presently on a flight to the rationality minicamp (hooray for free in-flight wifi!), and the passenger next to me has an interesting story to tell. He suffers from chronic renal failure, which has recently worsened, and is on a flight to Beijing to have an experimental stem-cell based treatment performed at Shijiazhuang Kidney Disease Hospital. His United States doctor, predictably, thinks this idea is crazy; the alternatives would be transplantation or dialysis, neither of which seems appealing. He's not particularly clear on the details, and isn't savvy enough to productively research the issue himself or to generate outcome probabilities.&nbsp;My first reflex, upon hearing this story, was to jump on the internet and spend an hour on PubMed.</p>\n<p>There are two interesting questions to consider here. The first is: is getting the experimental treatment a good idea or not? And the second is: is attempting to arrive at an answer to the first question a good idea or not? He is already sufficiently committed that a \"no\" answer would almost certainly be ignored, unless it had an extremely compelling justification behind it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4z2SRmvs7jiggfiK9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 7.20415767559712e-07, "legacy": true, "legacyId": "7693", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-28T16:07:39.641Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Marginally Zero-Sum Efforts", "slug": "seq-rerun-marginally-zero-sum-efforts", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uMTGmbYfNeGxLonJQ/seq-rerun-marginally-zero-sum-efforts", "pageUrlRelative": "/posts/uMTGmbYfNeGxLonJQ/seq-rerun-marginally-zero-sum-efforts", "linkUrl": "https://www.lesswrong.com/posts/uMTGmbYfNeGxLonJQ/seq-rerun-marginally-zero-sum-efforts", "postedAtFormatted": "Saturday, May 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Marginally%20Zero-Sum%20Efforts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Marginally%20Zero-Sum%20Efforts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMTGmbYfNeGxLonJQ%2Fseq-rerun-marginally-zero-sum-efforts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Marginally%20Zero-Sum%20Efforts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMTGmbYfNeGxLonJQ%2Fseq-rerun-marginally-zero-sum-efforts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMTGmbYfNeGxLonJQ%2Fseq-rerun-marginally-zero-sum-efforts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>Today's post, <a href=\"/lw/hj/marginally_zerosum_efforts/\">Marginally Zero-Sum Efforts</a> was originally published on April 11, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>After a point, labeling a problem as \"important\" is a commons problem. Rather than increasing the total resources devoted to important problems, resources are taken from other projects. Some grants proposals need to be written, but eventually this process becomes zero- or negative-sum on the margin.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5x7/seq_rerun_futuristic_predictions_as_consumable/\">Futuristic Predictions as Consumable Goods</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uMTGmbYfNeGxLonJQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 7.20501163823357e-07, "legacy": true, "legacyId": "7694", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hcKCrYTW7Zbzmv29g", "4oN3hLPfCTffDQ4bs", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-28T22:24:25.727Z", "modifiedAt": null, "url": null, "title": "Sequence translations: Seeking feedback/collaboration", "slug": "sequence-translations-seeking-feedback-collaboration", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:59.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o8vCJ86ExLaWjFAZf/sequence-translations-seeking-feedback-collaboration", "pageUrlRelative": "/posts/o8vCJ86ExLaWjFAZf/sequence-translations-seeking-feedback-collaboration", "linkUrl": "https://www.lesswrong.com/posts/o8vCJ86ExLaWjFAZf/sequence-translations-seeking-feedback-collaboration", "postedAtFormatted": "Saturday, May 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sequence%20translations%3A%20Seeking%20feedback%2Fcollaboration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASequence%20translations%3A%20Seeking%20feedback%2Fcollaboration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8vCJ86ExLaWjFAZf%2Fsequence-translations-seeking-feedback-collaboration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sequence%20translations%3A%20Seeking%20feedback%2Fcollaboration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8vCJ86ExLaWjFAZf%2Fsequence-translations-seeking-feedback-collaboration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8vCJ86ExLaWjFAZf%2Fsequence-translations-seeking-feedback-collaboration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 227, "htmlBody": "<p>(...and also <em>permission</em>&nbsp;from Eliezer, who has the right to veto this whole idea should he so desire.)&nbsp;</p>\n<p>In the hope of launching a collaborative project, I've set up Wordpress sites for translations of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a> into <a href=\"http://rationalite.wordpress.com\">French</a>, <a href=\"http://xrazionalita.wordpress.com\">Italian</a>, and <a href=\"http://xracionalidad.wordpress.com\">Spanish</a>; and to get things started, I've put up <a href=\"http://rationalite.wordpress.com/2011/05/28/001-lart-martial-de-la-rationalite/\">my</a> <a href=\"http://xrazionalita.wordpress.com/2011/05/28/001-l%E2%80%99arte-marziale-della-razionalita/\">own</a> <a href=\"http://xracionalidad.wordpress.com/2011/05/28/el-arte-marcial-de-le-racionalidad/\">attempts</a> at translating the first post, <a href=\"/lw/gn/the_martial_art_of_rationality/\">The Martial Art of Rationality</a>.&nbsp;</p>\n<p>I'm looking for collaborators. At the very least, I'm hoping to find folks who would be willing to proofread my translations and help improve them -- in particular to help me purge them of the inevitable mistakes, non-native shibboleths, and typos with which they're bound to be infused. Beyond that, it would of course be nice to have people contribute translations of their own of entire posts. (I'll keep plugging away myself no doubt, but if I'm the only one, it will take a very long time to complete.) &nbsp;</p>\n<p>If you'd like to translate articles yourself into one of these languages, send me a PM so I can add you as an author on the appropriate blog(s). Proofreading my work can be done either privately or in comments here on LW; I'd probably prefer to reserve the comments sections of the actual sites for discussion of the posts' content (in the target languages), although this isn't an inflexible demand.&nbsp;</p>\n<p>Also, one would like to add more languages, of course...&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o8vCJ86ExLaWjFAZf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 7.206119954158034e-07, "legacy": true, "legacyId": "7695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["teaxCFgtmCQ3E9fy8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-28T23:41:49.958Z", "modifiedAt": null, "url": null, "title": "\"Thank you for updating\"", "slug": "thank-you-for-updating", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:01.399Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rysade", "createdAt": "2010-10-23T05:51:26.346Z", "isAdmin": false, "displayName": "rysade"}, "userId": "ZPxsNDAnPrQfo2yx2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S4BREsumkaRHSwWCD/thank-you-for-updating", "pageUrlRelative": "/posts/S4BREsumkaRHSwWCD/thank-you-for-updating", "linkUrl": "https://www.lesswrong.com/posts/S4BREsumkaRHSwWCD/thank-you-for-updating", "postedAtFormatted": "Saturday, May 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Thank%20you%20for%20updating%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Thank%20you%20for%20updating%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS4BREsumkaRHSwWCD%2Fthank-you-for-updating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Thank%20you%20for%20updating%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS4BREsumkaRHSwWCD%2Fthank-you-for-updating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS4BREsumkaRHSwWCD%2Fthank-you-for-updating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<p>Would it be supercilious to thank someone for updating?&nbsp; I know I would feel uncomfortable doing it, but I often feel the urge to do so anyway.</p>\n<p>There seems to be something vicious about thanking them.&nbsp; My own estimation of my own belief has not changed in these situations.&nbsp; I feel fairly satisfied that the other has considered my view and has shakily come to agree with it. I worry it would be a little like saying 'screw your opinion, now you see I'm right.'</p>\n<p>However:</p>\n<p>It is both rational and polite to thank them.&nbsp; The gentlemen's agreement of rationality allows for one person to be wrong and not lose face at all.&nbsp; When someone concedes something to me (a point) I typically feel the need to thank them.</p>\n<p>What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S4BREsumkaRHSwWCD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 9, "extendedScore": null, "score": 7.206347685603291e-07, "legacy": true, "legacyId": "7696", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-29T06:04:47.121Z", "modifiedAt": null, "url": null, "title": "A discussion of an applictation of Bayes' theorem to everyday life", "slug": "a-discussion-of-an-applictation-of-bayes-theorem-to-everyday", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:08.596Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AdeleneDawner", "createdAt": "2009-04-28T14:40:00.131Z", "isAdmin": false, "displayName": "AdeleneDawner"}, "userId": "MeSREm4SMRGxeQ8X3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QLdPsErm75LAapMpi/a-discussion-of-an-applictation-of-bayes-theorem-to-everyday", "pageUrlRelative": "/posts/QLdPsErm75LAapMpi/a-discussion-of-an-applictation-of-bayes-theorem-to-everyday", "linkUrl": "https://www.lesswrong.com/posts/QLdPsErm75LAapMpi/a-discussion-of-an-applictation-of-bayes-theorem-to-everyday", "postedAtFormatted": "Sunday, May 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20discussion%20of%20an%20applictation%20of%20Bayes'%20theorem%20to%20everyday%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20discussion%20of%20an%20applictation%20of%20Bayes'%20theorem%20to%20everyday%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQLdPsErm75LAapMpi%2Fa-discussion-of-an-applictation-of-bayes-theorem-to-everyday%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20discussion%20of%20an%20applictation%20of%20Bayes'%20theorem%20to%20everyday%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQLdPsErm75LAapMpi%2Fa-discussion-of-an-applictation-of-bayes-theorem-to-everyday", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQLdPsErm75LAapMpi%2Fa-discussion-of-an-applictation-of-bayes-theorem-to-everyday", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1070, "htmlBody": "<p>[12:49:29 AM] Conversational Partner: actually, even if the praise is honest, it makes me uncomfortable if it seems excessive.&nbsp; that is, repeated too often, or made a big deal about.</p>\n<p>[12:49:58 AM] Adelene Dawner: 'Seems excessive' can actually be a cue for 'is insincere'.</p>\n<p>[12:50:05 AM] Conversational Partner: oh</p>\n<p>[12:50:25 AM] Adelene Dawner: That kind of praise tends to parse to me as someone trying to push my buttons.</p>\n<p>[12:51:53 AM | Edited 12:52:09 AM] Conversational Partner: is it at least theoretically possible that the praise is honest, and the other person just happens to think that the thing is more praiseworthy than I do?&nbsp; or if the other person has a different opinion than I do about how much praise is appropriate in general?</p>\n<p>[12:52:59 AM] Adelene Dawner: Of course.</p>\n<p>[12:53:13 AM] Adelene Dawner: This is a situation where looking at Bayes' theorem is useful.</p>\n<p><a id=\"more\"></a>[12:54:03 AM] Conversational Partner: ooh, that might be the first situation I've ever encountered where applying Bayes' theorem directly is useful...&nbsp; or did you not mean applying it directly?</p>\n<p>[12:55:18 AM] Adelene Dawner: You might not be able to apply it directly in terms of plugging in numbers and getting a result, but you can use it in terms of 'these things make the probability that this signal is strong evidence higher, and these things make it lower', which then makes the whole situation make more sense.</p>\n<p>[12:55:50 AM] Conversational Partner: oh, thanks for explaining that</p>\n<p>[12:55:56 AM] Conversational Partner: also, what were you saying before about \"costly signal\"?</p>\n<p>[12:56:16 AM] Adelene Dawner: That actually ties into the Bayes' theorem thing. I'll expand.</p>\n<p>[12:56:52 AM] Adelene Dawner: The relevant thing here is the probability that you did something right given that you were praised for it by a particular person or in a particular way, right?</p>\n<p>[12:57:38 AM] Conversational Partner: yes</p>\n<p>[12:57:44 AM] Conversational Partner: or at least, I think so</p>\n<p>[12:59:02 AM] Adelene Dawner: Okay. So A is 'I did the thing right' and B is 'I was praised for A in a particular way'. Bayes' theorem says that P(A given B) is P(B given A) times P(A) over P(B). P(B given A) and P(A) make it go up, P(B) makes it go down.</p>\n<p>[1:02:29 AM] Adelene Dawner: So, if I praise you in a particular way every time you do a particular thing right, without fail, P(B given A) is 1 and the chance that you did the thing right given that I praised you goes up. If you always do the thing right, P(A) is 1 and the chance that you did it right given that I praised you goes up. If I always praise you no matter whether you've done the thing right or not, P(B) is 1 and the chance that you did it right given that I praised you goes down.</p>\n<p>[1:04:19 AM] Adelene Dawner: (If I always praise you whether you've done the thing right or not, then P(B given A) and P(B) are both 1, and P(A given B) is the same as P(A).)</p>\n<p>[1:04:42 AM] Adelene Dawner: You follow so far?</p>\n<p>[1:04:57 AM] Conversational Partner: I think so, yes</p>\n<p>[1:14:18 AM] Adelene Dawner: Say I have a project where every week on Monday I give you a piece of spec, and on Friday you give me code and I test it to see if it meets that spec. Also on Friday, I roll a 10-sided die. If the code seems to meet the spec, I give you $100 on Saturday. Also, if the die comes up 10, I give you $100 on Saturday. (If the code meets spec and the die comes up 10, I give you $200.) I don't tell you whether the code met spec, though. Also say that you've been testing it yourself, and it seems to you that your code meets spec 20% of the time, and I'm 90% accurate at judging whether code meets spec.<br /><br />A is 'code meets spec', B is 'you get $100'.</p>\n<p>[1:19:05 AM] Adelene Dawner: So, P(A given B) is P(B given A) times P(A) over P(B) - in this case, .9 times .2 over (.1 (dice came up 10) plus .2 (code was right)), or .11 over .3, or .366 [Ed note: I know it wouldn't be exactly .1 + .2]</p>\n<p>[1:23:19 AM] Adelene Dawner: Now, say that I'm having financial trouble, and can't afford to give you $100 for no reason every time a d10 comes up 10. So I switch it for a d20, and give you $100 if the d20 comes up 20. This is the 'costly signaling' scenario, since giving you $100 for the correct code costs me more, too. In this case, the math is .9 times .2 over (.05 plus .2), or .11 over .25, or .44. In this case, if you get $100 on Friday, there's a much higher chance that it's actually because you got the code right.</p>\n<p>[1:24:48 AM] Conversational Partner: yay for Bayes Theorem :)&nbsp; yay for being smart enough to know how to use it :)</p>\n<p>[1:24:57 AM] Adelene Dawner: ^^</p>\n<p>[1:25:19 AM] Adelene Dawner: The trick is, you can do that without pluging numbers in at all.</p>\n<p>[1:25:35 AM] Conversational Partner: yes, you explained how.</p>\n<p>[1:25:37 AM] Conversational Partner: thanks</p>\n<p>[1:25:38 AM] Adelene Dawner: ^^</p>\n<p>[1:25:42 AM] Conversational Partner: :)</p>\n<p>[1:26:46 AM] Adelene Dawner: So the problem with someone giving you a lot of praise is that it raises P(B), which lowers P(A given B) - it means that any instance of praise is less meaningful.</p>\n<p>[1:28:39 AM] Conversational Partner: sorry, but I still don't see where the \"cost\" comes in for just verbal praise</p>\n<p>[1:29:53 AM] Adelene Dawner: Being seen to praise someone can have social costs. Also paying enough attention to give praise at all is costly, though that affects P(B given A) more than P(B).</p>\n<p>[1:31:14 AM] Conversational Partner: oh.&nbsp; so you weren't implying that praising selectively is more costly than praising unconditionally?</p>\n<p>[1:33:14 AM] Adelene Dawner: It can be, but again that all shows up in P(B given A) and P(B). False negatives lower P(B given A) and false positives raise P(B).</p>\n<p>[1:39:15 AM] Conversational Partner: checking if there are any beliefs I need to update as a result of all this...&nbsp; no, I don't think there are any false beliefs that need to be corrected, but this information is likely to be helpful next time I'm in a situation where I'm trying to decide the appropriate response to praise.&nbsp; it should be more trustworthy, and more satisfying, when P(A given B) is high.&nbsp; thanks :)</p>\n<p>[1:39:35 AM] Adelene Dawner: Yep! ^^</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QLdPsErm75LAapMpi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 7.207474566585813e-07, "legacy": true, "legacyId": "7697", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-29T06:20:02.382Z", "modifiedAt": null, "url": null, "title": "Calculus textbook recommendation?", "slug": "calculus-textbook-recommendation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:17.126Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rMaKrW5wKwRfW9afw/calculus-textbook-recommendation", "pageUrlRelative": "/posts/rMaKrW5wKwRfW9afw/calculus-textbook-recommendation", "linkUrl": "https://www.lesswrong.com/posts/rMaKrW5wKwRfW9afw/calculus-textbook-recommendation", "postedAtFormatted": "Sunday, May 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Calculus%20textbook%20recommendation%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACalculus%20textbook%20recommendation%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrMaKrW5wKwRfW9afw%2Fcalculus-textbook-recommendation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Calculus%20textbook%20recommendation%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrMaKrW5wKwRfW9afw%2Fcalculus-textbook-recommendation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrMaKrW5wKwRfW9afw%2Fcalculus-textbook-recommendation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<p>I'm seeking suggestions for a calculus textbook that I could use to teach myself the subject.</p>\n<p>Details:</p>\n<p>[Personal details removed.] I know that most calculus textbooks are designed to be taught to classes, so I was wondering if anyone knew of a textbook specifically designed for autodidacts, or one that would be particularly useful for the purpose. (If you just know of a good general textbook, I'd be grateful to hear that as well.)</p>\n<p>Thanks to anyone who gives a suggestion.</p>\n<p>EDIT: Chose a Marvin-Gardner-edited version of Calculus Made Easy, accompanied by Khan Academy lessons.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rMaKrW5wKwRfW9afw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 7.20751946052151e-07, "legacy": true, "legacyId": "7698", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-29T07:33:36.218Z", "modifiedAt": null, "url": null, "title": "Advice request: Buying a car", "slug": "advice-request-buying-a-car", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.048Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hMnFxTwYHNRz8xKDM/advice-request-buying-a-car", "pageUrlRelative": "/posts/hMnFxTwYHNRz8xKDM/advice-request-buying-a-car", "linkUrl": "https://www.lesswrong.com/posts/hMnFxTwYHNRz8xKDM/advice-request-buying-a-car", "postedAtFormatted": "Sunday, May 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20request%3A%20Buying%20a%20car&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20request%3A%20Buying%20a%20car%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMnFxTwYHNRz8xKDM%2Fadvice-request-buying-a-car%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20request%3A%20Buying%20a%20car%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMnFxTwYHNRz8xKDM%2Fadvice-request-buying-a-car", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMnFxTwYHNRz8xKDM%2Fadvice-request-buying-a-car", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 293, "htmlBody": "<p>So I'm looking at buying a car. At the moment I am using my parents' old car, however when I move out of home I will not be able to take it with me, and it also lacks some of the features I would like (cruise control in particular).</p>\n<p>I'm looking at buying a small car, probably a hatchback. At the moment I have saved around AU$12,000. My parents are willing to lend me some amount, probably up to $5000, and I work for a bank which can give me a loan at a very favourable staff rate. I earn a bit over $325 per week at the moment and I have few living expenses beyond luxuries, that income is bolstered semifrequently when I can work extra shifts. At the beginning of 2013 I will be seeking a full time job as a high school teacher.</p>\n<p>Looking at a few car websites (in particular carsales.com.au) it appears I can buy a fairly good second hand car for around $15,000, or a brand new car that seems quite good for around $24,000. This is a <em>very</em> rough guide to local prices.</p>\n<p>I am aware that my decision process in this judgement is very fallible because I don't know much about buying cars, e.g. the potential pitfalls of a second hand one, and any other things I need to take into account.</p>\n<p>Being as this is by far the most money I will ever have spent on a single thing and it is likely to last me for most of a decade at least, I am strongly motivated to make this decision rationally.</p>\n<p>Does anybody have any advice they can give on how I can decide how much money to spend, or things I should consider when comparing potential cars?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hMnFxTwYHNRz8xKDM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 1, "extendedScore": null, "score": 7.20773596853601e-07, "legacy": true, "legacyId": "7699", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-29T09:34:17.739Z", "modifiedAt": null, "url": null, "title": "Training for math olympiads", "slug": "training-for-math-olympiads", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:17.599Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Malik", "createdAt": "2011-01-05T12:45:17.182Z", "isAdmin": false, "displayName": "D_Malik"}, "userId": "9dhw3PngyAWKqTymS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t5abtwcfEb7APnfTY/training-for-math-olympiads", "pageUrlRelative": "/posts/t5abtwcfEb7APnfTY/training-for-math-olympiads", "linkUrl": "https://www.lesswrong.com/posts/t5abtwcfEb7APnfTY/training-for-math-olympiads", "postedAtFormatted": "Sunday, May 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Training%20for%20math%20olympiads&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATraining%20for%20math%20olympiads%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft5abtwcfEb7APnfTY%2Ftraining-for-math-olympiads%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Training%20for%20math%20olympiads%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft5abtwcfEb7APnfTY%2Ftraining-for-math-olympiads", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft5abtwcfEb7APnfTY%2Ftraining-for-math-olympiads", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 285, "htmlBody": "<p>Lately I've resolved to try harder at teaching myself math so I have a better shot at the international olympiad (IMO). These basically involve getting, say, three really hard math <a href=\"http://www.imo-official.org/problems.aspx\">problems</a> and trying your best to solve them within 5 hours.</p>\n<p>My current state:</p>\n<ul>\n<li>I have worked through a general math problem-solving guide (Art and Craft of Problem-Solving), a general math olympiad guide (A Primer for Mathematics Competitions) and practice problems.</li>\n<li>I've added all problems and solutions and theorems and techniques into an Anki deck. When reviewing, I do not re-solve the problem, I only try to remember any key insights and outline the solution method.</li>\n<li>I am doing <a href=\"http://www.gwern.net/N-back%20FAQ\">n-back</a>, ~20 sessions (1 hour) daily, in an attempt to increase my general intelligence (my IQ is ~125, sd 15).</li>\n<li>I am working almost permanently; akrasia is not much of a problem.</li>\n<li>I am not _yet_ at the level of IMO medallists.</li>\n</ul>\n<p>What does the intrumental-rationality skill of LWers have to say about this? What recommendations do you guys have for improving problem-solving ability, in general and specifically for olympiad-type environments? Specifically,</p>\n<ul>\n<li>How should I spread my time between n-backing, solving problems, and learning more potentially-useful math?</li>\n<li>Should I take any nootropics? I am currently looking to procure some fish oil (I don't consume any normally) and perhaps a racetam. I have been experimenting with cycling caffeine weekends on, weekdays off (to prevent tolerance being developed), with moderate success (Monday withdrawal really sucks, but Saturday is awesome).</li>\n<li>Should I add the problems to Anki? It takes time to create the cards and review them; is that time better spent doing more problems?</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t5abtwcfEb7APnfTY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 7.208091205613966e-07, "legacy": true, "legacyId": "7700", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-29T10:57:45.362Z", "modifiedAt": null, "url": null, "title": "Overcoming suffering: Emotional acceptance", "slug": "overcoming-suffering-emotional-acceptance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.394Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tNnhxNYcXYdJYtQRh/overcoming-suffering-emotional-acceptance", "pageUrlRelative": "/posts/tNnhxNYcXYdJYtQRh/overcoming-suffering-emotional-acceptance", "linkUrl": "https://www.lesswrong.com/posts/tNnhxNYcXYdJYtQRh/overcoming-suffering-emotional-acceptance", "postedAtFormatted": "Sunday, May 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Overcoming%20suffering%3A%20Emotional%20acceptance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOvercoming%20suffering%3A%20Emotional%20acceptance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtNnhxNYcXYdJYtQRh%2Fovercoming-suffering-emotional-acceptance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Overcoming%20suffering%3A%20Emotional%20acceptance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtNnhxNYcXYdJYtQRh%2Fovercoming-suffering-emotional-acceptance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtNnhxNYcXYdJYtQRh%2Fovercoming-suffering-emotional-acceptance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1749, "htmlBody": "<p><strong>Follow-up to:</strong> <a href=\"/lw/5r9/suffering_as_attentionallocational_conflict/\">Suffering as attention-allocational conflict</a>.</p>\r\n<p>In many cases, it may be possible to end an attention-allocational conflict by looking at the content of the conflict and resolving it. However, there are also many cases where this simply won't work. If you're afraid of public speaking, say, the \"I don't want to do this\" signal is going to keep repeating itself regardless of how you try to resolve the conflict. Instead, you have to treat the conflict in a non-content-focused way.</p>\r\n<p>In a nutshell, this is just the map-territory distinction as applied to emotions. Your emotions have evolved as a feedback and attention control mechanism: their purpose is to modify your behavior. If you're afraid of a dog, this is a fact about you, not about the dog. Nothing in the world is inherently scary, bad or good. Furthermore, emotions aren't inherently good or bad either, unless we choose to treat them as such.<br /><br />We all know this, right? But we don't consistently apply it to our thinking of emotions. In particular, this has two major implications:<br /><br /><strong>1. You are not the world</strong><strong>:</strong> It's always alright to feel good. Whether you're feeling good or bad won't change the state of the world: the world is only changed by the actual actions you take. You're never obligated to feel bad, or guilty, or ashamed. In particular, since you can only influence the world through your actions, you will accomplish more and be happier if your emotions are tied to your actions, not states of the world. <br /><strong>2. Emotional acceptance:</strong> At the same time, \"negative\" emotions are <a href=\"http://www.psychologytoday.com/blog/insight-therapy/201009/emotional-acceptance-why-feeling-bad-is-good\">not something to suppress or flinch away from</a>. They're a feedback mechanism which <a href=\"/lw/37z/applied_cognitive_science_learning_from_a_faux_pas/\">imprints lessons</a> directly into your automatic behavior (your <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">elephant</a>). With your subconsciousness having been trained to act better in the future, your conscious mind is free to concentrate on other things. If the feedback system is broken and teaching you bad lessons, then you should act to correct it. But if the pain is about some real mistake or real loss you suffered, then you should welcome it.<br /><br />Internalizing these lessons can have some very powerful effects. I've been making very good progress on consistently feeling better after starting to train myself to think like this. But some LW posters are even farther along; witness Will Ryan:<a id=\"more\"></a></p>\r\n<blockquote>\r\n<p>I internalized a number of different conclusions during this period, although piecing together the exact time frame is somewhat difficult without rereading all of my old notes. The biggest conclusion was probably <strong>acceptance of the world as it is</strong>, or eliminating affective judgments about reality as a whole. I wanted to become an agent who was never harmed by receiving true information. Denying reality does not change it, only decreases our effectiveness in interacting with the world. An important piece of this acceptance is that the past is immutable, I realized that I should <strong>only have <em>prospective</em> emotions</strong>, since they are there to guide our future behavior. [...]<br /><br />More things fell into place in early 2010, during a period in which I was breaking up with Katie at the same time our cat was dying of cancer. I learned to <strong>only have emotions about situations that were within my immediate control</strong> - between calls with the vet making life-or-death decisions about my pet, I was going to parties and incredibly enjoying myself. This immediately eliminated chronic stress of any kind, which has been greatly beneficial for my overall happiness and effectiveness. I felt alive in a way that I hadn't experienced before, living every moment with its own intensity. I don't (yet) experience this state constantly, but it does seem to happen much more frequently than it used to. This moment-intensity also induces incredible subjective time dilation, which I appreciate quite a bit.<br /><br />I am not sure exactly when, but sometime during this period I began to develop sadness asymbolia - <strong>sadness lost its negative affect</strong>, and so I no longer avoided experiencing it. I came to the realization that sadness was precisely the right emotion I needed to internalize negative updates! Being able to internalize bad news about the world without fear or suffering is one of my biggest hacks to date, as far as I am concerned. I think this was related to internalizing the general idea of <strong>emotions as feedback</strong>, instead of some kind of intrinsic truth about the world.<br /><br />In April 2010 I came to the realization that my systematic avoidance of certain things was in fact the emotion of fear. It seems obvious when stated this way in retrospect, but for most of my life I had prioritized thought over emotion, and at that time did not have particularly good access to my emotional state. Once I came to this realization, I also realized that <strong>my fear pointed towards my biggest areas of potential growth</strong>. Although I have not yet developed fear asymbolia, I have developed a habit of directing myself straight towards my biggest fears as soon as I recognize them. [...]</p>\r\n<p>...my subjective experience of the pain-sensation did not seem to change much, what changed was a mental aversion to the stimulus.</p>\r\n<p>Sadness... oh such sweet sadness! My enjoyment of all emotions scales with its intensity, so I actively try to cultivate more sadness when it occurs. I long for the feeling of warm tears rolling down my cheeks, my breath and body racked with sobbing... Emotional pain now feels euphorically pleasurable to release, and in its aftermath I am left with warmth and contentment. It is almost as though the pain realizes I have incorporated its lessons through my acknowledgment and expression, and then no longer demands my attention. The sensation itself is difficult to describe... it is definitely painful, but in no way aversive.</p>\r\n</blockquote>\r\n<p>Some other LW posters who've made considerable progress on this are Jasen Murray, Frank Adamek and Michael Vassar. I invite them to post their experiences in this thread, and in future posts of their own.<br /><br />How does one actually achieve emotional acceptance? It is a way of thought that has to be learned with practice. There are various techniques which help in this: I will cover one in this post, and others in future ones.<br /><br /><strong><span style=\"text-decoration: underline;\">Mindfulness practice</span></strong></p>\r\n<blockquote>\r\n<p>\"Many [mindfulness exercises] encourage individuals to attend to the internal experiences occurring in each moment, such as bodily sensations, thoughts, and emotions. Others encourage attention to aspects of the environment, such as sights and sounds ... All suggest that mindfulness should be practiced with an attitude of nonjudgmental acceptance. That is, phenomena that enter the individual&rsquo;s awareness during mindfulness practice, such as perceptions, cognitions, emotions, or sensations, are observed carefully but are not evaluated as good or bad, true or false, healthy or sick, or important or trivial ... Thus, mindfulness is the nonjudgmental observation of the ongoing stream of internal and external stimuli as they arise.\" -- <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.168.5601&amp;rep=rep1&amp;type=pdf\">Mindfulness Training as a Clinical Intervention: A Conceptual and Empirical Review</a> (R.A. Baer 2003, in <em>Clinical psychology: Science and practice</em>).</p>\r\n</blockquote>\r\n<p>Mindfulness techniques are very useful in realizing that your thoughts and emotions are just things constructed by your mind:</p>\r\n<blockquote>\r\n<p>\"Several authors have noted that the practice of mindfulness may lead to changes in thought patterns, or in attitudes about one&rsquo;s thoughts. For example, Kabat-Zinn (1982, 1990) suggests that nonjudgmental observation of pain and anxiety-related thoughts may lead to the understanding that they are &ldquo;just thoughts,&rdquo; rather than reflections of truth or reality, and do not necessitate escape or avoidance behavior. Similarly, Linehan (1993a, 1993b) notes that observing one&rsquo;s thoughts and feelings and applying descriptive labels to them encourages the understanding that they are not always accurate reflections of reality. For example, feeling afraid does not necessarily mean that danger is imminent, and thinking &ldquo;I am a failure&rdquo; does not make it true. Kristeller and Hallett (1999), in a study of MBSR in patients with binge eating disorder, cite Heatherton and Baumeister&rsquo;s (1991) theory of binge eating as an escape from self-awareness and suggest that mindfulness training might develop nonjudgmental acceptance of the aversive cognitions that binge-eaters are thought to be avoiding, such as unfavorable comparisons of self to others and perceived inability to meet others&rsquo; demands.\"<br /><br />\"All of the treatment programs reviewed here include acceptance of pain, thoughts, feelings, urges, or other bodily, cognitive, and emotional phenomena, without trying to change, escape, or avoid them. Kabat-Zinn (1990) describes acceptance as one of several foundations of mindfulness practice. DBT provides explicit training in several mindfulness techniques designed to promote acceptance of reality. Thus, it appears that mindfulness training may provide a method for teaching acceptance skills.\"</p>\r\n</blockquote>\r\n<p>It also has clear promise in reducing suffering:</p>\r\n<blockquote>\r\n<p>\"According to Salmon, Santorelli, and Kabat-Zinn (1998), over 240 hospitals and clinics in the United States and abroad were offering stress reduction programs based on mindfulness training as of 1997. ... The empirical literature on the effects of mindfulness training contains many methodological weaknesses, but it suggests that mindfulness interventions may lead to reductions in a variety of problematic conditions, including pain, stress, anxiety, depressive relapse, and disordered eating.\"</p>\r\n</blockquote>\r\n<p>I recommend the linked paper for a good survey about various therapies utilizing mindfulness, their effects and theoretical explanations for how they work.<br /><br />While I haven't personally looked at any of the referenced therapies, I've found great benefit from the simple practice of turning my attention to any source of physical or emotional discomfort and simply nonjudgementally observing it. Frequently, this changes the pain from something that feels negative to something that feels neutral. My hypothesis is that this eliminates an <a href=\"/lw/5r9/suffering_as_attentionallocational_conflict/\">attention-allocational conflict</a>. The pain acts as a signal to concentrate on and pay attention to this source of discomfort, and once I do so, the signal has accomplished its purpose.<br /><br />However, often I can do even better than just making the sensation neutral. If I make a <em>conscious decision </em>to experience this now-neutral sensation as something actively positive, <em>that often works</em>. Obviously, there are limits to the degree to which I can do this - the stronger the discomfort, the harder it is to just passively observe it and experience it as neutral. So far my accomplishments have been relatively mild, such as carrying several heavy bags and changing it from something uncomfortable to something enjoyable. But I keep becoming better at it with practice.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LaDu5bKDpe8LxaR7C": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tNnhxNYcXYdJYtQRh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 52, "extendedScore": null, "score": 9.7e-05, "legacy": true, "legacyId": "7701", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9XBhs3dS4XnBwZRan", "Ti2SW9GoZLCq36zEz", "du395YvCnQXBPSJax"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-29T15:36:07.552Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Priors as Mathematical Objects", "slug": "seq-rerun-priors-as-mathematical-objects", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tyrrell_McAllister", "createdAt": "2009-03-05T19:59:57.157Z", "isAdmin": false, "displayName": "Tyrrell_McAllister"}, "userId": "HSANMQBsHiGrZzwTB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uxfGirs9njFB9dG5b/seq-rerun-priors-as-mathematical-objects", "pageUrlRelative": "/posts/uxfGirs9njFB9dG5b/seq-rerun-priors-as-mathematical-objects", "linkUrl": "https://www.lesswrong.com/posts/uxfGirs9njFB9dG5b/seq-rerun-priors-as-mathematical-objects", "postedAtFormatted": "Sunday, May 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Priors%20as%20Mathematical%20Objects&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Priors%20as%20Mathematical%20Objects%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuxfGirs9njFB9dG5b%2Fseq-rerun-priors-as-mathematical-objects%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Priors%20as%20Mathematical%20Objects%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuxfGirs9njFB9dG5b%2Fseq-rerun-priors-as-mathematical-objects", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuxfGirs9njFB9dG5b%2Fseq-rerun-priors-as-mathematical-objects", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 244, "htmlBody": "<p>Today's post, <a href=\"/lw/hk/priors_as_mathematical_objects/\">Priors as Mathematical Objects</a>, was originally published on 12 April 2007. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>As a mathematical object, a Bayesian \"prior\" is a probability distribution over sequences of observations. That is, the prior assigns a probability to every possible sequence of observations. In principle, you could then use the prior to compute the probability of any event by summing the probabilities of all observation-sequences in which that event occurs. Formally, the prior is just a giant look-up table. However, an actual Bayesian reasoner wouldn't literally implement a giant look-up table. Nonetheless, the formal definition of a prior is sometimes convenient. For example, if you are uncertain about which distribution to use, you can just use a weighted sum of distributions, which directly gives another distribution.</blockquote>\n<p>Discuss the post here (rather than in the comments to the original post).</p>\n<p><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/hj/marginally_zerosum_efforts/\">Marginally Zero-Sum Efforts</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.</em></p>\n<p><em>Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uxfGirs9njFB9dG5b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 7.209156378786987e-07, "legacy": true, "legacyId": "7703", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jzf4Rcienrm6btRyt", "hcKCrYTW7Zbzmv29g", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-29T23:50:28.495Z", "modifiedAt": null, "url": null, "title": "Nature: Red, in Truth and Qualia", "slug": "nature-red-in-truth-and-qualia", "viewCount": null, "lastCommentedAt": "2022-01-30T12:25:28.428Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gyz2MsHM9GKxX62hj/nature-red-in-truth-and-qualia", "pageUrlRelative": "/posts/gyz2MsHM9GKxX62hj/nature-red-in-truth-and-qualia", "linkUrl": "https://www.lesswrong.com/posts/gyz2MsHM9GKxX62hj/nature-red-in-truth-and-qualia", "postedAtFormatted": "Sunday, May 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nature%3A%20Red%2C%20in%20Truth%20and%20Qualia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANature%3A%20Red%2C%20in%20Truth%20and%20Qualia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgyz2MsHM9GKxX62hj%2Fnature-red-in-truth-and-qualia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nature%3A%20Red%2C%20in%20Truth%20and%20Qualia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgyz2MsHM9GKxX62hj%2Fnature-red-in-truth-and-qualia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgyz2MsHM9GKxX62hj%2Fnature-red-in-truth-and-qualia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1682, "htmlBody": "<p><strong>Previously:</strong> <a href=\"/lw/5n9/qualia_a_new_hope/\">Seeing Red: Dissolving Mary's Room and Qualia</a>, <a href=\"/lw/5op/qualia_strike_back/\">A Study of Scarlet: The Conscious Mental Graph<br /></a></p>\n<p>When we left off, we'd introduced a hypothetical organism called Martha whose actions are directed by a mobile graph of simple mental agents. The tip of the iceberg, consisting of the agents that are connected to Martha's language centers, we called the conscious subgraph. Now we're going to place Martha into a situation like Mary's Room: we'll say that a large unconscious agent of hers (like color vision) has never been active, we'll grant her an excellent conscious understanding of that agent, and then we'll see what happens when we activate it for the first time.</p>\n<p>But first, there's one more mental agent we need to introduce, one which serves a key purpose in Martha's evolutionary history: a simple agent that <em>identifies learning</em>.</p>\n<p><a id=\"more\"></a>We recall that Martha's species had evolved ever-better fluency with language, and along the way their minds developed a conscious/unconscious distinction, based on the functional difference (in the ancestral environment) between things that might be beneficial to communicate and things that weren't. Any new conscious node or connection has the potential to be communicated, but not all such elements are created equal; some of them (like a key bit of gossip, or a pattern suddenly grasped) have much larger effects on the full graph (and the conscious subgraph) than others.</p>\n<p>Since the minds of Martha's species are similar in structure, it's likely that the new conscious ideas that rearrange Martha's graph might have similar effects on her friends' graphs- and therefore, Martha's species has evolved to give such connections high priority for communication.</p>\n<p>One way to do this is to have a simple mental agent which tells Martha when she's learned something of significance. That is, it becomes active when the conscious subgraph rearranges on a large scale<sup>1</sup>. When this happens, it traces the cascade of changes backward until it finds the most basic new node or connection that started the revolution. When it finds the culprit, it forms the conscious idea \"I just learned X\", where X is the new node or connection; thus, when Martha is thinking or talking about related ideas, the important bit is more likely to occur to her.</p>\n<p>In our toy example from yesterday's post, at the moment that the DREAD VILLAIN reveals himself as the LONE STALWART's father, a number of changes ripple through Martha's graph (once all the subconscious processes that transform patterns of sound into conscious concepts have gone through). The learning agent activates and seeks out the root cause; it finds the new connection that DV = AS, and the attached memory of the DREAD VILLAIN's mechanical bass voice revealing the secret. The learning agent creates a stronger memory of this idea and the moment it occurred, and strengthens the connection from these to the language centers. Then, when Martha retells the story, the nearby agents will trigger this particular memory (and Martha will include it unless she has other reasons not to). Everything functions as it ought.</p>\n<p>So what happens when Martha has a fundamentally new experience?</p>\n<h4>Martha's Room<br /></h4>\n<p>Let's put Martha in the situation of Mary's Room. She has all of the mental equipment for color vision, but has never had it activated by external experience. She has factual knowledge of the correspondence between particular objects and their colors. Her conscious graph has linked the names of cherry red and firetruck red and lime green to their proper RGB triplets, and knows which colors are between which. And she has factual knowledge of the structure of her subconscious visual-processing agents, and her mind in general- that is, her conscious graph contains a subgraph that is a good (though of course not infinitely detailed) model of the full graph, and this subgraph can model quite well what the full graph will do once color vision is activated.</p>\n<p>At last, we introduce color vision for the first time. Vast subconscious agents form new connections to each other, rearranging Martha's mind in myriad ways; as in the third image in the previous post, the connections formed by subconscious agents add to the strength of the conscious connections, altering the conscious graph like a puppeteer moves a puppet. The learning agent activates and then starts looking for the culprit. It zeroes in on the epicenter of change in the conscious subgraph: Martha's representations of color knowledge.</p>\n<p>And there it gets stuck. No new <em>conscious</em> connection formed between, say, \"apple\" and \"red\", and the learning agent is only looking for information in the conscious subgraph. The learning agent goes through a dozen false starts looking for something that isn't there. Martha articulates the thought of having learned something, but has nothing to say about what that new knowledge might be, except that it's something about the colors. This new knowledge is, somehow,<em> ineffable</em> to her.</p>\n<p>However, this is no contradiction to her complete factual knowledge about the brain! Martha's mind has a conscious representation of what will happen to her mental graph once she sees colors, but this doesn't itself rearrange her subconscious visual processing agents, any more than drawing a new bridge on your map of California causes its construction in reality. (The potential for <a href=\"http://wiki.lesswrong.com/wiki/Map_and_territory\">confusion of levels</a>, it seems to me, gives the Mary's Room argument much of its persuasive force.)</p>\n<p>Martha would not be surprised at this new rearrangement (since she could model in advance this snafu with her learning agent), but her mind would nonetheless have the reaction of learning something ineffable. No amount of prior factual knowledge would suffice to prevent the learning agent from malfunctioning in this way. Furthermore, this ineffability has the same boundaries as that we wanted to investigate: when Martha digests a new food, it doesn&rsquo;t strongly affect agents that are part of the conscious subgraph, so the learning-agent isn't activated; when she counts sheep, anything she learns is a trivial development of her conscious knowledge of quantity<sup>2</sup>.</p>\n<p>Since our model&ndash;which seems to be thoroughly reductionistic&ndash;gives rise to this reaction without paradox or mystery, the Mary's Room argument cannot be a logically valid argument against physical reductionism. But there's more to it than that...</p>\n<h3>Martha in the Mirror<br /></h3>\n<p>Now that we've seen how the model gives rise to thoughts and reactions like those of human beings, why not carry the correspondence farther? There's perhaps no need for the <a href=\"http://en.wikipedia.org/wiki/Cartesian_theater\">Cartesian theater</a> at all, if our ineffable qualia can arise from a similar interplay between vast subconscious agents and the conscious graph<sup>3</sup>. In the case of familiar qualia, we might look not at an agent which notices learning, but rather one which traces patterns of activation; as before, our conscious connections aren't enough to cause the pattern of activation by themselves, and another mental agent might well characterize this as ineffability in the same fashion.</p>\n<p>But by focusing on the conscious thoughts of ineffability, are we neglecting the actual essence of the problem? It seems to me that once we remove the seeming paradox, the conclusion that different mental agents feel different to us is hardly a mysterious one. Your mileage may, perhaps, vary.</p>\n<p>This is by no means a full or completed theory, but I find it promising enough as a start on the hard problem&ndash; and I think it might be helpful for those who find subjective experience a stumbling block on the way to reductionism. Thanks to several people on Less Wrong and elsewhere who've given me feedback on early versions, and thanks for all your thoughts now!</p>\n<h3>Addenda:<br /></h3>\n<p>We should also consider the question of what we're actually doing when we <em>think</em> about the thought experiment, since we form our intuitions on Mary's Room well in advance of any experimental test. We're probably using our subconscious mental machinery to simulate what such a person might think and feel by <a href=\"/lw/xs/sympathetic_minds/\">empathetically feeling it ourselves</a>, a process which is extremely useful on an evolutionary level for predicting other people's future actions, and which doesn't restrict itself to simulating only those aspects of psychology which we consciously understand<sup>4</sup>. (One might suspect that this ability to model other minds is the real origin of the <a href=\"http://blogs.discovermagazine.com/gnxp/2010/11/the-inevitable-social-brain/\">recent arms race in brain size among hominids</a>.)</p>\n<p>Our subconscious model includes our conscious subgraph, the effect of a new sense experience, and the agents that recognize learning, and it rightly recognizes the feeling of ineffability that would result. But it's not designed to pass along a conscious understanding of the structure behind this feeling, so it's understandable that we naturally take it as a mysterious property of subjective experience rather than as a hiccup in our cognitive algorithms.</p>\n<p>Finally, how much should we worry about Occam's Razor? Well, while the concepts I've introduced have taken some careful explanations, they're relatively basic entities which we could (if we wanted) program on a computer if we so chose. The phenomenon arises from the interaction of the following entities: a mental graph that rearranges upon new connections, the subgraph connected to language, and the simple learning agent. As far as cognitive science is concerned, we're being quite parsimonious.</p>\n<p><strong>Footnotes:</strong></p>\n<p><strong>1.</strong> I'm positing that it focuses on the conscious subgraph, because there's no benefit to communicate events that only affect the agents that are useless to communicate about.</p>\n<p><strong>2.</strong> Come to think of it, I suppose there should be a qualia response to counting a new order of magnitude- \"Ah! So that's what a crowd of 100,000 looks like.\" But intermediate numbers between known quantities should still be relatively qualia-free. It turns out to be really difficult to construct an example of mental processing that doesn't have a qualia aspect...</p>\n<p><strong>3.</strong> Our conscious/subconscious distinction may be a bit more complex than Martha's, but in the main it seems to correspond well to the boundary between what we evolved to communicate and what we did not.</p>\n<p><strong>4.</strong> Note that this is quite different from the <em>conscious</em> modeling of the human mind we posited for Mary and Martha. It's crucial, from an evolutionary perspective, that our subconscious models of human action aren't handicapped by the paltry state of our conscious notions of psychology. The model needs to <em>actually get the right prediction</em> to be successful.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8e9e8fzXuW5gGBS3F": 2, "XSryTypw5Hszpa4TS": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gyz2MsHM9GKxX62hj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 64, "extendedScore": null, "score": 0.000123, "legacy": true, "legacyId": "7373", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 65, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Previously:</strong> <a href=\"/lw/5n9/qualia_a_new_hope/\">Seeing Red: Dissolving Mary's Room and Qualia</a>, <a href=\"/lw/5op/qualia_strike_back/\">A Study of Scarlet: The Conscious Mental Graph<br></a></p>\n<p>When we left off, we'd introduced a hypothetical organism called Martha whose actions are directed by a mobile graph of simple mental agents. The tip of the iceberg, consisting of the agents that are connected to Martha's language centers, we called the conscious subgraph. Now we're going to place Martha into a situation like Mary's Room: we'll say that a large unconscious agent of hers (like color vision) has never been active, we'll grant her an excellent conscious understanding of that agent, and then we'll see what happens when we activate it for the first time.</p>\n<p>But first, there's one more mental agent we need to introduce, one which serves a key purpose in Martha's evolutionary history: a simple agent that <em>identifies learning</em>.</p>\n<p><a id=\"more\"></a>We recall that Martha's species had evolved ever-better fluency with language, and along the way their minds developed a conscious/unconscious distinction, based on the functional difference (in the ancestral environment) between things that might be beneficial to communicate and things that weren't. Any new conscious node or connection has the potential to be communicated, but not all such elements are created equal; some of them (like a key bit of gossip, or a pattern suddenly grasped) have much larger effects on the full graph (and the conscious subgraph) than others.</p>\n<p>Since the minds of Martha's species are similar in structure, it's likely that the new conscious ideas that rearrange Martha's graph might have similar effects on her friends' graphs- and therefore, Martha's species has evolved to give such connections high priority for communication.</p>\n<p>One way to do this is to have a simple mental agent which tells Martha when she's learned something of significance. That is, it becomes active when the conscious subgraph rearranges on a large scale<sup>1</sup>. When this happens, it traces the cascade of changes backward until it finds the most basic new node or connection that started the revolution. When it finds the culprit, it forms the conscious idea \"I just learned X\", where X is the new node or connection; thus, when Martha is thinking or talking about related ideas, the important bit is more likely to occur to her.</p>\n<p>In our toy example from yesterday's post, at the moment that the DREAD VILLAIN reveals himself as the LONE STALWART's father, a number of changes ripple through Martha's graph (once all the subconscious processes that transform patterns of sound into conscious concepts have gone through). The learning agent activates and seeks out the root cause; it finds the new connection that DV = AS, and the attached memory of the DREAD VILLAIN's mechanical bass voice revealing the secret. The learning agent creates a stronger memory of this idea and the moment it occurred, and strengthens the connection from these to the language centers. Then, when Martha retells the story, the nearby agents will trigger this particular memory (and Martha will include it unless she has other reasons not to). Everything functions as it ought.</p>\n<p>So what happens when Martha has a fundamentally new experience?</p>\n<h4 id=\"Martha_s_Room\">Martha's Room<br></h4>\n<p>Let's put Martha in the situation of Mary's Room. She has all of the mental equipment for color vision, but has never had it activated by external experience. She has factual knowledge of the correspondence between particular objects and their colors. Her conscious graph has linked the names of cherry red and firetruck red and lime green to their proper RGB triplets, and knows which colors are between which. And she has factual knowledge of the structure of her subconscious visual-processing agents, and her mind in general- that is, her conscious graph contains a subgraph that is a good (though of course not infinitely detailed) model of the full graph, and this subgraph can model quite well what the full graph will do once color vision is activated.</p>\n<p>At last, we introduce color vision for the first time. Vast subconscious agents form new connections to each other, rearranging Martha's mind in myriad ways; as in the third image in the previous post, the connections formed by subconscious agents add to the strength of the conscious connections, altering the conscious graph like a puppeteer moves a puppet. The learning agent activates and then starts looking for the culprit. It zeroes in on the epicenter of change in the conscious subgraph: Martha's representations of color knowledge.</p>\n<p>And there it gets stuck. No new <em>conscious</em> connection formed between, say, \"apple\" and \"red\", and the learning agent is only looking for information in the conscious subgraph. The learning agent goes through a dozen false starts looking for something that isn't there. Martha articulates the thought of having learned something, but has nothing to say about what that new knowledge might be, except that it's something about the colors. This new knowledge is, somehow,<em> ineffable</em> to her.</p>\n<p>However, this is no contradiction to her complete factual knowledge about the brain! Martha's mind has a conscious representation of what will happen to her mental graph once she sees colors, but this doesn't itself rearrange her subconscious visual processing agents, any more than drawing a new bridge on your map of California causes its construction in reality. (The potential for <a href=\"http://wiki.lesswrong.com/wiki/Map_and_territory\">confusion of levels</a>, it seems to me, gives the Mary's Room argument much of its persuasive force.)</p>\n<p>Martha would not be surprised at this new rearrangement (since she could model in advance this snafu with her learning agent), but her mind would nonetheless have the reaction of learning something ineffable. No amount of prior factual knowledge would suffice to prevent the learning agent from malfunctioning in this way. Furthermore, this ineffability has the same boundaries as that we wanted to investigate: when Martha digests a new food, it doesn\u2019t strongly affect agents that are part of the conscious subgraph, so the learning-agent isn't activated; when she counts sheep, anything she learns is a trivial development of her conscious knowledge of quantity<sup>2</sup>.</p>\n<p>Since our model\u2013which seems to be thoroughly reductionistic\u2013gives rise to this reaction without paradox or mystery, the Mary's Room argument cannot be a logically valid argument against physical reductionism. But there's more to it than that...</p>\n<h3 id=\"Martha_in_the_Mirror\">Martha in the Mirror<br></h3>\n<p>Now that we've seen how the model gives rise to thoughts and reactions like those of human beings, why not carry the correspondence farther? There's perhaps no need for the <a href=\"http://en.wikipedia.org/wiki/Cartesian_theater\">Cartesian theater</a> at all, if our ineffable qualia can arise from a similar interplay between vast subconscious agents and the conscious graph<sup>3</sup>. In the case of familiar qualia, we might look not at an agent which notices learning, but rather one which traces patterns of activation; as before, our conscious connections aren't enough to cause the pattern of activation by themselves, and another mental agent might well characterize this as ineffability in the same fashion.</p>\n<p>But by focusing on the conscious thoughts of ineffability, are we neglecting the actual essence of the problem? It seems to me that once we remove the seeming paradox, the conclusion that different mental agents feel different to us is hardly a mysterious one. Your mileage may, perhaps, vary.</p>\n<p>This is by no means a full or completed theory, but I find it promising enough as a start on the hard problem\u2013 and I think it might be helpful for those who find subjective experience a stumbling block on the way to reductionism. Thanks to several people on Less Wrong and elsewhere who've given me feedback on early versions, and thanks for all your thoughts now!</p>\n<h3 id=\"Addenda_\">Addenda:<br></h3>\n<p>We should also consider the question of what we're actually doing when we <em>think</em> about the thought experiment, since we form our intuitions on Mary's Room well in advance of any experimental test. We're probably using our subconscious mental machinery to simulate what such a person might think and feel by <a href=\"/lw/xs/sympathetic_minds/\">empathetically feeling it ourselves</a>, a process which is extremely useful on an evolutionary level for predicting other people's future actions, and which doesn't restrict itself to simulating only those aspects of psychology which we consciously understand<sup>4</sup>. (One might suspect that this ability to model other minds is the real origin of the <a href=\"http://blogs.discovermagazine.com/gnxp/2010/11/the-inevitable-social-brain/\">recent arms race in brain size among hominids</a>.)</p>\n<p>Our subconscious model includes our conscious subgraph, the effect of a new sense experience, and the agents that recognize learning, and it rightly recognizes the feeling of ineffability that would result. But it's not designed to pass along a conscious understanding of the structure behind this feeling, so it's understandable that we naturally take it as a mysterious property of subjective experience rather than as a hiccup in our cognitive algorithms.</p>\n<p>Finally, how much should we worry about Occam's Razor? Well, while the concepts I've introduced have taken some careful explanations, they're relatively basic entities which we could (if we wanted) program on a computer if we so chose. The phenomenon arises from the interaction of the following entities: a mental graph that rearranges upon new connections, the subgraph connected to language, and the simple learning agent. As far as cognitive science is concerned, we're being quite parsimonious.</p>\n<p><strong id=\"Footnotes_\">Footnotes:</strong></p>\n<p><strong>1.</strong> I'm positing that it focuses on the conscious subgraph, because there's no benefit to communicate events that only affect the agents that are useless to communicate about.</p>\n<p><strong>2.</strong> Come to think of it, I suppose there should be a qualia response to counting a new order of magnitude- \"Ah! So that's what a crowd of 100,000 looks like.\" But intermediate numbers between known quantities should still be relatively qualia-free. It turns out to be really difficult to construct an example of mental processing that doesn't have a qualia aspect...</p>\n<p><strong>3.</strong> Our conscious/subconscious distinction may be a bit more complex than Martha's, but in the main it seems to correspond well to the boundary between what we evolved to communicate and what we did not.</p>\n<p><strong>4.</strong> Note that this is quite different from the <em>conscious</em> modeling of the human mind we posited for Mary and Martha. It's crucial, from an evolutionary perspective, that our subconscious models of human action aren't handicapped by the paltry state of our conscious notions of psychology. The model needs to <em>actually get the right prediction</em> to be successful.</p>", "sections": [{"title": "Martha's Room", "anchor": "Martha_s_Room", "level": 2}, {"title": "Martha in the Mirror", "anchor": "Martha_in_the_Mirror", "level": 1}, {"title": "Addenda:", "anchor": "Addenda_", "level": 1}, {"title": "Footnotes:", "anchor": "Footnotes_", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "64 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYjyQ839MDsZ6E3L", "pi5DAEZWJK3c9NAhW", "NLMo5FZWFFq652MNe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T00:54:48.882Z", "modifiedAt": null, "url": null, "title": "Raw silicon ore of perfect emptiness", "slug": "raw-silicon-ore-of-perfect-emptiness", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:09.181Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pavitra", "createdAt": "2009-09-22T08:32:44.250Z", "isAdmin": false, "displayName": "Pavitra"}, "userId": "yC2JgX3ENu7mionKh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oAmJS7rq3HzcfiKsa/raw-silicon-ore-of-perfect-emptiness", "pageUrlRelative": "/posts/oAmJS7rq3HzcfiKsa/raw-silicon-ore-of-perfect-emptiness", "linkUrl": "https://www.lesswrong.com/posts/oAmJS7rq3HzcfiKsa/raw-silicon-ore-of-perfect-emptiness", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Raw%20silicon%20ore%20of%20perfect%20emptiness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARaw%20silicon%20ore%20of%20perfect%20emptiness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoAmJS7rq3HzcfiKsa%2Fraw-silicon-ore-of-perfect-emptiness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Raw%20silicon%20ore%20of%20perfect%20emptiness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoAmJS7rq3HzcfiKsa%2Fraw-silicon-ore-of-perfect-emptiness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoAmJS7rq3HzcfiKsa%2Fraw-silicon-ore-of-perfect-emptiness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p>Does building a computer count as <a href=\"/lw/rn/no_universally_compelling_arguments/\">explaining something to a rock</a>?</p>\n<p>&nbsp;</p>\n<p>(If we still had open threads, I would have posted this there. As it is, I figure this is better than not saying anything.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oAmJS7rq3HzcfiKsa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 7.210801621047239e-07, "legacy": true, "legacyId": "7705", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PtoQdG7E8MxYJrigu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T04:01:00.757Z", "modifiedAt": null, "url": null, "title": "A puzzle on the ASVAB", "slug": "a-puzzle-on-the-asvab", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:09.287Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Hul-Gil", "createdAt": "2011-05-02T22:16:10.898Z", "isAdmin": false, "displayName": "Hul-Gil"}, "userId": "a67SuoZfWaXPzjvzy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CrGW8PC49TCjWZ6dh/a-puzzle-on-the-asvab", "pageUrlRelative": "/posts/CrGW8PC49TCjWZ6dh/a-puzzle-on-the-asvab", "linkUrl": "https://www.lesswrong.com/posts/CrGW8PC49TCjWZ6dh/a-puzzle-on-the-asvab", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20puzzle%20on%20the%20ASVAB&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20puzzle%20on%20the%20ASVAB%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCrGW8PC49TCjWZ6dh%2Fa-puzzle-on-the-asvab%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20puzzle%20on%20the%20ASVAB%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCrGW8PC49TCjWZ6dh%2Fa-puzzle-on-the-asvab", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCrGW8PC49TCjWZ6dh%2Fa-puzzle-on-the-asvab", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<p><img src=\"http://img862.imageshack.us/img862/2877/asvabquestioncopy.jpg\" alt=\"\" width=\"396\" height=\"800\" /></p>\n<p>&nbsp;I was linked to this on another forum. No instructions were given, apparently - just this picture. What's the deal?</p>\n<p>It seems to me the answer is clearly C, not A as the test indicates; and the members in the original thread appear to agree. However, attempted justifications of A have been made, none of which are very convincing to me - mainly because if there are no instructions and an obvious answer, there's not really any benefit for them to reward a different interpretation, which would almost certainly involve arbitrary assumptions regarding the rules they <em>really</em> want you to apply.</p>\n<p>&nbsp;Trick questions on exams seem to rely on failure to pay close attention to instructions, or insufficiently rigorously apply rules; when there are no instructions, what justification would anyone have for not choosing the most obvious interpretation? Any could be right!</p>\n<p>What do the geniuses here at MoreRight think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CrGW8PC49TCjWZ6dh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 7.211350086105999e-07, "legacy": true, "legacyId": "7717", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T04:57:29.780Z", "modifiedAt": null, "url": null, "title": "Austin, TX LW Meetup, 6/4 1:30PM", "slug": "austin-tx-lw-meetup-6-4-1-30pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:21.591Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2KXjidpRFNDdoePgw/austin-tx-lw-meetup-6-4-1-30pm", "pageUrlRelative": "/posts/2KXjidpRFNDdoePgw/austin-tx-lw-meetup-6-4-1-30pm", "linkUrl": "https://www.lesswrong.com/posts/2KXjidpRFNDdoePgw/austin-tx-lw-meetup-6-4-1-30pm", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Austin%2C%20TX%20LW%20Meetup%2C%206%2F4%201%3A30PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAustin%2C%20TX%20LW%20Meetup%2C%206%2F4%201%3A30PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2KXjidpRFNDdoePgw%2Faustin-tx-lw-meetup-6-4-1-30pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Austin%2C%20TX%20LW%20Meetup%2C%206%2F4%201%3A30PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2KXjidpRFNDdoePgw%2Faustin-tx-lw-meetup-6-4-1-30pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2KXjidpRFNDdoePgw%2Faustin-tx-lw-meetup-6-4-1-30pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<p>After a brief hiatus, the <a href=\"/lw/5a7/austin_less_wrong_meetup_saturday_april_23rd_1200/\">Austin LW meetup</a> is back together and stepping things up: for the foreseeable future, we plan on meeting every week. The next meeting is:</p>\n<p><strong>Saturday 6/4, 1:30 PM, </strong><span class=\"pp-place-title\"><span><strong><a href=\"http://caffemedici.com/guadalupe.html\">Caff&eacute; Medici</a></strong>; right across the street from UT. Parking is available on the street on in one of the garages around it on the street that runs parallel to Guadalupe.</span></span></p>\n<p><span class=\"pp-place-title\"><span>I plan to be there at least until ~4:20, but we may end up staying longer. (We've had <em>a lot</em> to talk about at past meetups.) Hope to see you there!<br /></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2KXjidpRFNDdoePgw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.211516479862861e-07, "legacy": true, "legacyId": "7718", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["APRJhh2WMjjRdzWma"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T05:09:03.725Z", "modifiedAt": null, "url": null, "title": "A simple counterexample to deBlanc 2007?", "slug": "a-simple-counterexample-to-deblanc-2007", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:23.645Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JyYgTPTrgeYuuNdk7/a-simple-counterexample-to-deblanc-2007", "pageUrlRelative": "/posts/JyYgTPTrgeYuuNdk7/a-simple-counterexample-to-deblanc-2007", "linkUrl": "https://www.lesswrong.com/posts/JyYgTPTrgeYuuNdk7/a-simple-counterexample-to-deblanc-2007", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20simple%20counterexample%20to%20deBlanc%202007%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20simple%20counterexample%20to%20deBlanc%202007%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyYgTPTrgeYuuNdk7%2Fa-simple-counterexample-to-deblanc-2007%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20simple%20counterexample%20to%20deBlanc%202007%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyYgTPTrgeYuuNdk7%2Fa-simple-counterexample-to-deblanc-2007", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyYgTPTrgeYuuNdk7%2Fa-simple-counterexample-to-deblanc-2007", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 926, "htmlBody": "<p>Peter de Blanc submitted a paper to arXiv.org in 2007 called \"<a href=\"http://arxiv.org/abs/0712.4318\">Convergence of Expected Utilities with Algorithmic Probability Distributions</a>.\"&nbsp; It claims to show that a computable utility function can have an expected value only if the utility function is bounded.</p>\n<p>This is important because it implies that, if a utility function is unbounded, it is useless.&nbsp; The purpose of a utility function is to compare possible actions <em>k</em> by choosing the <em>k</em> for which <em>U(k)</em> is maximal.&nbsp; You can't do this if <em>U(k)</em> is undefined for any <em>k</em>, let alone for every <em>k</em>.</p>\n<p>I don't know whether any agent we contemplate can have a truly unbounded utility function, since the universe is finite.&nbsp; (The multiverse, supposing you believe in that, might not be finite; but as the utility function is meant to choose a single universe from the multiverse, I doubt that's relevant.)&nbsp; But it is worth exploring, as computable functions are worth exploring despite not having infinitely long tapes for our Turing machines.&nbsp; I previously objected that the decision process is not computable; but this is not important - we want to know whether the expected value exists, before asking how to compute (or approximate) it.</p>\n<p class=\"title\">The math in the paper was too difficult for me to follow all the way through; so instead, I tried to construct a counterexample.&nbsp; This counterexample does not work; the flaw is explained in one of comments below.&nbsp; Can you find the flaw yourself?&nbsp; This type of error is both subtle and common.&nbsp; (The problem is not that the theorem actually proves that for any unbounded utility function, there is some set of possible worlds for which the expected value does not converge.)<a id=\"more\"></a></p>\n<p class=\"title\">The abstract says:</p>\n<blockquote>\n<p>We consider an agent interacting with an unknown environment. The environment is a function which maps natural numbers to natural numbers; the agent's set of hypotheses about the environment contains all such functions which are computable and compatible with a finite set of known input-output pairs, and the agent assigns a positive probability to each such hypothesis. We do not require that this probability distribution be computable, but it must be bounded below by a positive computable function. The agent has a utility function on outputs from the environment. We show that if this utility function is bounded below in absolute value by an unbounded computable function, then the expected utility of any input is undefined. This implies that a computable utility function will have convergent expected utilities iff that function is bounded.</p>\n</blockquote>\n<p>This is the formalism the paper uses as I understand it:</p>\n<p>The world is described by a function <em>h</em>:N &rarr;N, where N represents the integers, and <em>h </em>is a function from a countable set of possible actions, into a countable set of possible observations.</p>\n<p>H is the set of possible functions <em>h</em>.&nbsp; p:H &rarr;[0 .. 1] is a probability distribution for H.&nbsp; S<sub>I</sub> is a subset of H representing all possible worlds (possible values for <em>h</em>) that are consistent with prior observations, meaning that for f in S<sub>I</sub>, p(f) &gt; 0, and for f not in S<sub>I</sub>, p(f) = 0.</p>\n<p>The agent considers a countable number of possible actions.&nbsp; The expected utility of action <em>k</em> is the sum, over all possible worlds <em>f</em> in S<sub>I</sub>, of p(f)U(f(k)).&nbsp; Theorem 1 says that this sum does not converge, given some conditions which are supposed to have the meaning, \"U is unbounded\".</p>\n<p>First, note that S<sub>I</sub> must be countable.&nbsp; This is because the sum over f in S<sub>I</sub> of p(f) = 1, and as Planetmath.org says, <a href=\"http://planetmath.org/encyclopedia/UncountableSumsOfPositiveNumbers.html\">a sum can be finite only if the number of items summed is countable</a>.&nbsp; (Also, they are all computable functions, and the number of computable functions is countable; and they are already indexed with G&ouml;del numbers in the paper.)&nbsp; I will renumber the set S<sub>I</sub>.&nbsp; Instead of indexing them with G&ouml;del numbers, as the paper does, I will index them so that f<sub>i</sub> (the ith member of S<sub>I</sub>) is a function of i in a different way.</p>\n<p>I choose a probability function p:S<sub>I</sub>&rarr;[0 .. 1].&nbsp; The sum of p(f<sub>i</sub>) must be 1.&nbsp; I choose the function p(f<sub>i</sub>) = 3/4<sup>i</sup>.&nbsp; This sums to 1, since the infinite series 1/4<sup>i</sup> (starting from i=1) sums to 1/3.&nbsp; (To see this, let the sum from i=0 to infinity be S; see that S/4 = S - 1, 4S-S = 4. s(4-1)=4, S = 4/3.)</p>\n<p>Since the theorem is supposed to hold for all cases, I choose the case where the set of possible worlds consistent with observation is S<sub>I</sub> = { f<sub>i </sub>| f<sub>i</sub>(k) = (2(1-1/2<sup>k</sup>))<sup>i</sup> }.&nbsp; Note that for every i, for every k<sub>2</sub> &gt; k<sub>1</sub>, f<sub>i</sub>(k<sub>2</sub>)&nbsp;&ge; f<sub>i</sub>(k<sub>1</sub>), and the limit as <em>k</em> approaches infinity of f<sub>i</sub>(k) is 2<sup>i</sup>.&nbsp; So f<sub>i</sub>(k) is bounded above by 2<sup>i</sup>.</p>\n<p>I choose the unbounded utility function U(f(k)) = f(k).&nbsp;&nbsp;Since f<sub>i</sub>(k) is bounded above by 2<sup>i</sup>, p(f<sub>i</sub>)U(f<sub>i</sub>(k)) is bounded above by (3/4<sup>i</sup>)2<sup>i</sup>.&nbsp; The sum from i=1 to infinity of (3/4<sup>i</sup>)2<sup>i</sup> is 3&times;(sum from i=1 to infinity of 2<sup>i</sup>/4<sup>i</sup>) = 3&times;(sum from i=1 to infinity of 1/2<sup>i</sup>) = 3.</p>\n<p>So, for this case, the utility function is U(n) = n, which is unbounded; all other criteria are met; and the expected value is always bounded above by 3.</p>\n<p>Can you find any reason why this is not a counterexample?</p>\n<p>The only step I'm suspicious of is the step where I choose the set of possible worlds.&nbsp; Does the theorem implicitly assume that the utility function may be evaluated in any countable set of possible worlds (meaning it is actually proving that, for any unbounded utility function, there is some set of possible worlds for which the expected value does not converge)?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JyYgTPTrgeYuuNdk7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 7.211550551756074e-07, "legacy": true, "legacyId": "7714", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T06:12:18.646Z", "modifiedAt": null, "url": null, "title": "Existing Absurd Technologies", "slug": "existing-absurd-technologies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:59.044Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Desrtopa", "createdAt": "2009-07-08T00:36:51.471Z", "isAdmin": false, "displayName": "Desrtopa"}, "userId": "vmhCKZoik2GFo5yAJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WeDr3cpp8PY2P5qaz/existing-absurd-technologies", "pageUrlRelative": "/posts/WeDr3cpp8PY2P5qaz/existing-absurd-technologies", "linkUrl": "https://www.lesswrong.com/posts/WeDr3cpp8PY2P5qaz/existing-absurd-technologies", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Existing%20Absurd%20Technologies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExisting%20Absurd%20Technologies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWeDr3cpp8PY2P5qaz%2Fexisting-absurd-technologies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Existing%20Absurd%20Technologies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWeDr3cpp8PY2P5qaz%2Fexisting-absurd-technologies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWeDr3cpp8PY2P5qaz%2Fexisting-absurd-technologies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p>When attempting to introduce non-rationalists to the ideas of cryonics or Strong AI, it appears that their primary objections tend to be rooted in the absurdity heuristic. They don't believe they inhabit a universe where such <em>weird</em> technologies could actually work. To deal with this, I thought it would be useful to have a cache of examples of technologies that have actually been implemented that did, or ideally, still do, challenge our intuitions about the way the universe works.</p>\n<p>The first example that comes to my mind is computers in general; imagine what Ernest Rutherford, let alone Benjamin Franklin, would have thought of a machine that uses electricity to calculate, and do those calculations so fast that they can express nearly <em>anything</em> as calculations. Nothing we know about how the universe works says it shouldn't be possible, indeed it obviously is knowing what we do now, but imagine how weird this would have seemed back when we were just coming to grips with how electricity actually worked.</p>\n<p>I suspect there may be better examples to challenge the intuitions of people who've grown up in an age where computers are commonplace though. So does anyone have any to volunteer?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb19d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WeDr3cpp8PY2P5qaz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 34, "extendedScore": null, "score": 7.211736884053413e-07, "legacy": true, "legacyId": "7720", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T10:26:49.552Z", "modifiedAt": null, "url": null, "title": "Non-axiomatic math reasoning with naive Bayes", "slug": "non-axiomatic-math-reasoning-with-naive-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:08.899Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/idNFgnNr5uTLHMcys/non-axiomatic-math-reasoning-with-naive-bayes", "pageUrlRelative": "/posts/idNFgnNr5uTLHMcys/non-axiomatic-math-reasoning-with-naive-bayes", "linkUrl": "https://www.lesswrong.com/posts/idNFgnNr5uTLHMcys/non-axiomatic-math-reasoning-with-naive-bayes", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Non-axiomatic%20math%20reasoning%20with%20naive%20Bayes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANon-axiomatic%20math%20reasoning%20with%20naive%20Bayes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FidNFgnNr5uTLHMcys%2Fnon-axiomatic-math-reasoning-with-naive-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Non-axiomatic%20math%20reasoning%20with%20naive%20Bayes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FidNFgnNr5uTLHMcys%2Fnon-axiomatic-math-reasoning-with-naive-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FidNFgnNr5uTLHMcys%2Fnon-axiomatic-math-reasoning-with-naive-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 676, "htmlBody": "<p><strong>ETA:</strong> this post is WRONG. Read on only if you like to find mistakes in other people's reasoning. Sorry.</p>\n<p>\n<hr />\n</p>\n<p>So I've been <a href=\"/lw/2tk/whats_a_natural_number/\">thinking</a>&nbsp;how to teach machines to reason about <a href=\"/lw/5jv/no_coinductive_datatype_of_integers/\">integers</a>&nbsp;without running into <a href=\"/lw/5f2/attempts_to_work_around_goedels_theorem_by_using/\">Goedelian limitations</a>. A couple days ago I suddenly got this painfully obvious idea that I'm sure others have already developed, but I don't know how to search for prior work, so I'm posting it here.</p>\n<p>Here's a simple statement about the integers:</p>\n<p><strong>5 &gt; 4</strong></p>\n<p>It's easy enough to check by direct calculation if your CPU can do arithmetic. Here's a more complex statement:</p>\n<p><strong>For any x, x+1 &gt; x</strong></p>\n<p>How do you check this? Let's denote the whole statement as S, and the individual statements \"x + 1 &gt; x\" as S(x). The statements S(1), S(2) and so on are all directly checkable like the first one. For simplicity we can make the \"naive Bayesian\" assumption that they're all independent, so P(S) = P(S(1))*P(S(2))*... (ETA: this is shady, though it doesn't break the construction. See AlephNeil's comment and my reply for a slightly better way.) After manually checking that S(1) holds, we update its probability to 1, so our estimate of P(S) increases. Then we try S(2), etc. After a while P(S) will become as close to 1 as we want, if S is true.</p>\n<p>We can also use a different quantifier:</p>\n<p><strong>There exists x such that x*x = 9</strong></p>\n<p>You can check this one by naive Bayesian reasoning too, except \"exists\" corresponds to probabilistic \"or\", whereas \"for any\" corresponded to probabilistic \"and\". So you check S(1), S(2) and so on, revising P(S) downward at each step, until you stumble on the right x and P(S) jumps to 1.</p>\n<p>Here's a still more complex statement:</p>\n<p><strong>For any x, there exists y such that x&nbsp;&ge;&nbsp;y*y and x &lt; (y+1)*(y+1)</strong></p>\n<p>How do you check this one? If we again denote it as S and the individual parts as S(x), we already know how to approximately check S(x) for each value of x, because it's a statement of the previous \"level\"! So we know how to check the whole S approximately, too.</p>\n<p>Hopefully you get the general idea by now: add as many quantifiers as you wish. The fancy math name for this method of organizing statements is the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Arithmetical_hierarchy\">arithmetical hierarchy</a>. The calculations become expensive, but in the limit of enough computing power the system will get closer and closer to correct reasoning about the integers. As time passes, it will come to believe pretty strongly in all the axioms of PA (or at least the correct ones :-)), the consistency of PA (if it's consistent), <a href=\"http://rjlipton.wordpress.com/2009/05/27/arithmetic-hierarchy-and-pnp/\">P&ne;NP</a> (if it's true), etc. The Goedelian limitation doesn't apply because we don't get proofs, only ever-stronger corroborations. The arithmetical hierarchy doesn't cover tricky second-order questions like the continuum hypothesis, but perhaps we can live without those?</p>\n<p>The flip side is that you need a whole lot of computational resources, in fact you need so much that no computable function can estimate how long it'll take our system to converge to the \"actual truth\". Given an arithmetical statement that's guaranteed to have a counterexample, the number of CPU cycles required to find the counterexample grows uncomputably fast with the size of the statement, I believe. So it's pretty far from a practical AI, and more like Solomonoff induction - something to be approximated.</p>\n<p>Some optimizations immediately spring to mind. If you spend some time calculating and come to strongly believe a bunch of statements involving quantifiers, you can adopt them as heuristics (prospective axioms) while investigating other statements, search for proofs or contradictions using ordinary first-order logic, etc. Such tricks seem to fit nicely into the Bayesian framework as long as you're willing to introduce some \"naivety\" here and there.</p>\n<p>This sort of non-axiomatic reasoning looks like a good approximation of our math intuitions. The axioms of PA sound reasonable to me only because I can try some examples in my mind and everything works out. So I was pretty excited to find a simple computational process that, given enough time, settles on the axioms of PA and much more besides. What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "idNFgnNr5uTLHMcys", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7724", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p><strong>ETA:</strong> this post is WRONG. Read on only if you like to find mistakes in other people's reasoning. Sorry.</p>\n<p>\n</p><hr>\n<p></p>\n<p>So I've been <a href=\"/lw/2tk/whats_a_natural_number/\">thinking</a>&nbsp;how to teach machines to reason about <a href=\"/lw/5jv/no_coinductive_datatype_of_integers/\">integers</a>&nbsp;without running into <a href=\"/lw/5f2/attempts_to_work_around_goedels_theorem_by_using/\">Goedelian limitations</a>. A couple days ago I suddenly got this painfully obvious idea that I'm sure others have already developed, but I don't know how to search for prior work, so I'm posting it here.</p>\n<p>Here's a simple statement about the integers:</p>\n<p><strong id=\"5___4\">5 &gt; 4</strong></p>\n<p>It's easy enough to check by direct calculation if your CPU can do arithmetic. Here's a more complex statement:</p>\n<p><strong id=\"For_any_x__x_1___x\">For any x, x+1 &gt; x</strong></p>\n<p>How do you check this? Let's denote the whole statement as S, and the individual statements \"x + 1 &gt; x\" as S(x). The statements S(1), S(2) and so on are all directly checkable like the first one. For simplicity we can make the \"naive Bayesian\" assumption that they're all independent, so P(S) = P(S(1))*P(S(2))*... (ETA: this is shady, though it doesn't break the construction. See AlephNeil's comment and my reply for a slightly better way.) After manually checking that S(1) holds, we update its probability to 1, so our estimate of P(S) increases. Then we try S(2), etc. After a while P(S) will become as close to 1 as we want, if S is true.</p>\n<p>We can also use a different quantifier:</p>\n<p><strong id=\"There_exists_x_such_that_x_x___9\">There exists x such that x*x = 9</strong></p>\n<p>You can check this one by naive Bayesian reasoning too, except \"exists\" corresponds to probabilistic \"or\", whereas \"for any\" corresponded to probabilistic \"and\". So you check S(1), S(2) and so on, revising P(S) downward at each step, until you stumble on the right x and P(S) jumps to 1.</p>\n<p>Here's a still more complex statement:</p>\n<p><strong id=\"For_any_x__there_exists_y_such_that_x___y_y_and_x____y_1___y_1_\">For any x, there exists y such that x&nbsp;\u2265&nbsp;y*y and x &lt; (y+1)*(y+1)</strong></p>\n<p>How do you check this one? If we again denote it as S and the individual parts as S(x), we already know how to approximately check S(x) for each value of x, because it's a statement of the previous \"level\"! So we know how to check the whole S approximately, too.</p>\n<p>Hopefully you get the general idea by now: add as many quantifiers as you wish. The fancy math name for this method of organizing statements is the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Arithmetical_hierarchy\">arithmetical hierarchy</a>. The calculations become expensive, but in the limit of enough computing power the system will get closer and closer to correct reasoning about the integers. As time passes, it will come to believe pretty strongly in all the axioms of PA (or at least the correct ones :-)), the consistency of PA (if it's consistent), <a href=\"http://rjlipton.wordpress.com/2009/05/27/arithmetic-hierarchy-and-pnp/\">P\u2260NP</a> (if it's true), etc. The Goedelian limitation doesn't apply because we don't get proofs, only ever-stronger corroborations. The arithmetical hierarchy doesn't cover tricky second-order questions like the continuum hypothesis, but perhaps we can live without those?</p>\n<p>The flip side is that you need a whole lot of computational resources, in fact you need so much that no computable function can estimate how long it'll take our system to converge to the \"actual truth\". Given an arithmetical statement that's guaranteed to have a counterexample, the number of CPU cycles required to find the counterexample grows uncomputably fast with the size of the statement, I believe. So it's pretty far from a practical AI, and more like Solomonoff induction - something to be approximated.</p>\n<p>Some optimizations immediately spring to mind. If you spend some time calculating and come to strongly believe a bunch of statements involving quantifiers, you can adopt them as heuristics (prospective axioms) while investigating other statements, search for proofs or contradictions using ordinary first-order logic, etc. Such tricks seem to fit nicely into the Bayesian framework as long as you're willing to introduce some \"naivety\" here and there.</p>\n<p>This sort of non-axiomatic reasoning looks like a good approximation of our math intuitions. The axioms of PA sound reasonable to me only because I can try some examples in my mind and everything works out. So I was pretty excited to find a simple computational process that, given enough time, settles on the axioms of PA and much more besides. What do you think?</p>", "sections": [{"title": "5 > 4", "anchor": "5___4", "level": 1}, {"title": "For any x, x+1 > x", "anchor": "For_any_x__x_1___x", "level": 1}, {"title": "There exists x such that x*x = 9", "anchor": "There_exists_x_such_that_x_x___9", "level": 1}, {"title": "For any x, there exists y such that x\u00a0\u2265\u00a0y*y and x < (y+1)*(y+1)", "anchor": "For_any_x__there_exists_y_such_that_x___y_y_and_x____y_1___y_1_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DurJh5k3Br3xFSHpe", "gzzDoayg9cESFiuXX", "qvsTmpkWSroJyN6MS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T15:13:58.848Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Lotteries: A Waste of Hope", "slug": "seq-rerun-lotteries-a-waste-of-hope", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:38.935Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7ZrAjzSRJqWKAHvPe/seq-rerun-lotteries-a-waste-of-hope", "pageUrlRelative": "/posts/7ZrAjzSRJqWKAHvPe/seq-rerun-lotteries-a-waste-of-hope", "linkUrl": "https://www.lesswrong.com/posts/7ZrAjzSRJqWKAHvPe/seq-rerun-lotteries-a-waste-of-hope", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Lotteries%3A%20A%20Waste%20of%20Hope&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Lotteries%3A%20A%20Waste%20of%20Hope%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ZrAjzSRJqWKAHvPe%2Fseq-rerun-lotteries-a-waste-of-hope%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Lotteries%3A%20A%20Waste%20of%20Hope%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ZrAjzSRJqWKAHvPe%2Fseq-rerun-lotteries-a-waste-of-hope", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ZrAjzSRJqWKAHvPe%2Fseq-rerun-lotteries-a-waste-of-hope", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 222, "htmlBody": "<p>Today's post, <a href=\"/lw/hl/lotteries_a_waste_of_hope/\">Lotteries: A Waste of Hope</a> was originally published on April 13, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>Some defend lottery-ticket buying as a rational purchase of fantasy &mdash; paying a dollar for a day's worth of pleasant anticipation.  But then your valuable brain is occupied with a fantasy whose real probability is nearly zero, investing emotional energy.  Without the lottery, people might fantasize about things to actually do, which then might lead to making the fantasy a reality. To work around a bias, you must first notice it, analyze it, and decide that it is bad.  Many people, such as the lottery advocates above, often fail to complete the third step.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5xz/seq_rerun_priors_as_mathematical_objects/\">Priors as Mathematical Objects</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb137": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7ZrAjzSRJqWKAHvPe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 7.21333300902281e-07, "legacy": true, "legacyId": "7725", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vYsuM8cpuRgZS5rYB", "uxfGirs9njFB9dG5b", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T16:21:08.597Z", "modifiedAt": null, "url": null, "title": "Natural wireheadings: formal request.", "slug": "natural-wireheadings-formal-request", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.964Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrMind", "createdAt": "2011-04-19T08:43:22.388Z", "isAdmin": false, "displayName": "MrMind"}, "userId": "LJ4br8GWFXetsXkM8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F6se594nycg8fYfhp/natural-wireheadings-formal-request", "pageUrlRelative": "/posts/F6se594nycg8fYfhp/natural-wireheadings-formal-request", "linkUrl": "https://www.lesswrong.com/posts/F6se594nycg8fYfhp/natural-wireheadings-formal-request", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Natural%20wireheadings%3A%20formal%20request.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANatural%20wireheadings%3A%20formal%20request.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF6se594nycg8fYfhp%2Fnatural-wireheadings-formal-request%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Natural%20wireheadings%3A%20formal%20request.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF6se594nycg8fYfhp%2Fnatural-wireheadings-formal-request", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF6se594nycg8fYfhp%2Fnatural-wireheadings-formal-request", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<p>This post is a formal request for everybody in this forum, who are the most likely humans to produce an FAI in the future, that in the possible resulting utopia some activity will still be available, even if they are not based on challenges of increasing complexity. I call these activity natural wireheadings, and since <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\" target=\"_blank\">it is my right</a> to determine my own fun-generating activities, I hereby formally request that some simple pleasure, listened below, will still be available at *any* point in my future cone, and that I will consider a dystopia any future which deprives me of these natural wireheadings (if anyone is still around caring for those things).<br />A (non-exhaustive) list of them includes the following:<br />- sex<br />- eating food/drinking<br />- feeling relieved for having emptied my bowels<br />- dancing<br />- the pleasure of physical activity (wood-carving, sculpting, running, etc)<br />- the rapture I feel when in the presence of a safe ancestral environment<br />- social laughter<br />- the pleasure of talking in a small, same-minded crowd<br />- listening to pop/metal/rap music<br />- the pleasure of resting when tired<br />- scratching an itch<br />-...<br /><br />More will come!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F6se594nycg8fYfhp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -5, "extendedScore": null, "score": -1.2e-05, "legacy": true, "legacyId": "7726", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pK4HTxuv6mftHXWC3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T17:55:59.721Z", "modifiedAt": null, "url": null, "title": "Wiki edits by spambots", "slug": "wiki-edits-by-spambots", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:17.964Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Desrtopa", "createdAt": "2009-07-08T00:36:51.471Z", "isAdmin": false, "displayName": "Desrtopa"}, "userId": "vmhCKZoik2GFo5yAJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WpwHt4xx3rSK2QnsD/wiki-edits-by-spambots", "pageUrlRelative": "/posts/WpwHt4xx3rSK2QnsD/wiki-edits-by-spambots", "linkUrl": "https://www.lesswrong.com/posts/WpwHt4xx3rSK2QnsD/wiki-edits-by-spambots", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wiki%20edits%20by%20spambots&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWiki%20edits%20by%20spambots%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpwHt4xx3rSK2QnsD%2Fwiki-edits-by-spambots%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wiki%20edits%20by%20spambots%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpwHt4xx3rSK2QnsD%2Fwiki-edits-by-spambots", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpwHt4xx3rSK2QnsD%2Fwiki-edits-by-spambots", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>We seem to have at least one bot <a href=\"http://wiki.lesswrong.com/mediawiki/index.php?title=How_To_Get_Your_Ex_Back_-_How_To_Get_An_Ex_Boyfriend_To_Leave_Your_Girlfriend_Alone\" target=\"_self\">editing the Less Wrong Wiki</a>. Is this something that could easily be fixed by instituting a karma requirement for wiki editing?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WpwHt4xx3rSK2QnsD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 7.213810534332515e-07, "legacy": true, "legacyId": "7727", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T18:35:12.858Z", "modifiedAt": null, "url": null, "title": "Existing Absurd Ideas", "slug": "existing-absurd-ideas", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "endoself", "createdAt": "2011-01-02T09:26:40.389Z", "isAdmin": false, "displayName": "endoself"}, "userId": "e4JPxEMj36oRwTALQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7HCrSgMFpATE9Zuke/existing-absurd-ideas", "pageUrlRelative": "/posts/7HCrSgMFpATE9Zuke/existing-absurd-ideas", "linkUrl": "https://www.lesswrong.com/posts/7HCrSgMFpATE9Zuke/existing-absurd-ideas", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Existing%20Absurd%20Ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExisting%20Absurd%20Ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HCrSgMFpATE9Zuke%2Fexisting-absurd-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Existing%20Absurd%20Ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HCrSgMFpATE9Zuke%2Fexisting-absurd-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HCrSgMFpATE9Zuke%2Fexisting-absurd-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7, "htmlBody": "<p><strong>Inspired by</strong>: <a href=\"/r/discussion/lw/5yg/existing_absurd_technologies\">Existing Absurd Technologies</a></p>\n<p>&nbsp;</p>\n<p>People rely on</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb19d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7HCrSgMFpATE9Zuke", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7728", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WeDr3cpp8PY2P5qaz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-30T22:34:39.020Z", "modifiedAt": null, "url": null, "title": "Defeating Mundane Holocausts With Robots", "slug": "defeating-mundane-holocausts-with-robots", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.552Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q7mC4k9wDwcGNYRnP/defeating-mundane-holocausts-with-robots", "pageUrlRelative": "/posts/Q7mC4k9wDwcGNYRnP/defeating-mundane-holocausts-with-robots", "linkUrl": "https://www.lesswrong.com/posts/Q7mC4k9wDwcGNYRnP/defeating-mundane-holocausts-with-robots", "postedAtFormatted": "Monday, May 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Defeating%20Mundane%20Holocausts%20With%20Robots&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADefeating%20Mundane%20Holocausts%20With%20Robots%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7mC4k9wDwcGNYRnP%2Fdefeating-mundane-holocausts-with-robots%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Defeating%20Mundane%20Holocausts%20With%20Robots%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7mC4k9wDwcGNYRnP%2Fdefeating-mundane-holocausts-with-robots", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7mC4k9wDwcGNYRnP%2Fdefeating-mundane-holocausts-with-robots", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 647, "htmlBody": "<p>Causes of death such as malaria and hunger are certainly worth allocating resources towards preventing, for today's results-oriented philanthropist. It's almost ridiculous to realize we can put $1000 towards mosquito netting and save a human life. However, these kinds of things will eventually run out of low-hanging fruit, especially as countries become more developed. By advancing the adoption of certain key near-term technologies just a little sooner, we can make rather significant gains even in developed countries where the causes of death are more complex and occur later in life.</p>\n<p>According to Brad Templeton's <a href=\"http://www.templetons.com/brad/robocars/\">executive summary</a> of the case for robotic cars:</p>\n<blockquote>\n<p>Every year we delay deploying robocars (and related technology) in the USA, human driving will kill another 35,000, and 1.2 million worldwide.</p>\n</blockquote>\n<p>Anything in the range of a million people per year definitely qualifies as a holocaust! And yet this is actually a fairly small percentage of the world death rate (about 57 million per year) overall. (Most <a href=\"http://en.wikipedia.org/wiki/List_of_causes_of_death_by_rate\">deaths</a> are caused by heart disease or infectious diseases.) Nonetheless, self-driving cars strike me as an attractive initial goal for the following reasons:</p>\n<ul>\n<li>More reliable EMS. Self-driving cars could coordinate with self-driving ambulances, leading to sooner arrival times and lower death rates.</li>\n<li>Higher world GDP. Robin Hanson <a href=\"http://www.overcomingbias.com/2010/11/who-will-pioneer-auto-autos.html\">estimates</a> 5-20% long term gains due to road use efficiency alone. Additional economic benefits would include reduced dependence on oil, and fewer deaths (from auto accidents) among the working (pre-retirement-age) population.</li>\n<li>Near-term / the world is ready. Self-driving cars seem science-fictional, but they don't trip deep philosophical flags the way cryonics or curing aging does. And there are youtube videos of cars driving themselves safely all over the country, so people can't argue that they are vaporware.</li>\n</ul>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Powered_exoskeleton\">robotic exo-suit</a> is another near-term source for dramatically increased GDP -- a person wearing one can perform manual labor tasks with greater endurance and reduced danger of physical injury, without undergoing painful physical conditioning. It can delay forced retirement age, as a feeble body will no longer be an obstacle to a number of tasks. Furthermore, powered suits may prove the key to truly comfortable hermetically sealed environments -- something that can be very handy when old age hits and your immune system declines. It can also be useful for keeping infectious diseases <em>in</em>.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Artificial_heart\">artificial heart</a> is something else that can reduce instances of death <em>significantly</em>. We are kept alive by two pounds of throbbing muscle just waiting to explode on us. Removing that risk from the picture would have a huge impact on the death rate in the developed world.</p>\n<p>Another huge risk-reducer would be wider adoption of <a href=\"http://en.wikipedia.org/wiki/Robotic_surgery\">robotic surgery</a>. This enables surgical interventions to take place under far more controlled circumstances, without hand-tremors and human error to complicate matters. As surgery becomes more safe and noninvasive, it can be used for preventative maintenance, rather than being conserved for when something is going wrong.</p>\n<p>Cryonics and robust rejuvenation treatments still are very significant from a life-extension perspective. But proof that they will work is not necessarily going to be available until it is too late to convince people (in this generation) to start allocating significant resources to them. A better strategy might be to invest first in these less radical technologies (while still maintaining a healthy activist base for life extension memes) and use the economic gains to jump on the growing life extension market as it starts to open up.</p>\n<p>Another thing to bear in mind is that cheap, accessible robotics can lead to cheaper, more accessible cryonics and life-extension drugs. These things tend to synergize. A factory where the workers are equipped with exo-suits can produce chemicals, drugs, and mechanical parts more quickly and cheaply. The more easily a new piece of robotic equipment can be prototyped and tested, the more likely it will see use sooner, resulting in the earlier introduction of safety and economic gains for humans.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "qAvbtzdG2A2RBn7in": 1, "pGqRLe9bFDX2G2kXY": 1, "EeSkeTcT4wtW2fWsL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q7mC4k9wDwcGNYRnP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 34, "extendedScore": null, "score": 7.214631979579412e-07, "legacy": true, "legacyId": "7729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-31T04:35:00.004Z", "modifiedAt": null, "url": null, "title": "Overcoming Suffering & Buddhism", "slug": "overcoming-suffering-and-buddhism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:39.000Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Hul-Gil", "createdAt": "2011-05-02T22:16:10.898Z", "isAdmin": false, "displayName": "Hul-Gil"}, "userId": "a67SuoZfWaXPzjvzy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8yH3kuFb99ZLwdG6K/overcoming-suffering-and-buddhism", "pageUrlRelative": "/posts/8yH3kuFb99ZLwdG6K/overcoming-suffering-and-buddhism", "linkUrl": "https://www.lesswrong.com/posts/8yH3kuFb99ZLwdG6K/overcoming-suffering-and-buddhism", "postedAtFormatted": "Tuesday, May 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Overcoming%20Suffering%20%26%20Buddhism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOvercoming%20Suffering%20%26%20Buddhism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8yH3kuFb99ZLwdG6K%2Fovercoming-suffering-and-buddhism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Overcoming%20Suffering%20%26%20Buddhism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8yH3kuFb99ZLwdG6K%2Fovercoming-suffering-and-buddhism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8yH3kuFb99ZLwdG6K%2Fovercoming-suffering-and-buddhism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 727, "htmlBody": "<p>The recent post (http://lesswrong.com/lw/5xx/overcoming_suffering_emotional_acceptance) by Kaj_Sotala is very reminiscent of Buddhism to me. Since no one has commented with similar sentiments, and since I get the impression Buddhism is not a common topic of discussion here, I thought I'd make a quick article for the curious. I'm not exactly a Buddhist myself, but I have a good few books about the topic and have experienced mild success with meditation.</p>\n<p>Buddhism is one of the few religious belief systems not entirely repellent to me, for a couple of reasons. For one, Buddhism - or some traditions thereof, including the \"original\" (Theravada), I believe - encourages adherents to be skeptical. The emphasis is not on faith, gods, or symbolism, but rather on actual practice and experience: in other words, on obtaining evidence. You can see for yourself whether or not the system works, because the reward is not in another life. It is the cessation of suffering in this one.<br /><br />For two, that emphasis on the problem of suffering seems very reasonable to me. Buddhism holds that the problem with this world is suffering, and that suffering can be alleviated by methods somewhat similar to the ones in Kaj_Sotala's post. (The choice of the word \"mindfulness\" - was that a coincidence, or a reference to the Buddhist concept of the same name?) The idea is that suffering results from unfulfilled desires, themselves a product of an uncontrolled mind. You become upset when the world is This Way, but you want it to be That Way; and even if you try to accept the world-as-it-is, your brain is rebellious. Unpleasant feelings arise, unbidden and unwelcome.</p>\n<p>The solution, according to Buddhism, is meditation. There are many different types of meditation, both in technique and in topic meditated upon, but I won't go into them here. Meditation appears to be physically healthy just on its own; a quick Google search on \"meditation brain\" will bring up hundreds of articles about how it affects the thinking organ. However, the main goals of Buddhist meditation are a.) attaining control over your own mind (i.e., learning to separate sense impressions from emotions and values, so that harsh words or even blows cause no corresponding mental disturbance), and b.) attaining insight into Buddhist thought about subjects such as love, impermanence, mindfulness, or skillfulness.</p>\n<p>Buddhist thought on some subjects (see next-to-final paragraph) I can leave, but mindfulness and skillfulness seem appropriate to LessWrong. As I understand it, the idea behind mindfulness is simply to be aware of what you're doing, rather than going through the motions - and to be aware of, and fix, cognitive biases. For beliefs and mental processes, failing to hit the \"Explain\" button (to steal from Mr. Yudkowsky) could be considered un-mindful. Things you don't think about are things you could be getting wrong. Skillfulness is related; it's not about skill at some particular task - it's about maximizing utility, to put it simply. The goal is no wasted or mistaken actions. Your actions should not result in unintended consequences, and your intended consequences should never fail to advance your goals in some way. Rationality is thus a very big part of Buddhism, since it is necessary to be rational to be mindful and skillful!</p>\n<p>**One important note:** Buddhism has many traditions, and many, many different beliefs. A great deal of it is about as credible as any other religion. For instance, Buddhism holds that there is no \"self\", ultimately; however, it also holds that people are reincarnated... so what is it that is being reincarnated? I'm sure there is an apology for this somewhere, but the only explanation I've read made less sense than the question. Karma is also a silly idea, in my opinion. I've picked and chosen regarding Buddhist beliefs, and I'm no expert, so if it turns out what I've written isn't orthodox - well, I've warned you!</p>\n<p>That's about all I have to say on the subject. Buddhist methods for overcoming suffering have served me well; it is from Buddhism that I first learned to fight depression over things I can do nothing about, and that regret is only useful insofar as it can inspire you to change, and that there is no excuse for being unskillful and unmindful even in the smallest task. I hope this post has served to impart some knowledge, and/or satisfy (or impart!) some curiosity.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LaDu5bKDpe8LxaR7C": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8yH3kuFb99ZLwdG6K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 3, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "7744", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-31T05:05:05.575Z", "modifiedAt": null, "url": null, "title": "Houston Hackerspace Meetup : Saturday June 4, 2:00PM", "slug": "houston-hackerspace-meetup-saturday-june-4-2-00pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.896Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cog", "createdAt": "2011-04-25T04:58:53.803Z", "isAdmin": false, "displayName": "Cog"}, "userId": "xkp87vCZ56dp2tWnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tvWmas9xmiMGhtgpQ/houston-hackerspace-meetup-saturday-june-4-2-00pm", "pageUrlRelative": "/posts/tvWmas9xmiMGhtgpQ/houston-hackerspace-meetup-saturday-june-4-2-00pm", "linkUrl": "https://www.lesswrong.com/posts/tvWmas9xmiMGhtgpQ/houston-hackerspace-meetup-saturday-june-4-2-00pm", "postedAtFormatted": "Tuesday, May 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Houston%20Hackerspace%20Meetup%20%3A%20Saturday%20June%204%2C%202%3A00PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHouston%20Hackerspace%20Meetup%20%3A%20Saturday%20June%204%2C%202%3A00PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtvWmas9xmiMGhtgpQ%2Fhouston-hackerspace-meetup-saturday-june-4-2-00pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Houston%20Hackerspace%20Meetup%20%3A%20Saturday%20June%204%2C%202%3A00PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtvWmas9xmiMGhtgpQ%2Fhouston-hackerspace-meetup-saturday-june-4-2-00pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtvWmas9xmiMGhtgpQ%2Fhouston-hackerspace-meetup-saturday-june-4-2-00pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p style=\"margin-bottom: 0in;\"><em>Saturday June 4, 2:00PM</em></p>\n<p style=\"margin-bottom: 0in;\"><em>TX/RX Labs Hackerspace</em></p>\n<p style=\"margin-bottom: 0in;\"><em>2010 Commerce St</em></p>\n<p style=\"margin-bottom: 0in;\"><em>Houston, TX 77002</em></p>\n<p>&nbsp;</p>\n<p>The third meeting of the Houston Less Wrong group will be this coming Saturday at TX/RX Labs at 2:00PM. We had some success last time, with Orange Cat (I apologize if I got the formatting of your handle wrong) showing up during the meeting time, and another prospective member who showed up earlier in the day. Hopefully Saturday will be easier for some people.</p>\n<p>&nbsp;</p>\n<p>We have decided to do a reading group on Jayne's \"Probability Theory: The Logic of Science\". We're still knocking around the reading schedule, so if you want to get involved, let me know. We will hopefully be doing other things as well, but I'll start scheduling those as we get more interest.</p>\n<p>&nbsp;</p>\n<p>Pizza or prepared food are a possibility, if people show up hungry. We have a full kitchen in the hackerspace.</p>\n<p><strong>Directions</strong></p>\n<p><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">A pictoral view</p>\n<p><img src=\"http://images.lesswrong.com/t3_5pp_1.png\" alt=\"Front\" width=\"645\" height=\"470\" /></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This is the set of buildings that the hackerspace is in. It's difficult to see our front from this angle - unfortunately google maps decided to map everything but our little section of commerce street. It's near where the white truck and red motorcycle are. Currently, there is an old military vehicle and generator in front.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><img src=\"http://images.lesswrong.com/t3_5pp_0.png\" alt=\"Empy Lot\" width=\"644\" height=\"465\" /></p>\n<p style=\"margin-bottom: 0in;\">And this is the empty lot that you can park in if all the nearby marked spots are taken..</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For more reference:</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><a href=\"http://maps.google.com/maps?client=ubuntu&amp;channel=fs&amp;q=2010\">http://maps.google.com/maps?client=ubuntu&amp;channel=fs&amp;q=2010</a>+Commerce+St.+Houston,+Tx+77002&amp;oe=utf-8&amp;um=1&amp;ie=UTF-8&amp;hq=&amp;hnear=0x8640bed8ed95625d:0x4c9af214d2032035,2010+Commerce+St,+Houston,+TX+77002&amp;gl=us&amp;ei=C9LRTYHvE8fL0QGu8OjlCw&amp;sa=X&amp;oi=geocode_result&amp;ct=title&amp;resnum=1&amp;ved=0CBkQ8gEwAA</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tvWmas9xmiMGhtgpQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 7.215783239468423e-07, "legacy": true, "legacyId": "7745", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-31T09:27:20.419Z", "modifiedAt": null, "url": null, "title": "[Career advice] Machine learning jobs", "slug": "career-advice-machine-learning-jobs", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:16.492Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "outlier", "createdAt": "2011-05-21T23:00:23.393Z", "isAdmin": false, "displayName": "outlier"}, "userId": "Ni8Zc3YNmRdjKZRWu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xC3jsX4bdYkdNZzhc/career-advice-machine-learning-jobs", "pageUrlRelative": "/posts/xC3jsX4bdYkdNZzhc/career-advice-machine-learning-jobs", "linkUrl": "https://www.lesswrong.com/posts/xC3jsX4bdYkdNZzhc/career-advice-machine-learning-jobs", "postedAtFormatted": "Tuesday, May 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BCareer%20advice%5D%20Machine%20learning%20jobs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BCareer%20advice%5D%20Machine%20learning%20jobs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxC3jsX4bdYkdNZzhc%2Fcareer-advice-machine-learning-jobs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BCareer%20advice%5D%20Machine%20learning%20jobs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxC3jsX4bdYkdNZzhc%2Fcareer-advice-machine-learning-jobs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxC3jsX4bdYkdNZzhc%2Fcareer-advice-machine-learning-jobs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<p>I'm about to get my masters degree in computer science. I've been interested in AI for a few years, studying mathematics and machine learning. I'm pretty sure that I would like to work as machine learning specialist. The question I have for you is: how do I do that?</p>\n<p>I live in Szczecin, Poland. It's just 150km from Berlin and I am willing to move there if necessary. My current plan is:</p>\n<ul>\n<li>Work as Java programmer with a friend of mine.</li>\n<li>Keep on learning maths and machine learning.</li>\n<li>Learn German.</li>\n<li>Earn some money to move to Berlin. </li>\n<li>Move to Berlin</li>\n<li>Find machine-learning related job.</li>\n</ul>\n<p>I'm still unsure if I should get phd. I don't know if machine learning jobs are common in private sector, but I think they will be in near future (am I wrong about this?). What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xC3jsX4bdYkdNZzhc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 7.216556685996267e-07, "legacy": true, "legacyId": "7751", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-31T13:09:13.236Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] New Improved Lottery", "slug": "seq-rerun-new-improved-lottery", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:33.757Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j7dxBNRrdCCKwdXpQ/seq-rerun-new-improved-lottery", "pageUrlRelative": "/posts/j7dxBNRrdCCKwdXpQ/seq-rerun-new-improved-lottery", "linkUrl": "https://www.lesswrong.com/posts/j7dxBNRrdCCKwdXpQ/seq-rerun-new-improved-lottery", "postedAtFormatted": "Tuesday, May 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20New%20Improved%20Lottery&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20New%20Improved%20Lottery%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj7dxBNRrdCCKwdXpQ%2Fseq-rerun-new-improved-lottery%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20New%20Improved%20Lottery%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj7dxBNRrdCCKwdXpQ%2Fseq-rerun-new-improved-lottery", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj7dxBNRrdCCKwdXpQ%2Fseq-rerun-new-improved-lottery", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Today's post, <a href=\"/lw/hm/new_improved_lottery/\">New Improved Lottery</a> was originally publeslished on April 13, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>If the opportunity to fantasize about winning was a rational justification for the lottery, we could design a \"New Improved Lottery\", where a single payment buys an epsilon chance of becoming a millionaire over the next five years. All your time could be spent thinking how you could become a millionaire at any moment.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5yl/seq_rerun_lotteries_a_waste_of_hope/\">Lotteries: A Waste of Hope</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j7dxBNRrdCCKwdXpQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 7.217211192701999e-07, "legacy": true, "legacyId": "7752", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QawvGzYWhqdyPWgBL", "7ZrAjzSRJqWKAHvPe", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-31T15:56:34.766Z", "modifiedAt": null, "url": null, "title": "Making projects happen", "slug": "making-projects-happen", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:17.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xdNQEXagwMTCbwwK4/making-projects-happen", "pageUrlRelative": "/posts/xdNQEXagwMTCbwwK4/making-projects-happen", "linkUrl": "https://www.lesswrong.com/posts/xdNQEXagwMTCbwwK4/making-projects-happen", "postedAtFormatted": "Tuesday, May 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Making%20projects%20happen&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaking%20projects%20happen%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxdNQEXagwMTCbwwK4%2Fmaking-projects-happen%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Making%20projects%20happen%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxdNQEXagwMTCbwwK4%2Fmaking-projects-happen", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxdNQEXagwMTCbwwK4%2Fmaking-projects-happen", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1101, "htmlBody": "<p>Judging by the number of upvotes, Brandon Reinhart's&nbsp;<a href=\"/lw/5il/siai_an_examination/\">analysis of SIAI's financial filings</a>&nbsp;is valuable to quite a few people. Similar analysis' of Alcor and the Cryonics Institute would be quite valuable. There has been talk of more work on condensing LW content and placing it on the wiki. I'm sure lots of people would like to know about the literature on&nbsp;<a href=\"/r/discussion/lw/5qm/living_forever_is_hard_or_the_gompertz_curve/477u\">low dose asprin</a>.&nbsp;People seem to want a front page more accessible to newcomers. Will these projects get accomplished? Some of them, but probably fewer than optimal. I think we can do better.&nbsp;</p>\n<p>I would like to look for ways to channel group willingness to contribute to a project into focused individual willingness to work on a project.</p>\n<h2>Observations about the problem space</h2>\n<p>The following is based on discussions at the Seattle Less Wrong meetup.</p>\n<p>Many people would get a moderate amount of benefit from such projects, but only a small number would end up putting in the hard work to make them happen.&nbsp;</p>\n<p>The people most enthusiastic about a given project may not be the best people to work on the project. Perhaps they have very time consuming jobs or have a hard time being objective about the topic (e.g. someone who gets especially emotional about Cryonics) or have too many other projects already or perhaps they are intellectually motivated but not emotionally motivated by the project which might make it difficult to Get Things Done.&nbsp;</p>\n<p>Trying to generalize too early is a risk here. Going out and building fancy tools or otherwise trying something elaborate is probably not a good idea at first. Better to try some concrete trials first and learn from those experiences.</p>\n<h2>Sources of motivation</h2>\n<p>There are three major potential sources of motivation:&nbsp;Money (the unit of caring), social status (Karma, kind words etc.), things (pizza, books, cookies,&nbsp;<a href=\"/lw/58u/specific_fiction_discusion_april_2011/3xqq\">pony pictures</a>).</p>\n<ul>\n<li>Money \n<ul>\n<li>Transfers of money (the&nbsp;<a href=\"/lw/65/money_the_unit_of_caring/\">unit of caring</a>) are often much more efficient than transfers of other goods.</li>\n<li>Extrinsic rewards (especially money) can reduce intrinsic motivation.&nbsp;</li>\n<li>Large monetary rewards can also make relationship between the project contributors and the project sponsors less social.&nbsp;</li>\n<li>Many Less Wrong people are high paid \n<ul>\n<li>Less likely to be motivated by small monetary rewards</li>\n<li>Have more money to contribute to projects.&nbsp;</li>\n<li>Not all Less Wrong people are high paid.&nbsp;</li>\n</ul>\n</li>\n<li>There are services for collecting donations (<a href=\"http://www.chipin.com/\">link</a>).</li>\n</ul>\n</li>\n<li>Social rewards \n<ul>\n<li>Praise&nbsp;</li>\n<li>Karma</li>\n<li>Social status</li>\n</ul>\n</li>\n<li>Things \n<ul>\n<li>Pizza, books, cookies,&nbsp;<a href=\"/lw/58u/specific_fiction_discusion_april_2011/3xqq\">pony pictures</a></li>\n</ul>\n</li>\n<li>Social pressure \n<ul>\n<li>requests</li>\n<li>progress monitoring</li>\n</ul>\n</li>\n</ul>\n<p>Different motivators may work better for different kinds of projects. For example, money might be a counterproductive motivator for social projects but a great motivator for setting up a website.</p>\n<h2>How have others tackled this?</h2>\n<p>This is a problem others face as well. How do other similar groups and communities&nbsp;ameliorate&nbsp;it?</p>\n<ul>\n<li>Intrinsic motivation \n<ul>\n<li>Conferring social status on those who do valuable work</li>\n</ul>\n</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Sprint_(software_development)\">Sprints</a>:&nbsp;several people get together in a single place and work together on a project for a couple of days. \n<ul>\n<li>Main draw seems to be Fun</li>\n<li>Frequently used by Python projects</li>\n</ul>\n</li>\n<li>Competition/bounties (<a href=\"http://www.mckinsey.com/app_media/reports/sso/and_the_winner_is.pdf\">McKinsey survey of prize literature</a>) \n<ul>\n<li>Provides social and/or material rewards</li>\n<li>Sometimes used on LW (<a href=\"/lw/3a2/100_for_the_best_article_on_efficient_charty_the/\">link 1</a>,&nbsp;<a href=\"/lw/5hy/cryonics_promotional_video_contest_10_btc_prize/\">link 2</a>,&nbsp;<a href=\"/lw/422/295_bounty_for_new_singularity_institute_logo/\">link 3</a>).</li>\n<li>Work seem well for some larger open source software projects (<a href=\"http://intrepidusgroup.com/insight/2011/03/bug-bounties-do-they-work/\">link 1</a>,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Open_source_bounty\">link 2</a>,&nbsp;<a href=\"http://forum.vectorlinux.com/index.php?topic=11745.0;wap2\">link 3</a>), though some fail to get off the ground at all.</li>\n<li>Poorly arranged prizes can induce wasted effort</li>\n<li>Judging quality can be a serious issue especially when monetary rewards are involved \n<ul>\n<li>potential for social conflict</li>\n<li>some people are better at dealing with social conflicts than others</li>\n<li>pre-designated&nbsp;arbiters more likely to be trusted than others</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Miscellaneous&nbsp;observations</h2>\n<ul style=\"font-family: Verdana, Arial, Helvetica, sans-serif; margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; \">\n<li>Working groups or otherwise close contact sometimes increase people's motivations via peer pressure.</li>\n<li>Personally requesting someone work on a project can increase their motivation to do so.</li>\n<li>With certain kinds of motivation you often get people agreeing to work on a project and then getting slightly stuck and delaying it indefinitely. (Patri Friedman has given&nbsp;<a href=\"/lw/z8/image_vs_impact_can_public_commitment_be\">one reason</a>&nbsp;why this might happen)</li>\n<li>Different incentives might work better/worse for different kinds of projects.&nbsp;</li>\n<li>Monitoring project progress could help motivation (it might also have other benefits, such as knowing when to rethink the project or to find another person to work on it).</li>\n<li>Splitting up a project into a number of small clear tasks that individuals can pick up and complete decreases the costs of working on projects. The very fact of announcing, specifying and taskifying a project can induce interest.&nbsp;</li>\n<li>Open projects (Wikipedia, open source projects) are often primarily worked on by a small group of highly dedicated contributors.</li>\n<li>Want to encourage quality</li>\n<li>sometimes something is better than nothing&nbsp;</li>\n<li>sometimes drafts and large output volume is useful for future work</li>\n<li>People most interested in the results of a project are not always the people best suited to do the project.&nbsp;</li>\n<li>High visibility projects&nbsp;</li>\n<li>Increase interest in working on projects&nbsp;</li>\n<li>Completed projects give social rewards to completors&nbsp;</li>\n<li>Completed projects serve as templates for future related projects</li>\n<li>Quantifying aggregate interest (both in terms of number and intensity) is useful for deciding what projects are most important&nbsp;</li>\n<li>Aggregating what skills potential project contributors have is useful for determining what projects are possible</li>\n</ul>\n<p>In the interest of Holding Off On Proposing Solutions, please take a moment to try to identify features of the problem space that I have not mentioned before reading the comments. Please mention any features you notice as well as any potential solutions or parts of solutions in the comments. I have some ideas, and I will propose them in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xdNQEXagwMTCbwwK4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 7.21770494040467e-07, "legacy": true, "legacyId": "7753", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Judging by the number of upvotes, Brandon Reinhart's&nbsp;<a href=\"/lw/5il/siai_an_examination/\">analysis of SIAI's financial filings</a>&nbsp;is valuable to quite a few people. Similar analysis' of Alcor and the Cryonics Institute would be quite valuable. There has been talk of more work on condensing LW content and placing it on the wiki. I'm sure lots of people would like to know about the literature on&nbsp;<a href=\"/r/discussion/lw/5qm/living_forever_is_hard_or_the_gompertz_curve/477u\">low dose asprin</a>.&nbsp;People seem to want a front page more accessible to newcomers. Will these projects get accomplished? Some of them, but probably fewer than optimal. I think we can do better.&nbsp;</p>\n<p>I would like to look for ways to channel group willingness to contribute to a project into focused individual willingness to work on a project.</p>\n<h2 id=\"Observations_about_the_problem_space\">Observations about the problem space</h2>\n<p>The following is based on discussions at the Seattle Less Wrong meetup.</p>\n<p>Many people would get a moderate amount of benefit from such projects, but only a small number would end up putting in the hard work to make them happen.&nbsp;</p>\n<p>The people most enthusiastic about a given project may not be the best people to work on the project. Perhaps they have very time consuming jobs or have a hard time being objective about the topic (e.g. someone who gets especially emotional about Cryonics) or have too many other projects already or perhaps they are intellectually motivated but not emotionally motivated by the project which might make it difficult to Get Things Done.&nbsp;</p>\n<p>Trying to generalize too early is a risk here. Going out and building fancy tools or otherwise trying something elaborate is probably not a good idea at first. Better to try some concrete trials first and learn from those experiences.</p>\n<h2 id=\"Sources_of_motivation\">Sources of motivation</h2>\n<p>There are three major potential sources of motivation:&nbsp;Money (the unit of caring), social status (Karma, kind words etc.), things (pizza, books, cookies,&nbsp;<a href=\"/lw/58u/specific_fiction_discusion_april_2011/3xqq\">pony pictures</a>).</p>\n<ul>\n<li>Money \n<ul>\n<li>Transfers of money (the&nbsp;<a href=\"/lw/65/money_the_unit_of_caring/\">unit of caring</a>) are often much more efficient than transfers of other goods.</li>\n<li>Extrinsic rewards (especially money) can reduce intrinsic motivation.&nbsp;</li>\n<li>Large monetary rewards can also make relationship between the project contributors and the project sponsors less social.&nbsp;</li>\n<li>Many Less Wrong people are high paid \n<ul>\n<li>Less likely to be motivated by small monetary rewards</li>\n<li>Have more money to contribute to projects.&nbsp;</li>\n<li>Not all Less Wrong people are high paid.&nbsp;</li>\n</ul>\n</li>\n<li>There are services for collecting donations (<a href=\"http://www.chipin.com/\">link</a>).</li>\n</ul>\n</li>\n<li>Social rewards \n<ul>\n<li>Praise&nbsp;</li>\n<li>Karma</li>\n<li>Social status</li>\n</ul>\n</li>\n<li>Things \n<ul>\n<li>Pizza, books, cookies,&nbsp;<a href=\"/lw/58u/specific_fiction_discusion_april_2011/3xqq\">pony pictures</a></li>\n</ul>\n</li>\n<li>Social pressure \n<ul>\n<li>requests</li>\n<li>progress monitoring</li>\n</ul>\n</li>\n</ul>\n<p>Different motivators may work better for different kinds of projects. For example, money might be a counterproductive motivator for social projects but a great motivator for setting up a website.</p>\n<h2 id=\"How_have_others_tackled_this_\">How have others tackled this?</h2>\n<p>This is a problem others face as well. How do other similar groups and communities&nbsp;ameliorate&nbsp;it?</p>\n<ul>\n<li>Intrinsic motivation \n<ul>\n<li>Conferring social status on those who do valuable work</li>\n</ul>\n</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Sprint_(software_development)\">Sprints</a>:&nbsp;several people get together in a single place and work together on a project for a couple of days. \n<ul>\n<li>Main draw seems to be Fun</li>\n<li>Frequently used by Python projects</li>\n</ul>\n</li>\n<li>Competition/bounties (<a href=\"http://www.mckinsey.com/app_media/reports/sso/and_the_winner_is.pdf\">McKinsey survey of prize literature</a>) \n<ul>\n<li>Provides social and/or material rewards</li>\n<li>Sometimes used on LW (<a href=\"/lw/3a2/100_for_the_best_article_on_efficient_charty_the/\">link 1</a>,&nbsp;<a href=\"/lw/5hy/cryonics_promotional_video_contest_10_btc_prize/\">link 2</a>,&nbsp;<a href=\"/lw/422/295_bounty_for_new_singularity_institute_logo/\">link 3</a>).</li>\n<li>Work seem well for some larger open source software projects (<a href=\"http://intrepidusgroup.com/insight/2011/03/bug-bounties-do-they-work/\">link 1</a>,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Open_source_bounty\">link 2</a>,&nbsp;<a href=\"http://forum.vectorlinux.com/index.php?topic=11745.0;wap2\">link 3</a>), though some fail to get off the ground at all.</li>\n<li>Poorly arranged prizes can induce wasted effort</li>\n<li>Judging quality can be a serious issue especially when monetary rewards are involved \n<ul>\n<li>potential for social conflict</li>\n<li>some people are better at dealing with social conflicts than others</li>\n<li>pre-designated&nbsp;arbiters more likely to be trusted than others</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Miscellaneous_observations\">Miscellaneous&nbsp;observations</h2>\n<ul style=\"font-family: Verdana, Arial, Helvetica, sans-serif; margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; \">\n<li>Working groups or otherwise close contact sometimes increase people's motivations via peer pressure.</li>\n<li>Personally requesting someone work on a project can increase their motivation to do so.</li>\n<li>With certain kinds of motivation you often get people agreeing to work on a project and then getting slightly stuck and delaying it indefinitely. (Patri Friedman has given&nbsp;<a href=\"/lw/z8/image_vs_impact_can_public_commitment_be\">one reason</a>&nbsp;why this might happen)</li>\n<li>Different incentives might work better/worse for different kinds of projects.&nbsp;</li>\n<li>Monitoring project progress could help motivation (it might also have other benefits, such as knowing when to rethink the project or to find another person to work on it).</li>\n<li>Splitting up a project into a number of small clear tasks that individuals can pick up and complete decreases the costs of working on projects. The very fact of announcing, specifying and taskifying a project can induce interest.&nbsp;</li>\n<li>Open projects (Wikipedia, open source projects) are often primarily worked on by a small group of highly dedicated contributors.</li>\n<li>Want to encourage quality</li>\n<li>sometimes something is better than nothing&nbsp;</li>\n<li>sometimes drafts and large output volume is useful for future work</li>\n<li>People most interested in the results of a project are not always the people best suited to do the project.&nbsp;</li>\n<li>High visibility projects&nbsp;</li>\n<li>Increase interest in working on projects&nbsp;</li>\n<li>Completed projects give social rewards to completors&nbsp;</li>\n<li>Completed projects serve as templates for future related projects</li>\n<li>Quantifying aggregate interest (both in terms of number and intensity) is useful for deciding what projects are most important&nbsp;</li>\n<li>Aggregating what skills potential project contributors have is useful for determining what projects are possible</li>\n</ul>\n<p>In the interest of Holding Off On Proposing Solutions, please take a moment to try to identify features of the problem space that I have not mentioned before reading the comments. Please mention any features you notice as well as any potential solutions or parts of solutions in the comments. I have some ideas, and I will propose them in the comments.</p>", "sections": [{"title": "Observations about the problem space", "anchor": "Observations_about_the_problem_space", "level": 1}, {"title": "Sources of motivation", "anchor": "Sources_of_motivation", "level": 1}, {"title": "How have others tackled this?", "anchor": "How_have_others_tackled_this_", "level": 1}, {"title": "Miscellaneous\u00a0observations", "anchor": "Miscellaneous_observations", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qqhdj3W3vSfB5E9ss", "ZpDnRCeef2CLEFeKM", "Rrg2sq75a5gBowgQb", "ojpjGWrBNSSFYiYG8", "S5YYkJdsjh6hpKLHn", "6ocujnKZL38thXn62"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-31T16:42:42.354Z", "modifiedAt": null, "url": null, "title": "Remaining human", "slug": "remaining-human", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:26.795Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tel", "createdAt": "2009-10-29T04:20:04.291Z", "isAdmin": false, "displayName": "tel"}, "userId": "g5N42nsQxJ3Q2jk7H", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3zQTWDwCLNZZyDhWw/remaining-human", "pageUrlRelative": "/posts/3zQTWDwCLNZZyDhWw/remaining-human", "linkUrl": "https://www.lesswrong.com/posts/3zQTWDwCLNZZyDhWw/remaining-human", "postedAtFormatted": "Tuesday, May 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Remaining%20human&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARemaining%20human%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3zQTWDwCLNZZyDhWw%2Fremaining-human%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Remaining%20human%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3zQTWDwCLNZZyDhWw%2Fremaining-human", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3zQTWDwCLNZZyDhWw%2Fremaining-human", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 399, "htmlBody": "<p>If our morality is <em>complex</em>&nbsp;and directly tied to what's human&mdash;if we're seeking to avoid building paperclip maximizers&mdash;how do you judge and quantify the danger in training yourself to become more rational if it should drift from being more human?</p>\n<hr />\n<p>My friend is a skeptical theist. She, for instance, scoffs mightily at Camping's little dilemma/psychosis but then argues from a position of comfort that Rapture it's a silly thing to predict because it's clearly stated that no one will know the day. And then she gives me a confused look because the psychological dissonance is clear.</p>\n<p>On one hand, my friend is in a prime position to take forward steps to self-examination and holding rational belief systems. On the other hand, she's an opera singer whose passion and profession require her to be able to empathize with and explore highly irrational human experiences. Since rationality is the art of <em>winning</em>, nobody can deny that the option that lets you have your cake and eat it too is best, but how do you navigate such a narrows?</p>\n<p>\n<hr />\n</p>\n<p>In another example, a <a href=\"/lw/5x8/teachable_rationality_skills/49e9\">recent comment thread</a>&nbsp;suggested the dangers of embracing human tendencies: catharsis might lead to promoting further emotional intensity. At the same time, catharsis is a well appreciated human communication strategy with roots in Greek stage. If rational action pulls you away from humanity, away from our complex morality, then how do we judge it worth doing?</p>\n<p>The most immediate resolution to this conundrum appears to me to be that human morality has no consistency constraint: we can want to be powerful and able to win while also want to retain our human tendencies which directly impinge on that goal. Is there a theory of metamorality which allows you to infer how such tradeoffs should be managed? Or is human morality, as a program, flawed with inconsistencies that lead to inescapable cognitive dissonance and dehumanization? If you interpret morality as a self-supporting strange loop, is it possible to have unresolvable, drifting interpretations based on how you focus you attentions?</p>\n<p>\n<hr />\n</p>\n<p>Dual to the problem of resolving a way forward is the problem of the interpreter. If there is a goal to at least marginally increase the rationality of humanity, but in order to discover the means to do so you have to become less capable of empathizing with and communicating with humanity, who acts as an interpreter between the two divergent mindsets?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3zQTWDwCLNZZyDhWw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7754", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-31T17:45:52.368Z", "modifiedAt": null, "url": null, "title": "Are the Sciences Better Than the Social Sciences For Training Rationalists?", "slug": "are-the-sciences-better-than-the-social-sciences-for", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.441Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2Mo5GmZzZdtaA3Get/are-the-sciences-better-than-the-social-sciences-for", "pageUrlRelative": "/posts/2Mo5GmZzZdtaA3Get/are-the-sciences-better-than-the-social-sciences-for", "linkUrl": "https://www.lesswrong.com/posts/2Mo5GmZzZdtaA3Get/are-the-sciences-better-than-the-social-sciences-for", "postedAtFormatted": "Tuesday, May 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20the%20Sciences%20Better%20Than%20the%20Social%20Sciences%20For%20Training%20Rationalists%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20the%20Sciences%20Better%20Than%20the%20Social%20Sciences%20For%20Training%20Rationalists%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Mo5GmZzZdtaA3Get%2Fare-the-sciences-better-than-the-social-sciences-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20the%20Sciences%20Better%20Than%20the%20Social%20Sciences%20For%20Training%20Rationalists%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Mo5GmZzZdtaA3Get%2Fare-the-sciences-better-than-the-social-sciences-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Mo5GmZzZdtaA3Get%2Fare-the-sciences-better-than-the-social-sciences-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 285, "htmlBody": "<p>\n<p class=\"MsoNoSpacing\">A <a href=\"http://online.wsj.com/article/SB10001424052702304520804576345632061434312.html?mod=WSJ_Opinion_LEFTTopOpinion\">Wall Street Journal article</a> by Harvard professor of government Harvey Mansfield claims that the social sciences and humanities are inferior to the sciences.<span style=\"mso-spacerun: yes;\">&nbsp; </span>The article implicitly urges undergraduates to major in science.<span style=\"mso-spacerun: yes;\">&nbsp; </span>From the article:</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<blockquote>\n<p class=\"MsoNoSpacing\">&ldquo;Science has knowledge of fact, and this makes it rigorous and hard.&rdquo;</p>\n</blockquote>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<blockquote>\n<p class=\"MsoNoSpacing\">&ldquo;Others try to imitate the sciences and call themselves &lsquo;social scientists.&rsquo; The best imitators of scientists are the economists. Among social scientists they rank highest in rigor, which means in mathematics... Just as Gender Studies taints the whole university with its sexless fantasies, so economists infect their neighbors with the imitation science they peddle. (Game theorists, I'm talking about you.)&rdquo;</p>\n</blockquote>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<p class=\"MsoNoSpacing\">Do you agree with this?<span style=\"mso-spacerun: yes;\">&nbsp; </span><a href=\"http://www.amazon.com/Game-Theory-Work-Outmaneuver-Competition/dp/0071400206/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1306863127&amp;sr=1-1\">As a game theorist</a> I probably have a rather biased view of the situation.<span style=\"mso-spacerun: yes;\">&nbsp; </span>It's certainly true that the ideal of the scientific method is vastly better than the practice of economists, but I think that majoring in economics provides better training for a rationalist than majoring in any of the sciences does.</p>\n<p class=\"MsoNoSpacing\">Economics explicitly considers what it means to be rational.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Although it infrequently considers ways in which humans are irrational, I'm under the impression that the hard sciences never do this.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Furthermore, because economists can almost never perform replicable experiments we have to rely on what everyone in the profession recognizes as messy data; therefore we&rsquo;re far more equipped than hard scientists to understand the limits of using statistical inference to draw conclusions from real world situations.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Although I have seen no data on this, I bet that a claim by nutritionists that they have found a strong causal link between some X and heart disease would be treated with far more skepticism by the average economist than the average hard scientist.</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2Mo5GmZzZdtaA3Get", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "7755", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-31T21:57:51.812Z", "modifiedAt": null, "url": null, "title": "This is a draft", "slug": "this-is-a-draft", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KPFippkXreQQhWzjp/this-is-a-draft", "pageUrlRelative": "/posts/KPFippkXreQQhWzjp/this-is-a-draft", "linkUrl": "https://www.lesswrong.com/posts/KPFippkXreQQhWzjp/this-is-a-draft", "postedAtFormatted": "Tuesday, May 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20This%20is%20a%20draft&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThis%20is%20a%20draft%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKPFippkXreQQhWzjp%2Fthis-is-a-draft%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=This%20is%20a%20draft%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKPFippkXreQQhWzjp%2Fthis-is-a-draft", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKPFippkXreQQhWzjp%2Fthis-is-a-draft", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9, "htmlBody": "<p>You're not supposed to be able to see this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KPFippkXreQQhWzjp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-31T23:23:32.998Z", "modifiedAt": null, "url": null, "title": "Generalizing From One Example & Evolutionary Game Theory", "slug": "generalizing-from-one-example-and-evolutionary-game-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:23.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3oZYqkQkHTmjmtocb/generalizing-from-one-example-and-evolutionary-game-theory", "pageUrlRelative": "/posts/3oZYqkQkHTmjmtocb/generalizing-from-one-example-and-evolutionary-game-theory", "linkUrl": "https://www.lesswrong.com/posts/3oZYqkQkHTmjmtocb/generalizing-from-one-example-and-evolutionary-game-theory", "postedAtFormatted": "Tuesday, May 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Generalizing%20From%20One%20Example%20%26%20Evolutionary%20Game%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGeneralizing%20From%20One%20Example%20%26%20Evolutionary%20Game%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3oZYqkQkHTmjmtocb%2Fgeneralizing-from-one-example-and-evolutionary-game-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Generalizing%20From%20One%20Example%20%26%20Evolutionary%20Game%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3oZYqkQkHTmjmtocb%2Fgeneralizing-from-one-example-and-evolutionary-game-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3oZYqkQkHTmjmtocb%2Fgeneralizing-from-one-example-and-evolutionary-game-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 411, "htmlBody": "<p>Back in April 2010 Robin Hanson wrote a post titled <a href=\"http://www.overcomingbias.com/2010/04/homo-hypocritus-signals.html\">Homo Hypocritus Signals</a>. Hal Finney <a href=\"http://www.overcomingbias.com/2010/04/homo-hypocritus-signals.html#comment-445612\">wrote a comment</a></p>\n<blockquote>\n<p>This reasoning does offer an explanation for why big brains might  have evolved, to help walk the line between acceptable and unacceptable  behavior. Still it seems like the basic puzzle remains: why is this  hypocrisy unconscious? Why do our conscious minds remain unaware of our  subconscious signaling?</p>\n</blockquote>\n<p>to which <a href=\"http://www.overcomingbias.com/2010/04/homo-hypocritus-signals.html#comment-445612\">Vladimir M responded</a>. This post is a short addendum to the discussion.</p>\n<p>In <a href=\"/lw/dr/generalizing_from_one_example/\">Generalizing From One Example</a> Yvain wrote</p>\n<blockquote>\n<p>There's some evidence that the usual method of interacting with people  involves something sorta like emulating them within our own brain. We  think about how we would react, adjust for the other person's  differences, and then assume the other person would react that way.</p>\n</blockquote>\n<p>It's plausible that the evolutionary pathway to developing an internal model of other people's minds involved bootstrapping from one's awareness of one's own mind. This would work well to the extent that there was <a href=\"/lw/rl/the_psychological_unity_of_humankind/\">psychological  unity of humankind</a>. In our evolutionary environment, people who interact with each other were more similar to one another than they are today.</p>\n<p>I don't understand many of the decision theory posts on Less Wrong,  but my impression is that the settings in which one is better off with  timeless decision theory or updateless decision theory than with casual  decision theory are situations in which the other agents have a good  model of the one's own internal wiring.</p>\n<p>This together with one's model of others being based on one's model of one's own mind and the psychological unity of humankind would push in the direction of the conscious mind adapting to something like timeless/updateless decision theory; based around cooperating with others. But the unconscious mind would then have free reign to push in the direction of defection (say, in one-shot prisoners' dilemma situations) because others would not have conscious access toward their own tendency toward defection and consequently would not properly emulate this tendency toward defection in their model of the other person.</p>\n<p>The analysis given here is overly simplistic; for example <a href=\"/lw/2jt/conflicts_between_mental_subagents_expanding_wei/2dbc\">quoting myself </a></p>\n<blockquote>\n<p>The conscious vs. unconscious division is not binary but gradualist.  There are aspects of one's thinking that one is very aware of, aspects  that one is somewhat aware of, aspects that one is obliquely aware of,  aspects that one could be aware of if one was willing to pay attention  to them, and aspects that one has no access to.</p>\n</blockquote>\n<p>but it rings true to me in some measure.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3oZYqkQkHTmjmtocb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 7.219023901055207e-07, "legacy": true, "legacyId": "7758", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ", "Cyj6wQLW6SeF6aGLy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-05-31T23:34:02.922Z", "modifiedAt": null, "url": null, "title": "West LA Weekly Meetups: Wednesday, June 8th", "slug": "west-la-weekly-meetups-wednesday-june-8th", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:26.103Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KF8thFYLZ9r3mEgMh/west-la-weekly-meetups-wednesday-june-8th", "pageUrlRelative": "/posts/KF8thFYLZ9r3mEgMh/west-la-weekly-meetups-wednesday-june-8th", "linkUrl": "https://www.lesswrong.com/posts/KF8thFYLZ9r3mEgMh/west-la-weekly-meetups-wednesday-june-8th", "postedAtFormatted": "Tuesday, May 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20West%20LA%20Weekly%20Meetups%3A%20Wednesday%2C%20June%208th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWest%20LA%20Weekly%20Meetups%3A%20Wednesday%2C%20June%208th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKF8thFYLZ9r3mEgMh%2Fwest-la-weekly-meetups-wednesday-june-8th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=West%20LA%20Weekly%20Meetups%3A%20Wednesday%2C%20June%208th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKF8thFYLZ9r3mEgMh%2Fwest-la-weekly-meetups-wednesday-june-8th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKF8thFYLZ9r3mEgMh%2Fwest-la-weekly-meetups-wednesday-june-8th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<p>ETA: <a href=\"/lw/6at/west_la_biweekly_meetups/\">Meetups are now every other Tuesday</a>.</p>\n<p><strong>When</strong>: 7pm - 9pm, Wednesday, June 8th.</p>\n<p><strong>Where</strong>: The <a href=\"http://maps.google.com/maps?q=10800+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\">Westside Pavillion</a> - on the bridge, which connects Nordstrom 3rd floor with Barnes &amp; Noble / Landmark Theatres 3rd floor.</p>\n<p><img src=\"http://images.lesswrong.com/t3_5zj_0.png?v=7c96cc847af6bc2b4999cff885e15461\" alt=\"\" width=\"511\" height=\"320\" /></p>\n<p><strong>Parking</strong> is free for 3 hours.</p>\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! If enough people want to change the time or place for future meetups, we will - send me a message.</p>\n<p><strong>I will be the guy with the map.</strong></p>\n<p>&nbsp;</p>\n<p>See also: <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine weekly meetups</a>, <a href=\"http://groups.google.com/group/LW-SoCal-Announce\">SoCal google group</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KF8thFYLZ9r3mEgMh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 7.219054886643337e-07, "legacy": true, "legacyId": "7759", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tHFu6kvy2HMvQBEhW", "pAHo9zSFXygp5A5dL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T00:09:43.010Z", "modifiedAt": null, "url": null, "title": "Proposal: Systematic Search for Useful Ideas", "slug": "proposal-systematic-search-for-useful-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HyD6kz9H2PLkNeP8P/proposal-systematic-search-for-useful-ideas", "pageUrlRelative": "/posts/HyD6kz9H2PLkNeP8P/proposal-systematic-search-for-useful-ideas", "linkUrl": "https://www.lesswrong.com/posts/HyD6kz9H2PLkNeP8P/proposal-systematic-search-for-useful-ideas", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%3A%20Systematic%20Search%20for%20Useful%20Ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%3A%20Systematic%20Search%20for%20Useful%20Ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHyD6kz9H2PLkNeP8P%2Fproposal-systematic-search-for-useful-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%3A%20Systematic%20Search%20for%20Useful%20Ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHyD6kz9H2PLkNeP8P%2Fproposal-systematic-search-for-useful-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHyD6kz9H2PLkNeP8P%2Fproposal-systematic-search-for-useful-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 798, "htmlBody": "<p>LessWrong is a font of good ideas, but the topics and interests usually expressed and explored here tend to cluster over few areas. As such, high-value topics may still be present for the community in other fields which can be systematically explored,&nbsp;<a href=\"/lw/1ul/for_progress_to_be_by_accumulation_and_not_by\">rather than waiting for a random encounter</a>. Additionally, there seems to be interest here in&nbsp;<a href=\"/r/discussion/lw/5ro/what_bothers_you_about_less_wrong/47iv\">examining a wider variety of topics</a>. In order to do this, I suggest creating a community list of areas to look into (besides the usual AI, Cog Sci, Comp Sci, Econ, Math, Philosophy, Psych, Statistics,&nbsp;etc.) and then reading a bit on the basics of these fields. In additional to potentially uncovering useful ideas&nbsp;per se, this also might offer the opportunity to populate the&nbsp;<a href=\"/lw/3gu/the_best_textbooks_on_every_subject\">textbooks resource list</a>&nbsp;and engage in not-random acts of <a href=\"/lw/4sn/costs_and_benefits_of_scholarship\">scholarship</a>.</p>\n<p>\n<h4>Everyone Split Up, There&rsquo;s a A Lot of Ideosphere to Cover</h4>\nA rough sketch of how I think the project will work follows. I&rsquo;ll be proceeding with this and tackling at least one or two subjects as long as there&rsquo;s at least a few other people interested in working on it too.</p>\n<p>Step 1, Community Evaluation: Using&nbsp;<a href=\"http://www.allourideas.org/\">All Our Ideas</a>&nbsp;or similar, generate a list of fields to investigate.<br />Step 2, Sign-Up: People have the best sense of what they already know and their abilities, so at this point anyone that wants to can pick a subject that&rsquo;s best for them to look into.<br />Step 3, Study: I imagine this will mostly involve self-directed reading of a handful of texts, watching some online videos, and&nbsp;maybe&nbsp;calling up one or two people -- in other words,&nbsp;nothing too dramatic. If a vein of something interesting is found, it&rsquo;s probably better that it&rsquo;s &ldquo;marked&rdquo; for further follow-up rather than&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar\">further examined alone</a>.<br />Step 4, Post: Some these investigations will not reveal anything -- that&rsquo;s actually a good thing (explained below); for these, a short &ldquo;Looked into it, nothing here&rdquo; sort of comment should suffice. Subjects with bigger findings should get bigger, more detailed comments/posts.</p>\n<p>\n<h4>Evaluation of Proposal</h4>\nAs a first step, I&rsquo;ll use a variation of the Heilmeier questions which is an (admittedly idiosyncratic) mix of the original version and&nbsp;<a href=\"/lw/4wu/the_heilmeier_questions\">gregv&rsquo;s enhanced version</a>.</p>\n<ul>\n<li><strong>What are you trying to do? Articulate your objectives using absolutely no jargon.</strong><br />Produce comments or posts providing very brief overviews of fields of knowledge, not previously discussed here, with notes pertaining to Less Wrong topics and interests.</li>\n<li><strong>Who cares? How many people will benefit?</strong><br /><em>This post is partially an attempt to determine that</em>, but there seems to be at least some interest in more variety on the site (see above). Additionally, the posts should be a good general resource for anyone that stumbles across them, and might even make&nbsp;<a href=\"/tag/seo\">good content for search purposes</a>.</li>\n<li><strong>Why hasn't someone already solved this problem? What makes you think what stopped them won't stop you?</strong><br />The idea is roughly&nbsp;<a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/XMeetsY\">book club meets Wikipedia</a>, but with an emphasis on creating a small evaluative body of knowledge rather than a massive descriptive encyclopedia, and with a LessWrong twist. The sharper focus should make the results more useful to go through than just hitting &ldquo;random page&rdquo; in yon encyclopedia.</li>\n<li><strong>How much have projects like this cost (time equivalent)?</strong><br />Some have the ability to take on &ldquo;<a href=\"/lw/5me/scholarship_how_to_do_it_efficiently\">whole fields of knowledge in mere weeks</a>&rdquo; but that&rsquo;s not typical -- investigating a subject in this case is roughly comparable in complexity to taking an introductory class or two, which people without any previous training normally accomplish over a period of about three to four months at a pace which is not especially strenuous, and with fairly light monetary costs beyond tuition/fees (which aren't applicable here).</li>\n<li><strong>What are the midterm and final \"exams\" to check for success?</strong><br />For each individual investigation, a good &ldquo;midterm&rdquo; check would be for the person looking into a field to have an list of resources or texts they&rsquo;re working on. The final &ldquo;exam&rdquo; is a posting indicating if anything useful or interesting was found, and if so, what.</li>\n<li><strong>If y [this community search] fails to solve x [uncover useful knowledge in fields previously under-examined on LessWrong], what would that teach you that you (hopefully) didn't know at the beginning?</strong><br />Quite possibly, this could be a good thing -- it indicates that the mix of topics on LessWrong is approximately right, and things can continue on. In this case, we&rsquo;d end up seeing a bunch of short &ldquo;nothing interesting here&rdquo; comments, and can rest more or less assured that further investigation into even more minute detail in unnecessarily. This is conditional on not-terrible scholarship and a reasonably good priority list from step 1.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HyD6kz9H2PLkNeP8P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 7.219160158021308e-07, "legacy": true, "legacyId": "7760", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>LessWrong is a font of good ideas, but the topics and interests usually expressed and explored here tend to cluster over few areas. As such, high-value topics may still be present for the community in other fields which can be systematically explored,&nbsp;<a href=\"/lw/1ul/for_progress_to_be_by_accumulation_and_not_by\">rather than waiting for a random encounter</a>. Additionally, there seems to be interest here in&nbsp;<a href=\"/r/discussion/lw/5ro/what_bothers_you_about_less_wrong/47iv\">examining a wider variety of topics</a>. In order to do this, I suggest creating a community list of areas to look into (besides the usual AI, Cog Sci, Comp Sci, Econ, Math, Philosophy, Psych, Statistics,&nbsp;etc.) and then reading a bit on the basics of these fields. In additional to potentially uncovering useful ideas&nbsp;per se, this also might offer the opportunity to populate the&nbsp;<a href=\"/lw/3gu/the_best_textbooks_on_every_subject\">textbooks resource list</a>&nbsp;and engage in not-random acts of <a href=\"/lw/4sn/costs_and_benefits_of_scholarship\">scholarship</a>.</p>\n<p>\n</p><h4 id=\"Everyone_Split_Up__There_s_a_A_Lot_of_Ideosphere_to_Cover\">Everyone Split Up, There\u2019s a A Lot of Ideosphere to Cover</h4>\nA rough sketch of how I think the project will work follows. I\u2019ll be proceeding with this and tackling at least one or two subjects as long as there\u2019s at least a few other people interested in working on it too.<p></p>\n<p>Step 1, Community Evaluation: Using&nbsp;<a href=\"http://www.allourideas.org/\">All Our Ideas</a>&nbsp;or similar, generate a list of fields to investigate.<br>Step 2, Sign-Up: People have the best sense of what they already know and their abilities, so at this point anyone that wants to can pick a subject that\u2019s best for them to look into.<br>Step 3, Study: I imagine this will mostly involve self-directed reading of a handful of texts, watching some online videos, and&nbsp;maybe&nbsp;calling up one or two people -- in other words,&nbsp;nothing too dramatic. If a vein of something interesting is found, it\u2019s probably better that it\u2019s \u201cmarked\u201d for further follow-up rather than&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar\">further examined alone</a>.<br>Step 4, Post: Some these investigations will not reveal anything -- that\u2019s actually a good thing (explained below); for these, a short \u201cLooked into it, nothing here\u201d sort of comment should suffice. Subjects with bigger findings should get bigger, more detailed comments/posts.</p>\n<p>\n</p><h4 id=\"Evaluation_of_Proposal\">Evaluation of Proposal</h4>\nAs a first step, I\u2019ll use a variation of the Heilmeier questions which is an (admittedly idiosyncratic) mix of the original version and&nbsp;<a href=\"/lw/4wu/the_heilmeier_questions\">gregv\u2019s enhanced version</a>.<p></p>\n<ul>\n<li><strong>What are you trying to do? Articulate your objectives using absolutely no jargon.</strong><br>Produce comments or posts providing very brief overviews of fields of knowledge, not previously discussed here, with notes pertaining to Less Wrong topics and interests.</li>\n<li><strong>Who cares? How many people will benefit?</strong><br><em>This post is partially an attempt to determine that</em>, but there seems to be at least some interest in more variety on the site (see above). Additionally, the posts should be a good general resource for anyone that stumbles across them, and might even make&nbsp;<a href=\"/tag/seo\">good content for search purposes</a>.</li>\n<li><strong>Why hasn't someone already solved this problem? What makes you think what stopped them won't stop you?</strong><br>The idea is roughly&nbsp;<a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/XMeetsY\">book club meets Wikipedia</a>, but with an emphasis on creating a small evaluative body of knowledge rather than a massive descriptive encyclopedia, and with a LessWrong twist. The sharper focus should make the results more useful to go through than just hitting \u201crandom page\u201d in yon encyclopedia.</li>\n<li><strong>How much have projects like this cost (time equivalent)?</strong><br>Some have the ability to take on \u201c<a href=\"/lw/5me/scholarship_how_to_do_it_efficiently\">whole fields of knowledge in mere weeks</a>\u201d but that\u2019s not typical -- investigating a subject in this case is roughly comparable in complexity to taking an introductory class or two, which people without any previous training normally accomplish over a period of about three to four months at a pace which is not especially strenuous, and with fairly light monetary costs beyond tuition/fees (which aren't applicable here).</li>\n<li><strong>What are the midterm and final \"exams\" to check for success?</strong><br>For each individual investigation, a good \u201cmidterm\u201d check would be for the person looking into a field to have an list of resources or texts they\u2019re working on. The final \u201cexam\u201d is a posting indicating if anything useful or interesting was found, and if so, what.</li>\n<li><strong>If y [this community search] fails to solve x [uncover useful knowledge in fields previously under-examined on LessWrong], what would that teach you that you (hopefully) didn't know at the beginning?</strong><br>Quite possibly, this could be a good thing -- it indicates that the mix of topics on LessWrong is approximately right, and things can continue on. In this case, we\u2019d end up seeing a bunch of short \u201cnothing interesting here\u201d comments, and can rest more or less assured that further investigation into even more minute detail in unnecessarily. This is conditional on not-terrible scholarship and a reasonably good priority list from step 1.</li>\n</ul>", "sections": [{"title": "Everyone Split Up, There\u2019s a A Lot of Ideosphere to Cover", "anchor": "Everyone_Split_Up__There_s_a_A_Lot_of_Ideosphere_to_Cover", "level": 1}, {"title": "Evaluation of Proposal", "anchor": "Evaluation_of_Proposal", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ufBYjpi9gK6uvtkh5", "xg3hXCYQPJkwHyik2", "TFbAMgeiCLjdX4Smw", "TyhxgRCD3F9cpanDL", "37sHjeisS9uJufi4u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T00:59:30.115Z", "modifiedAt": null, "url": null, "title": "Pluralistic Moral Reductionism", "slug": "pluralistic-moral-reductionism", "viewCount": null, "lastCommentedAt": "2021-06-25T12:39:47.889Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "lukeprog", "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3zDX3f3QTepNeZHGc/pluralistic-moral-reductionism", "pageUrlRelative": "/posts/3zDX3f3QTepNeZHGc/pluralistic-moral-reductionism", "linkUrl": "https://www.lesswrong.com/posts/3zDX3f3QTepNeZHGc/pluralistic-moral-reductionism", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pluralistic%20Moral%20Reductionism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APluralistic%20Moral%20Reductionism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3zDX3f3QTepNeZHGc%2Fpluralistic-moral-reductionism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pluralistic%20Moral%20Reductionism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3zDX3f3QTepNeZHGc%2Fpluralistic-moral-reductionism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3zDX3f3QTepNeZHGc%2Fpluralistic-moral-reductionism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4562, "htmlBody": "<p>Part of the sequence: <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">No-Nonsense Metaethics</a></p><blockquote>Disputes over the definition of morality... are disputes over words which raise no really significant issues. [Of course,] lack of clarity about the meaning of words is an important source of error\u2026 My complaint is that what should be regarded as something to be got out of the way in the introduction to a work of moral philosophy has become the subject matter of almost the whole of moral philosophy...</blockquote><blockquote><a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Singer-The-Triviality-of-the-Debate-Over-Is-Ought-and-the-Definition-of-Moral.pdf\">Peter Singer</a></blockquote><p>If a tree falls in the forest, and no one hears it, does it make a sound? If by &#x27;sound&#x27; you mean &#x27;acoustic vibrations in the air&#x27;, the answer is &#x27;Yes.&#x27; But if by &#x27;sound&#x27; you mean an auditory experience in the brain, the answer is &#x27;No.&#x27;</p><p>We might call this straightforward solution <em>pluralistic sound reductionism</em>. If people use the word &#x27;sound&#x27; to mean different things, and people have different intuitions about the meaning of the word &#x27;sound&#x27;, then <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/\">we needn&#x27;t endlessly debate which definition is &#x27;correct&#x27;</a>.1 We can be pluralists about the meanings of &#x27;sound&#x27;. </p><p>To facilitate communication, we can <a href=\"https://www.lesserwrong.com/lw/nu/taboo_your_words/\">taboo</a> and <a href=\"https://www.lesserwrong.com/lw/on/reductionism/\">reduce</a>: we can <a href=\"https://www.lesserwrong.com/lw/nv/replace_the_symbol_with_the_substance/\">replace the symbol with the substance</a> and talk about facts and <a href=\"https://www.lesserwrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">anticipations</a>, not definitions. We can avoid using the word &#x27;sound&#x27; and instead talk about &#x27;acoustic vibrations&#x27; or &#x27;auditory brain experiences.&#x27;</p><p>Still, some definitions can be <em>wrong</em>:</p><blockquote>Alex: If a tree falls in the forest, and no one hears it, does it make a sound?</blockquote><p>Austere MetaAcousticist: Tell me what you mean by &#x27;sound&#x27;, and I will tell you the answer.</p><p>Alex: By &#x27;sound&#x27; I mean &#x27;acoustic messenger fairies flying through the ether&#x27;.</p><p>Austere MetaAcousticist: There&#x27;s no such thing. Now, if you had asked me about this other definition of &#x27;sound&#x27;...</p><p>There are <a href=\"https://www.lesserwrong.com/lw/od/37_ways_that_words_can_be_wrong/\">other ways</a> for words to be wrong, too. But once we admit to multiple potentially useful reductions of &#x27;sound&#x27;, it is not hard to see how we could admit to multiple useful reductions of <em>moral</em> terms.</p><h1>Many Moral Reductionisms</h1><p>Moral terms are used in a greater variety of ways than sound terms are. There is little hope of arriving at the One True Theory of Morality by <a href=\"https://www.lesserwrong.com/lw/nr/the_argument_from_common_usage/\">analyzing common usage</a> or by <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/#moral_terms\">triangulating from the platitudes of folk moral discourse</a>. But we can use stipulation, and we can taboo and reduce. We can use <em>pluralistic moral reductionism</em>2 (for <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory#austere_empathic\">austere metaethics, not for empathic metaethics</a>).</p><p>Example #1:</p><blockquote>Neuroscientist <a href=\"http://www.amazon.com/dp/1439171211/\">Sam Harris</a>: Which is better? Religious totalitarianism or the Northern European welfare state?</blockquote><p>Austere Metaethicist: What do you mean by &#x27;better&#x27;?</p><p>Harris: By &#x27;better&#x27; I mean &#x27;that which tends to maximize the well-being of conscious creatures&#x27;.</p><p>Austere Metaethicist: Assuming we have similar reductions of &#x27;well-being&#x27; and &#x27;conscious creatures&#x27; in mind, the evidence I know of suggests that the Northern European welfare state is more likely to maximize the well-being of conscious creatures than religious totalitarianism.</p><p>Example #2:</p><blockquote>Philosopher <a href=\"http://commonsenseatheism.com/?p=15253\">Peter Railton</a>: Is capitalism the best economic system?</blockquote><blockquote>Austere Metaethicist: What do you mean by &#x27;best&#x27;?</blockquote><blockquote>Railton: By &#x27;best&#x27; I mean &#x27;would be approved of by an ideally instrumentally rational and fully informed agent considering the question \u2018How best to maximize the amount of non-moral goodness?&#x27; from a social point of view in which the interests of all potentially affected individuals are counted equally.</blockquote><blockquote>Austere Metaethicist: Assuming we agree on the meaning of &#x27;ideally instrumentally rational&#x27; and &#x27;fully informed&#x27; and &#x27;agent&#x27; and &#x27;non-moral goodness&#x27; and a few other things, the evidence I know of suggests that capitalism would not be approved of by an ideally instrumentally rational and fully informed agent considering the question \u2018How best to maximize the amount of non-moral goodness?&#x27; from a social point of view in which the interests of all potentially affected individuals were counted equally.</blockquote><p>Example #3:</p><blockquote>Theologian <a href=\"http://www.reasonablefaith.org/site/News2?page=NewsArticle&id=8215\">Bill Craig</a>: Ought we to give 50% of our income to efficient charities?</blockquote><blockquote>Austere Metaethicist: What do you mean by &#x27;ought&#x27;?</blockquote><blockquote>Craig: By &#x27;ought&#x27; I mean &#x27;approved of by an essentially just and loving God&#x27;.</blockquote><blockquote>Austere Metaethicist: Your definition doesn&#x27;t connect to reality. <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/46ma\">It&#x27;s like</a> talking about atom-for-atom &#x27;indexical identity&#x27; even though the world <a href=\"https://www.lesserwrong.com/lw/qx/timeless_identity/\">is</a> made of configurations and amplitudes instead of Newtonian billiard balls. <a href=\"https://www.lesserwrong.com/lw/11m/atheism_untheism_antitheism\">Gods don&#x27;t exist.</a></blockquote><p>But before we get to empathic metaethics, let&#x27;s examine the <a href=\"https://www.lesserwrong.com/lw/5eh/what_is_metaethics/\">standard problems</a> of metaethics using the framework of pluralistic moral reductionism.</p><h1>Cognitivism vs. Noncognitivism</h1><p>One standard debate in metaethics is cognitivism vs. noncognitivism. Alexander Miller explains:</p><blockquote>Consider a particular moral judgement, such as the judgement that murder is wrong. What sort of psychological state does this express? Some philosophers, called cognitivists, think that a moral judgement such as this expresses a belief.</blockquote><blockquote>Beliefs can be true or false: they are truth-apt, or apt to be assessed in terms of truth and falsity. So cognitivists think that moral judgements are capable of being true or false. </blockquote><blockquote>On the other hand, non-cognitivists think that moral judgements express non-cognitive states such as emotions or desires. Desires and emotions are not truth-apt. So moral judgements are not capable of being true or false.3</blockquote><p>But why should we expect all people to use moral judgments like &quot;Stealing is wrong&quot; to express the same thing?4</p><p>Some people who say &quot;Stealing is wrong&quot; are <em>really</em> just trying to express emotions: &quot;Stealing? Yuck!&quot; Others use moral judgments like &quot;Stealing is wrong&quot; to express commands: &quot;Don&#x27;t steal!&quot; Still others use moral judgments like &quot;Stealing is wrong&quot; to assert factual claims, such as &quot;stealing is against the will of God&quot; or &quot;stealing is a practice that usually adds pain rather than pleasure to the world.&quot;</p><p>It may be interesting to study all such uses of moral discourse, but this post focuses on addressing <em>cognitivists</em>, who use moral judgments to assert factual claims. We ask: Are those claims true or false? What are their implications?</p><h1>Objective vs. Subjective Morality</h1><p>Is morality objective or subjective? It depends which moral reductionism you have in mind, and what you mean by &#x27;objective&#x27; and &#x27;subjective&#x27;.</p><p>Here are some common5 uses of the objective/subjective distinction in ethics:</p><ul><li>Moral facts are objective1 if they are made true or false by mind-independent facts, otherwise they are subjective1.</li><li>Moral facts are objective2 if they are made true or false by facts independent of the opinions of sentient beings, otherwise they are subjective2.</li><li>Moral facts are objective3 if they are made true or false by facts independent of the opinions of humans, otherwise they are subjective3.</li></ul><p>Now, consider Harris&#x27; reduction of morality to facts about the well-being of conscious creatures. His theory of morality is objective3 and objective2, because facts about well-being are independent of anyone&#x27;s opinion. Even if the Nazis had won WWII and brainwashed everybody to have the opinion that torturing Jews was moral, it would remain true that torturing Jews does not increase the average well-being of conscious creatures. But Harris&#x27; theory of morality is not objective1, because facts about the well-being of conscious creatures are mind-<em>dependent</em> facts.</p><p>Or, consider Craig&#x27;s theory of morality in terms of divine approval. His theory doesn&#x27;t connect to reality, but still: is it objective or subjective? Craig&#x27;s theory says that moral facts are objective3, because they don&#x27;t depend on human opinion (God isn&#x27;t human). But his theory <em>doesn&#x27;t</em> say that morality is objective2 or objective1, because for him, moral facts depend on the opinion of a sentient being: God.</p><p>A warning: ambiguous terms like &#x27;objective&#x27; and &#x27;subjective&#x27; are attractors for <a href=\"https://www.lesserwrong.com/lw/ny/sneaking_in_connotations/\">sneaking in connotations</a>. Craig himself provides an example. In his writings and public appearances, Craig insists that only God-based morality can be objective.6 What does he mean by &#x27;objective&#x27;? On a single page,7 he uses &#x27;objective&#x27; to mean &quot;independent of <em>people&#x27;s</em> opinions&quot; (objective2) and also to mean &quot;independent of <em>human</em> opinion&quot; (objective3). I&#x27;ll assume he means that only God-based morality can be objective3, because God-based morality is clearly not objective2 (Craig&#x27;s God is a <em>person</em>, a sentient being).</p><p>And yet, Craig says that we need God in order to have objective3 morality as if this should be a <em>big deal</em>. But hold on. Even a moral code defined in terms of the preferences of <a href=\"http://en.wikipedia.org/wiki/Washoe_(chimpanzee)\">Washoe the chimpanzee</a> is objective3. So not only is Bill&#x27;s claim that only God-based morality can be objective3 <em>false </em>(because Harris&#x27; moral theory is also objective3), but also it&#x27;s trivially easy to come up with a moral theory that is &#x27;objective&#x27; in <em>Craig&#x27;s</em> (apparent) sense of the term (that is, objective3).8</p><p>Moreover, Harris&#x27; theory of morality is objective in a &#x27;stronger&#x27; sense than Craig&#x27;s theory of morality is. Harris&#x27; theory is objective3 and objective2, while Craig&#x27;s theory is merely objective3. Whether he&#x27;s doing it consciously or not, I wonder if Craig is using the word &#x27;objective&#x27; to try to <a href=\"https://www.lesserwrong.com/lw/ny/sneaking_in_connotations/\">sneak in connotations</a> that don&#x27;t actually apply to his claims once you pay attention to what Craig <em>actually</em> means by the word &#x27;objective&#x27;. If Craig told his audience that we need God for morality to be &#x27;objective&#x27; in the same sense that morality defined in terms of the preferences of a chimpanzee is &#x27;objective&#x27;, would this still still have his desired effect on his audience? I doubt it.</p><p>Once you&#x27;ve stipulated your use of &#x27;objective&#x27; and &#x27;subjective&#x27;, it is often trivial to determine whether a given moral reductionism is &#x27;objective&#x27; or &#x27;subjective&#x27;. But what of it? What force should those words carry after you&#x27;ve <a href=\"https://www.lesserwrong.com/lw/nu/taboo_your_words/\">tabooed</a> them? Be careful not to sneak in connotations that don&#x27;t belong.</p><h1>Relative vs. Absolute Morality</h1><p>Is morality relative or absolute? Again, it depends which moral reductionism you have in mind, and what you mean by &#x27;relative&#x27; and &#x27;absolute&#x27;. Again, we must be careful about sneaking in connotations.</p><h2>Moore&#x27;s Open Question Argument</h2><p>&quot;He&#x27;s an unmarried man, but is he a bachelor?&quot; This is a &#x27;closed&#x27; question. The answer is obviously &quot;Yes.&quot;</p><p>In contrast, said G.E. Moore, all questions of the type &quot;Such and such is X, but is it good?&quot; are <em>open</em> questions. It feels like you can always ask, &quot;Yes, but is it good?&quot; In this way, Moore resists the identification of &#x27;morally good&#x27; with any set of natural facts. This is Moore&#x27;s <a href=\"http://en.wikipedia.org/wiki/Open_Question_Argument\">Open Question Argument</a>. Because some moral reductionisms <em>do</em> identify &#x27;good&#x27; or &#x27;right&#x27; with a particular X, those reductionisms had better have an answer to Moore.</p><p>The Yudkowskian response is to point out that when cognitivists use the term &#x27;good&#x27;, their intuitive notion of &#x27;good&#x27; is captured by a massive logical function that can&#x27;t be expressed in simple statements like &quot;maximize pleasure&quot; or &quot;act only in accordance with maxims you could wish to be a universal law without contradiction.&quot; Even if you <em>think</em> everything you want (or rather, <em>want</em> to want) can be realized by (say) <a href=\"http://www.amazon.com/Moral-Landscape-Science-Determine-Values/dp/1439171211/\">maximizing the well-being of conscious creatures</a>, you&#x27;re wrong. <a href=\"https://www.lesserwrong.com/lw/ld/the_hidden_complexity_of_wishes/\">Your values are more complex than that</a>, and <a href=\"https://www.lesserwrong.com/lw/5sk/inferring_our_desires/\">we can&#x27;t see</a> the structure of our values. <em>That</em> is why it feels like an open question remains no matter which simplistic identification of &quot;Good = X&quot; you choose.</p><p>The problem is not that there <em>is</em> no way to identify &#x27;good&#x27; or &#x27;right&#x27; (as used intuitively, without tabooing) with a certain X. The problem is that <a href=\"https://www.lesserwrong.com/lw/sm/the_meaning_of_right/\">X is huge and complicated</a> and we don&#x27;t (yet) have access to its structure.</p><p>But that&#x27;s the response to Moore <em>after</em> <a href=\"https://www.lesserwrong.com/lw/oh/righting_a_wrong_question/\">righting a wrong question</a> - that is, when doing <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory#austere_empathic\">empathic metaethics</a>. When doing mere <em>pluralistic moral reductionism</em>, Moore&#x27;s argument doesn&#x27;t apply. If we taboo and reduce, then the question of &quot;...but is it good?&quot; is out of place. The reply is: &quot;Yes it is, because I just told you <em>that&#x27;s what I mean to communicate</em> when I use the word-tool &#x27;good&#x27; for this discussion. I&#x27;m not here to <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/\">debate definitions</a>; I&#x27;m here to get something done.&quot;9</p><h2>The Is-Ought Gap</h2><p>(This section <a href=\"https://www.lesserwrong.com/lw/5u2/pluralistic_moral_reductionism/4cli\">rewritten</a> for clarity.)</p><p>Many claim that you cannot infer an &#x27;ought&#x27; statement from a series of &#x27;is&#x27; statements. The objection comes from Hume, who said he was surprised whenever an argument made of <em>is</em> and <em>is not</em> propositions suddenly shifted to an <em>ought</em> or <em>ought not</em> claim, without explanation.10</p><p>The solution is to make explicit the bridge from &#x27;ought&#x27; statements to &#x27;is&#x27; statements.</p><p>Perhaps the arguer means something non-natural by &#x27;ought&#x27;, such as &#x27;commanded by God&#x27; or &#x27;in accord with irreducible, non-natural facts about goodness&#x27; (see <a href=\"http://plato.stanford.edu/entries/moral-non-naturalism/\">Moore</a>). If so, I would reject that premise of the argument, because I&#x27;m a reductionist. At this point, our discussion might need to shift to a debate over the merits of <a href=\"https://www.lesserwrong.com/lw/on/reductionism/\">reductionism</a>.</p><p>Or perhaps by &#x27;you ought to X&#x27; the arguer means something fully natural, such as:</p><ul><li>&quot;X is obligatory (by <a href=\"http://plato.stanford.edu/entries/logic-deontic/\">deontic logic</a>) if you assume axiomatic imperatives Y and Z.&quot;</li><li>Or: &quot;X tends to maximizes <a href=\"http://www.scholarpedia.org/article/Reward_signals\">reward signals</a> in agents exhibiting <a href=\"http://en.wikipedia.org/wiki/Multiple_Drafts_Model\">multiple-drafts consciousness</a>&quot; (or, as Sam Harris more broadly <a href=\"http://www.amazon.com/Moral-Landscape-Science-Determine-Values/dp/1439171211/\">puts it</a>, &quot;X tends to maximize well-being in conscious creatures&quot;).</li><li>Or: &quot;X is what a Bayes-rational and Hubble-volume-omniscient agent would do if it was motivated to maximize the amount of non-moral goodness from a view in which the interests of all potentially affected individuals were counted equally, where &#x27;non-moral goodness&#x27; refers to what an agent would want if it were he to contemplate its present situation from a standpoint fully and vividly informed about itself and its circumstances, and entirely free of cognitive error or lapses of instrumental rationality&quot; (see <a href=\"http://commonsenseatheism.com/?p=15253\">Railton&#x27;s</a> <a href=\"http://commonsenseatheism.com/?p=15264\">metaethics</a>).</li><li>Or: &quot;X maximizes the complicated function that can be computed by extrapolating (in a particular way) the motivations encoded by my brain&quot; (see <a href=\"http://intelligence.org/upload/CEV.html\">CEV</a>).</li><li>Or: &quot;[insert here whatever statement, if believed, would motivate one to do X]&quot; (see <a href=\"https://www.lesserwrong.com/lw/64j/a_defense_of_naive_metaethics/4cce\">Will Sawin</a>).</li></ul><p>Or, the speaker may have in mind a common ought-reductionism known as the <em>hypothetical imperative</em>. This is an ought of the kind: &quot;<em>If</em> you desire to lose weight, <em>then</em> you <em>ought</em> to consume fewer calories than your burn.&quot; (But usually, people leave off the implied <em>if</em> statement, and simply say &quot;You should eat less and exercise more.&quot;)</p><p>A hypothetical imperative (as some use it) can be translated from &#x27;ought&#x27; to &#x27;is&#x27; in a straightforward way: &quot;If you desire to lose weight, then you <em>ought</em> to consume fewer calories than you burn&quot; translates to the claim &quot;If you consume fewer calories than you burn, then you <em>will</em> (or are, <em><a href=\"http://www.hss.cmu.edu/philosophy/glymour/glymour2002.pdf\">ceteris paribus</a></em>, more likely to) fulfill your desire to lose weight.&quot;11</p><p>Or, the speaker may be using &#x27;ought&#x27; to communicate something only about other symbols (example: Bayes&#x27; Rule), leaving the bridge from &#x27;ought&#x27; to &#x27;is&#x27; to be built when the logical function represented by his use of &#x27;ought&#x27; is plugged into a theory that refers to the world.</p><p>But one must not fall into the trap of thinking that a definition you&#x27;ve stipulated (aloud or in your head) for &#x27;ought&#x27; must match up to your intended meaning of &#x27;ought&#x27; (to which you don&#x27;t have introspective access). In fact, I suspect it <em>never</em> does, which is why the <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/\">conceptual analysis</a> of &#x27;ought&#x27; language can <a href=\"https://www.lesserwrong.com/lw/64j/a_defense_of_naive_metaethics/4hi7\">go in circles</a> for centuries, and why any stipulated meaning of &#x27;ought&#x27; is a <a href=\"https://www.lesserwrong.com/lw/lq/fake_utility_functions/\">fake utility function</a>. To see clearly to our intuitive concept of ought, we&#x27;ll have to try empathic metaethics (see below).</p><p>But whatever our intended meaning of &#x27;ought&#x27; is, the same reasoning applies. Either our intended meaning of &#x27;ought&#x27; refers (eventually) to the world of math and physics (in which case the is-ought gap is bridged), or else it doesn&#x27;t (in which case it fails to refer).12</p><h1>Moral realism vs. Anti-realism</h1><p>So, does all this mean that we can embrace moral realism, or does it doom us to moral anti-realism? Again, it depends on what you mean by &#x27;realism&#x27; and &#x27;anti-realism&#x27;.</p><p>In a sense, pluralistic moral reductionism can be considered a robust form of moral &#x27;realism&#x27;, in the same way that pluralistic sound reductionism is a robust form of <em>sound realism</em>. &quot;Yes, there really <em>is</em> sound, and we can locate it in reality \u2014 either as vibrations in the air or as mental auditory experiences, however you are using the term.&quot; In the same way: &quot;Yes, there really <em>is</em> morality, and we can locate it in reality \u2014 either as a set of facts about the well-being of conscious creatures, or as a set of facts about what an ideally rational and perfectly informed agent would prefer, or as some other set of natural facts.&quot;</p><p>But in another sense, pluralistic moral reductionism is &#x27;anti-realist&#x27;. It suggests that there is no One True Theory of Morality. (We use moral terms in a variety of ways, and some of those ways refer to different sets of natural facts.) And as a reductionist approach to morality, it might also leave no room for moral theories which say there are <em>universally binding</em> moral rules for which the universe (e.g. via a God) will hold us accountable.</p><p>What matters are the facts, not whether labels like &#x27;realism&#x27; or &#x27;anti-realism&#x27; apply to &#x27;morality&#x27;.</p><h1>Toward Empathic Metaethics</h1><p>But pluralistic moral reductionism satisfies only a would-be austere metaethicist, not an empathic metaethicist.</p><p><a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory#austere_empathic\">Recall that</a> when Alex asks how she can do what is right, the Austere Metaethicist replies:</p><blockquote>Tell me what you mean by &#x27;right&#x27;, and I will tell you what is the right thing to do. If by &#x27;right&#x27; you mean X, then Y is the right thing to do. If by &#x27;right&#x27; you mean P, then Z is the right thing to do. But if you can&#x27;t tell me what you mean by &#x27;right&#x27;, then you have failed to ask a coherent question, and no one can answer an incoherent question.</blockquote><p>Alex may reply to the Austere Metaethicist:</p><blockquote>Okay, I&#x27;m not sure exactly what I mean by &#x27;right&#x27;. So how do I do what is right if I&#x27;m not sure what I mean by &#x27;right&#x27;?</blockquote><p>The Austere Metaethicist refuses to answer this question. The Empathic Metaethicist, however, is willing to go the extra mile. He says to Alex:</p><blockquote>You may not know what you mean by &#x27;right.&#x27; But let&#x27;s not stop there. Here, let me come alongside you and help decode the cognitive algorithms that generated your question in the first place, and then we&#x27;ll be able to answer your question. Then we can tell you what the right thing to do is.</blockquote><p>This may seem like too much work. Would we be motivated to decode the cognitive algorithms producing Albert and Barry&#x27;s use of the word &#x27;sound&#x27;? Would we try to solve &#x27;empathic meta-acoustics&#x27;? Probably not. We can simply taboo and reduce &#x27;sound&#x27; and then get some work done.</p><p>But moral terms and value terms are about what we <em>want</em>. And unfortunately, we <a href=\"https://www.lesserwrong.com/lw/5sk/inferring_our_desires/\">often</a> don&#x27;t <em>know</em> what we want. As such, we&#x27;re unlikely to get what we <em>really</em> want if the world is <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">re-engineered</a> in accordance with our current best <em>guess</em> as to what we want. That&#x27;s why we need to decode the cognitive algorithms that generate our questions about value and morality.</p><p>So how can the Empathic Metaethicist answer Alex&#x27;s question? We don&#x27;t know the details yet. For example, we don&#x27;t have a completed cognitive neuroscience. But we have some ideas, and we know of some open problems that may admit of progress once more people understand them. In the next few posts, we&#x27;ll take our first look at empathic metaethics.13</p><p>Previous post: <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/\">Conceptual Analysis and Moral Theory</a></p><h2>Notes</h2><p>1 Some have <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/48b4\">objected</a> that the conceptual analysis argued against in <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/\">Conceptual Analysis and Moral Theory</a> is not just a battle over definitions. But a definition <a href=\"http://dictionary.reference.com/browse/definition\">is</a> &quot;the formal statement of the meaning or significance of a word, phrase, etc.&quot;, and a conceptual analysis is (usually) a &quot;formal statement of the meaning or significance of a word, phrase, etc.&quot; in terms of necessary and sufficient conditions. The goal of a conceptual analysis is to arrive at a definition for a term that captures our intuitions about its meaning. The process is to bash our intuitions against others&#x27; intuitions until we converge upon a set of necessary and sufficient conditions that captures them all. But consider Barry and Albert&#x27;s debate over the definition of &#x27;sound&#x27;. Why think Albert and Barry have the same concept in mind? Words mean slightly different things in different cultures, subcultures, and small communities. We develop different intuitions about their meaning based on divergent life experiences. Our intuitions differ from each other&#x27;s due to the specifics of <a href=\"https://www.lesserwrong.com/lw/59v/intuition_and_unconscious_learning/\">unconscious associative learning</a> and <a href=\"https://www.lesserwrong.com/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">attribution substitution heuristics</a>. What is the point of bashing our intuitions about the meaning of terms against each other for thousands of pages, in the hopes that we&#x27;ll converge on a precise set of necessary and sufficient conditions? Even if we can get Albert and Barry to agree, what happens when Susan wants to use the same term, but has slightly differing intuitions about its meaning? And, let&#x27;s say we arrive at a messy set of 6 necessary and sufficient conditions for the intuitive meaning of the term. Is that going to be as <a href=\"https://www.lesserwrong.com/lw/o0/where_to_draw_the_boundary/\">useful for communication</a> as one we consciously chose because it <a href=\"https://www.lesserwrong.com/lw/nl/the_cluster_structure_of_thingspace/\">carved-up thingspace well</a>? I doubt it. The IAU&#x27;s <a href=\"http://api.viglink.com/api/click?format=go&key=9f37ca02a1e3cbd4f3d0a3618a39fbca&loc=http%3A%2F%2Flesswrong.com%2Flw%2F5kn%2Fconceptual_analysis_and_moral_theory%2F&v=1&libid=1306248621230&out=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FIAU_definition_of_planet&ref=http%3A%2F%2Fwww.google.com%2Fsearch%3Fsourceid%3Dchrome%26ie%3DUTF-8%26q%3Dconceptual%2Banalysis%2Band%2Bmoral%2Btheory&title=Conceptual%20Analysis%20and%20Moral%20Theory%20-%20Less%20Wrong&txt=IAU%27s%20definition%20of%20%27planet%27\">definition of &#x27;planet&#x27;</a> is more useful than the folk-intuitions definition of &#x27;planet&#x27;. Folk intuitions about &#x27;planet&#x27; evolved over thousands of years and different people have different intuitions which may not always converge. In 2006, the IAU used modern astronomical knowledge to carve up thingspace in a more useful and informed way than our intuitions do.</p><p>A passage from Bertrand Russell (1953) is appropriate. Russell said that many philosophers reminded him of</p><blockquote>the shopkeeper of whom I once asked the shortest way to Winchester. He called to a man in the back premises:</blockquote><p>&quot;Gentleman wants to know the shortest way to Winchester.&quot;</p><p>&quot;Winchester?&quot; an unseen voice replied.</p><p>&quot;Aye.&quot;</p><p>&quot;Way to Winchester?&quot;</p><p>&quot;Aye.&quot;</p><p>&quot;Shortest way?&quot;</p><p>&quot;Aye.&quot;</p><p>&quot;Dunno.&quot;</p><p>He wanted to get the nature of the question clear, but took no interest in answering it. This is exactly what modern philosophy does for the earnest seeker after truth. Is it surprising that young people turn to other studies?</p><p>2 Compare also to the biologist&#x27;s &#x27;species concept pluralism&#x27; and the philosopher&#x27;s &#x27;art concept pluralism.&#x27; See Uidhir &amp; Magnus (2011). Also see &#x27;causal pluralism&#x27; (Godfrey-Smith, 2009; Cartwright, 2007), &#x27;theory concept pluralism&#x27; (Magnus, 2009) and, especially, &#x27;metaethical contextualism&#x27; (Bjornsson &amp; Finlay, 2010) or &#x27;metaethical pluralism&#x27; or &#x27;metaethical ambivalence&#x27; (Joyce, 2011). Joyce quotes Lewis (1989), who wrote that some concepts of value refer to things that really exist, and some concepts don&#x27;t, and what you make of this situation is largely a matter of temperament:</p><blockquote>What to make of the situation is mainly a matter of temperament. You can bang the drum about how philosophy has uncovered a terrible secret: there are no values! ... Or you can think it better for public safety to keep quiet and hope people will go on as before. Or you can declare that there are no values, but that nevertheless it is legitimate\u2014and not just expedient\u2014for us to carry on with value-talk, since we can make it all go smoothly if we just give the name of value to claimants that don&#x27;t quite deserve it... Or you can think it an empty question whether there are values: say what you please, speak strictly or loosely. When it comes to deserving a name, there&#x27;s better and worse but who&#x27;s to say how good is good enough? Or you can think it clear that the imperfect deservers of the name are good enough, but only just, and say that although there are values we are still terribly wrong about them. Or you can calmly say that value (like simultaneity) is not quite as some of us sometimes thought. Myself, I prefer the calm and conservative responses. But as far as the analysis of value goes, they&#x27;re all much of a muchness.</blockquote><p>Joyce concludes that, for example, the moral naturalist and the moral error theorist may agree with each other (when adopting each other&#x27;s own language):</p><blockquote>[Metaethical ambivalence] begins with a kind of metametaethical enlightenment. The moral naturalist espouses moral naturalism, but this espousal reflects a mature decision, by which I mean that the moral naturalist doesn&#x27;t claim to have latched on to an incontrovertiblerealm of moral facts of which the skeptic is foolishly ignorant, but rather acknowledges that this moral naturalism has been achieved only via a non-mandatory piece of conceptual precisification. Likewise, the moral skeptic champions moral skepticism, but this too is a sophisticated verdict: not the simple declaration that there are no moral values and that the naturalist is gullibly uncritical, but rather a decision that recognizes that this skepticism has been earned only by making certain non-obligatory but permissible conceptual clarifications.</blockquote><p>...The enlightened moral naturalist doesn&#x27;t merely (grudgingly) admit that the skeptic is warranted in his or her views, but is able to adopt the skeptical position in order to gain the insights that come from recognizing that we live in a world without values. And the enlightened moral skeptic goes beyond (grudgingly) conceding that moral naturalism is reasonable, but is capable of assuming that perspective in order to gain whatever benefits come from enjoying epistemic access to a realm of moral facts.</p><p>3 Miller (2003), p. 3.</p><p>4 I changed the example moral judgment from &quot;murder is wrong&quot; to &quot;stealing is wrong&quot; because the former invites confusion. &#x27;Murder&#x27; often means <em>wrongful</em> killing.</p><p>5 Also see Jacobs (2002), starting on p. 2.</p><p>6 The first premise of one of <a href=\"http://thegospelcoalition.org/pdf-articles/Craig_Atheism.pdf\">his favorite arguments for God&#x27;s existence</a> is &quot;If God does not exist, objective moral values and duties do not exist.&quot;</p><p>7 Craig (2010), p. 11.</p><p>8 It&#x27;s also possible that Craig intended a different sense of objective than the ones explicitly given in his article. Perhaps he meant objective4: &quot;morality is objective4 if it is not grounded in the opinion of non-divine persons.&quot;</p><p>9 Also see <a href=\"http://commonsenseatheism.com/?p=15336\">Moral Reductionism and Moore&#x27;s Open Question Argument</a>.</p><p>10 Hume (1739), p. 469. The famous paragraph is:</p><blockquote>In every system of morality, which I have hitherto met with, I have always remarked, that the author proceeds for some time in the ordinary ways of reasoning, and establishes the being of a God, or makes observations concerning human affairs; when all of a sudden I am surprised to find, that instead of the usual copulations of propositions, is, and is not, I meet with no proposition that is not connected with an <em>ought</em>, or an ought not. This change is imperceptible; but is however, of the last consequence. For as this ought, or ought not, expresses some new relation or affirmation, &#x27;tis necessary that it should be observed and explained; and at the same time that a reason should be given; for what seems altogether inconceivable, how this new relation can be a deduction from others, which are entirely different from it.</blockquote><p>11 For more on reducing certain kinds of normative statements, see Finlay (2010).</p><p>12 Assuming reductionism is true. If reductionism is false, then of course there are problems for pluralistic moral reductionism as a theory of austere (but not empathic) metaethics. The clarifications in the last three paragraphs of this section are due to discussions with Wei Dai and Vladimir Nesov.</p><p>13 My thanks to Steve Rayhawk and Will Newsome for their feedback on early drafts of this post.</p><h2>References</h2><p>Bjornsson &amp; Finlay (2010). <a href=\"http://peasoup.typepad.com/peasoup/files/MetaethicalContextualismDefended.pdf\">Metaethical contextualism defended</a>. <em>Ethics, 121</em>: 7-36.</p><p>Craig (2010). <a href=\"http://thegospelcoalition.org/blogs/tgc/2010/02/04/william-lane-craig-five-arguments-for-god/\">Five Arguments for God</a>. The Gospel Coalition.</p><p>Cartwright (2007).<em><a href=\"http://www.amazon.com/Hunting-Causes-Using-Them-Approaches/dp/052167798X/\">Hunting Causes and Using Them: Approaches in Philosophy and Economics</a></em>. Cambridge University Press.</p><p>Godfrey-Smith (2009). <a href=\"http://www.people.fas.harvard.edu/~pgs/CausalPluralismPGS-07-Final.pdf\">Causal pluralism</a>. In Beebee, Hitchcock, &amp; Menzies (eds.), <em>The Oxford Handbook of Causation</em> (pp. 326-337). Oxford University Press.</p><p>Hume (1739). <em><a href=\"http://books.google.com.au/books?id=-Sp8B0ZdyAYC&pg=PA335#v=onepage&q&f=false\">A Treatise on Human Nature</a></em>. John Noon.</p><p>Finlay (2010). <a href=\"http://www-bcf.usc.edu/~finlay/NNT.pdf\">Normativity, Necessity and Tense: A Recipe for Homebaked Normativity</a>. In Shafer-Landau (ed.), <em>Oxford Studies in Metaethics 5</em> (pp. 57-85). Oxford University Press.</p><p>Jacobs (2002). <em><a href=\"http://www.amazon.com/Dimensions-Moral-Theory-Introduction-Metaethics/dp/0631229639/\">Dimensions of Moral Theory</a></em>. Wiley-Blackwell.</p><p>Joyce (2011).<a href=\"http://www.victoria.ac.nz/staff/richard_joyce/acrobat/joyce_metaethical.pluralism.pdf\">Metaethical pluralism: How both moral naturalism and moral skepticism may be permissible positions</a>. In Nuccetelli &amp; Seay (eds.), <em>Ethical Naturalism: Current Debates</em>. Cambridge University Press.</p><p>Lewis (1989). Dispositional theories of value. Part II. <em>Proceedings of the Aristotelian Society, supplementary vol. 63</em>: 113-137.</p><p>Magnus (2009). <a href=\"http://www.fecundity.com/job/species-analogy.pdf\">What species can teach us about theory</a>. </p><p>Miller (2003). <em><a href=\"http://www.amazon.com/Introduction-Contemporary-Metaethics-Alex-Miller/dp/074562345X/\">An Introduction to Contemporary Metaethics</a></em>. Polity.</p><p>Russell (1953). <a href=\"http://www.sfu.ca/~jeffpell/Phil467/RussellOrdLang53.pdf\">The cult of common usage</a>. <em>British Journal for the Philosophy of Science, 12</em>: 305-306.</p><p>Uidhir &amp; Magnus (2011). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Uidhir-Art-Concept-Pluralism.pdf\">Art concept pluralism</a>. <em>Metaphilosophy, 42</em>: 83-97.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z8wZZLeLMJ3NSK7kR": 1, "nSHiKwWyMZFdZg5qt": 1, "FtT2T9bRbECCGYxrL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3zDX3f3QTepNeZHGc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": 62, "extendedScore": null, "score": 0.000119, "legacy": true, "legacyId": "7562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "bQgRsy23biR52poMf", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "FZ5aTJFXZMpPL7ycK", "canonicalPrevPostSlug": "2YPbdHgcjt7g5ZaFN", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 62, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Part of the sequence: <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">No-Nonsense Metaethics</a></p><blockquote>Disputes over the definition of morality... are disputes over words which raise no really significant issues. [Of course,] lack of clarity about the meaning of words is an important source of error\u2026 My complaint is that what should be regarded as something to be got out of the way in the introduction to a work of moral philosophy has become the subject matter of almost the whole of moral philosophy...</blockquote><blockquote><a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Singer-The-Triviality-of-the-Debate-Over-Is-Ought-and-the-Definition-of-Moral.pdf\">Peter Singer</a></blockquote><p>If a tree falls in the forest, and no one hears it, does it make a sound? If by 'sound' you mean 'acoustic vibrations in the air', the answer is 'Yes.' But if by 'sound' you mean an auditory experience in the brain, the answer is 'No.'</p><p>We might call this straightforward solution <em>pluralistic sound reductionism</em>. If people use the word 'sound' to mean different things, and people have different intuitions about the meaning of the word 'sound', then <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/\">we needn't endlessly debate which definition is 'correct'</a>.1 We can be pluralists about the meanings of 'sound'. </p><p>To facilitate communication, we can <a href=\"https://www.lesserwrong.com/lw/nu/taboo_your_words/\">taboo</a> and <a href=\"https://www.lesserwrong.com/lw/on/reductionism/\">reduce</a>: we can <a href=\"https://www.lesserwrong.com/lw/nv/replace_the_symbol_with_the_substance/\">replace the symbol with the substance</a> and talk about facts and <a href=\"https://www.lesserwrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">anticipations</a>, not definitions. We can avoid using the word 'sound' and instead talk about 'acoustic vibrations' or 'auditory brain experiences.'</p><p>Still, some definitions can be <em>wrong</em>:</p><blockquote>Alex: If a tree falls in the forest, and no one hears it, does it make a sound?</blockquote><p>Austere MetaAcousticist: Tell me what you mean by 'sound', and I will tell you the answer.</p><p>Alex: By 'sound' I mean 'acoustic messenger fairies flying through the ether'.</p><p>Austere MetaAcousticist: There's no such thing. Now, if you had asked me about this other definition of 'sound'...</p><p>There are <a href=\"https://www.lesserwrong.com/lw/od/37_ways_that_words_can_be_wrong/\">other ways</a> for words to be wrong, too. But once we admit to multiple potentially useful reductions of 'sound', it is not hard to see how we could admit to multiple useful reductions of <em>moral</em> terms.</p><h1 id=\"Many_Moral_Reductionisms\">Many Moral Reductionisms</h1><p>Moral terms are used in a greater variety of ways than sound terms are. There is little hope of arriving at the One True Theory of Morality by <a href=\"https://www.lesserwrong.com/lw/nr/the_argument_from_common_usage/\">analyzing common usage</a> or by <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/#moral_terms\">triangulating from the platitudes of folk moral discourse</a>. But we can use stipulation, and we can taboo and reduce. We can use <em>pluralistic moral reductionism</em>2 (for <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory#austere_empathic\">austere metaethics, not for empathic metaethics</a>).</p><p>Example #1:</p><blockquote>Neuroscientist <a href=\"http://www.amazon.com/dp/1439171211/\">Sam Harris</a>: Which is better? Religious totalitarianism or the Northern European welfare state?</blockquote><p>Austere Metaethicist: What do you mean by 'better'?</p><p>Harris: By 'better' I mean 'that which tends to maximize the well-being of conscious creatures'.</p><p>Austere Metaethicist: Assuming we have similar reductions of 'well-being' and 'conscious creatures' in mind, the evidence I know of suggests that the Northern European welfare state is more likely to maximize the well-being of conscious creatures than religious totalitarianism.</p><p>Example #2:</p><blockquote>Philosopher <a href=\"http://commonsenseatheism.com/?p=15253\">Peter Railton</a>: Is capitalism the best economic system?</blockquote><blockquote>Austere Metaethicist: What do you mean by 'best'?</blockquote><blockquote>Railton: By 'best' I mean 'would be approved of by an ideally instrumentally rational and fully informed agent considering the question \u2018How best to maximize the amount of non-moral goodness?' from a social point of view in which the interests of all potentially affected individuals are counted equally.</blockquote><blockquote>Austere Metaethicist: Assuming we agree on the meaning of 'ideally instrumentally rational' and 'fully informed' and 'agent' and 'non-moral goodness' and a few other things, the evidence I know of suggests that capitalism would not be approved of by an ideally instrumentally rational and fully informed agent considering the question \u2018How best to maximize the amount of non-moral goodness?' from a social point of view in which the interests of all potentially affected individuals were counted equally.</blockquote><p>Example #3:</p><blockquote>Theologian <a href=\"http://www.reasonablefaith.org/site/News2?page=NewsArticle&amp;id=8215\">Bill Craig</a>: Ought we to give 50% of our income to efficient charities?</blockquote><blockquote>Austere Metaethicist: What do you mean by 'ought'?</blockquote><blockquote>Craig: By 'ought' I mean 'approved of by an essentially just and loving God'.</blockquote><blockquote>Austere Metaethicist: Your definition doesn't connect to reality. <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/46ma\">It's like</a> talking about atom-for-atom 'indexical identity' even though the world <a href=\"https://www.lesserwrong.com/lw/qx/timeless_identity/\">is</a> made of configurations and amplitudes instead of Newtonian billiard balls. <a href=\"https://www.lesserwrong.com/lw/11m/atheism_untheism_antitheism\">Gods don't exist.</a></blockquote><p>But before we get to empathic metaethics, let's examine the <a href=\"https://www.lesserwrong.com/lw/5eh/what_is_metaethics/\">standard problems</a> of metaethics using the framework of pluralistic moral reductionism.</p><h1 id=\"Cognitivism_vs__Noncognitivism\">Cognitivism vs. Noncognitivism</h1><p>One standard debate in metaethics is cognitivism vs. noncognitivism. Alexander Miller explains:</p><blockquote>Consider a particular moral judgement, such as the judgement that murder is wrong. What sort of psychological state does this express? Some philosophers, called cognitivists, think that a moral judgement such as this expresses a belief.</blockquote><blockquote>Beliefs can be true or false: they are truth-apt, or apt to be assessed in terms of truth and falsity. So cognitivists think that moral judgements are capable of being true or false. </blockquote><blockquote>On the other hand, non-cognitivists think that moral judgements express non-cognitive states such as emotions or desires. Desires and emotions are not truth-apt. So moral judgements are not capable of being true or false.3</blockquote><p>But why should we expect all people to use moral judgments like \"Stealing is wrong\" to express the same thing?4</p><p>Some people who say \"Stealing is wrong\" are <em>really</em> just trying to express emotions: \"Stealing? Yuck!\" Others use moral judgments like \"Stealing is wrong\" to express commands: \"Don't steal!\" Still others use moral judgments like \"Stealing is wrong\" to assert factual claims, such as \"stealing is against the will of God\" or \"stealing is a practice that usually adds pain rather than pleasure to the world.\"</p><p>It may be interesting to study all such uses of moral discourse, but this post focuses on addressing <em>cognitivists</em>, who use moral judgments to assert factual claims. We ask: Are those claims true or false? What are their implications?</p><h1 id=\"Objective_vs__Subjective_Morality\">Objective vs. Subjective Morality</h1><p>Is morality objective or subjective? It depends which moral reductionism you have in mind, and what you mean by 'objective' and 'subjective'.</p><p>Here are some common5 uses of the objective/subjective distinction in ethics:</p><ul><li>Moral facts are objective1 if they are made true or false by mind-independent facts, otherwise they are subjective1.</li><li>Moral facts are objective2 if they are made true or false by facts independent of the opinions of sentient beings, otherwise they are subjective2.</li><li>Moral facts are objective3 if they are made true or false by facts independent of the opinions of humans, otherwise they are subjective3.</li></ul><p>Now, consider Harris' reduction of morality to facts about the well-being of conscious creatures. His theory of morality is objective3 and objective2, because facts about well-being are independent of anyone's opinion. Even if the Nazis had won WWII and brainwashed everybody to have the opinion that torturing Jews was moral, it would remain true that torturing Jews does not increase the average well-being of conscious creatures. But Harris' theory of morality is not objective1, because facts about the well-being of conscious creatures are mind-<em>dependent</em> facts.</p><p>Or, consider Craig's theory of morality in terms of divine approval. His theory doesn't connect to reality, but still: is it objective or subjective? Craig's theory says that moral facts are objective3, because they don't depend on human opinion (God isn't human). But his theory <em>doesn't</em> say that morality is objective2 or objective1, because for him, moral facts depend on the opinion of a sentient being: God.</p><p>A warning: ambiguous terms like 'objective' and 'subjective' are attractors for <a href=\"https://www.lesserwrong.com/lw/ny/sneaking_in_connotations/\">sneaking in connotations</a>. Craig himself provides an example. In his writings and public appearances, Craig insists that only God-based morality can be objective.6 What does he mean by 'objective'? On a single page,7 he uses 'objective' to mean \"independent of <em>people's</em> opinions\" (objective2) and also to mean \"independent of <em>human</em> opinion\" (objective3). I'll assume he means that only God-based morality can be objective3, because God-based morality is clearly not objective2 (Craig's God is a <em>person</em>, a sentient being).</p><p>And yet, Craig says that we need God in order to have objective3 morality as if this should be a <em>big deal</em>. But hold on. Even a moral code defined in terms of the preferences of <a href=\"http://en.wikipedia.org/wiki/Washoe_(chimpanzee)\">Washoe the chimpanzee</a> is objective3. So not only is Bill's claim that only God-based morality can be objective3 <em>false </em>(because Harris' moral theory is also objective3), but also it's trivially easy to come up with a moral theory that is 'objective' in <em>Craig's</em> (apparent) sense of the term (that is, objective3).8</p><p>Moreover, Harris' theory of morality is objective in a 'stronger' sense than Craig's theory of morality is. Harris' theory is objective3 and objective2, while Craig's theory is merely objective3. Whether he's doing it consciously or not, I wonder if Craig is using the word 'objective' to try to <a href=\"https://www.lesserwrong.com/lw/ny/sneaking_in_connotations/\">sneak in connotations</a> that don't actually apply to his claims once you pay attention to what Craig <em>actually</em> means by the word 'objective'. If Craig told his audience that we need God for morality to be 'objective' in the same sense that morality defined in terms of the preferences of a chimpanzee is 'objective', would this still still have his desired effect on his audience? I doubt it.</p><p>Once you've stipulated your use of 'objective' and 'subjective', it is often trivial to determine whether a given moral reductionism is 'objective' or 'subjective'. But what of it? What force should those words carry after you've <a href=\"https://www.lesserwrong.com/lw/nu/taboo_your_words/\">tabooed</a> them? Be careful not to sneak in connotations that don't belong.</p><h1 id=\"Relative_vs__Absolute_Morality\">Relative vs. Absolute Morality</h1><p>Is morality relative or absolute? Again, it depends which moral reductionism you have in mind, and what you mean by 'relative' and 'absolute'. Again, we must be careful about sneaking in connotations.</p><h2 id=\"Moore_s_Open_Question_Argument\">Moore's Open Question Argument</h2><p>\"He's an unmarried man, but is he a bachelor?\" This is a 'closed' question. The answer is obviously \"Yes.\"</p><p>In contrast, said G.E. Moore, all questions of the type \"Such and such is X, but is it good?\" are <em>open</em> questions. It feels like you can always ask, \"Yes, but is it good?\" In this way, Moore resists the identification of 'morally good' with any set of natural facts. This is Moore's <a href=\"http://en.wikipedia.org/wiki/Open_Question_Argument\">Open Question Argument</a>. Because some moral reductionisms <em>do</em> identify 'good' or 'right' with a particular X, those reductionisms had better have an answer to Moore.</p><p>The Yudkowskian response is to point out that when cognitivists use the term 'good', their intuitive notion of 'good' is captured by a massive logical function that can't be expressed in simple statements like \"maximize pleasure\" or \"act only in accordance with maxims you could wish to be a universal law without contradiction.\" Even if you <em>think</em> everything you want (or rather, <em>want</em> to want) can be realized by (say) <a href=\"http://www.amazon.com/Moral-Landscape-Science-Determine-Values/dp/1439171211/\">maximizing the well-being of conscious creatures</a>, you're wrong. <a href=\"https://www.lesserwrong.com/lw/ld/the_hidden_complexity_of_wishes/\">Your values are more complex than that</a>, and <a href=\"https://www.lesserwrong.com/lw/5sk/inferring_our_desires/\">we can't see</a> the structure of our values. <em>That</em> is why it feels like an open question remains no matter which simplistic identification of \"Good = X\" you choose.</p><p>The problem is not that there <em>is</em> no way to identify 'good' or 'right' (as used intuitively, without tabooing) with a certain X. The problem is that <a href=\"https://www.lesserwrong.com/lw/sm/the_meaning_of_right/\">X is huge and complicated</a> and we don't (yet) have access to its structure.</p><p>But that's the response to Moore <em>after</em> <a href=\"https://www.lesserwrong.com/lw/oh/righting_a_wrong_question/\">righting a wrong question</a> - that is, when doing <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory#austere_empathic\">empathic metaethics</a>. When doing mere <em>pluralistic moral reductionism</em>, Moore's argument doesn't apply. If we taboo and reduce, then the question of \"...but is it good?\" is out of place. The reply is: \"Yes it is, because I just told you <em>that's what I mean to communicate</em> when I use the word-tool 'good' for this discussion. I'm not here to <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/\">debate definitions</a>; I'm here to get something done.\"9</p><h2 id=\"The_Is_Ought_Gap\">The Is-Ought Gap</h2><p>(This section <a href=\"https://www.lesserwrong.com/lw/5u2/pluralistic_moral_reductionism/4cli\">rewritten</a> for clarity.)</p><p>Many claim that you cannot infer an 'ought' statement from a series of 'is' statements. The objection comes from Hume, who said he was surprised whenever an argument made of <em>is</em> and <em>is not</em> propositions suddenly shifted to an <em>ought</em> or <em>ought not</em> claim, without explanation.10</p><p>The solution is to make explicit the bridge from 'ought' statements to 'is' statements.</p><p>Perhaps the arguer means something non-natural by 'ought', such as 'commanded by God' or 'in accord with irreducible, non-natural facts about goodness' (see <a href=\"http://plato.stanford.edu/entries/moral-non-naturalism/\">Moore</a>). If so, I would reject that premise of the argument, because I'm a reductionist. At this point, our discussion might need to shift to a debate over the merits of <a href=\"https://www.lesserwrong.com/lw/on/reductionism/\">reductionism</a>.</p><p>Or perhaps by 'you ought to X' the arguer means something fully natural, such as:</p><ul><li>\"X is obligatory (by <a href=\"http://plato.stanford.edu/entries/logic-deontic/\">deontic logic</a>) if you assume axiomatic imperatives Y and Z.\"</li><li>Or: \"X tends to maximizes <a href=\"http://www.scholarpedia.org/article/Reward_signals\">reward signals</a> in agents exhibiting <a href=\"http://en.wikipedia.org/wiki/Multiple_Drafts_Model\">multiple-drafts consciousness</a>\" (or, as Sam Harris more broadly <a href=\"http://www.amazon.com/Moral-Landscape-Science-Determine-Values/dp/1439171211/\">puts it</a>, \"X tends to maximize well-being in conscious creatures\").</li><li>Or: \"X is what a Bayes-rational and Hubble-volume-omniscient agent would do if it was motivated to maximize the amount of non-moral goodness from a view in which the interests of all potentially affected individuals were counted equally, where 'non-moral goodness' refers to what an agent would want if it were he to contemplate its present situation from a standpoint fully and vividly informed about itself and its circumstances, and entirely free of cognitive error or lapses of instrumental rationality\" (see <a href=\"http://commonsenseatheism.com/?p=15253\">Railton's</a> <a href=\"http://commonsenseatheism.com/?p=15264\">metaethics</a>).</li><li>Or: \"X maximizes the complicated function that can be computed by extrapolating (in a particular way) the motivations encoded by my brain\" (see <a href=\"http://intelligence.org/upload/CEV.html\">CEV</a>).</li><li>Or: \"[insert here whatever statement, if believed, would motivate one to do X]\" (see <a href=\"https://www.lesserwrong.com/lw/64j/a_defense_of_naive_metaethics/4cce\">Will Sawin</a>).</li></ul><p>Or, the speaker may have in mind a common ought-reductionism known as the <em>hypothetical imperative</em>. This is an ought of the kind: \"<em>If</em> you desire to lose weight, <em>then</em> you <em>ought</em> to consume fewer calories than your burn.\" (But usually, people leave off the implied <em>if</em> statement, and simply say \"You should eat less and exercise more.\")</p><p>A hypothetical imperative (as some use it) can be translated from 'ought' to 'is' in a straightforward way: \"If you desire to lose weight, then you <em>ought</em> to consume fewer calories than you burn\" translates to the claim \"If you consume fewer calories than you burn, then you <em>will</em> (or are, <em><a href=\"http://www.hss.cmu.edu/philosophy/glymour/glymour2002.pdf\">ceteris paribus</a></em>, more likely to) fulfill your desire to lose weight.\"11</p><p>Or, the speaker may be using 'ought' to communicate something only about other symbols (example: Bayes' Rule), leaving the bridge from 'ought' to 'is' to be built when the logical function represented by his use of 'ought' is plugged into a theory that refers to the world.</p><p>But one must not fall into the trap of thinking that a definition you've stipulated (aloud or in your head) for 'ought' must match up to your intended meaning of 'ought' (to which you don't have introspective access). In fact, I suspect it <em>never</em> does, which is why the <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/\">conceptual analysis</a> of 'ought' language can <a href=\"https://www.lesserwrong.com/lw/64j/a_defense_of_naive_metaethics/4hi7\">go in circles</a> for centuries, and why any stipulated meaning of 'ought' is a <a href=\"https://www.lesserwrong.com/lw/lq/fake_utility_functions/\">fake utility function</a>. To see clearly to our intuitive concept of ought, we'll have to try empathic metaethics (see below).</p><p>But whatever our intended meaning of 'ought' is, the same reasoning applies. Either our intended meaning of 'ought' refers (eventually) to the world of math and physics (in which case the is-ought gap is bridged), or else it doesn't (in which case it fails to refer).12</p><h1 id=\"Moral_realism_vs__Anti_realism\">Moral realism vs. Anti-realism</h1><p>So, does all this mean that we can embrace moral realism, or does it doom us to moral anti-realism? Again, it depends on what you mean by 'realism' and 'anti-realism'.</p><p>In a sense, pluralistic moral reductionism can be considered a robust form of moral 'realism', in the same way that pluralistic sound reductionism is a robust form of <em>sound realism</em>. \"Yes, there really <em>is</em> sound, and we can locate it in reality \u2014 either as vibrations in the air or as mental auditory experiences, however you are using the term.\" In the same way: \"Yes, there really <em>is</em> morality, and we can locate it in reality \u2014 either as a set of facts about the well-being of conscious creatures, or as a set of facts about what an ideally rational and perfectly informed agent would prefer, or as some other set of natural facts.\"</p><p>But in another sense, pluralistic moral reductionism is 'anti-realist'. It suggests that there is no One True Theory of Morality. (We use moral terms in a variety of ways, and some of those ways refer to different sets of natural facts.) And as a reductionist approach to morality, it might also leave no room for moral theories which say there are <em>universally binding</em> moral rules for which the universe (e.g. via a God) will hold us accountable.</p><p>What matters are the facts, not whether labels like 'realism' or 'anti-realism' apply to 'morality'.</p><h1 id=\"Toward_Empathic_Metaethics\">Toward Empathic Metaethics</h1><p>But pluralistic moral reductionism satisfies only a would-be austere metaethicist, not an empathic metaethicist.</p><p><a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory#austere_empathic\">Recall that</a> when Alex asks how she can do what is right, the Austere Metaethicist replies:</p><blockquote>Tell me what you mean by 'right', and I will tell you what is the right thing to do. If by 'right' you mean X, then Y is the right thing to do. If by 'right' you mean P, then Z is the right thing to do. But if you can't tell me what you mean by 'right', then you have failed to ask a coherent question, and no one can answer an incoherent question.</blockquote><p>Alex may reply to the Austere Metaethicist:</p><blockquote>Okay, I'm not sure exactly what I mean by 'right'. So how do I do what is right if I'm not sure what I mean by 'right'?</blockquote><p>The Austere Metaethicist refuses to answer this question. The Empathic Metaethicist, however, is willing to go the extra mile. He says to Alex:</p><blockquote>You may not know what you mean by 'right.' But let's not stop there. Here, let me come alongside you and help decode the cognitive algorithms that generated your question in the first place, and then we'll be able to answer your question. Then we can tell you what the right thing to do is.</blockquote><p>This may seem like too much work. Would we be motivated to decode the cognitive algorithms producing Albert and Barry's use of the word 'sound'? Would we try to solve 'empathic meta-acoustics'? Probably not. We can simply taboo and reduce 'sound' and then get some work done.</p><p>But moral terms and value terms are about what we <em>want</em>. And unfortunately, we <a href=\"https://www.lesserwrong.com/lw/5sk/inferring_our_desires/\">often</a> don't <em>know</em> what we want. As such, we're unlikely to get what we <em>really</em> want if the world is <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">re-engineered</a> in accordance with our current best <em>guess</em> as to what we want. That's why we need to decode the cognitive algorithms that generate our questions about value and morality.</p><p>So how can the Empathic Metaethicist answer Alex's question? We don't know the details yet. For example, we don't have a completed cognitive neuroscience. But we have some ideas, and we know of some open problems that may admit of progress once more people understand them. In the next few posts, we'll take our first look at empathic metaethics.13</p><p>Previous post: <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/\">Conceptual Analysis and Moral Theory</a></p><h2 id=\"Notes\">Notes</h2><p>1 Some have <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/48b4\">objected</a> that the conceptual analysis argued against in <a href=\"https://www.lesserwrong.com/lw/5kn/conceptual_analysis_and_moral_theory/\">Conceptual Analysis and Moral Theory</a> is not just a battle over definitions. But a definition <a href=\"http://dictionary.reference.com/browse/definition\">is</a> \"the formal statement of the meaning or significance of a word, phrase, etc.\", and a conceptual analysis is (usually) a \"formal statement of the meaning or significance of a word, phrase, etc.\" in terms of necessary and sufficient conditions. The goal of a conceptual analysis is to arrive at a definition for a term that captures our intuitions about its meaning. The process is to bash our intuitions against others' intuitions until we converge upon a set of necessary and sufficient conditions that captures them all. But consider Barry and Albert's debate over the definition of 'sound'. Why think Albert and Barry have the same concept in mind? Words mean slightly different things in different cultures, subcultures, and small communities. We develop different intuitions about their meaning based on divergent life experiences. Our intuitions differ from each other's due to the specifics of <a href=\"https://www.lesserwrong.com/lw/59v/intuition_and_unconscious_learning/\">unconscious associative learning</a> and <a href=\"https://www.lesserwrong.com/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">attribution substitution heuristics</a>. What is the point of bashing our intuitions about the meaning of terms against each other for thousands of pages, in the hopes that we'll converge on a precise set of necessary and sufficient conditions? Even if we can get Albert and Barry to agree, what happens when Susan wants to use the same term, but has slightly differing intuitions about its meaning? And, let's say we arrive at a messy set of 6 necessary and sufficient conditions for the intuitive meaning of the term. Is that going to be as <a href=\"https://www.lesserwrong.com/lw/o0/where_to_draw_the_boundary/\">useful for communication</a> as one we consciously chose because it <a href=\"https://www.lesserwrong.com/lw/nl/the_cluster_structure_of_thingspace/\">carved-up thingspace well</a>? I doubt it. The IAU's <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5kn%2Fconceptual_analysis_and_moral_theory%2F&amp;v=1&amp;libid=1306248621230&amp;out=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FIAU_definition_of_planet&amp;ref=http%3A%2F%2Fwww.google.com%2Fsearch%3Fsourceid%3Dchrome%26ie%3DUTF-8%26q%3Dconceptual%2Banalysis%2Band%2Bmoral%2Btheory&amp;title=Conceptual%20Analysis%20and%20Moral%20Theory%20-%20Less%20Wrong&amp;txt=IAU%27s%20definition%20of%20%27planet%27\">definition of 'planet'</a> is more useful than the folk-intuitions definition of 'planet'. Folk intuitions about 'planet' evolved over thousands of years and different people have different intuitions which may not always converge. In 2006, the IAU used modern astronomical knowledge to carve up thingspace in a more useful and informed way than our intuitions do.</p><p>A passage from Bertrand Russell (1953) is appropriate. Russell said that many philosophers reminded him of</p><blockquote>the shopkeeper of whom I once asked the shortest way to Winchester. He called to a man in the back premises:</blockquote><p>\"Gentleman wants to know the shortest way to Winchester.\"</p><p>\"Winchester?\" an unseen voice replied.</p><p>\"Aye.\"</p><p>\"Way to Winchester?\"</p><p>\"Aye.\"</p><p>\"Shortest way?\"</p><p>\"Aye.\"</p><p>\"Dunno.\"</p><p>He wanted to get the nature of the question clear, but took no interest in answering it. This is exactly what modern philosophy does for the earnest seeker after truth. Is it surprising that young people turn to other studies?</p><p>2 Compare also to the biologist's 'species concept pluralism' and the philosopher's 'art concept pluralism.' See Uidhir &amp; Magnus (2011). Also see 'causal pluralism' (Godfrey-Smith, 2009; Cartwright, 2007), 'theory concept pluralism' (Magnus, 2009) and, especially, 'metaethical contextualism' (Bjornsson &amp; Finlay, 2010) or 'metaethical pluralism' or 'metaethical ambivalence' (Joyce, 2011). Joyce quotes Lewis (1989), who wrote that some concepts of value refer to things that really exist, and some concepts don't, and what you make of this situation is largely a matter of temperament:</p><blockquote>What to make of the situation is mainly a matter of temperament. You can bang the drum about how philosophy has uncovered a terrible secret: there are no values! ... Or you can think it better for public safety to keep quiet and hope people will go on as before. Or you can declare that there are no values, but that nevertheless it is legitimate\u2014and not just expedient\u2014for us to carry on with value-talk, since we can make it all go smoothly if we just give the name of value to claimants that don't quite deserve it... Or you can think it an empty question whether there are values: say what you please, speak strictly or loosely. When it comes to deserving a name, there's better and worse but who's to say how good is good enough? Or you can think it clear that the imperfect deservers of the name are good enough, but only just, and say that although there are values we are still terribly wrong about them. Or you can calmly say that value (like simultaneity) is not quite as some of us sometimes thought. Myself, I prefer the calm and conservative responses. But as far as the analysis of value goes, they're all much of a muchness.</blockquote><p>Joyce concludes that, for example, the moral naturalist and the moral error theorist may agree with each other (when adopting each other's own language):</p><blockquote>[Metaethical ambivalence] begins with a kind of metametaethical enlightenment. The moral naturalist espouses moral naturalism, but this espousal reflects a mature decision, by which I mean that the moral naturalist doesn't claim to have latched on to an incontrovertiblerealm of moral facts of which the skeptic is foolishly ignorant, but rather acknowledges that this moral naturalism has been achieved only via a non-mandatory piece of conceptual precisification. Likewise, the moral skeptic champions moral skepticism, but this too is a sophisticated verdict: not the simple declaration that there are no moral values and that the naturalist is gullibly uncritical, but rather a decision that recognizes that this skepticism has been earned only by making certain non-obligatory but permissible conceptual clarifications.</blockquote><p>...The enlightened moral naturalist doesn't merely (grudgingly) admit that the skeptic is warranted in his or her views, but is able to adopt the skeptical position in order to gain the insights that come from recognizing that we live in a world without values. And the enlightened moral skeptic goes beyond (grudgingly) conceding that moral naturalism is reasonable, but is capable of assuming that perspective in order to gain whatever benefits come from enjoying epistemic access to a realm of moral facts.</p><p>3 Miller (2003), p. 3.</p><p>4 I changed the example moral judgment from \"murder is wrong\" to \"stealing is wrong\" because the former invites confusion. 'Murder' often means <em>wrongful</em> killing.</p><p>5 Also see Jacobs (2002), starting on p. 2.</p><p>6 The first premise of one of <a href=\"http://thegospelcoalition.org/pdf-articles/Craig_Atheism.pdf\">his favorite arguments for God's existence</a> is \"If God does not exist, objective moral values and duties do not exist.\"</p><p>7 Craig (2010), p. 11.</p><p>8 It's also possible that Craig intended a different sense of objective than the ones explicitly given in his article. Perhaps he meant objective4: \"morality is objective4 if it is not grounded in the opinion of non-divine persons.\"</p><p>9 Also see <a href=\"http://commonsenseatheism.com/?p=15336\">Moral Reductionism and Moore's Open Question Argument</a>.</p><p>10 Hume (1739), p. 469. The famous paragraph is:</p><blockquote>In every system of morality, which I have hitherto met with, I have always remarked, that the author proceeds for some time in the ordinary ways of reasoning, and establishes the being of a God, or makes observations concerning human affairs; when all of a sudden I am surprised to find, that instead of the usual copulations of propositions, is, and is not, I meet with no proposition that is not connected with an <em>ought</em>, or an ought not. This change is imperceptible; but is however, of the last consequence. For as this ought, or ought not, expresses some new relation or affirmation, 'tis necessary that it should be observed and explained; and at the same time that a reason should be given; for what seems altogether inconceivable, how this new relation can be a deduction from others, which are entirely different from it.</blockquote><p>11 For more on reducing certain kinds of normative statements, see Finlay (2010).</p><p>12 Assuming reductionism is true. If reductionism is false, then of course there are problems for pluralistic moral reductionism as a theory of austere (but not empathic) metaethics. The clarifications in the last three paragraphs of this section are due to discussions with Wei Dai and Vladimir Nesov.</p><p>13 My thanks to Steve Rayhawk and Will Newsome for their feedback on early drafts of this post.</p><h2 id=\"References\">References</h2><p>Bjornsson &amp; Finlay (2010). <a href=\"http://peasoup.typepad.com/peasoup/files/MetaethicalContextualismDefended.pdf\">Metaethical contextualism defended</a>. <em>Ethics, 121</em>: 7-36.</p><p>Craig (2010). <a href=\"http://thegospelcoalition.org/blogs/tgc/2010/02/04/william-lane-craig-five-arguments-for-god/\">Five Arguments for God</a>. The Gospel Coalition.</p><p>Cartwright (2007).<em><a href=\"http://www.amazon.com/Hunting-Causes-Using-Them-Approaches/dp/052167798X/\">Hunting Causes and Using Them: Approaches in Philosophy and Economics</a></em>. Cambridge University Press.</p><p>Godfrey-Smith (2009). <a href=\"http://www.people.fas.harvard.edu/~pgs/CausalPluralismPGS-07-Final.pdf\">Causal pluralism</a>. In Beebee, Hitchcock, &amp; Menzies (eds.), <em>The Oxford Handbook of Causation</em> (pp. 326-337). Oxford University Press.</p><p>Hume (1739). <em><a href=\"http://books.google.com.au/books?id=-Sp8B0ZdyAYC&amp;pg=PA335#v=onepage&amp;q&amp;f=false\">A Treatise on Human Nature</a></em>. John Noon.</p><p>Finlay (2010). <a href=\"http://www-bcf.usc.edu/~finlay/NNT.pdf\">Normativity, Necessity and Tense: A Recipe for Homebaked Normativity</a>. In Shafer-Landau (ed.), <em>Oxford Studies in Metaethics 5</em> (pp. 57-85). Oxford University Press.</p><p>Jacobs (2002). <em><a href=\"http://www.amazon.com/Dimensions-Moral-Theory-Introduction-Metaethics/dp/0631229639/\">Dimensions of Moral Theory</a></em>. Wiley-Blackwell.</p><p>Joyce (2011).<a href=\"http://www.victoria.ac.nz/staff/richard_joyce/acrobat/joyce_metaethical.pluralism.pdf\">Metaethical pluralism: How both moral naturalism and moral skepticism may be permissible positions</a>. In Nuccetelli &amp; Seay (eds.), <em>Ethical Naturalism: Current Debates</em>. Cambridge University Press.</p><p>Lewis (1989). Dispositional theories of value. Part II. <em>Proceedings of the Aristotelian Society, supplementary vol. 63</em>: 113-137.</p><p>Magnus (2009). <a href=\"http://www.fecundity.com/job/species-analogy.pdf\">What species can teach us about theory</a>. </p><p>Miller (2003). <em><a href=\"http://www.amazon.com/Introduction-Contemporary-Metaethics-Alex-Miller/dp/074562345X/\">An Introduction to Contemporary Metaethics</a></em>. Polity.</p><p>Russell (1953). <a href=\"http://www.sfu.ca/~jeffpell/Phil467/RussellOrdLang53.pdf\">The cult of common usage</a>. <em>British Journal for the Philosophy of Science, 12</em>: 305-306.</p><p>Uidhir &amp; Magnus (2011). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/Uidhir-Art-Concept-Pluralism.pdf\">Art concept pluralism</a>. <em>Metaphilosophy, 42</em>: 83-97.</p>", "sections": [{"title": "Many Moral Reductionisms", "anchor": "Many_Moral_Reductionisms", "level": 1}, {"title": "Cognitivism vs. Noncognitivism", "anchor": "Cognitivism_vs__Noncognitivism", "level": 1}, {"title": "Objective vs. Subjective Morality", "anchor": "Objective_vs__Subjective_Morality", "level": 1}, {"title": "Relative vs. Absolute Morality", "anchor": "Relative_vs__Absolute_Morality", "level": 1}, {"title": "Moore's Open Question Argument", "anchor": "Moore_s_Open_Question_Argument", "level": 2}, {"title": "The Is-Ought Gap", "anchor": "The_Is_Ought_Gap", "level": 2}, {"title": "Moral realism vs. Anti-realism", "anchor": "Moral_realism_vs__Anti_realism", "level": 1}, {"title": "Toward Empathic Metaethics", "anchor": "Toward_Empathic_Metaethics", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 2}, {"title": "References", "anchor": "References", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "327 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 327, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2YPbdHgcjt7g5ZaFN", "WBdvyyHLdxZSAMmoz", "tPqQdLCuxanjhoaNs", "GKfPL6LQFgB49FEnv", "a7n8GdKiAZRX86T5A", "FaJaCgqBKphrDzDSj", "9ZooAqfh2TC9SBDvq", "924arDrTu3QRHFA5r", "PYtus925Gcg7cqTEq", "s4Mcg9aLMeRwdW7fh", "yuKaWPRTxZoov4z8K", "4ARaTpNX62uaL86j6", "2G7AH92pHyj3nC32T", "fG3g3764tSubr6xvs", "rQEwySCcLtdKHkrHp", "NnohDYHNnKDtbiMyp", "6Cc3TWZjAnrNWokWY", "du395YvCnQXBPSJax", "d5NyJ2Lf6N22AD9PB", "WBw8dDkAWohFjWQSk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T03:00:55.221Z", "modifiedAt": null, "url": null, "title": "(Possibly Bayesian) Statistics Question", "slug": "possibly-bayesian-statistics-question", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.249Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "b1shop", "createdAt": "2010-07-21T22:58:23.412Z", "isAdmin": false, "displayName": "b1shop"}, "userId": "YYM9ouBdPbNFxvF2G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vprWjfbCCeEuwhCyS/possibly-bayesian-statistics-question", "pageUrlRelative": "/posts/vprWjfbCCeEuwhCyS/possibly-bayesian-statistics-question", "linkUrl": "https://www.lesswrong.com/posts/vprWjfbCCeEuwhCyS/possibly-bayesian-statistics-question", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20(Possibly%20Bayesian)%20Statistics%20Question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A(Possibly%20Bayesian)%20Statistics%20Question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvprWjfbCCeEuwhCyS%2Fpossibly-bayesian-statistics-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=(Possibly%20Bayesian)%20Statistics%20Question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvprWjfbCCeEuwhCyS%2Fpossibly-bayesian-statistics-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvprWjfbCCeEuwhCyS%2Fpossibly-bayesian-statistics-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p>&nbsp;</p>\n<p>This question exists in the awkward space between \"things undergrads google for homework\" and \"things on the cutting edge,\" so google isn't being super helpful.</p>\n<p>I have a number I want a computer to estimate. Right now I have two regression models and an insider methodology. The former can be used to create two normal curves. The latter creates a point estimate only, but I can back into a confidence interval/normal curve with an acceptable amount of arbitrary hand-waving. If necessary, this could be conceived of as a prior.</p>\n<p>How can I automatically weight the three curves into a single point estimate? I vaguely remember something from an econometrics class about weighting forecasts in a way that minimized total standard error, but I tried to work the math out myself and I didn&rsquo;t know how to deal with the covariances of the forecasts. Can I simply assume the forecast covariances are zero?</p>\n<p>This seems like a good place to use Bayes&rsquo; law, but I don't know how to formally set it up.</p>\n<p>&nbsp;</p>\n<p><strong>Edit to Add:</strong>&nbsp;Bayesian statistics is still new to me, so forgive me for being a bit dense.<a href=\"http://i.imgur.com/C9Lmh.png\"> Here's my understanding of the methodology right now.</a></p>\n<p>What exactly is D in this scenario?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vprWjfbCCeEuwhCyS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 7.21966548916546e-07, "legacy": true, "legacyId": "7769", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T03:52:04.379Z", "modifiedAt": null, "url": null, "title": "Bitcoin Bounty: Advice Requested", "slug": "bitcoin-bounty-advice-requested", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.056Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pccBf7FCWgBHCgrjS/bitcoin-bounty-advice-requested", "pageUrlRelative": "/posts/pccBf7FCWgBHCgrjS/bitcoin-bounty-advice-requested", "linkUrl": "https://www.lesswrong.com/posts/pccBf7FCWgBHCgrjS/bitcoin-bounty-advice-requested", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bitcoin%20Bounty%3A%20Advice%20Requested&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABitcoin%20Bounty%3A%20Advice%20Requested%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpccBf7FCWgBHCgrjS%2Fbitcoin-bounty-advice-requested%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bitcoin%20Bounty%3A%20Advice%20Requested%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpccBf7FCWgBHCgrjS%2Fbitcoin-bounty-advice-requested", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpccBf7FCWgBHCgrjS%2Fbitcoin-bounty-advice-requested", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 354, "htmlBody": "<p>About a month ago, I started a <a href=\"/lw/5hy/cryonics_promotional_video_contest_10_btc_prize/\">prize</a> for Cryonics promotional videos published to Youtube during the month of May. There was a fair bit of interest, but nobody actually submitted anything. So the 14.75 bitcoins are thus far unclaimed.</p>\n<p>I want to use this money (worth ~$125) to start another contest, but I thought that this time it would be a good idea to solicit some feedback on how best to do so.</p>\n<ul>\n<li>What is the best medium to use? \n<ul>\n<li>It may be that few less wrongers are comfortable with videos</li>\n<li>Other media might be just as effective or more so</li>\n<li>Youtube may not be the best site</li>\n</ul>\n</li>\n<li>What is the best way of judging? \n<ul>\n<li>Likes on youtube or some other site</li>\n<li>Karma votes here on LW</li>\n<li>A panel of judges</li>\n</ul>\n</li>\n<li>What is the best timeframe? \n<ul>\n<li>Long, 6-month projects (more money needed?)</li>\n<li>Quick projects 15-30 minutes (less money? specific amounts?)</li>\n<li>Repeating prizes (monthly, weekly, annually, etc.)</li>\n</ul>\n</li>\n<li>Is regular cash a better prize than bitcoin? \n<ul>\n<li>May vary from person to person</li>\n<li>People already know what cash is worth</li>\n<li>Bitcoin might have novelty value or geek appeal</li>\n<li>Bitcoin is ridiculously easy to transfer without the hassle of fees</li>\n</ul>\n</li>\n<li>Is cryonics really the best topic? \n<ul>\n<li>Maybe we would get more creative output from other topics</li>\n<li>Other prize funds could be started for other topics</li>\n</ul>\n</li>\n</ul>\n<p>So anyways, I'd love to hear some opinions. I think making the contest more fun will probably get more results, so basically what I want to know is, what would be the most fun?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pccBf7FCWgBHCgrjS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 7.219816484529154e-07, "legacy": true, "legacyId": "7773", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ojpjGWrBNSSFYiYG8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T07:31:17.482Z", "modifiedAt": null, "url": null, "title": "Heuristics for Deciding What to Work On", "slug": "heuristics-for-deciding-what-to-work-on", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:25.014Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/youHg69dvzzWDvTuL/heuristics-for-deciding-what-to-work-on", "pageUrlRelative": "/posts/youHg69dvzzWDvTuL/heuristics-for-deciding-what-to-work-on", "linkUrl": "https://www.lesswrong.com/posts/youHg69dvzzWDvTuL/heuristics-for-deciding-what-to-work-on", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Heuristics%20for%20Deciding%20What%20to%20Work%20On&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHeuristics%20for%20Deciding%20What%20to%20Work%20On%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyouHg69dvzzWDvTuL%2Fheuristics-for-deciding-what-to-work-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Heuristics%20for%20Deciding%20What%20to%20Work%20On%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyouHg69dvzzWDvTuL%2Fheuristics-for-deciding-what-to-work-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyouHg69dvzzWDvTuL%2Fheuristics-for-deciding-what-to-work-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 901, "htmlBody": "<p>If you're like me, you have way more ideas for things to do than time, energy, and willpower to do them with. &nbsp;(And if you're not like me, you might very well become like me if you just kept track of all the times you or someone else said \"Hey, that might be a worthwhile project.\") &nbsp;To give you an idea of what I'm talking about, here are some entries on my things-to-possibly-do list: give speed reading another shot; improve the Less Wrong codebase and add a feature that helps users find old, good posts they haven't read; experiment with online freelancing work; try my hand at e-commerce; work as a salesperson to build social skills.</p>\n<p>One of the things I've learned from keeping a things-to-possibly-do list is that doing stuff inevitably takes longer than I intuitively think it will. &nbsp;For example, the main thing I did during the past 3-day weekend was write&nbsp;36 Anki cards and&nbsp;220 lines of Python to program myself and my computer to help me keep a resolution. &nbsp;In past years, I might have gotten demoralized halfway through, thinking things were taking too long, but I've gradually gotten used to things taking longer than I expect.</p>\n<p>Given that things take such a long time to get done, it seems worthwhile to spend a decent amount of time deciding what to work on. &nbsp;But the standard objective of doing whatever has the highest expected utility is often computationally intractable in practice. &nbsp;For example, what's the expected utility of building social skills?</p>\n<p><a id=\"more\"></a></p>\n<p>Given this, I'm working on a list of heuristics for the computationally intractable problem of what to work on. &nbsp;Here's my current list; feel free to suggest additions in the comments.</p>\n<ul>\n<li>Watch for investments that pay for themselves. &nbsp;For example, I have two close-to-identical keys that I use multiple times daily. &nbsp;About a week ago I taped some paper to one of them. &nbsp;I wouldn't be surprised if this time investment will have paid itself back within a few weeks, and start generating value from then on.</li>\n<li>Find things that separate those who succeed from those who fail, and focus on getting those right. &nbsp;For example, I've observed that men who have succeeded in business seem more likely than regular people to display high-status behavioral cues.</li>\n<li>Find things that will give you a lot of new information in an important domain even if their immediate expected value may be low. &nbsp;(An example might be buying a few websites on <a href=\"https://flippa.com/\">Flippa</a>&nbsp;to learn what strategies work on the web instead of going through the relatively laborious process of writing a website that might fail yourself.)</li>\n<li>Find things that will start generating continuous benefit as soon as you do them, and do them first if they're worth doing at all. &nbsp;For example, I'd guess that for most, installing&nbsp;<a href=\"https://addons.mozilla.org/en-US/firefox/addon/leechblock/\">LeechBlock</a> or a similar anti-time-wasting tool can potentially generate continuous benefit for quite a while, especially if you install it while in a resolution-making mindset. &nbsp;(Of course, an argument against doing <strong>all</strong> activities of this type first is that you might be optimizing prematurely. &nbsp;Consider the case of a self-help reader who judges self-help techniques based on the number of hours they help him spend reading self-help books.)</li>\n<li>Work on things you've been doing recently, so you aren't penalized by context switches. &nbsp;(Unless you've reached some sort of intellectual dead end where revisiting the problem later would actually result in <strong>increased</strong> effectiveness.)</li>\n<li>Do things at the best possible time. &nbsp;For example, maybe you're a PhD student with an unpublished book you've written and you're not sure whether you want to spend your summer trying to get your book published or travelling. &nbsp;Using this heuristic, you might decide to put off trying to get your book published until you've got your PhD, with the rationale that it will be easier to find a publisher then. &nbsp;(Of course, it's possible that publishing your book is so high-value that you <strong>shouldn't</strong>&nbsp;put it off, and given the scale of this example it's probably worth doing more computation to determine this.)</li>\n<li>Do what you want to do--people tend to be better at things they like doing.</li>\n<li>Intuitive estimation of expected value.</li>\n</ul>\n<div>And here are some ways you could use these heuristics:</div>\n<ul>\n<li>Rank everything on your to-possibly-do list according to each heuristic. &nbsp;Do the top item according to each heuristic. &nbsp;(The rationale being that even if some heuristics turn out to be way better than others, this approach will generate a decently good return.)</li>\n<li>Determine some weighting for each heuristic. &nbsp;Score each item on your list according to how well it satisfies each heuristic, then use the weightings to determine an item's overall score.</li>\n<li>(Probably the best...) &nbsp;Intuitively rank the items on your list, keeping these heuristics in mind. &nbsp;Spend relatively more time figuring out the positions of the top items on your list, since those are the ones you might actually have time to do.</li>\n</ul>\n<div><br /></div>\n<h3>Further Reading</h3>\n<ul>\n<li>Tom McCabe's&nbsp;<a href=\"/lw/58g/levels_of_action/\">Levels of Action</a>&nbsp;post touches on the idea of activities that provide new information being high-value.</li>\n<li>Cal Newport&nbsp;<a href=\"http://calnewport.com/blog/2008/06/27/dangerous-ideas-getting-started-is-overrated/\">says</a>&nbsp;you're best off putting a lot of effort in to a very small number of projects. &nbsp;This implies that choosing one's projects is important, and is also related to the idea of things taking longer than expected to get done.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "youHg69dvzzWDvTuL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 24, "extendedScore": null, "score": 7.220463655192147e-07, "legacy": true, "legacyId": "7776", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["guDcrPqLsnhEjrPZj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T08:17:07.695Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes: June 2011", "slug": "rationality-quotes-june-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:37.713Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oscar_Cunningham", "createdAt": "2009-09-18T13:28:22.764Z", "isAdmin": false, "displayName": "Oscar_Cunningham"}, "userId": "G2SZuAiaBaNPg9rBt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DxzJY2rpsYYSFZXqb/rationality-quotes-june-2011", "pageUrlRelative": "/posts/DxzJY2rpsYYSFZXqb/rationality-quotes-june-2011", "linkUrl": "https://www.lesswrong.com/posts/DxzJY2rpsYYSFZXqb/rationality-quotes-june-2011", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%3A%20June%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%3A%20June%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDxzJY2rpsYYSFZXqb%2Frationality-quotes-june-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%3A%20June%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDxzJY2rpsYYSFZXqb%2Frationality-quotes-june-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDxzJY2rpsYYSFZXqb%2Frationality-quotes-june-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p>Y'all know the rules:</p>\n<ul style=\"margin: 10px 2em; list-style-type: disc; list-style-position: outside; padding: 0px;\">\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp; (If they are strongly related, reply to your own comments.&nbsp; If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DxzJY2rpsYYSFZXqb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 7.220598986454999e-07, "legacy": true, "legacyId": "7778", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 475, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T14:04:57.022Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Your Rationality is My Business", "slug": "seq-rerun-your-rationality-is-my-business", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.034Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/geEHc379o2rHzT7cY/seq-rerun-your-rationality-is-my-business", "pageUrlRelative": "/posts/geEHc379o2rHzT7cY/seq-rerun-your-rationality-is-my-business", "linkUrl": "https://www.lesswrong.com/posts/geEHc379o2rHzT7cY/seq-rerun-your-rationality-is-my-business", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Your%20Rationality%20is%20My%20Business&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Your%20Rationality%20is%20My%20Business%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgeEHc379o2rHzT7cY%2Fseq-rerun-your-rationality-is-my-business%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Your%20Rationality%20is%20My%20Business%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgeEHc379o2rHzT7cY%2Fseq-rerun-your-rationality-is-my-business", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgeEHc379o2rHzT7cY%2Fseq-rerun-your-rationality-is-my-business", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 190, "htmlBody": "<p>Today's post, <a href=\"/lw/hn/your_rationality_is_my_business/\">Your Rationality is My Business</a> was originally published on April 15, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>As humans, we have an interest in the future of human civilization, including the human pursuit of truth.  That makes your rationality my business.  However, calling out others as wrong can be a dangerous action. Some turn to relativism to avoid it, but this is too extreme. Disagreements should be met with experiments and arguments, not ignored or met with violence and edicts.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/5zc/seq_rerun_new_improved_lottery/\">New Improved Lottery</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "geEHc379o2rHzT7cY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.221626062743059e-07, "legacy": true, "legacyId": "7779", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["anCubLdggTWjnEvBS", "j7dxBNRrdCCKwdXpQ", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T15:55:37.121Z", "modifiedAt": null, "url": null, "title": "The Landmark Forum \u2014 a rationalist's first impression", "slug": "the-landmark-forum-a-rationalist-s-first-impression", "viewCount": null, "lastCommentedAt": "2021-08-11T08:58:11.115Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s4JE7cNqxqwxFmFe2/the-landmark-forum-a-rationalist-s-first-impression", "pageUrlRelative": "/posts/s4JE7cNqxqwxFmFe2/the-landmark-forum-a-rationalist-s-first-impression", "linkUrl": "https://www.lesswrong.com/posts/s4JE7cNqxqwxFmFe2/the-landmark-forum-a-rationalist-s-first-impression", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Landmark%20Forum%20%E2%80%94%20a%20rationalist's%20first%20impression&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Landmark%20Forum%20%E2%80%94%20a%20rationalist's%20first%20impression%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs4JE7cNqxqwxFmFe2%2Fthe-landmark-forum-a-rationalist-s-first-impression%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Landmark%20Forum%20%E2%80%94%20a%20rationalist's%20first%20impression%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs4JE7cNqxqwxFmFe2%2Fthe-landmark-forum-a-rationalist-s-first-impression", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs4JE7cNqxqwxFmFe2%2Fthe-landmark-forum-a-rationalist-s-first-impression", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 613, "htmlBody": "<p>Last week I attended an introductory seminar to the Landmark Forum. Landmark is the modern form of <a href=\"http://en.wikipedia.org/wiki/Erhard_Seminars_Training\">est</a>, a seminar series started by Werner Erhard in the 1970s. Like the Less Wrong community, this is an organization which claims to empower people and enable them to achieve their goals. I heard people recount how Landmark had enabled them to become a responsible adult, to build a relationship with their estranged son, to repair their relationship with their wife, to decide to quit their job and become funded as a grad student.</p>\n<p>It's quite successful in attracting members (more than 1 million participants), so you may be interested to know how it works. Note that I only attended one session; this is a first impression. If you have more experience with Landmark, please tell us about it in the comments.</p>\n<p>Also, a word of warning: Landmark Education is a for-profit employee-owned company. If you go to one of these things, beware the Dark Arts. The purpose of the free introductory session is to persuade you to sign up for the weekend-long Forum retreat, which costs around $500. And after that retreat, there are more advanced seminars to sign up for. Forum graduates, who are universally enthusiastic about the positive change Landmark has effected in their lives, are encouraged to recruit their friends. (By the way, this is a really good way to recruit people.) I had precommitted to not make any purchasing decisions while at the seminar.</p>\n<h2>An optimistic worldview</h2>\n<p>The seminar was mostly a lecture conveying an empowering, optimistic worldview. The message was that you can achieve anything, basically. Or, put more charitably, the message is that you can construct a personal narrative in which your actions are guided by goals and possibilities, rather than being limited by constraints. The speaker evoked laughter in some of the veterans &mdash; not humorous laughter, but the kind of giggle that comes from feeling all warm and fuzzy inside. The speaker was pretty good, admittedly. But her words and ideas were fuzzy, blunt, imprecise. Aspiring rationalists could do much better.</p>\n<h2>The exercises</h2>\n<p>Religions are also good at conveying a comforting worldview in lectures. But unlike in the religions I'm familiar with, every participant in the Landmark Forum is promised personal growth or rebirth. To be sure, the realization of this promise is facilitated by a placebo effect &mdash; everyone who signs up and pays for the Landmark seminars expects to get a lot out of it. But at least as important are the instrumental rationality exercises. These exercises are meant to internalize one's <a href=\"/lw/50p/reflections_on_rationality_a_year_out/3sh1\">locus of control</a>, manage one's dispositions, manage one's personal narratives, communicate effectively, and become more effective in general.</p>\n<p>In accordance with <a href=\"/lw/5ln/building_rationalist_communities_a_series_overview/\">Bhagwat's Law of Commitment</a>, they led my fellow participants in an exercise in <a href=\"/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/\">taskifying goals</a>. I won't describe the exercise exactly; but if you spend five minutes thinking of a first exercise in taskifying goals, you'll come up with it yourself.<a name=\"body1\"></a><sup><a href=\"/lw/5zh/the_landmark_forum_a_rationalists_first_impression/#foot1\">1</a></sup> Anyways, the exercise is very basic, but surprisingly effective if you don't already have the skill of turning goals into tasks. We were told beforehand that a common reaction to the exercise is \"Wow, my goal is so easy to achieve now! I am relieved.\" Of course, this expectation made the exercise seem super effective (and probably made it more effective in fact).</p>\n<h2>Conclusion</h2>\n<p>Exercises are important. We <a href=\"/lw/5lm/building_rationalist_communities_lessons_from_the/\">already</a> <a href=\"/lw/5kz/the_5second_level/\">knew</a> <a href=\"/lw/5x8/teachable_rationality_skills/\">that</a>, right?</p>\n<p>&nbsp;</p>\n<hr />\n<p><sup><a href=\"/lw/5zh/the_landmark_forum_a_rationalists_first_impression/#body1\">1</a><a name=\"foot1\"></a></sup>Landmark has intellectual property rights to its curricula. And they also have money. If you start using Landmark intellectual property at your Less Wrong meetup, they might sue you. I won't publish Landmark exercises on Less Wrong unless I'm sure it won't hurt Less Wrong or the Singularity Institute. I'm happy to talk about this stuff privately.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LJxkaxERSGYKBBJp2": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s4JE7cNqxqwxFmFe2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 24, "extendedScore": null, "score": 5.2e-05, "legacy": true, "legacyId": "7757", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Last week I attended an introductory seminar to the Landmark Forum. Landmark is the modern form of <a href=\"http://en.wikipedia.org/wiki/Erhard_Seminars_Training\">est</a>, a seminar series started by Werner Erhard in the 1970s. Like the Less Wrong community, this is an organization which claims to empower people and enable them to achieve their goals. I heard people recount how Landmark had enabled them to become a responsible adult, to build a relationship with their estranged son, to repair their relationship with their wife, to decide to quit their job and become funded as a grad student.</p>\n<p>It's quite successful in attracting members (more than 1 million participants), so you may be interested to know how it works. Note that I only attended one session; this is a first impression. If you have more experience with Landmark, please tell us about it in the comments.</p>\n<p>Also, a word of warning: Landmark Education is a for-profit employee-owned company. If you go to one of these things, beware the Dark Arts. The purpose of the free introductory session is to persuade you to sign up for the weekend-long Forum retreat, which costs around $500. And after that retreat, there are more advanced seminars to sign up for. Forum graduates, who are universally enthusiastic about the positive change Landmark has effected in their lives, are encouraged to recruit their friends. (By the way, this is a really good way to recruit people.) I had precommitted to not make any purchasing decisions while at the seminar.</p>\n<h2 id=\"An_optimistic_worldview\">An optimistic worldview</h2>\n<p>The seminar was mostly a lecture conveying an empowering, optimistic worldview. The message was that you can achieve anything, basically. Or, put more charitably, the message is that you can construct a personal narrative in which your actions are guided by goals and possibilities, rather than being limited by constraints. The speaker evoked laughter in some of the veterans \u2014 not humorous laughter, but the kind of giggle that comes from feeling all warm and fuzzy inside. The speaker was pretty good, admittedly. But her words and ideas were fuzzy, blunt, imprecise. Aspiring rationalists could do much better.</p>\n<h2 id=\"The_exercises\">The exercises</h2>\n<p>Religions are also good at conveying a comforting worldview in lectures. But unlike in the religions I'm familiar with, every participant in the Landmark Forum is promised personal growth or rebirth. To be sure, the realization of this promise is facilitated by a placebo effect \u2014 everyone who signs up and pays for the Landmark seminars expects to get a lot out of it. But at least as important are the instrumental rationality exercises. These exercises are meant to internalize one's <a href=\"/lw/50p/reflections_on_rationality_a_year_out/3sh1\">locus of control</a>, manage one's dispositions, manage one's personal narratives, communicate effectively, and become more effective in general.</p>\n<p>In accordance with <a href=\"/lw/5ln/building_rationalist_communities_a_series_overview/\">Bhagwat's Law of Commitment</a>, they led my fellow participants in an exercise in <a href=\"/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/\">taskifying goals</a>. I won't describe the exercise exactly; but if you spend five minutes thinking of a first exercise in taskifying goals, you'll come up with it yourself.<a name=\"body1\"></a><sup><a href=\"/lw/5zh/the_landmark_forum_a_rationalists_first_impression/#foot1\">1</a></sup> Anyways, the exercise is very basic, but surprisingly effective if you don't already have the skill of turning goals into tasks. We were told beforehand that a common reaction to the exercise is \"Wow, my goal is so easy to achieve now! I am relieved.\" Of course, this expectation made the exercise seem super effective (and probably made it more effective in fact).</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>Exercises are important. We <a href=\"/lw/5lm/building_rationalist_communities_lessons_from_the/\">already</a> <a href=\"/lw/5kz/the_5second_level/\">knew</a> <a href=\"/lw/5x8/teachable_rationality_skills/\">that</a>, right?</p>\n<p>&nbsp;</p>\n<hr>\n<p><sup><a href=\"/lw/5zh/the_landmark_forum_a_rationalists_first_impression/#body1\">1</a><a name=\"foot1\"></a></sup>Landmark has intellectual property rights to its curricula. And they also have money. If you start using Landmark intellectual property at your Less Wrong meetup, they might sue you. I won't publish Landmark exercises on Less Wrong unless I'm sure it won't hurt Less Wrong or the Singularity Institute. I'm happy to talk about this stuff privately.</p>", "sections": [{"title": "An optimistic worldview", "anchor": "An_optimistic_worldview", "level": 1}, {"title": "The exercises", "anchor": "The_exercises", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "58 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Zasc7RXAEosWLYf8E", "Pmfk7ruhWaHj9diyv", "su7bXsXYpY55vwphc", "JcpzFpPBSmzuksmWM", "f4CZNEHirweN3XEjs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T17:55:42.229Z", "modifiedAt": null, "url": null, "title": "What is/are the definition(s) of \"Should\"?", "slug": "what-is-are-the-definition-s-of-should", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:23.354Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Sawin", "createdAt": "2010-06-20T00:58:25.645Z", "isAdmin": false, "displayName": "Will_Sawin"}, "userId": "EWLubmrSBK6cnmDwd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hG59vonPheZ4xRmQz/what-is-are-the-definition-s-of-should", "pageUrlRelative": "/posts/hG59vonPheZ4xRmQz/what-is-are-the-definition-s-of-should", "linkUrl": "https://www.lesswrong.com/posts/hG59vonPheZ4xRmQz/what-is-are-the-definition-s-of-should", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%2Fare%20the%20definition(s)%20of%20%22Should%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%2Fare%20the%20definition(s)%20of%20%22Should%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhG59vonPheZ4xRmQz%2Fwhat-is-are-the-definition-s-of-should%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%2Fare%20the%20definition(s)%20of%20%22Should%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhG59vonPheZ4xRmQz%2Fwhat-is-are-the-definition-s-of-should", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhG59vonPheZ4xRmQz%2Fwhat-is-are-the-definition-s-of-should", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1310, "htmlBody": "<p><span style=\"font-family: arial;\"> </span></p>\n<div>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\"><strong>My Model</strong></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Consider an AI. This AI goes out into the world, observing things and doing things. This is a special AI, though. In converting observations into actions, it first transforms them into beliefs in some kind of propositional language. This may or may not be the optimal way to build an AI. Regardless, that's how it works.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The AI has a database, filled with propositions. The AI also has some code.</p>\n<ul>\n<li>It has code for turning propositions into logically equivalent propositions.</li>\n<li>It has code for turning observations about the world into propositions about these observations, like \"The pixel at location 343, 429 of image #8765 is red\"</li>\n<li>It has code for turning propositions about observations into propositions about the state of the world, like \"The apple in front of my camera is red.\"</li>\n<li>It has code for turning those propositions into propositions that express prediction, like \"There will still be a red apple there until someone moves it.\"</li>\n<li>It has code for turning those propositions into propositions about shouldness, like \"I should tell the scientists about that apple.\"</li>\n<li>It has code for turning propositions about shouldness into actions.</li>\n</ul>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">What is special about this code is that it can't be expressed as propositions. Just as one <a href=\"/lw/rs/created_already_in_motion/\">can't argue morality into a rock</a>, the AI doesn't function if it doesn't have this code, no matter what propositions are stored in its memory. The classic example of this would be the Tortoise, from <a href=\"http://en.wikipedia.org/wiki/What_the_Tortoise_Said_to_Achilles\">What the Tortoise said to Achilles</a>.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\"><strong>Axioms - Assumptions and Definitions</strong></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">When we, however, observe the AI, we can put everything into words. We can express its starting state, both the propositions and the code, as a set of axioms. We can then watch as it logically draws conclusions from these axioms, ending on a decision of what action to take.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The important thing, therefore, is to check that its initial axioms are correct. There is a key distinction here, because it seems like there are two kinds of axioms going on. Consider two from Euclid:</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">First, his definition of a point, which I believe is \"a point is that which has no part\". It does not seem like this could be wrong. If it turns out that nothing has no part, then that just means that there aren't any points.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Second, one of the postulates, like the parallel postulate. This one could be wrong. Because of General Relativity, when applied to the real world, it is, in fact, wrong.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">This boundary can be somewhat fluid when we interpret the propositions in a new light. For instance, if we prefix each axiom with, \"In a Euclidean space, \", then the whole system is just the definition of a Euclidean space.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">A necessary condition for some axiom to be a definition of a term is that you can't draw any new conclusions that don't involve that term from the definition. This also applies to groups of definitions. That is:</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">\"Stars are the things that produce the lights we see in the sky at night that stay fixed relative to each other, and also the Sun\" and \"Stars are gigantic balls of plasma undergoing nuclear reactions\" can both be definitions of the word \"Star\", since we know them to be equivalent. If, however, we did not know those concepts to be equivalent (if we needed to be really really really confident in our conclusions, for instance) then only one of those could be the definition.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\"><strong>What are the AI's definitions?</strong></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Logic: It seems to me that this code both defines logical terms and makes assumptions about their properties. In logic it is very hard to tell the difference.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Observation: Here it is easier. The meaning of a statement like \"I see something red\" is given by the causal process that leads to seeing something red. This code, therefore, provides a definition.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The world: What is the meaning of a statement about the world? This meaning, as we know from the <a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">litany of Tarski</a>, comes from the world. One can't define facts about the world into existence, so this code must consist of assumptions.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Predictions: The meaning of a prediction, clearly, is not determined by the process used to create it. It's determined by what sort of events would confirm or falsify a prediction. The code that deduces predictions from states of the world might include part of the definition of those states. For instance, red things are defined as those things predicted to create the appearance of redness. It cannot, though, define the future - it can only assume that its process for producing predictions is accurate.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Shouldness: Now we come to the fun part. There are two separate ways to define \"should\" here. We can define it by the code that produces should statements, or by the code that uses them.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">When you have two different definitions, one thing you can do is to decide that they're defining two different words. Let's call the first one AI_should and the second Actually_Should.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">ETA: Another way to state this is \"Which kinds of statement can be reduced, through definition, to which others kinds of statements?\" I argue that while one can reduce facts about observations to facts about the world (as long as qualia don't exist), one cannot reduce statements backwards - something new is introduced at each step. People think that we should be able to reduce ethical statements to the previous kinds of statements. Why, when so many other reductions are not possible?</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\"><strong>Is the second definition acceptable?</strong></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The first claim I would like to make is that Actually_Should is a well-defined term - that AI_should and me_should and you_should are not all that there is.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">One counter-claim that can be made against it is that it is ambiguous. But if it were, then one would presumably be able to clarify the definition with additional statements about it. But this cannot be done, because then one would be able to define certain actions into existence.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\"><strong>Is the second definition better?</strong></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">I'm not sure \"better\" is well-defined here, but I'm going to argue that it is. I think these arguments should at least shed light on the first question.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The first thing that strikes me as off about AI_should is that if you use it, then your definition will be very long and complex (because <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">Human Value is Complex</a>) but your assumption will be short (because the code that takes \"You should do X\" and then does X can be minimal). This is backwards - definitions should be clean, elegant, and simple, while assumptions often have to be messy and complex.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The second thing is that it doesn't seem to be very useful for communication. When I tell someone something, it is usually because I want them to do something. I might tell someone that there is a deadly snake nearby, so that they will be wary. I may not always know exactly what someone will do with information, but I can guess that some information will probably improve the quality of their decisions.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">How useful, then to communicate directly in \"Actually_should\", to say to Alice the proposition that, if Alice believed it, would cause her to do X, from which she conclude that \"Bob believes the proposition that, if I believed it, would cause me to do X\".</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">If, on the other hand, Bob says that Alice Bob_should do X, then Alice might respond with \"that's interesting, but I don't care\", and would not be able to respond with \"You're wrong.\" This would paper over the very real moral disagreement between them. This is often advisable in social situations, but rarely useful epistemically.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">How do we know that there is a disagreement? Well, if the issue is significant enough, both sides would feel justified in starting a war over it. This is true even if they agree on Alice_Should and Bob_Should and so on for everyone on Earth. That seems like a pretty real disagreement to me.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hG59vonPheZ4xRmQz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 10, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "7780", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"font-family: arial;\"> </span></p>\n<div>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\"><strong id=\"My_Model\">My Model</strong></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Consider an AI. This AI goes out into the world, observing things and doing things. This is a special AI, though. In converting observations into actions, it first transforms them into beliefs in some kind of propositional language. This may or may not be the optimal way to build an AI. Regardless, that's how it works.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The AI has a database, filled with propositions. The AI also has some code.</p>\n<ul>\n<li>It has code for turning propositions into logically equivalent propositions.</li>\n<li>It has code for turning observations about the world into propositions about these observations, like \"The pixel at location 343, 429 of image #8765 is red\"</li>\n<li>It has code for turning propositions about observations into propositions about the state of the world, like \"The apple in front of my camera is red.\"</li>\n<li>It has code for turning those propositions into propositions that express prediction, like \"There will still be a red apple there until someone moves it.\"</li>\n<li>It has code for turning those propositions into propositions about shouldness, like \"I should tell the scientists about that apple.\"</li>\n<li>It has code for turning propositions about shouldness into actions.</li>\n</ul>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">What is special about this code is that it can't be expressed as propositions. Just as one <a href=\"/lw/rs/created_already_in_motion/\">can't argue morality into a rock</a>, the AI doesn't function if it doesn't have this code, no matter what propositions are stored in its memory. The classic example of this would be the Tortoise, from <a href=\"http://en.wikipedia.org/wiki/What_the_Tortoise_Said_to_Achilles\">What the Tortoise said to Achilles</a>.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\"><strong id=\"Axioms___Assumptions_and_Definitions\">Axioms - Assumptions and Definitions</strong></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">When we, however, observe the AI, we can put everything into words. We can express its starting state, both the propositions and the code, as a set of axioms. We can then watch as it logically draws conclusions from these axioms, ending on a decision of what action to take.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The important thing, therefore, is to check that its initial axioms are correct. There is a key distinction here, because it seems like there are two kinds of axioms going on. Consider two from Euclid:</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">First, his definition of a point, which I believe is \"a point is that which has no part\". It does not seem like this could be wrong. If it turns out that nothing has no part, then that just means that there aren't any points.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Second, one of the postulates, like the parallel postulate. This one could be wrong. Because of General Relativity, when applied to the real world, it is, in fact, wrong.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">This boundary can be somewhat fluid when we interpret the propositions in a new light. For instance, if we prefix each axiom with, \"In a Euclidean space, \", then the whole system is just the definition of a Euclidean space.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">A necessary condition for some axiom to be a definition of a term is that you can't draw any new conclusions that don't involve that term from the definition. This also applies to groups of definitions. That is:</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">\"Stars are the things that produce the lights we see in the sky at night that stay fixed relative to each other, and also the Sun\" and \"Stars are gigantic balls of plasma undergoing nuclear reactions\" can both be definitions of the word \"Star\", since we know them to be equivalent. If, however, we did not know those concepts to be equivalent (if we needed to be really really really confident in our conclusions, for instance) then only one of those could be the definition.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\"><strong id=\"What_are_the_AI_s_definitions_\">What are the AI's definitions?</strong></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Logic: It seems to me that this code both defines logical terms and makes assumptions about their properties. In logic it is very hard to tell the difference.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Observation: Here it is easier. The meaning of a statement like \"I see something red\" is given by the causal process that leads to seeing something red. This code, therefore, provides a definition.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The world: What is the meaning of a statement about the world? This meaning, as we know from the <a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">litany of Tarski</a>, comes from the world. One can't define facts about the world into existence, so this code must consist of assumptions.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Predictions: The meaning of a prediction, clearly, is not determined by the process used to create it. It's determined by what sort of events would confirm or falsify a prediction. The code that deduces predictions from states of the world might include part of the definition of those states. For instance, red things are defined as those things predicted to create the appearance of redness. It cannot, though, define the future - it can only assume that its process for producing predictions is accurate.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">Shouldness: Now we come to the fun part. There are two separate ways to define \"should\" here. We can define it by the code that produces should statements, or by the code that uses them.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">When you have two different definitions, one thing you can do is to decide that they're defining two different words. Let's call the first one AI_should and the second Actually_Should.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">ETA: Another way to state this is \"Which kinds of statement can be reduced, through definition, to which others kinds of statements?\" I argue that while one can reduce facts about observations to facts about the world (as long as qualia don't exist), one cannot reduce statements backwards - something new is introduced at each step. People think that we should be able to reduce ethical statements to the previous kinds of statements. Why, when so many other reductions are not possible?</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\"><strong id=\"Is_the_second_definition_acceptable_\">Is the second definition acceptable?</strong></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The first claim I would like to make is that Actually_Should is a well-defined term - that AI_should and me_should and you_should are not all that there is.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">One counter-claim that can be made against it is that it is ambiguous. But if it were, then one would presumably be able to clarify the definition with additional statements about it. But this cannot be done, because then one would be able to define certain actions into existence.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\"><strong id=\"Is_the_second_definition_better_\">Is the second definition better?</strong></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">I'm not sure \"better\" is well-defined here, but I'm going to argue that it is. I think these arguments should at least shed light on the first question.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The first thing that strikes me as off about AI_should is that if you use it, then your definition will be very long and complex (because <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">Human Value is Complex</a>) but your assumption will be short (because the code that takes \"You should do X\" and then does X can be minimal). This is backwards - definitions should be clean, elegant, and simple, while assumptions often have to be messy and complex.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">The second thing is that it doesn't seem to be very useful for communication. When I tell someone something, it is usually because I want them to do something. I might tell someone that there is a deadly snake nearby, so that they will be wary. I may not always know exactly what someone will do with information, but I can guess that some information will probably improve the quality of their decisions.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">How useful, then to communicate directly in \"Actually_should\", to say to Alice the proposition that, if Alice believed it, would cause her to do X, from which she conclude that \"Bob believes the proposition that, if I believed it, would cause me to do X\".</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">If, on the other hand, Bob says that Alice Bob_should do X, then Alice might respond with \"that's interesting, but I don't care\", and would not be able to respond with \"You're wrong.\" This would paper over the very real moral disagreement between them. This is often advisable in social situations, but rarely useful epistemically.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; min-height: 15.0px;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial;\">How do we know that there is a disagreement? Well, if the issue is significant enough, both sides would feel justified in starting a war over it. This is true even if they agree on Alice_Should and Bob_Should and so on for everyone on Earth. That seems like a pretty real disagreement to me.</p>\n</div>", "sections": [{"title": "My Model", "anchor": "My_Model", "level": 1}, {"title": "Axioms - Assumptions and Definitions", "anchor": "Axioms___Assumptions_and_Definitions", "level": 1}, {"title": "What are the AI's definitions?", "anchor": "What_are_the_AI_s_definitions_", "level": 1}, {"title": "Is the second definition acceptable?", "anchor": "Is_the_second_definition_acceptable_", "level": 1}, {"title": "Is the second definition better?", "anchor": "Is_the_second_definition_better_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "46 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CuSTqHgeK4CMpWYTe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T22:06:30.950Z", "modifiedAt": null, "url": null, "title": "Can't downvote", "slug": "can-t-downvote", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:28.652Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Document", "createdAt": "2010-02-08T04:14:47.949Z", "isAdmin": false, "displayName": "Document"}, "userId": "vaMNHjzaCGqF8yTMS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mXKJXs8twt9mzAA2J/can-t-downvote", "pageUrlRelative": "/posts/mXKJXs8twt9mzAA2J/can-t-downvote", "linkUrl": "https://www.lesswrong.com/posts/mXKJXs8twt9mzAA2J/can-t-downvote", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can't%20downvote&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan't%20downvote%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmXKJXs8twt9mzAA2J%2Fcan-t-downvote%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can't%20downvote%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmXKJXs8twt9mzAA2J%2Fcan-t-downvote", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmXKJXs8twt9mzAA2J%2Fcan-t-downvote", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 28, "htmlBody": "<p>Downvote buttons seem to have just stopped working for me, saying I need at least 1 karma. I'm posting in case it leads someone to fix the problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mXKJXs8twt9mzAA2J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 7.223047062478348e-07, "legacy": true, "legacyId": "7781", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-01T23:40:28.052Z", "modifiedAt": null, "url": null, "title": "[Link] Enhanced Autodidacticism for the Chronically Lazy and Hyperactive", "slug": "link-enhanced-autodidacticism-for-the-chronically-lazy-and", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/38FydKdaWcmpCiSJB/link-enhanced-autodidacticism-for-the-chronically-lazy-and", "pageUrlRelative": "/posts/38FydKdaWcmpCiSJB/link-enhanced-autodidacticism-for-the-chronically-lazy-and", "linkUrl": "https://www.lesswrong.com/posts/38FydKdaWcmpCiSJB/link-enhanced-autodidacticism-for-the-chronically-lazy-and", "postedAtFormatted": "Wednesday, June 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Enhanced%20Autodidacticism%20for%20the%20Chronically%20Lazy%20and%20Hyperactive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Enhanced%20Autodidacticism%20for%20the%20Chronically%20Lazy%20and%20Hyperactive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F38FydKdaWcmpCiSJB%2Flink-enhanced-autodidacticism-for-the-chronically-lazy-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Enhanced%20Autodidacticism%20for%20the%20Chronically%20Lazy%20and%20Hyperactive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F38FydKdaWcmpCiSJB%2Flink-enhanced-autodidacticism-for-the-chronically-lazy-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F38FydKdaWcmpCiSJB%2Flink-enhanced-autodidacticism-for-the-chronically-lazy-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p>Takeaway seems to be: stay light on your feet; keep everything in the short term, but build habits that will serve you in the long term; make sure you're always doing something that holds your interest.</p>\n<p>&nbsp;</p>\n<p>http://somebeautifulplace.tumblr.com/post/6074297771/enhanced-autodidactism-for-the-chronically-lazy-and</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "38FydKdaWcmpCiSJB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 7.223326062349536e-07, "legacy": true, "legacyId": "7782", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T00:10:01.205Z", "modifiedAt": null, "url": null, "title": "(Philosophical) Disagreements are not Rational", "slug": "philosophical-disagreements-are-not-rational", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.394Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nnAZBLfKBnF276cqe/philosophical-disagreements-are-not-rational", "pageUrlRelative": "/posts/nnAZBLfKBnF276cqe/philosophical-disagreements-are-not-rational", "linkUrl": "https://www.lesswrong.com/posts/nnAZBLfKBnF276cqe/philosophical-disagreements-are-not-rational", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20(Philosophical)%20Disagreements%20are%20not%20Rational&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A(Philosophical)%20Disagreements%20are%20not%20Rational%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnnAZBLfKBnF276cqe%2Fphilosophical-disagreements-are-not-rational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=(Philosophical)%20Disagreements%20are%20not%20Rational%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnnAZBLfKBnF276cqe%2Fphilosophical-disagreements-are-not-rational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnnAZBLfKBnF276cqe%2Fphilosophical-disagreements-are-not-rational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 439, "htmlBody": "<p>This is a combination news-announcement and begging for someone with academic subscriptions to maybe jailbreak a PDF for us.</p>\n<p><a href=\"http://www.sciencedirect.com/science/article/pii/S1053810011001231\">\"Persistent bias in expert judgments about free will and moral responsibility: A test of the expertise defense\"</a> (emphasis added):</p>\n<blockquote>\n<p>\"Many philosophers appeal to intuitions to support some philosophical views. However, there is reason to be concerned about this practice as scientific evidence has documented systematic bias in philosophically relevant intuitions as a function of seemingly irrelevant features (e.g., personality). One popular defense used to insulate philosophers from these concerns holds that philosophical expertise eliminates the influence of these extraneous factors. Here, we test this assumption. We present data suggesting that verifiable philosophical expertise in the free will debate&mdash;as measured by a reliable and validated test of expert knowledge&mdash;does not eliminate the influence of one important extraneous feature (i.e., the <em>heritable</em> personality trait extraversion) on judgments concerning freedom and moral responsibility. These results suggest that, in at least some important cases, the expertise defense fails. Implications for the practice of philosophy, experimental philosophy, and applied ethics are discussed.\"</p>\n</blockquote>\n<p>Linked from http://experimentalphilosophy.typepad.com/experimental_philosophy/2011/06/failure-of-the-expertise-defense-persistent-bias-in-expert-intuitions-.html which elaborates:</p>\n<blockquote>\n<p>\"For example, our research suggests that heritable personality traits predict bias in some fundamental philosophically relevant intuitions (Feltz &amp; Cokely <a href=\"http://faculty.schreiner.edu/adfeltz/Papers/pp329_feltz.pdf\">2008</a>, <a href=\"http://www.sciencedirect.com/science/article/pii/S1053810008001220\">2009</a>; Cokely &amp; Feltz, <a href=\"http://www.sciencedirect.com/science/article/pii/S0092656608001451\">2009</a>; Feltz, Perez, &amp; Harris, in press; Feltz, Harris, &amp; Perez, <a href=\"http://faculty.schreiner.edu/adfeltz/Papers/Feltz,%20Harris,%20&amp;%20Perez_Feb%206.pdf\">2010</a>). In response to these findings, &ldquo;philosophical expertise&rdquo; has been used to shield some parts of standard philosophical practice from the worries presented by experimental philosophers (e.g., Ludwig, <a href=\"http://ahpc-jp30.st-and.ac.uk/%7Eahwiki/pub/Arche/MethodologySeminar19Dec2008/mws2007ludwig.pdf\">2007</a>; Kauppinen <a href=\"http://www.helsinki.fi/%7Eamkauppi/phil/The_Rise_and_Fall_of_Experimental_Philosophy.pdf\">2007</a>; Horvarth, <a href=\"http://www.informaworld.com/smpp/content%7Econtent=a925856616%7Edb=all%7Ejumptype=rss\">2010</a>; Sosa, <a href=\"http://www.informaworld.com/smpp/content%7Econtent=a925857953%7Edb=all%7Ejumptype=rss\">2010</a>; Williamson, <a href=\"http://www.wiley.com/WileyCDA/WileyTitle/productCd-1405133961.html\">2007</a>, <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9973.2011.01685.x/abstract\">2011</a>). One important part of the &ldquo;Expertise Defense&rdquo; is that philosophers are assumed to be relevantly different from the folk (e.g., as a result of their years of training) and consequently philosophers' intuitions shouldn&rsquo;t display the same (or similar) biases.</p>\n<p>But more recently, there have been serious concerns raised by experimental philosophers about the Expertise Defense. Some have used indirect strategies suggesting that philosophical expertise is unlike expertise in areas known to result in the relevant differences (e.g., in chess) (Weinberg, Gonnerman, Buckner, &amp; Alexander, <a href=\"http://www.informaworld.com/smpp/content%7Econtent=a923357364%7Edb=all%7Ejumptype=rss\">2010</a> see related discussion <a href=\"http://experimentalphilosophy.typepad.com/experimental_philosophy/2009/08/are-philosophers-expert-intuiters.html\">here</a>). Others have opted for direct strategies showing that for many important everyday behaviors (e.g., voting, returning library books, showing common courtesy) philosophers often display the same (or similar) biases as the folk (Schwitzgebel <a href=\"http://www.faculty.ucr.edu/%7Eeschwitz/SchwitzAbs/EthicsBooks.htm\">2009</a>; Schwitzgebel &amp; Rust, <a href=\"http://escholarship.org/uc/item/887203mm#page-1\">2010</a>, <a href=\"http://www.faculty.ucr.edu/%7Eeschwitz/SchwitzAbs/PeerOpinion.htm\">2009</a>; Schwitzgebel &amp; Cushman,<a href=\"http://www.faculty.ucr.edu/%7Eeschwitz/SchwitzAbs/EthOrder.htm\"> in press</a>). In a new paper (Schulz, Cokely, &amp; Feltz,<a href=\"http://www.sciencedirect.com/science/article/pii/S1053810011001231\"> in press</a>), we also adopt the direct strategy and present the first evidence that personality predicts persistent bias in verifiable expert intuitions about free will and moral responsibility. These results suggest that, in at least some important fundamental philosophical debates, the Expertise Defense fails\"</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nnAZBLfKBnF276cqe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "7783", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T01:33:26.292Z", "modifiedAt": null, "url": null, "title": "DC Meetup June 5th, 1 PM", "slug": "dc-meetup-june-5th-1-pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.762Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q5bmsGX2soxiLpocq/dc-meetup-june-5th-1-pm", "pageUrlRelative": "/posts/Q5bmsGX2soxiLpocq/dc-meetup-june-5th-1-pm", "linkUrl": "https://www.lesswrong.com/posts/Q5bmsGX2soxiLpocq/dc-meetup-june-5th-1-pm", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20DC%20Meetup%20June%205th%2C%201%20PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADC%20Meetup%20June%205th%2C%201%20PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ5bmsGX2soxiLpocq%2Fdc-meetup-june-5th-1-pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=DC%20Meetup%20June%205th%2C%201%20PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ5bmsGX2soxiLpocq%2Fdc-meetup-june-5th-1-pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ5bmsGX2soxiLpocq%2Fdc-meetup-june-5th-1-pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"> </span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><span style=\"font-style: italic;\">Sunday June 5th</span><br /><span style=\"font-style: italic;\">Core hours: 1 PM - 3PM</span><br /><span style=\"font-style: italic;\">Chipotle Mexican Grill<br /><span style=\"font-style: italic;\">7600 Old Georgetown Road</span><br /><span style=\"font-style: italic;\">Bethesda, MD 20814</span></span></span></em></p>\n<p><em style=\"font-style: italic;\"> </em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><strong>Notes:</strong><br />Core hours are 1-3, but many people are willing to stay around longer than that.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><em style=\"font-style: italic;\"><span style=\"font-style: normal;\"><strong style=\"font-weight: bold;\">Directions:</strong><br />The Chipotle is near the Bethesda Metro station, just follow&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://maps.google.com/maps?f=d&amp;source=s_d&amp;saddr=Old+Georgetown+Rd&amp;daddr=7600+Old+Georgetown+Rd,+Bethesda,+MD+20814&amp;hl=en&amp;geocode=FVXcUgId66Fn-w%3BFarfUgIdn5tn-ynZQi-7ZMm3iTG95vklOa-npw&amp;mra=me&amp;mrsp=0&amp;sz=18&amp;sll=38.985195,-77.0952&amp;sspn=0.002243,0.003449&amp;ie=UTF8&amp;t=h&amp;z=18\">these</a>&nbsp;directions. Go out of the station and walk down Old Georgetown Road. The Chipotle is next to the waterfall fountain, and outdoor seating area.<br /><br />We will be sitting towards the back of the restaurant (basically, keep going in the direction you've been walking to get to the Chipotle), in the corner with a wraparound bench.<br /><br />See us also at lesswrong-dc@googlegroups.com</span></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q5bmsGX2soxiLpocq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "7784", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T02:51:20.589Z", "modifiedAt": null, "url": null, "title": "Brief question about Conway's Game of LIfe and AI", "slug": "brief-question-about-conway-s-game-of-life-and-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:08.358Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Lh82F4Xa5unwAbMqv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dg8gfsiCNWanFRDm9/brief-question-about-conway-s-game-of-life-and-ai", "pageUrlRelative": "/posts/dg8gfsiCNWanFRDm9/brief-question-about-conway-s-game-of-life-and-ai", "linkUrl": "https://www.lesswrong.com/posts/dg8gfsiCNWanFRDm9/brief-question-about-conway-s-game-of-life-and-ai", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brief%20question%20about%20Conway's%20Game%20of%20LIfe%20and%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrief%20question%20about%20Conway's%20Game%20of%20LIfe%20and%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdg8gfsiCNWanFRDm9%2Fbrief-question-about-conway-s-game-of-life-and-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brief%20question%20about%20Conway's%20Game%20of%20LIfe%20and%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdg8gfsiCNWanFRDm9%2Fbrief-question-about-conway-s-game-of-life-and-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdg8gfsiCNWanFRDm9%2Fbrief-question-about-conway-s-game-of-life-and-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 10pt;\"><span style=\"font-family: Calibri;\"> </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 10pt;\">Conway&rsquo;s Game of Life is Turing-complete. Therefore, it is possible to create an AI in it. If you created a 3^^3 by 3^^3 Life board, setting the initial state at random, presumably somewhere an AI would be created. Would this AI somehow take over the whole game board, if given enough time?</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 10pt;\">Would this be visible from the top, as it were?</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 10pt;\">EDIT: I probably meant 3^^^3, sorry. Also, by generating at random, I meant 50% chance on. But any other chance would work too, I suspect.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dg8gfsiCNWanFRDm9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "7788", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T10:08:00.643Z", "modifiedAt": null, "url": null, "title": "Tricksy probability problem", "slug": "tricksy-probability-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:17.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Lh82F4Xa5unwAbMqv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jePF6Ko8r4vge96N3/tricksy-probability-problem", "pageUrlRelative": "/posts/jePF6Ko8r4vge96N3/tricksy-probability-problem", "linkUrl": "https://www.lesswrong.com/posts/jePF6Ko8r4vge96N3/tricksy-probability-problem", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tricksy%20probability%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATricksy%20probability%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjePF6Ko8r4vge96N3%2Ftricksy-probability-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tricksy%20probability%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjePF6Ko8r4vge96N3%2Ftricksy-probability-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjePF6Ko8r4vge96N3%2Ftricksy-probability-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<p>I came up with this problem the other day. I don't have nearly enough math to solve it. Do any of you wise folk have insight?</p>\n<p>Imagine a sequence of randomly generated 0s and 1s. I start from nothing and generate this sequence one term at a time. I stop when I'm at the end of a streak of 1s which is at least one half the length of the total sequence.</p>\n<p>For example:</p>\n<p>01</p>\n<p>0010110101101011111111111111</p>\n<p>010110111111 -<strong>EDIT:</strong> Obviously, this is wrong, because it would terminate at 01. Sorry.</p>\n<p>What is the average length of such sequences?</p>\n<p>I know that all of these sequences will eventually terminate. I just don't know if the length diverges, and I'm not sure how to deal with the problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jePF6Ko8r4vge96N3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 7.225180544833967e-07, "legacy": true, "legacyId": "7805", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T11:39:24.484Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Consolidated Nature of Morality Thread", "slug": "seq-rerun-consolidated-nature-of-morality-thread", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/722r3hyGkgMCTDsZY/seq-rerun-consolidated-nature-of-morality-thread", "pageUrlRelative": "/posts/722r3hyGkgMCTDsZY/seq-rerun-consolidated-nature-of-morality-thread", "linkUrl": "https://www.lesswrong.com/posts/722r3hyGkgMCTDsZY/seq-rerun-consolidated-nature-of-morality-thread", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Consolidated%20Nature%20of%20Morality%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Consolidated%20Nature%20of%20Morality%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F722r3hyGkgMCTDsZY%2Fseq-rerun-consolidated-nature-of-morality-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Consolidated%20Nature%20of%20Morality%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F722r3hyGkgMCTDsZY%2Fseq-rerun-consolidated-nature-of-morality-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F722r3hyGkgMCTDsZY%2Fseq-rerun-consolidated-nature-of-morality-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 295, "htmlBody": "<p>Today's post, <a href=\"/lw/ho/consolidated_nature_of_morality_thread/\">Consolidated Nature of Morality Thread</a> was originally published on April 15, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>Disputes about the nature of morality tend to overwhelm other discussions, so this post was intended to be a home for those tangential thoughts.<br /><br /> Examples of questions to be discussed here include: What is the difference between \"is\" and \"ought\" statements? Why do some preferences seem voluntary? Do children believe God can choose what is moral? Is there a systematic direction to the development of moral beliefs in history, and, if so, what is the causal explanation of this? Does Tarski's definition of truth extend to moral statements? If you were physically altered to prefer killing, would \"killing is good\" become true? If the truth value of a moral claim cannot be changed by any physical act, does this make the claim stronger or weaker than other claims? What are the referents of moral claims, or are they empty of content? Are there \"pure\" ought-statements, or do they all have is-statements mixed into them? Are there pure aesthetic judgments or preferences?</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/603/seq_rerun_your_rationality_is_my_business/\">Your Rationality is My Business</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "722r3hyGkgMCTDsZY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 7.225450707866425e-07, "legacy": true, "legacyId": "7808", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rfDS25Pdoij2ZFeJ9", "geEHc379o2rHzT7cY", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T13:20:09.041Z", "modifiedAt": null, "url": null, "title": "[link] Reasoning Under Uncertainty Videos", "slug": "link-reasoning-under-uncertainty-videos", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qQrgQ2DhceMoRZbhM/link-reasoning-under-uncertainty-videos", "pageUrlRelative": "/posts/qQrgQ2DhceMoRZbhM/link-reasoning-under-uncertainty-videos", "linkUrl": "https://www.lesswrong.com/posts/qQrgQ2DhceMoRZbhM/link-reasoning-under-uncertainty-videos", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Reasoning%20Under%20Uncertainty%20Videos&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Reasoning%20Under%20Uncertainty%20Videos%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqQrgQ2DhceMoRZbhM%2Flink-reasoning-under-uncertainty-videos%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Reasoning%20Under%20Uncertainty%20Videos%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqQrgQ2DhceMoRZbhM%2Flink-reasoning-under-uncertainty-videos", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqQrgQ2DhceMoRZbhM%2Flink-reasoning-under-uncertainty-videos", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>I just stumbled across this youtube \"lecture series\" by Trond Reitan which looks as a promising intro to Bayesian statistics &nbsp; <a href=\"http://www.youtube.com/trondreitan\">http://www.youtube.com/trondreitan</a>. Curious what others here think, could be a good resource to refer people to. Look for the \"reasoning under uncertainty\" playlist on the right. As an aside the guy seems to fall well into the LW rationalist cluster based on the videos and home page&nbsp;<a href=\"http://folk.uio.no/trondr/research.html\">http://folk.uio.no/trondr/research.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qQrgQ2DhceMoRZbhM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 7.22574851556335e-07, "legacy": true, "legacyId": "7809", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T14:59:00.325Z", "modifiedAt": null, "url": null, "title": "Action and habit", "slug": "action-and-habit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:06.242Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XNhfw5Bqsi4SGNNBk/action-and-habit", "pageUrlRelative": "/posts/XNhfw5Bqsi4SGNNBk/action-and-habit", "linkUrl": "https://www.lesswrong.com/posts/XNhfw5Bqsi4SGNNBk/action-and-habit", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Action%20and%20habit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAction%20and%20habit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXNhfw5Bqsi4SGNNBk%2Faction-and-habit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Action%20and%20habit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXNhfw5Bqsi4SGNNBk%2Faction-and-habit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXNhfw5Bqsi4SGNNBk%2Faction-and-habit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1344, "htmlBody": "<p>I remember a poster that hung on the wall of my seventh grade classroom. It went like this:</p>\n<blockquote>\n<p>Watch your thoughts, for they become words.<br />Watch your words, for they become actions.<br />Watch your actions, for they become habits.<br />Watch your habits, for they become your character.<br />Watch your character, for it becomes your destiny.</p>\n</blockquote>\n<p>It was as a competitive swimmer that these words were the most meaningful to me. Most sports are ultimately about the practice, about repeating an action over and over and over again, so that actions become habits and habits become character. The fleeting thought that <em>I really hate getting up at 5:00 am for swim practice </em>is just that: a fleeting thought. But if I justified it with words, speaking it aloud to my parents or siblings or friends, it became a fact that others knew about me, much realer than just a wispy thought. The action of forgetting-on-purpose to set my alarm, or faking sick, was a logical next step. And one missed practice might not be huge, in the long run, but it led easily to a habit of missing practice, say, once a week. A year of this, and I would start to think of myself as the <em>kind of person</em> who missed practice once a week, because after all, isn&rsquo;t it silly of anyone to expect a twelve-year-old to get up at 5:00 three times a week? And that attitude could very easily have led, over a couple of years, to quitting the team.&nbsp;<a id=\"more\"></a></p>\n<p>As a matter of fact, none of this happened. As a child I had a large measure of Conscientiousness, and putting long-term goals, like getting best times and earning my coach&rsquo;s approval, ahead of short-term goals like sleeping another three hours, came to me without too much difficulty. The cycle went the other way. My habitual response to the brief temptation to sleep in, namely <em>screw sleep, this is how you&rsquo;re going to get faster, </em>my verbal statements to just about everyone that I loved swimming, and the action of getting up three times a week and trekking to the pool after school another&nbsp;three times all reinforced the habit of working hard...which, over the five or so years that I competed, did become a fairly permanent character trait that generalized to things like school and work.</p>\n<p>Of course, the quote doesn&rsquo;t only apply to hard work. It applies to being generous or to being thrifty, to kindness or anger. A thought that happens once leaves a small trail. If it happens a thousand times, it leaves a deep trench. As positive (<em>I can do anything I set my mind to!) </em>or negative (<em>I always fail, no matter how hard I try) </em>thoughts become associated with given situations, they lend those situations their emotional colour. Swim practice, or school or work, becomes either positive or negative.</p>\n<h4>Actions and Habits</h4>\n<p>A lot of what I&rsquo;ve read on LessWrong about habits is in the context of <a href=\"/lw/2sh/break_your_habits_be_more_empirical/\">breaking</a> them. And yes, in some ways habits can act as a cognitive bias, a way of filtering the world that causes us to miss important opportunities, and habits are just as likely to be \"bad\" as to be good. (Maybe more likely.) But habits are also a powerful tool to <em>get stuff done</em>. As most of us know, <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">an intention to do something doesn't necessarily translate to doing it</a>. However, according to this article<sup>1</sup>, the strength of habit predicts how much students exercise, which their <em>intention</em> to exercise often to fails to predict.</p>\n<blockquote>\n<p>Habits are routinized behaviors that have been frequently paired with stable environmental contexts and, as a result of this pairing, are automatically rather than intentionally set in motion&hellip; Habit theory postulates that the intention&ndash;exercise relationship is a function of habit strength with a stronger intention&ndash;exercise relationship at lower levels of habit strength.</p>\n</blockquote>\n<p>Imagine the advantages of <em>automatically </em>setting aside an hour a day to exercise! Not only will you experience the health benefits, but if it&rsquo;s an automatic rather than an intentional behavior, you&rsquo;ll tend to exercise whether or not you feel motivated on given day, even under stress, even when you're tired and drained after a bad day.<sup>2</sup>&nbsp;And yes, this is a habit I&rsquo;ve (re)constructed in myself after a post-swim-team year of barely exercising at all. Having been active as a child and teenager, it was probably easier for me to build it into a habit than it would have been for a lifelong couch potato, but it would still be <em>possible </em>for them. Likewise, as far as I can tell from anecdotal evidence, it&rsquo;s much easier to stick to a long-term <em>habit </em>of healthy eating than to a temporary diet.</p>\n<p>How can you turn something into a habit, as opposed to a series of intentional actions? <a href=\"/lw/379/cheat_codes/\">This post</a> suggests planning for the long-term rather than the short term. &ldquo;If I&rsquo;m really good with my diet this month, I&rsquo;ll lose weight and then I can start eating whatever I like again&rdquo; is not a good <em>long term </em>motivational thought. Even in the short term I&rsquo;ve found that I <em>resent </em>the things I force myself to do with this excuse, whereas I don&rsquo;t resent my habitual behaviors like &ldquo;exercise every day&rdquo; and &ldquo;never buy fast food or unhealthy snacks.&rdquo;</p>\n<h4>Habits and Character</h4>\n<p>The habit of exercising doesn&rsquo;t necessarily influence other behaviors, but if maintained for long enough, it segues into the <em>character trait </em>of being a health-conscious person with good self-control. If I have evidence to present to myself that I have healthy habits (&ldquo;just look, I swam for an hour three to five times a week for a whole year, I <em>must </em>be the kind of person who&rsquo;s fit&rdquo;) then it becomes easier to start new &ldquo;good&rdquo; habits, like healthy eating. I can correct my fleeting thoughts of how tempting the free baked goods are, tell myself &ldquo;of course you have enough self-control not to eat those cookies, you&rsquo;re the kind of person who has healthy habits.&rdquo; At this point the motivational quote becomes circular; Habits and Character affect Thoughts, which affect Words and Actions. This isn&rsquo;t a logical paradox if it works, and it seems to work well for me. The more I exercise in a given month, the easier it is to have self-restraint in other areas.</p>\n<p>And even the fact that I have good self-control is, I think, partly based on <em>believing </em>it about myself (&ldquo;I got up at 5 am for swim practice three times a week for <em>five years</em>, I must have good self-control!) This seems to relate to the finding that willpower depletion depends on whether you <em>believe </em>your willpower will be depleted.<sup>3</sup></p>\n<h4>Conclusions</h4>\n<p>Anyone can develop any &ldquo;character trait.&rdquo; The requirement is simply enough years of thoughts becoming words becoming actions becoming habit. If you <em>believe </em>that something will get easier to maintain over time, it will. Not in the sense of time and resources&shy;; to get the continuing benefits of an hour&rsquo;s daily exercise, you have to pay the opportunity cost of that hour a day, no matter how many years you&rsquo;ve been doing it for; but in the sense of willpower and motivation. Your actions and habits will eventually change the person you believe yourself to be, which will affect just about everything else. I don&rsquo;t have any <em>direct </em>evidence that this process works if begun in adulthood, but intuitively it seems that it might work <em>better</em>, since adults are almost always <em>intrinsically</em> motivated&nbsp;in what they do,&nbsp;whereas children often do whatever activities their parents choose, whether or not it&rsquo;s something they&rsquo;re motivated to do.</p>\n<h4>References:</h4>\n<p>1. De Bruijn, G. J. , Rhodes, R. E.&nbsp;Exploring exercise behavior, intention and habit strength relationships. Scandinavian Journal of Medicine and Science in Sports. 2011: 21: 482&ndash;491.</p>\n<p><a href=\"http://pss.sagepub.com.proxy.bib.uottawa.ca/content/21/11/1686\"></a></p>\n<p>2. Schwabe, L., &amp; Wolf, O. T. (2011). Stress-induced modulation of instrumental behavior: From goal-directed to habitual control of action.<em> Behavioural Brain Research, 219</em>(2), 321-328.&nbsp;</p>\n<p>3. Job, V., Dweck, C. S., &amp; Walton, G. M. (2010). Ego depletion-is it all in your head? implicit theories about willpower affect self-regulation.<em> Psychological Science, 21</em>(11), 1686-1693.&nbsp;<a href=\"http://www.stanford.edu/~gwalton/home/Publications_files/Job,%20Dweck,%20%26%20Walton,%202010.pdf\">Link provided by Dr_Manhatten</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5Whwix4cZ3p5otshm": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XNhfw5Bqsi4SGNNBk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 101, "baseScore": 116, "extendedScore": null, "score": 0.000249, "legacy": true, "legacyId": "7810", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 116, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I remember a poster that hung on the wall of my seventh grade classroom. It went like this:</p>\n<blockquote>\n<p>Watch your thoughts, for they become words.<br>Watch your words, for they become actions.<br>Watch your actions, for they become habits.<br>Watch your habits, for they become your character.<br>Watch your character, for it becomes your destiny.</p>\n</blockquote>\n<p>It was as a competitive swimmer that these words were the most meaningful to me. Most sports are ultimately about the practice, about repeating an action over and over and over again, so that actions become habits and habits become character. The fleeting thought that <em>I really hate getting up at 5:00 am for swim practice </em>is just that: a fleeting thought. But if I justified it with words, speaking it aloud to my parents or siblings or friends, it became a fact that others knew about me, much realer than just a wispy thought. The action of forgetting-on-purpose to set my alarm, or faking sick, was a logical next step. And one missed practice might not be huge, in the long run, but it led easily to a habit of missing practice, say, once a week. A year of this, and I would start to think of myself as the <em>kind of person</em> who missed practice once a week, because after all, isn\u2019t it silly of anyone to expect a twelve-year-old to get up at 5:00 three times a week? And that attitude could very easily have led, over a couple of years, to quitting the team.&nbsp;<a id=\"more\"></a></p>\n<p>As a matter of fact, none of this happened. As a child I had a large measure of Conscientiousness, and putting long-term goals, like getting best times and earning my coach\u2019s approval, ahead of short-term goals like sleeping another three hours, came to me without too much difficulty. The cycle went the other way. My habitual response to the brief temptation to sleep in, namely <em>screw sleep, this is how you\u2019re going to get faster, </em>my verbal statements to just about everyone that I loved swimming, and the action of getting up three times a week and trekking to the pool after school another&nbsp;three times all reinforced the habit of working hard...which, over the five or so years that I competed, did become a fairly permanent character trait that generalized to things like school and work.</p>\n<p>Of course, the quote doesn\u2019t only apply to hard work. It applies to being generous or to being thrifty, to kindness or anger. A thought that happens once leaves a small trail. If it happens a thousand times, it leaves a deep trench. As positive (<em>I can do anything I set my mind to!) </em>or negative (<em>I always fail, no matter how hard I try) </em>thoughts become associated with given situations, they lend those situations their emotional colour. Swim practice, or school or work, becomes either positive or negative.</p>\n<h4 id=\"Actions_and_Habits\">Actions and Habits</h4>\n<p>A lot of what I\u2019ve read on LessWrong about habits is in the context of <a href=\"/lw/2sh/break_your_habits_be_more_empirical/\">breaking</a> them. And yes, in some ways habits can act as a cognitive bias, a way of filtering the world that causes us to miss important opportunities, and habits are just as likely to be \"bad\" as to be good. (Maybe more likely.) But habits are also a powerful tool to <em>get stuff done</em>. As most of us know, <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">an intention to do something doesn't necessarily translate to doing it</a>. However, according to this article<sup>1</sup>, the strength of habit predicts how much students exercise, which their <em>intention</em> to exercise often to fails to predict.</p>\n<blockquote>\n<p>Habits are routinized behaviors that have been frequently paired with stable environmental contexts and, as a result of this pairing, are automatically rather than intentionally set in motion\u2026 Habit theory postulates that the intention\u2013exercise relationship is a function of habit strength with a stronger intention\u2013exercise relationship at lower levels of habit strength.</p>\n</blockquote>\n<p>Imagine the advantages of <em>automatically </em>setting aside an hour a day to exercise! Not only will you experience the health benefits, but if it\u2019s an automatic rather than an intentional behavior, you\u2019ll tend to exercise whether or not you feel motivated on given day, even under stress, even when you're tired and drained after a bad day.<sup>2</sup>&nbsp;And yes, this is a habit I\u2019ve (re)constructed in myself after a post-swim-team year of barely exercising at all. Having been active as a child and teenager, it was probably easier for me to build it into a habit than it would have been for a lifelong couch potato, but it would still be <em>possible </em>for them. Likewise, as far as I can tell from anecdotal evidence, it\u2019s much easier to stick to a long-term <em>habit </em>of healthy eating than to a temporary diet.</p>\n<p>How can you turn something into a habit, as opposed to a series of intentional actions? <a href=\"/lw/379/cheat_codes/\">This post</a> suggests planning for the long-term rather than the short term. \u201cIf I\u2019m really good with my diet this month, I\u2019ll lose weight and then I can start eating whatever I like again\u201d is not a good <em>long term </em>motivational thought. Even in the short term I\u2019ve found that I <em>resent </em>the things I force myself to do with this excuse, whereas I don\u2019t resent my habitual behaviors like \u201cexercise every day\u201d and \u201cnever buy fast food or unhealthy snacks.\u201d</p>\n<h4 id=\"Habits_and_Character\">Habits and Character</h4>\n<p>The habit of exercising doesn\u2019t necessarily influence other behaviors, but if maintained for long enough, it segues into the <em>character trait </em>of being a health-conscious person with good self-control. If I have evidence to present to myself that I have healthy habits (\u201cjust look, I swam for an hour three to five times a week for a whole year, I <em>must </em>be the kind of person who\u2019s fit\u201d) then it becomes easier to start new \u201cgood\u201d habits, like healthy eating. I can correct my fleeting thoughts of how tempting the free baked goods are, tell myself \u201cof course you have enough self-control not to eat those cookies, you\u2019re the kind of person who has healthy habits.\u201d At this point the motivational quote becomes circular; Habits and Character affect Thoughts, which affect Words and Actions. This isn\u2019t a logical paradox if it works, and it seems to work well for me. The more I exercise in a given month, the easier it is to have self-restraint in other areas.</p>\n<p>And even the fact that I have good self-control is, I think, partly based on <em>believing </em>it about myself (\u201cI got up at 5 am for swim practice three times a week for <em>five years</em>, I must have good self-control!) This seems to relate to the finding that willpower depletion depends on whether you <em>believe </em>your willpower will be depleted.<sup>3</sup></p>\n<h4 id=\"Conclusions\">Conclusions</h4>\n<p>Anyone can develop any \u201ccharacter trait.\u201d The requirement is simply enough years of thoughts becoming words becoming actions becoming habit. If you <em>believe </em>that something will get easier to maintain over time, it will. Not in the sense of time and resources\u00ad; to get the continuing benefits of an hour\u2019s daily exercise, you have to pay the opportunity cost of that hour a day, no matter how many years you\u2019ve been doing it for; but in the sense of willpower and motivation. Your actions and habits will eventually change the person you believe yourself to be, which will affect just about everything else. I don\u2019t have any <em>direct </em>evidence that this process works if begun in adulthood, but intuitively it seems that it might work <em>better</em>, since adults are almost always <em>intrinsically</em> motivated&nbsp;in what they do,&nbsp;whereas children often do whatever activities their parents choose, whether or not it\u2019s something they\u2019re motivated to do.</p>\n<h4 id=\"References_\">References:</h4>\n<p>1. De Bruijn, G. J. , Rhodes, R. E.&nbsp;Exploring exercise behavior, intention and habit strength relationships. Scandinavian Journal of Medicine and Science in Sports. 2011: 21: 482\u2013491.</p>\n<p><a href=\"http://pss.sagepub.com.proxy.bib.uottawa.ca/content/21/11/1686\"></a></p>\n<p>2. Schwabe, L., &amp; Wolf, O. T. (2011). Stress-induced modulation of instrumental behavior: From goal-directed to habitual control of action.<em> Behavioural Brain Research, 219</em>(2), 321-328.&nbsp;</p>\n<p>3. Job, V., Dweck, C. S., &amp; Walton, G. M. (2010). Ego depletion-is it all in your head? implicit theories about willpower affect self-regulation.<em> Psychological Science, 21</em>(11), 1686-1693.&nbsp;<a href=\"http://www.stanford.edu/~gwalton/home/Publications_files/Job,%20Dweck,%20%26%20Walton,%202010.pdf\">Link provided by Dr_Manhatten</a></p>", "sections": [{"title": "Actions and Habits", "anchor": "Actions_and_Habits", "level": 1}, {"title": "Habits and Character", "anchor": "Habits_and_Character", "level": 1}, {"title": "Conclusions", "anchor": "Conclusions", "level": 1}, {"title": "References:", "anchor": "References_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "106 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 106, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iA25AvZqAr6G8mAXR", "5eRnAtwuirHC3uhua"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T16:24:14.154Z", "modifiedAt": null, "url": null, "title": "How exactly do you convince people that \"experts\" aren't the best people to consult?", "slug": "how-exactly-do-you-convince-people-that-experts-aren-t-the", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5BmWjPZxA5uKs8kuR/how-exactly-do-you-convince-people-that-experts-aren-t-the", "pageUrlRelative": "/posts/5BmWjPZxA5uKs8kuR/how-exactly-do-you-convince-people-that-experts-aren-t-the", "linkUrl": "https://www.lesswrong.com/posts/5BmWjPZxA5uKs8kuR/how-exactly-do-you-convince-people-that-experts-aren-t-the", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20exactly%20do%20you%20convince%20people%20that%20%22experts%22%20aren't%20the%20best%20people%20to%20consult%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20exactly%20do%20you%20convince%20people%20that%20%22experts%22%20aren't%20the%20best%20people%20to%20consult%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BmWjPZxA5uKs8kuR%2Fhow-exactly-do-you-convince-people-that-experts-aren-t-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20exactly%20do%20you%20convince%20people%20that%20%22experts%22%20aren't%20the%20best%20people%20to%20consult%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BmWjPZxA5uKs8kuR%2Fhow-exactly-do-you-convince-people-that-experts-aren-t-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BmWjPZxA5uKs8kuR%2Fhow-exactly-do-you-convince-people-that-experts-aren-t-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<p>Frankly, it's a question that I've been dealing with for a long time, and that has caused very tense relationships between myself and the educational+medical establishments.&nbsp;</p>\n<p>And it has damaged/destroyed several of my friendships too. Frankly, I trust sites like Imminst over so-called \"expert advice\", and I generally find that I have a much better command of the research literature than any of the experts I consult (see http://www.citeulike.org/user/InquilineKea to see a list of research papers I've read - it's a small fraction of all of them, of course).&nbsp;</p>\n<p>For all that matters, there hasn't even been a study showing the efficacy of most \"expert advice\". Many people who do it their own way may fail, of course, but most of them don't go deep into the research journals (and are consequently prone to falling for pseudoscience).</p>\n<p>Just see the debate at http://www.reddit.com/r/askscience/comments/hptkc/are_animal_products_bad_for_you/, for example.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5BmWjPZxA5uKs8kuR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -3, "extendedScore": null, "score": 7.226292750477366e-07, "legacy": true, "legacyId": "7811", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T17:03:20.400Z", "modifiedAt": null, "url": null, "title": "Edinburgh LW meetup Saturday 4th of June, 2pm", "slug": "edinburgh-lw-meetup-saturday-4th-of-june-2pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.177Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Elias_Kunnas", "createdAt": "2010-06-01T14:05:45.901Z", "isAdmin": false, "displayName": "Elias_Kunnas"}, "userId": "mwGJBaWGMHZKzvnSd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LwY7RvzbLuGWCYeqH/edinburgh-lw-meetup-saturday-4th-of-june-2pm", "pageUrlRelative": "/posts/LwY7RvzbLuGWCYeqH/edinburgh-lw-meetup-saturday-4th-of-june-2pm", "linkUrl": "https://www.lesswrong.com/posts/LwY7RvzbLuGWCYeqH/edinburgh-lw-meetup-saturday-4th-of-june-2pm", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Edinburgh%20LW%20meetup%20Saturday%204th%20of%20June%2C%202pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEdinburgh%20LW%20meetup%20Saturday%204th%20of%20June%2C%202pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLwY7RvzbLuGWCYeqH%2Fedinburgh-lw-meetup-saturday-4th-of-june-2pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Edinburgh%20LW%20meetup%20Saturday%204th%20of%20June%2C%202pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLwY7RvzbLuGWCYeqH%2Fedinburgh-lw-meetup-saturday-4th-of-june-2pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLwY7RvzbLuGWCYeqH%2Fedinburgh-lw-meetup-saturday-4th-of-june-2pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<p>Another meetup at the Delhi cafe on Saturday at 2pm (67 Nicolson Street - see <a href=\"http://goo.gl/Wwws0\">http://goo.gl/Wwws0</a>).<br /><br />No set topic really this time, though suggestions are welcome (for now and for the long-term). Not sure what kind of things would work best in maintaining interest in these...<br />I would personally be somewhat interest in the mindfulness practice as well discussing the somewhat associated <a href=\"/lw/5xx/overcoming_suffering_emotional_acceptance/\">recent post by Kaj Sotala</a>.<br /><br />I will have the book on the Flow with me:</p>\n<p><a href=\"http://www.amazon.co.uk/Flow-Psychology-Happiness-Classic-Achieve/dp/0712657592\"><img src=\"http://sitb-images-eu.amazon.com/Qffs+v35lerRoLjrIzicPzMa9CIWmaaDtWVBo9YlMmBqmFJMz2k16ogS8uo9EFFlh1yHVLV3+kg=\" alt=\"Flow: The Psychology of Happiness\" width=\"200\" /></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LwY7RvzbLuGWCYeqH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 7.226408368253294e-07, "legacy": true, "legacyId": "7812", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tNnhxNYcXYdJYtQRh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T19:40:06.751Z", "modifiedAt": null, "url": null, "title": "Minicamp Confidence Intervals", "slug": "minicamp-confidence-intervals", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.053Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mass_Driver", "createdAt": "2010-03-30T15:48:06.997Z", "isAdmin": false, "displayName": "Mass_Driver"}, "userId": "62rKjNqA2LCJ6RthR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tBqTCQuGYrrgw4sZc/minicamp-confidence-intervals", "pageUrlRelative": "/posts/tBqTCQuGYrrgw4sZc/minicamp-confidence-intervals", "linkUrl": "https://www.lesswrong.com/posts/tBqTCQuGYrrgw4sZc/minicamp-confidence-intervals", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Minicamp%20Confidence%20Intervals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMinicamp%20Confidence%20Intervals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtBqTCQuGYrrgw4sZc%2Fminicamp-confidence-intervals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Minicamp%20Confidence%20Intervals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtBqTCQuGYrrgw4sZc%2Fminicamp-confidence-intervals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtBqTCQuGYrrgw4sZc%2Fminicamp-confidence-intervals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 21, "htmlBody": "<p>I predict, with 50% confidence, that this comment will have between +1 and +4 karma at 9 pm PST on 6/2/2011.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tBqTCQuGYrrgw4sZc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": -24, "extendedScore": null, "score": -1.1e-05, "legacy": true, "legacyId": "7813", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T20:34:42.693Z", "modifiedAt": null, "url": null, "title": "About addition and truth", "slug": "about-addition-and-truth", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:28.183Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qp2G7TuKeGAZbZiSH/about-addition-and-truth", "pageUrlRelative": "/posts/qp2G7TuKeGAZbZiSH/about-addition-and-truth", "linkUrl": "https://www.lesswrong.com/posts/qp2G7TuKeGAZbZiSH/about-addition-and-truth", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20About%20addition%20and%20truth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAbout%20addition%20and%20truth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqp2G7TuKeGAZbZiSH%2Fabout-addition-and-truth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=About%20addition%20and%20truth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqp2G7TuKeGAZbZiSH%2Fabout-addition-and-truth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqp2G7TuKeGAZbZiSH%2Fabout-addition-and-truth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 345, "htmlBody": "<p>This is intended to explore a a thought I had, rather than making any particular argument about truth.</p>\n<p>The canonical example of a thing which is true without any obvious physical referent is the statement 2+2=4. It is true about fingers, sheep, particles, and galaxies; but intuitively it does not seem that any of those truths encapsulates the full meaning of the statement. Moreover, it certainly seems that there is nothing anyone could do to make the statement untrue; it seems that it would have to hold in any universe whatsoever.</p>\n<p>Now my thought: How do we know that the physical universe operates on this sort of arithmetic, and not arithmetic modulo some obscenely large number? Suppose we repeat the experiment that convinces us 2+2=4 (and let's note that babies are presumably not born knowing this; they learn it by counting on their fingers, even if they do so at too young an age to express it in words), but with much larger integers. Perhaps we might find that, when we take 3^^^^3 particles, and add 1, we are left with 3^^^^3 particles without any awareness that any particles have disappeared. And what is more, if we take three sets of 3^^^^3 particles, and measure their mass separately and then together, we find that we get the same mass. After some long sequence of such experiments, perhaps we might convince ourselves that physics actually operates on integer arithmetic modulo 3^^^^3. (Which would be unexpected in that the physics we know operates on complex numbers, not integers, but perhaps that's an approximation to some fantastically-finegrained two-dimensional integer grid.)</p>\n<p>What would this mean, if anything, for the truth of such statements as 2+2=4? It seems that it would then be a contingent truth, not a universal one; that there could in principle exist a universe whose physics operated on arithmetic modulo 3, so that 2+2=1. (Presumably such a universe would not have any sentient beings in it.) What if 2+2=4 is an observed fact about our universe on the same order as the electromagnetic constant or the speed of light?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qp2G7TuKeGAZbZiSH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -1, "extendedScore": null, "score": 7.227033380445102e-07, "legacy": true, "legacyId": "7814", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T21:33:39.783Z", "modifiedAt": null, "url": null, "title": "Consistency links", "slug": "consistency-links", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:21.880Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "QJcy5NDAd8HTiZYQq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pwtRR7btw7PQQsN23/consistency-links", "pageUrlRelative": "/posts/pwtRR7btw7PQQsN23/consistency-links", "linkUrl": "https://www.lesswrong.com/posts/pwtRR7btw7PQQsN23/consistency-links", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consistency%20links&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsistency%20links%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpwtRR7btw7PQQsN23%2Fconsistency-links%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consistency%20links%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpwtRR7btw7PQQsN23%2Fconsistency-links", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpwtRR7btw7PQQsN23%2Fconsistency-links", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p>There's an ongoing discussion on math sites about Vladimir Voevodsky's Fall 2010 lecture expressing doubts that math is consistent, i.e. doubts that it is not possible to deduce formally correct proofs of false statements starting from standard axioms.</p>\n<p><a href=\"http://video.ias.edu/voevodsky-80th\">Voevodsky's original lecture</a></p>\n<p><a href=\"http://www.cs.nyu.edu/pipermail/fom/2011-May/thread.html\">FOM discussion</a></p>\n<p><a href=\"http://mathoverflow.net/questions/66121/is-pa-consistent-do-we-know-it\">mathoverflow discussion</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pwtRR7btw7PQQsN23", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "7815", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T21:48:48.949Z", "modifiedAt": null, "url": null, "title": "Reminder: London meetup, Sunday 5th June 2pm, Cargo Shoreditch", "slug": "reminder-london-meetup-sunday-5th-june-2pm-cargo-shoreditch", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.283Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taryneast", "createdAt": "2010-11-29T20:51:06.328Z", "isAdmin": false, "displayName": "taryneast"}, "userId": "xD8wjhiTvwbXdKirW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XunvLnQyMqHQMDG3J/reminder-london-meetup-sunday-5th-june-2pm-cargo-shoreditch", "pageUrlRelative": "/posts/XunvLnQyMqHQMDG3J/reminder-london-meetup-sunday-5th-june-2pm-cargo-shoreditch", "linkUrl": "https://www.lesswrong.com/posts/XunvLnQyMqHQMDG3J/reminder-london-meetup-sunday-5th-june-2pm-cargo-shoreditch", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reminder%3A%20London%20meetup%2C%20Sunday%205th%20June%202pm%2C%20Cargo%20Shoreditch&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReminder%3A%20London%20meetup%2C%20Sunday%205th%20June%202pm%2C%20Cargo%20Shoreditch%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXunvLnQyMqHQMDG3J%2Freminder-london-meetup-sunday-5th-june-2pm-cargo-shoreditch%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reminder%3A%20London%20meetup%2C%20Sunday%205th%20June%202pm%2C%20Cargo%20Shoreditch%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXunvLnQyMqHQMDG3J%2Freminder-london-meetup-sunday-5th-june-2pm-cargo-shoreditch", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXunvLnQyMqHQMDG3J%2Freminder-london-meetup-sunday-5th-june-2pm-cargo-shoreditch", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<p>Next non-official London meetup is scheduled for this Sunday at Cargo:</p>\n<p><span class=\"fixed_width\" style=\"font-family: Courier,Monospaced;\">83 Rivington Street, <br /> Shoreditch, <br /> London, <br /> EC2A 3AY <br /> </span></p>\n<p><span class=\"fixed_width\" style=\"font-family: Courier,Monospaced;\"><a rel=\"nofollow\" href=\"http://www.google.com/url?sa=D&amp;q=http://www.cargo-london.com/\" target=\"_blank\">http://www.cargo-london.com/</a> <br /> </span></p>\n<p><span class=\"fixed_width\" style=\"font-family: Courier,Monospaced;\"><a rel=\"nofollow\" href=\"http://www.google.com/url?sa=D&amp;q=http://www.viewlondon.co.uk/clubs/cargo-review-14298.html\" target=\"_blank\">http://www.viewlondon.co.uk/clubs/cargo-review-14298.html</a></span></p>\n<p><span class=\"fixed_width\" style=\"font-family: Courier,Monospaced;\"><a title=\"google map\" href=\"http://maps.google.co.uk/maps?f=q&amp;source=s_q&amp;hl=en&amp;q=51.526368,-0.078536&amp;sll=51.526368,-0.078536&amp;sspn=0.012162,0.012531&amp;ie=UTF8&amp;hnear=London+EC2A+3AY,+United+Kingdom&amp;rq=1&amp;split=0&amp;ll=51.526274,-0.079007&amp;spn=0.005487,0.012531&amp;z=16&amp;layer=c&amp;cbll=51.526269,-0.079007&amp;panoid=M1Qo-ZL9LAGJcRGf0YZWvA&amp;cbp=11,24.95,,0,-1.7\">Google map</a></span></p>\n<p>As always, we'll have a picture of an extra-swirly paperclip on the table so you can find us.</p>\n<p>See you there :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XunvLnQyMqHQMDG3J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 7.227252524680866e-07, "legacy": true, "legacyId": "7806", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-02T22:34:45.038Z", "modifiedAt": null, "url": null, "title": "real life sample: why am I so scarred of roller coasters", "slug": "real-life-sample-why-am-i-so-scarred-of-roller-coasters", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.624Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MartinB", "createdAt": "2009-04-20T11:11:22.800Z", "isAdmin": false, "displayName": "MartinB"}, "userId": "2BGK5dWpTXzCE7iwF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gHH5ufus6tey6WuQD/real-life-sample-why-am-i-so-scarred-of-roller-coasters", "pageUrlRelative": "/posts/gHH5ufus6tey6WuQD/real-life-sample-why-am-i-so-scarred-of-roller-coasters", "linkUrl": "https://www.lesswrong.com/posts/gHH5ufus6tey6WuQD/real-life-sample-why-am-i-so-scarred-of-roller-coasters", "postedAtFormatted": "Thursday, June 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20real%20life%20sample%3A%20why%20am%20I%20so%20scarred%20of%20roller%20coasters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Areal%20life%20sample%3A%20why%20am%20I%20so%20scarred%20of%20roller%20coasters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgHH5ufus6tey6WuQD%2Freal-life-sample-why-am-i-so-scarred-of-roller-coasters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=real%20life%20sample%3A%20why%20am%20I%20so%20scarred%20of%20roller%20coasters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgHH5ufus6tey6WuQD%2Freal-life-sample-why-am-i-so-scarred-of-roller-coasters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgHH5ufus6tey6WuQD%2Freal-life-sample-why-am-i-so-scarred-of-roller-coasters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 240, "htmlBody": "<p>I currently visit a theme park with some friends. There I noticed that I am highly afraid of rides that involve heights and fast moving like roller coasters. We rode a few of them, and of course nothing happened, as it is supposed to be a lot of fun. But for me it is more like a constant situation with fear of falling.</p>\n<p>Empirically I know that the park has been around for a while, that very few accidents happen, and that there is no particular reason to actually be afraid. The park makes its money by offering its rides to many visitors, and they really seem to know what they are doing. So why am I afraid? In the past I ONCE had an issue with my non-standard height being a minor problem. But I routinely check the size of other visitors, and the rides seem to be designed in a one-size-fits-all-unisex way that works for all shapes and forms there are.</p>\n<p>I am also somewhat afraid of heights in general, which for the most part serves me pretty well. Should I turn that off while in a theme park? Is it reasonable to not ride the roller coaster?</p>\n<p>There is probably a design principle in rides that elicits strong reactions in the user. And I am not sure if I would want to self modify to actually enjoy the ride. But being afraid seems also pretty useless. It is not rational.</p>\n<p>Comments?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gHH5ufus6tey6WuQD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 7.227388370875804e-07, "legacy": true, "legacyId": "7816", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-03T08:45:19.330Z", "modifiedAt": null, "url": null, "title": "The Multiverse Interpretation of Quantum Mechanics [link]", "slug": "the-multiverse-interpretation-of-quantum-mechanics-link", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.597Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g96BX7ddmdvmtFiy3/the-multiverse-interpretation-of-quantum-mechanics-link", "pageUrlRelative": "/posts/g96BX7ddmdvmtFiy3/the-multiverse-interpretation-of-quantum-mechanics-link", "linkUrl": "https://www.lesswrong.com/posts/g96BX7ddmdvmtFiy3/the-multiverse-interpretation-of-quantum-mechanics-link", "postedAtFormatted": "Friday, June 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Multiverse%20Interpretation%20of%20Quantum%20Mechanics%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Multiverse%20Interpretation%20of%20Quantum%20Mechanics%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg96BX7ddmdvmtFiy3%2Fthe-multiverse-interpretation-of-quantum-mechanics-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Multiverse%20Interpretation%20of%20Quantum%20Mechanics%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg96BX7ddmdvmtFiy3%2Fthe-multiverse-interpretation-of-quantum-mechanics-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg96BX7ddmdvmtFiy3%2Fthe-multiverse-interpretation-of-quantum-mechanics-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://arxiv.org/abs/1105.3796\">http://arxiv.org/abs/1105.3796</a></p>\n<p><a href=\"http://www.newscientist.com/article/mg21028154.200-when-the-multiverse-and-manyworlds-collide.html?full=true\">http://www.newscientist.com/article/mg21028154.200-when-the-multiverse-and-manyworlds-collide.html?full=true</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g96BX7ddmdvmtFiy3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 7.229194483548649e-07, "legacy": true, "legacyId": "7838", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-03T12:22:34.601Z", "modifiedAt": null, "url": null, "title": "What would you do with infinite willpower?", "slug": "what-would-you-do-with-infinite-willpower", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:36.564Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Malik", "createdAt": "2011-01-05T12:45:17.182Z", "isAdmin": false, "displayName": "D_Malik"}, "userId": "9dhw3PngyAWKqTymS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B3qejyoC8NnqyNtDt/what-would-you-do-with-infinite-willpower", "pageUrlRelative": "/posts/B3qejyoC8NnqyNtDt/what-would-you-do-with-infinite-willpower", "linkUrl": "https://www.lesswrong.com/posts/B3qejyoC8NnqyNtDt/what-would-you-do-with-infinite-willpower", "postedAtFormatted": "Friday, June 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20would%20you%20do%20with%20infinite%20willpower%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20would%20you%20do%20with%20infinite%20willpower%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB3qejyoC8NnqyNtDt%2Fwhat-would-you-do-with-infinite-willpower%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20would%20you%20do%20with%20infinite%20willpower%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB3qejyoC8NnqyNtDt%2Fwhat-would-you-do-with-infinite-willpower", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB3qejyoC8NnqyNtDt%2Fwhat-would-you-do-with-infinite-willpower", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<p>For the past few years my willpower has been steadily increasing. If it lasts and I use it to accomplish something noteworthy, I might write a post about it.</p>\n<p>Anyway, what should a rational preference utilitarian with infinite willpower do? Assume that there are no negative effects (unhappiness, stress) with using this willpower, and that they can control their emotions at will.</p>\n<p>Clearly they should work a lot more, not spend time on recreation (movies, TV, games), stand instead of sitting, etc..</p>\n<p>What else? Should they listen to music? Should they keep their muscles flexed 24/7 ? What should they learn, where would they have the most relative advantage? How much time would be worth spending on social interaction?</p>\n<p>I can figure out these things on my own, but those questions are important and good ideas are very valuable.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B3qejyoC8NnqyNtDt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 7.229837330752847e-07, "legacy": true, "legacyId": "7841", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-03T12:25:27.635Z", "modifiedAt": null, "url": null, "title": "Should a rationalist be concerned about habitat loss/biodiversity loss?", "slug": "should-a-rationalist-be-concerned-about-habitat-loss", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.088Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fbwGJ789eZtLifc84/should-a-rationalist-be-concerned-about-habitat-loss", "pageUrlRelative": "/posts/fbwGJ789eZtLifc84/should-a-rationalist-be-concerned-about-habitat-loss", "linkUrl": "https://www.lesswrong.com/posts/fbwGJ789eZtLifc84/should-a-rationalist-be-concerned-about-habitat-loss", "postedAtFormatted": "Friday, June 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20a%20rationalist%20be%20concerned%20about%20habitat%20loss%2Fbiodiversity%20loss%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20a%20rationalist%20be%20concerned%20about%20habitat%20loss%2Fbiodiversity%20loss%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfbwGJ789eZtLifc84%2Fshould-a-rationalist-be-concerned-about-habitat-loss%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20a%20rationalist%20be%20concerned%20about%20habitat%20loss%2Fbiodiversity%20loss%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfbwGJ789eZtLifc84%2Fshould-a-rationalist-be-concerned-about-habitat-loss", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfbwGJ789eZtLifc84%2Fshould-a-rationalist-be-concerned-about-habitat-loss", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 325, "htmlBody": "<p>It's an interesting question that I'm pondering.</p>\n<p>Now, while I do question the intellectual honesty of this blog, I'll link to it anyways, since the evidence does seem interesting, at the very least:&nbsp;<a href=\"http://wattsupwiththat.com/2010/01/04/where-are-the-corpses/\">http://wattsupwiththat.com/2010/01/04/where-are-the-corpses/</a></p>\n<p><a href=\"http://wattsupwiththat.com/2011/05/19/species-extinction-hype-fundamentally-flawed/\">http://wattsupwiththat.com/2011/05/19/species-extinction-hype-fundamentally-flawed/</a></p>\n<p>It does seem that environmentalism can mimic some qualities of religion (I know, since I used to be an environmentalist myself). As such, it can cause many extremely intelligent people to reject evidence that goes against their worldview.&nbsp;</p>\n<p>Furthermore, it's also possible that computational chemistry may soon be our primary agent for drug discovery, rather than discovering more biological compounds in certain ecosystems (that being said, drug discovery is entirely different from drug synthesis, and discovering a gene that codes for a particular protein and splicing it into an E Coli bacterium is going to be far easier than anything computational chemistry can do in the near future).&nbsp;</p>\n<p>With that all being said, what now? I do believe that there is something of value that does get lost as habitat gets destroyed. But it's hard to quantify value in these cases. Certain animals, like crows, chimpanzees, orcas, and elephants, are cognitively advanced enough to have their own cultures. If one of their subcultures get destroyed (which can be done without a fullscale extinction), then is anything valuable that gets lost? (besides value for scientific research that has potential to be applicable elsewhere?) And is it more important to worry about these separate cultures, as compared to worrying about different subspecies of the same animal? Certainly, we're now beginning to discover novel social networks in dolphins and crows. But most of these animals are not at risk of extinction, and even the chimpanzees and bonobos will only get extinct in the wild (at the very worst). There are other less advanced animals that have a higher risk of permanent extinction.&nbsp;</p>\n<p>What we're prone to systematically underestimating, of course, is the possible permanent loss of micro-organisms. And of novel biological structures (and networks) that may be contained within them.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fbwGJ789eZtLifc84", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 9, "extendedScore": null, "score": 7.229845864088934e-07, "legacy": true, "legacyId": "7842", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-03T13:28:49.867Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Feeling Rational", "slug": "seq-rerun-feeling-rational", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.274Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sjtZJWqLNvwdQqZYt/seq-rerun-feeling-rational", "pageUrlRelative": "/posts/sjtZJWqLNvwdQqZYt/seq-rerun-feeling-rational", "linkUrl": "https://www.lesswrong.com/posts/sjtZJWqLNvwdQqZYt/seq-rerun-feeling-rational", "postedAtFormatted": "Friday, June 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Feeling%20Rational&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Feeling%20Rational%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsjtZJWqLNvwdQqZYt%2Fseq-rerun-feeling-rational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Feeling%20Rational%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsjtZJWqLNvwdQqZYt%2Fseq-rerun-feeling-rational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsjtZJWqLNvwdQqZYt%2Fseq-rerun-feeling-rational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>Today's post, <a href=\"/lw/hp/feeling_rational/\">Feeling Rational</a> was originally published on April 26, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>Strong emotions can be rational. A rational belief that something good happened leads to rational happiness. But your emotions ought not to change your beliefs about events that do not depend causally on your emotions.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/60w/seq_rerun_consolidated_nature_of_morality_thread/\">Consolidated Nature of Morality Thread</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sjtZJWqLNvwdQqZYt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 7.230033394295271e-07, "legacy": true, "legacyId": "7843", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SqF8cHjJv43mvJJzx", "722r3hyGkgMCTDsZY", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-03T16:32:16.552Z", "modifiedAt": null, "url": null, "title": "LW/OB Rationality Quotes, June 2011", "slug": "lw-ob-rationality-quotes-june-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:51.943Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7phSLA2SuEvSu2872/lw-ob-rationality-quotes-june-2011", "pageUrlRelative": "/posts/7phSLA2SuEvSu2872/lw-ob-rationality-quotes-june-2011", "linkUrl": "https://www.lesswrong.com/posts/7phSLA2SuEvSu2872/lw-ob-rationality-quotes-june-2011", "postedAtFormatted": "Friday, June 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%2FOB%20Rationality%20Quotes%2C%20June%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%2FOB%20Rationality%20Quotes%2C%20June%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7phSLA2SuEvSu2872%2Flw-ob-rationality-quotes-june-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%2FOB%20Rationality%20Quotes%2C%20June%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7phSLA2SuEvSu2872%2Flw-ob-rationality-quotes-june-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7phSLA2SuEvSu2872%2Flw-ob-rationality-quotes-june-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>I saw <a href=\"/lw/153/lwob_rationality_quotes_august_2009/\">this article</a> and thought \"ah, that's what's been missing.\" There's many a <em>bon mot</em> posted here that's outside the domain of the usual rationality quotes thread.<em> Overcoming Bias</em> seems still to be excluded from those too, even if the two blogs have diverged.</p>\n<p>So:</p>\n<p>This is a thread for posting any interesting rationality-related quotes you've seen on LW/OB.</p>\n<ul>\n<li>Please post all quotes separately (so that they can be voted up/down separately) unless they are strongly related/ordered. </li>\n<li>Do not quote yourself. </li>\n<li>Do not post quotes that are NOT comments/posts on LW/OB - there is a separate thread for this. </li>\n<li>No more than 5 quotes per person per thread, please.</li>\n</ul>\n<p>(You may care to check the <a href=\"/r/discussion/tag/lwob_quotes/\">previous posts</a> for duplicates.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7phSLA2SuEvSu2872", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "7844", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bTdDKX4sK35Q9t4v2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-04T02:46:07.975Z", "modifiedAt": null, "url": null, "title": "[LINK] \"Straight and crooked thinking,\" by Robert H. Thouless", "slug": "link-straight-and-crooked-thinking-by-robert-h-thouless", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.469Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hairyfigment", "createdAt": "2010-10-14T22:12:59.035Z", "isAdmin": false, "displayName": "hairyfigment"}, "userId": "NesjW63eueLsbKrCY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BzMgaemS9WNgNQ6RW/link-straight-and-crooked-thinking-by-robert-h-thouless", "pageUrlRelative": "/posts/BzMgaemS9WNgNQ6RW/link-straight-and-crooked-thinking-by-robert-h-thouless", "linkUrl": "https://www.lesswrong.com/posts/BzMgaemS9WNgNQ6RW/link-straight-and-crooked-thinking-by-robert-h-thouless", "postedAtFormatted": "Saturday, June 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20%22Straight%20and%20crooked%20thinking%2C%22%20by%20Robert%20H.%20Thouless&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20%22Straight%20and%20crooked%20thinking%2C%22%20by%20Robert%20H.%20Thouless%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzMgaemS9WNgNQ6RW%2Flink-straight-and-crooked-thinking-by-robert-h-thouless%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20%22Straight%20and%20crooked%20thinking%2C%22%20by%20Robert%20H.%20Thouless%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzMgaemS9WNgNQ6RW%2Flink-straight-and-crooked-thinking-by-robert-h-thouless", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzMgaemS9WNgNQ6RW%2Flink-straight-and-crooked-thinking-by-robert-h-thouless", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<p>This book from 1930 or so (<a title=\"from Neglected Books\" href=\"http://www.neglectedbooks.com/Straight_and_Crooked_Thinking.pdf\">PDF</a>) seems chiefly concerned with describing ways thinking can go wrong. The author does not seem to know much of Bayes, but his book appears largely sound. It ends with practical suggestions for dealing with various \"dishonest tricks\" -- though the list assumes these tricks come from someone other than the reader -- and an imaginary conversation between advocates of different political views who all exhibit flawed thinking. People who sympathize with one side or another can get practice taking apart a bad argument for that side.</p>\n<p>Found via <a title=\"AiG\" href=\"http://answersingenes.blogspot.com/2007/11/straight-crooked-thinking.html\">recommendation</a> from Answers in Genes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BzMgaemS9WNgNQ6RW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 7.232393558275324e-07, "legacy": true, "legacyId": "7850", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-04T04:15:35.877Z", "modifiedAt": null, "url": null, "title": "[LINK] Wondermark comic talks about \"carpe diem\" vs. \"think more\"", "slug": "link-wondermark-comic-talks-about-carpe-diem-vs-think-more", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.521Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Document", "createdAt": "2010-02-08T04:14:47.949Z", "isAdmin": false, "displayName": "Document"}, "userId": "vaMNHjzaCGqF8yTMS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tSBRwYMEJWtL5FnrE/link-wondermark-comic-talks-about-carpe-diem-vs-think-more", "pageUrlRelative": "/posts/tSBRwYMEJWtL5FnrE/link-wondermark-comic-talks-about-carpe-diem-vs-think-more", "linkUrl": "https://www.lesswrong.com/posts/tSBRwYMEJWtL5FnrE/link-wondermark-comic-talks-about-carpe-diem-vs-think-more", "postedAtFormatted": "Saturday, June 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Wondermark%20comic%20talks%20about%20%22carpe%20diem%22%20vs.%20%22think%20more%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Wondermark%20comic%20talks%20about%20%22carpe%20diem%22%20vs.%20%22think%20more%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSBRwYMEJWtL5FnrE%2Flink-wondermark-comic-talks-about-carpe-diem-vs-think-more%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Wondermark%20comic%20talks%20about%20%22carpe%20diem%22%20vs.%20%22think%20more%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSBRwYMEJWtL5FnrE%2Flink-wondermark-comic-talks-about-carpe-diem-vs-think-more", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSBRwYMEJWtL5FnrE%2Flink-wondermark-comic-talks-about-carpe-diem-vs-think-more", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p>Comic <a href=\"http://wondermark.com/730/\">here</a>.</p>\n<p>I feel like I'm likely to hear that it's a false dichotomy, but if so I haven't successfully dissolved it. (I'm not sure I actually had a concise name for it before; now that I do I should start saving comments about it.)</p>\n<p>Edit: <a href=\"/lw/4fo/ability_to_react/\">Ability to react</a> may be related, possibly applying on shorter timescales.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tSBRwYMEJWtL5FnrE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 7.232658476622901e-07, "legacy": true, "legacyId": "7852", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["p67vi8kk3LJCjL2v7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-04T06:37:45.779Z", "modifiedAt": null, "url": null, "title": "Job Search Advice", "slug": "job-search-advice", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.446Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "zntneo", "createdAt": "2011-02-25T06:06:34.571Z", "isAdmin": false, "displayName": "zntneo"}, "userId": "AgBNRryLw7RWBT7uh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z7ihuKNTRfhxsqttn/job-search-advice", "pageUrlRelative": "/posts/z7ihuKNTRfhxsqttn/job-search-advice", "linkUrl": "https://www.lesswrong.com/posts/z7ihuKNTRfhxsqttn/job-search-advice", "postedAtFormatted": "Saturday, June 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Job%20Search%20Advice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJob%20Search%20Advice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz7ihuKNTRfhxsqttn%2Fjob-search-advice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Job%20Search%20Advice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz7ihuKNTRfhxsqttn%2Fjob-search-advice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz7ihuKNTRfhxsqttn%2Fjob-search-advice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 336, "htmlBody": "<p>Some background about me. I currently live in seaside,ca. Have a bs in psychology and an A.A.S in information technology network administration. I currently am a cashier at a gas station but want to find a better job for many reasons. I want a job that will fulfill my high need for&nbsp;analytical&nbsp;thought(high in need for cognition if you know what that means) and problem solving and that hopefully maximizes the amount of time i can be with my wife (who is in the military and \"works\" 7-3. I am pretty new to the job search thing because i spent 6 years in college with the same job as basically a system admin. (note of worry about all jobs have already developed carpal tunnel and had surgery and my symptoms may be returning</p>\n<p>also i'd like to add some interests of mine. During college I was active in my atheist group (after i became one) and have been a pretty big activist since starting college. I try to be as involved as i can think to be in the skeptical/atheist/lesswrong community.&nbsp;</p>\n<p>So my question is given this information what are the best methods/ resources to help me in my job search. What i have been doing is applying online using multiple job banks but have not even landed a interview for anything related to computers I tried looking my self but was overwhelmed by what seemed to be contradictory messages. Any help i can get will be&nbsp;appreciated.</p>\n<p>Edit:Thanks to advice from nickernst i will break down the above to a more manageable set of questions</p>\n<p>&nbsp;</p>\n<ol>\n<li>what types of jobs will i enjoy that i would have a chance at given my background</li>\n<li>related to one is there anything i could add that would let me get a job that i will really love</li>\n<li>what jobs are avaiable to me</li>\n<li>what would you suggest for the \"process\" of job searching to increase the&nbsp;likelihood&nbsp;of interviews</li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; background-color: #f7f7f8;\">What are some common failure modes of people in the same situation?</span></li>\n</ol>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z7ihuKNTRfhxsqttn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 5, "extendedScore": null, "score": 7.233079482399725e-07, "legacy": true, "legacyId": "7854", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-04T15:10:46.106Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Universal Fire", "slug": "seq-rerun-universal-fire", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.720Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3JuvFPJhxJMEBuSfc/seq-rerun-universal-fire", "pageUrlRelative": "/posts/3JuvFPJhxJMEBuSfc/seq-rerun-universal-fire", "linkUrl": "https://www.lesswrong.com/posts/3JuvFPJhxJMEBuSfc/seq-rerun-universal-fire", "postedAtFormatted": "Saturday, June 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Universal%20Fire&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Universal%20Fire%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3JuvFPJhxJMEBuSfc%2Fseq-rerun-universal-fire%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Universal%20Fire%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3JuvFPJhxJMEBuSfc%2Fseq-rerun-universal-fire", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3JuvFPJhxJMEBuSfc%2Fseq-rerun-universal-fire", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 141, "htmlBody": "<p>Today's post, <a href=\"/lw/hq/universal_fire/\">Universal Fire</a> was originally published on April 27, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>You can't change just one thing in the world and expect the rest to continue working as before.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/61v/seq_rerun_feeling_rational/\">Feeling Rational</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3JuvFPJhxJMEBuSfc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.234599051358577e-07, "legacy": true, "legacyId": "7858", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LaM5aTcXvXzwQSC2Q", "sjtZJWqLNvwdQqZYt", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-05T00:26:23.450Z", "modifiedAt": null, "url": null, "title": "Irvine Meetup Tuesday June 21", "slug": "irvine-meetup-tuesday-june-21", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:27.678Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/caqXbMKSpLWw2XEEq/irvine-meetup-tuesday-june-21", "pageUrlRelative": "/posts/caqXbMKSpLWw2XEEq/irvine-meetup-tuesday-june-21", "linkUrl": "https://www.lesswrong.com/posts/caqXbMKSpLWw2XEEq/irvine-meetup-tuesday-june-21", "postedAtFormatted": "Sunday, June 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Irvine%20Meetup%20Tuesday%20June%2021&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIrvine%20Meetup%20Tuesday%20June%2021%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcaqXbMKSpLWw2XEEq%2Firvine-meetup-tuesday-june-21%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Irvine%20Meetup%20Tuesday%20June%2021%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcaqXbMKSpLWw2XEEq%2Firvine-meetup-tuesday-june-21", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcaqXbMKSpLWw2XEEq%2Firvine-meetup-tuesday-june-21", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<p>There are some adjustments to the weekly meetups in Irvine:</p>\n<p>The meetup for June 15th is cancelled because the regular attendees are on vacation that week.</p>\n<p>The following week, the meetup will be Tuesday June 21, instead of Wednesday. We expect special guests Alicorn and Yvain to attend.</p>\n<p>Normal weekly meetups on Wednesdays will resume on June 29th.</p>\n<p>All meetups are from 6:00 to 8:00<sup>1</sup> at the <a title=\"Map\" href=\"http://maps.google.com/maps?q=33.650526,-117.838927&amp;num=1&amp;t=h&amp;sll=33.650288,-117.838666&amp;sspn=0.001684,0.002363&amp;ie=UTF8&amp;ll=33.650398,-117.838631&amp;spn=0.001634,0.002497&amp;z=19\" target=\"_blank\">outdoor food court</a> near the UCI Campus, at Campus and Bridge. Look for me with sign showing a diagram of a <a href=\"/lw/nn/neural_categories\">naive neural classifier of bleggs and rubes</a>.</p>\n<p>To see all scheduled meetups in Southern California, see the <a href=\"https://www.google.com/calendar/embed?src=h57ej586rdo3jmld14hrk51m1c%40group.calendar.google.com&amp;ctz=America/Los_Angeles\" target=\"_blank\">calendar</a>. To get announcements by email, join the <a href=\"http://groups.google.com/group/LW-SoCal-Announce\" target=\"_blank\">email group</a>.</p>\n<hr />\n<p>1. The end time is very soft. Once as we got up to leave around 11:20, someone remarked \"This is how we do 6 to 8.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "caqXbMKSpLWw2XEEq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 7.236245493189217e-07, "legacy": true, "legacyId": "7859", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yFDKvfN6D87Tf5J9f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-05T11:57:16.123Z", "modifiedAt": null, "url": null, "title": "Ask LW: Why no reproductive human cloning?", "slug": "ask-lw-why-no-reproductive-human-cloning", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:56.193Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DXeMobLuLPNFHcSbh/ask-lw-why-no-reproductive-human-cloning", "pageUrlRelative": "/posts/DXeMobLuLPNFHcSbh/ask-lw-why-no-reproductive-human-cloning", "linkUrl": "https://www.lesswrong.com/posts/DXeMobLuLPNFHcSbh/ask-lw-why-no-reproductive-human-cloning", "postedAtFormatted": "Sunday, June 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ask%20LW%3A%20Why%20no%20reproductive%20human%20cloning%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsk%20LW%3A%20Why%20no%20reproductive%20human%20cloning%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXeMobLuLPNFHcSbh%2Fask-lw-why-no-reproductive-human-cloning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ask%20LW%3A%20Why%20no%20reproductive%20human%20cloning%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXeMobLuLPNFHcSbh%2Fask-lw-why-no-reproductive-human-cloning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXeMobLuLPNFHcSbh%2Fask-lw-why-no-reproductive-human-cloning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p>From what I can tell human cloning for the purpose of, ya know, actually cloning a person in the Dolly sense, is legal in many parts of the United States. It looks hard to pull off but without conceptual problems. Seems likely that after the first few clones are born there'll be a huge backlash and it will get banned forever. My impression is that whoever does it first would get a lot of money and tons of media attention that would be useful for getting funding for some other biotech venture. They'd get extra publicity if they put a eugenics spin on it too, which I haven't seen <em>anyone</em>&nbsp;talking about from my few Google searches. I also haven't seen anything about a combination of cloning and genome design/tweaking of various kinds, for research or for creating less-misoptimized humans; I'm not at all familiar with the science/tech there, is there a reason no one thinks it's promising? I can't find a decent blog that covers any of the related topics.</p>\n<p>Who's familiar with this dormant technology and its social situation? Are there good blogs that cover it? What parts of the picture am I missing?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DXeMobLuLPNFHcSbh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 7.238293654155501e-07, "legacy": true, "legacyId": "7860", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-05T13:23:30.136Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Universal Law", "slug": "seq-rerun-universal-law", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.367Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qq4kp59XWJmySyRL3/seq-rerun-universal-law", "pageUrlRelative": "/posts/Qq4kp59XWJmySyRL3/seq-rerun-universal-law", "linkUrl": "https://www.lesswrong.com/posts/Qq4kp59XWJmySyRL3/seq-rerun-universal-law", "postedAtFormatted": "Sunday, June 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Universal%20Law&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Universal%20Law%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQq4kp59XWJmySyRL3%2Fseq-rerun-universal-law%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Universal%20Law%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQq4kp59XWJmySyRL3%2Fseq-rerun-universal-law", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQq4kp59XWJmySyRL3%2Fseq-rerun-universal-law", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/hr/universal_law/\">Universal Law</a> was originally published on April 29, 2007. A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>In our everyday lives, we are accustomed to rules with exceptions, but the basic laws of the universe apply everywhere without exception. Apparent violations exist only in our models, not in reality.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/62a/seq_rerun_universal_fire/\">Universal Fire</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qq4kp59XWJmySyRL3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 7.238549372096738e-07, "legacy": true, "legacyId": "7861", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7iTwGquBFZKttpEdE", "3JuvFPJhxJMEBuSfc", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-05T19:40:02.077Z", "modifiedAt": null, "url": null, "title": "Ottawa LW meetup, June 9, 7pm; two Bayesian Conspiracy sessions", "slug": "ottawa-lw-meetup-june-9-7pm-two-bayesian-conspiracy-sessions", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FtwqRGGpbmFvCWrPh/ottawa-lw-meetup-june-9-7pm-two-bayesian-conspiracy-sessions", "pageUrlRelative": "/posts/FtwqRGGpbmFvCWrPh/ottawa-lw-meetup-june-9-7pm-two-bayesian-conspiracy-sessions", "linkUrl": "https://www.lesswrong.com/posts/FtwqRGGpbmFvCWrPh/ottawa-lw-meetup-june-9-7pm-two-bayesian-conspiracy-sessions", "postedAtFormatted": "Sunday, June 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ottawa%20LW%20meetup%2C%20June%209%2C%207pm%3B%20two%20Bayesian%20Conspiracy%20sessions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOttawa%20LW%20meetup%2C%20June%209%2C%207pm%3B%20two%20Bayesian%20Conspiracy%20sessions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFtwqRGGpbmFvCWrPh%2Fottawa-lw-meetup-june-9-7pm-two-bayesian-conspiracy-sessions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ottawa%20LW%20meetup%2C%20June%209%2C%207pm%3B%20two%20Bayesian%20Conspiracy%20sessions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFtwqRGGpbmFvCWrPh%2Fottawa-lw-meetup-june-9-7pm-two-bayesian-conspiracy-sessions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFtwqRGGpbmFvCWrPh%2Fottawa-lw-meetup-june-9-7pm-two-bayesian-conspiracy-sessions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<p>&nbsp;</p>\n<p><strong><a id=\"more\"></a>Less Wrong meeting</strong>:</p>\n<p>Date: Thursday June 9, 7:00pm 'til whenever.</p>\n<p>Venue: Fox and Feather, upstairs, possibly in back room #1 if it's free. Look for the LW sign.</p>\n<p>&nbsp;</p>\n<p><strong>Bayes study group</strong>: Anyone in the region interested in learning how to do Bayesian statistics is welcome to join us. We'll be using the statistical package R (http://cran.r-project.org/) as a platform, so bring your laptop if you have one.</p>\n<p>Date: Thursday June 9, 9:00am to 10:30am. NB: 9 in the morning.</p>\n<p>Venue: Jeanne Mance Building, Tunney's Pasture. Meet me in the lobby.</p>\n<p>&nbsp;</p>\n<p><strong>Bayes study group</strong>: This session will run in parallel (but slightly delayed relative) to the sessions at Tunney's Pasture.&nbsp;</p>\n<p>Date: Tuesday June 7, 9:30pm</p>\n<p>Venue: wmiles's house; join the Google group for the address.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FtwqRGGpbmFvCWrPh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.239666132942159e-07, "legacy": true, "legacyId": "7862", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-05T20:31:00.250Z", "modifiedAt": null, "url": null, "title": "What are you working on? June 2011", "slug": "what-are-you-working-on-june-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:31.571Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/taN2YNDKjzuQWmioi/what-are-you-working-on-june-2011", "pageUrlRelative": "/posts/taN2YNDKjzuQWmioi/what-are-you-working-on-june-2011", "linkUrl": "https://www.lesswrong.com/posts/taN2YNDKjzuQWmioi/what-are-you-working-on-june-2011", "postedAtFormatted": "Sunday, June 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20working%20on%3F%20June%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20working%20on%3F%20June%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtaN2YNDKjzuQWmioi%2Fwhat-are-you-working-on-june-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20working%20on%3F%20June%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtaN2YNDKjzuQWmioi%2Fwhat-are-you-working-on-june-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtaN2YNDKjzuQWmioi%2Fwhat-are-you-working-on-june-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p>This is the third What Are You Working On? thread. Click on the <a href=\"/r/discussion/tag/waywo/\">'waywo' tag</a> to see previous threads. I'm going to do these every two months instead of every month from now on. So here's the question:</p>\n<p style=\"padding-left: 30px;\"><em><strong>What are you working on?&nbsp;</strong></em></p>\n<p>Here are some guidelines</p>\n<ul>\n<li>Focus on projects that you have recently made progress on, not projects that you're thinking about doing but haven't started, those are for a different thread.&nbsp;</li>\n<li>Why this project and not others? Mention reasons why you're doing the project and/or why others should contribute to your project (if applicable).</li>\n<li>Talk about your goals for the project.</li>\n<li>Any kind of project is fair game: personal improvement, research project, art project, whatever.</li>\n<li><strong>Link to your work if it's linkable</strong></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "taN2YNDKjzuQWmioi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 7.23981732735654e-07, "legacy": true, "legacyId": "7863", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-05T22:59:47.038Z", "modifiedAt": null, "url": null, "title": "Meanings of Mathematical Truths", "slug": "meanings-of-mathematical-truths", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "prase", "createdAt": "2009-02-28T10:00:10.260Z", "isAdmin": false, "displayName": "prase"}, "userId": "WAP32wvmNt9QdutSu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2pNMrzcGyW7Hgk4QC/meanings-of-mathematical-truths", "pageUrlRelative": "/posts/2pNMrzcGyW7Hgk4QC/meanings-of-mathematical-truths", "linkUrl": "https://www.lesswrong.com/posts/2pNMrzcGyW7Hgk4QC/meanings-of-mathematical-truths", "postedAtFormatted": "Sunday, June 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meanings%20of%20Mathematical%20Truths&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeanings%20of%20Mathematical%20Truths%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2pNMrzcGyW7Hgk4QC%2Fmeanings-of-mathematical-truths%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meanings%20of%20Mathematical%20Truths%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2pNMrzcGyW7Hgk4QC%2Fmeanings-of-mathematical-truths", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2pNMrzcGyW7Hgk4QC%2Fmeanings-of-mathematical-truths", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1218, "htmlBody": "<p><em>Related Sequence posts:</em> <em><a href=\"/lw/si/math_is_subjunctively_objective/\">Math is Subjunctively Objective</a></em>, <em><a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">How to Convince Me That 2+2=3</a></em></p>\n<p>Discussions whether mathematical theorems can be possibly disproved by observations have been relatively frequent on LessWrong. The last instance which motivated me to write this post was the discussion <a href=\"/lw/612/about_addition_and_truth/\">here</a>. In fact these discussions are closely related to philosophical disputes about contingent and necessary truths. Many standard philosophical disputes are considered solved on LessWrong and there is usually some reference post which dissolves the issue. I don't know of any post that conclusively summarises this problem, although it doesn't seem particularly controversial.</p>\n<p>To be concrete, let's take a single statement of arithmetic, say \"5324 + 2326 = 7650\". Most people will gladly agree that the statement is true. No matter how proficient the debaters are in intellectual sophistry (with the possible exception of postmodern philosophers), they will not dispute that 5324 + 2326 indeed equals to 7650. But the agreement is lost when it comes to the meaning of the discussed simple sentence. What does it refer to? Is it a necessary truth, or can it be empirically tested?</p>\n<p>One opinion states that statements about addition refer to counting apples, pigs, megabytes of data or whatever stuff one needs to count. Sets of these objects, either material or abstract, are the referents of those statements. If I take two groups of apples which happen to contain 5324 and 2326 items and put them together, the compound group will consist of 7650 apples. It is not self-evident (as any illiterate tribesman from New Guinea would confirm) that the result will be 7650 and not, for example, 7494. Therefore each putting of apples together counts as a test of the respective proposition about arithmetic. Of course, the number of apples in the compound group must be determined by a method different from counting the two subgroups separately and then adding the numbers on the paper &mdash; that would defeat the idea of testing. Also, some people may say that \"5324 + 2326\" is a <em>definition</em> of \"7650\"; not that we regularly encounter such opinions when speaking about numbers of this magnitude, but that \"1 + 1\" is a definition of \"2\" I have heard countless times. So we have to be careful to create a dictionary which translates \"1\" as \"S0\" and \"2\" as \"SS0\" and \"7650\" as something which I am not going to write here, and describe counting of apples as adding one S for each apple in the bag. After that we may go through the ordeal of converting our bag of apples to a horribly long string of S's and finally look up our product in the dictionary - just to see whether the corresponding translation really reads \"7650\". And if done in this torturously lengthy way, I assume there would be at least a tiny amount of pleasant surprise if we really found \"7650\" and not \"157\", and not only because the possibility of an \"experimental error\". So it seems to be a legitimate empirical test.</p>\n<p>Holders of the contrary opinion would certainly not deny that such tests can be arranged (they may dispute the \"surprise\", though). But their argument is: Even if we conducted the described experiment with apples, and repeatedly found that the result was 157 instead of 7650 (and suppose that the possible errors in counting or translation from \"5324\" to \"SSS...S0\" were ruled out), that has no bearing on the truth value of \"5324 + 2326 = 7650\" as a statement of arithmetic. It is imaginable that physical addition of apples followed some different rules, such that putting 5324 objects together with 2326 objects always yielded&nbsp; a set of 157 objects &mdash; but that doesn't mean that \"5324 + 2326 = 157\". There would be an isomorphism, in such a hypothetical world, which maps that string to a true statement about apples, nevertheless there is no way how to make it a statement <em>about arithmetic</em>. It would be better to invent a new symbol for the abstract operation which emulates physical addition in that hypothetical world, or even better whole set of new symbols for all digits, to avoid confusion. One may rather say \"%#@$ &macr; @#@^ = !%&amp;\" instead of \"5324 + 2326 = 157\". The former is a statement of a certain formal system X which models the modified apple addition and as such it is true, while the latter is false. No matter what apples do in that world, within arithmetic we can still formally prove that 5324 + 2326 is 7650, and nothing else. Even if the inhabitants of our strange hypothetical world called their formal counting system \"arithmetic\" instead of \"X\" and the existence of <em>real</em> arithmetic had never occured to them &mdash; even such a fact cannot change the universal truth that 5324 + 2326 = 7650.</p>\n<p>Although there is hardly any disagreement about expected anticipations, the debates on this question seldom appear conclusive. The apparent disagreement is almost certainly caused by different interpretation of something. It is perhaps not much different from the iconic <a href=\"/lw/np/disputing_definitions/\">sound definition dispute</a> or the <a href=\"/lw/5u2/pluralistic_moral_reductionism/\">disputes about morality</a>. In contrast to the sound definition case, where the disagreement is about what a single word \"sound\" refers to, here the source of misunderstanding is more difficult to locate. On the first sight it may appear that the meaning of \"arithmetic\" is disputed. However more probably it is the phrase \"5324 + 2326 = 7650\" with all other statements of arithmetic which is interpreted in several distinct ways. Let's be more specific in what the proposition can mean:</p>\n<ol>\n<li>If I take 5324 objects and add another 2326 objects, I get 7650 objects. It holds for a broad range of object types and all reasonable senses of \"adding together\", therefore it is sensible to express the fact as a general abstract relation between numbers. By the way, we can create a formal system which allows us to deduce similar true propositions, and we call it \"arithmetic\".</li>\n<li>The string \"5324 + 2326 = 7650\" is a theorem in a formal system given by the following axioms and rules: <em>(Here should stand the axioms and rules.)</em> By the way, we call the system \"arithmetic\", and it happens to be a good model of counting objects.</li>\n</ol>\n<p>(There might be a third interpretation, along the lines of the second one, but with less apparent arbitrariness of arithmetic: \"any intelligence necessarily includes representation of a formal system isomorphic to arithmetic, independently of the properties of the external world\". I didn't include that to the list, because it is either a very narrow constraint on the definition of intelligence or almost certainly false.)</p>\n<p>Because arithmetic actually works (as far as we know) as a model of counting, the two interpretations are equally good and for all practical purposes indistinguishable. It is no surprise that our intuitions can't reliably distinguish between practically equivalent interpretations. Rather, two intuitions come into conflict. The first one tells us that arithmetic isn't arbitrary at all, and thus the second interpretation must be false. The second intuition is based on the self-consistence of mathematics: mathematics has its own ways how to decide between truth and falsity, and those ways never defer to the external world; therefore the first interpretation must be false. But once the meaning is spelled out in sufficient detail, the apparent conflict should disappear.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2pNMrzcGyW7Hgk4QC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "7864", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WAQ3qMD4vdXheQmui", "6FmqiAgS8h4EJm86s", "qp2G7TuKeGAZbZiSH", "7X2j8HAkWdmMoS8PE", "3zDX3f3QTepNeZHGc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-06T06:20:25.185Z", "modifiedAt": null, "url": null, "title": "Emotional Installation of Software", "slug": "emotional-installation-of-software", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.263Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qF59yHCfb4SnwijrK/emotional-installation-of-software", "pageUrlRelative": "/posts/qF59yHCfb4SnwijrK/emotional-installation-of-software", "linkUrl": "https://www.lesswrong.com/posts/qF59yHCfb4SnwijrK/emotional-installation-of-software", "postedAtFormatted": "Monday, June 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Emotional%20Installation%20of%20Software&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEmotional%20Installation%20of%20Software%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqF59yHCfb4SnwijrK%2Femotional-installation-of-software%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Emotional%20Installation%20of%20Software%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqF59yHCfb4SnwijrK%2Femotional-installation-of-software", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqF59yHCfb4SnwijrK%2Femotional-installation-of-software", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 886, "htmlBody": "<p>I have recently been thinking about this question, \"what is it exactly that helps install religious software so deeply and dogmatically into the brain?\" Often those who are strongly religious fall into a few categories: (1) They were trained to believe in specific aspects of religion as children; (2) They entered into a very destitute part of their lives (i.e. severe depression, midlife crisis, loss of a job, death in the family, cancer, alcoholism, or other existential problems). <br /><br />What strikes me about these situations is that emotion generally dominates the decision-making process. I remember when I was a child and attended church camp at the encouragement of my family I was heavily pressured by the camp counselors to \"accept Christ\" and I saw that there was a positive correlation between my willingness to accept Christ, memorize Bible verses, and say certain statements about behavior in the context of Christian morals and the way that the camp counselors, my extended family, and other adults would treat me. As a result, it was not until many years later that my preference for rationalism and science was able to fully crack that emotionally-founded religious belief installed in me as a child. I know many people for whom a similar narrative is true regarding experiences with alcohol, etc., though it seems to be rare for someone to completely dismiss deeply and emotionally held beliefs from their youth.<br /><br />Emotion is something we have evolved to utilize. Generally speaking, we need emotion because we have to make split-second decisions sometimes in life and we don't have the opportunity to integrate our decision process on data. If someone attacks me I will become angry because anger will raise my adrenaline levels, temporarily reduce other biological needs like hunger or waste removal, and enable me to fight for survival. Essentially emotion is just a recorded previous decision that works on stereotypical data, or in probabilistic terms it is like basing a quick decision on solely the first moment of a bunch of previously experienced data. The first moment might not be the best descriptor of the data... but if you're in a computational bind you might not be able to do a whole lot better and you'll be biologically penalized for spending your CPU time trying to compute better descriptors of the data. But it is undeniable that decisions we all make based upon emotion are often some of the most powerful and deepest-seated beliefs that we have. <br /><br />With religion this is especially true. Very religious people, in my view, have this software installed emotionally and then spend years practicing the art of pushing the installed software ever closer to the very act of perception itself, until at some point it is almost the case that sensory data is literally passed through a religious filter before it is even processed and presented for perception. A sunset becomes a symbol of God's love so much so that there is (almost) no physical distinction between the literal viewing of photons depicting the sunset scene and the thinking of the thought \"This shows that God loves me.\" Emotionally installed software presents a very difficult problem. Depending on how close to the act of perception that it has been pushed, it implies there is a remarkably tiny window of opportunity for the presentation of data that could convincingly demonstrate that rational alternatives are better in a number of important senses. &nbsp;<br /><br />I'm sure many of you have had debates where you've run into circular logic and unavoidable walls that stifle all useful discussion. Can we as a community come up with a good theory on how sensory data can help to uninstall deep emotionally installed software in someone's brain? I really feel that this is an area that deserves some philosophical attention. Is it the case that if software is installed in someone's brain in conjunction with emotion (and by this I literally mean that the cyclic AMP cycles and other biological processes used for memory formation are made stronger and synaptic connections related to the library of belief concepts (e.g. religious) are reinforced by chemicals released in conjunction with the emotive force of the experience in which they are formed) can only be uninstalled by a similarly impactful emotional experience? It appears that slow-moving rationality and logical discussion are almost physically powerless to succeed as convincing mechanisms. And if this is the case, what should rationalists do to promote their ideas (aside from the obvious social pressure to stop installing religious software in the minds of children, etc.) <br /><br />Note that in the discussion above I use 'religion' as a specific example, but any irrationally held belief that derives from an emotionally impactful experience would serve the same purpose. And also, here we can assume 'religious' refers to ontological claims unsupported by any evidence and then purported to have day-to-day impacts on life and decision-making. I would be very grateful for any thoughts the community has and hopefully we can generate some useful techniques for understanding how to appropriately uninstall emotional software (in the instances when it's useful to do so)... even the kinds of emotional software that we ourselves (rationalists) often fall victim to in our own imperfect understanding of the world.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qF59yHCfb4SnwijrK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 7.241566157423825e-07, "legacy": true, "legacyId": "7865", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-06T06:46:29.210Z", "modifiedAt": null, "url": null, "title": "More info about the book Eliezer is writing?", "slug": "more-info-about-the-book-eliezer-is-writing", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:21.865Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MatthewBaker", "createdAt": "2011-06-03T22:19:50.449Z", "isAdmin": false, "displayName": "MatthewBaker"}, "userId": "xEPvhkraqrPSryfFr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G9a7qbF5soTxmSPwf/more-info-about-the-book-eliezer-is-writing", "pageUrlRelative": "/posts/G9a7qbF5soTxmSPwf/more-info-about-the-book-eliezer-is-writing", "linkUrl": "https://www.lesswrong.com/posts/G9a7qbF5soTxmSPwf/more-info-about-the-book-eliezer-is-writing", "postedAtFormatted": "Monday, June 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20More%20info%20about%20the%20book%20Eliezer%20is%20writing%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMore%20info%20about%20the%20book%20Eliezer%20is%20writing%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9a7qbF5soTxmSPwf%2Fmore-info-about-the-book-eliezer-is-writing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=More%20info%20about%20the%20book%20Eliezer%20is%20writing%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9a7qbF5soTxmSPwf%2Fmore-info-about-the-book-eliezer-is-writing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9a7qbF5soTxmSPwf%2Fmore-info-about-the-book-eliezer-is-writing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<p>I have heard about this book from multiple sources, but so far i have yet to find a good description of the book including prospective release dates, topics and other info. I thought i would ask LW for the appropriate links.</p>\n<p>Thanks in advance,</p>\n<p>Matt</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G9a7qbF5soTxmSPwf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 7.241643517182121e-07, "legacy": true, "legacyId": "7866", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-06T11:27:27.010Z", "modifiedAt": null, "url": null, "title": "Friendly AI - Being good vs. having great sex", "slug": "friendly-ai-being-good-vs-having-great-sex", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.409Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hq4xKwrb9LNZvfoqY/friendly-ai-being-good-vs-having-great-sex", "pageUrlRelative": "/posts/Hq4xKwrb9LNZvfoqY/friendly-ai-being-good-vs-having-great-sex", "linkUrl": "https://www.lesswrong.com/posts/Hq4xKwrb9LNZvfoqY/friendly-ai-being-good-vs-having-great-sex", "postedAtFormatted": "Monday, June 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendly%20AI%20-%20Being%20good%20vs.%20having%20great%20sex&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendly%20AI%20-%20Being%20good%20vs.%20having%20great%20sex%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHq4xKwrb9LNZvfoqY%2Ffriendly-ai-being-good-vs-having-great-sex%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendly%20AI%20-%20Being%20good%20vs.%20having%20great%20sex%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHq4xKwrb9LNZvfoqY%2Ffriendly-ai-being-good-vs-having-great-sex", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHq4xKwrb9LNZvfoqY%2Ffriendly-ai-being-good-vs-having-great-sex", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 294, "htmlBody": "<blockquote>\n<p>[...] I think an LW post is important and interesting in proportion to how much it helps construct a Friendly AI, how much it gets people to participate in the human project [...]</p>\n</blockquote>\n<p>-- <a href=\"/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/3qoa\">Eliezer Yudkowsky</a></p>\n<blockquote>\n<p>I&rsquo;m not going to wait for philosophers to cover this issue correctly, or for use in FAI design.</p>\n</blockquote>\n<p>-- <a href=\"http://commonsenseatheism.com/?p=15264#comment-110928\">Luke Muehlhauser</a></p>\n<hr />\n<p>The above quotes hint at the possibility that some of the content that can be found on lesswrong.com has been written in support of friendly AI research.</p>\n<p><strong>My question, of what importance is ethics when it comes to friendly AI research?</strong> If a friendly AI is one that does protect and cultivate human values, how does ethics help to achieve this?</p>\n<p>Let's assume that there exist some sort of objective <em>right</em>, no matter what that actually means. <em>If</em> humans desire to be <em>right</em>, isn't it the sort of human value that a friendly AI would seek to protect and cultivate?</p>\n<p>What difference is there between wanting to be <em>good</em> and wanting to have a lot of <em>great</em> sex? Both seem to be values that humans might desire, therefore both values have to be taken into account by a friendly AI.</p>\n<p>If a friendly AI has to be able to extrapolate the coherent volition of humanity, without any hard-coded knowledge of human values, why doesn't this extent to ethics as well?</p>\n<p>If we have to solve ethics before being able to design friendly AI, if we have to hard-code what it means to be <em>good</em>, how doesn't this apply to what it means to have <em>great</em> sex as well (or what it means to have <em>sex</em> anyway)?</p>\n<p>If a friendly AI is going to figure out what humans desire, by extrapolating their volition, might it conclude that our volition is immoral and therefore undesirable?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hq4xKwrb9LNZvfoqY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -3, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "7867", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-06T13:28:26.484Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Think Like Reality", "slug": "seq-rerun-think-like-reality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.523Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jmsGDEHQw2hLokhnw/seq-rerun-think-like-reality", "pageUrlRelative": "/posts/jmsGDEHQw2hLokhnw/seq-rerun-think-like-reality", "linkUrl": "https://www.lesswrong.com/posts/jmsGDEHQw2hLokhnw/seq-rerun-think-like-reality", "postedAtFormatted": "Monday, June 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Think%20Like%20Reality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Think%20Like%20Reality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjmsGDEHQw2hLokhnw%2Fseq-rerun-think-like-reality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Think%20Like%20Reality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjmsGDEHQw2hLokhnw%2Fseq-rerun-think-like-reality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjmsGDEHQw2hLokhnw%2Fseq-rerun-think-like-reality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>Today's post, <a href=\"/lw/hs/think_like_reality/\">Think Like Reality</a> was originally published on May 2, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>\"Quantum physics is not \"weird\". You are weird. You have the absolutely bizarre idea that reality ought to consist of little billiard balls bopping around, when in fact reality is a perfectly normal cloud of complex amplitude in configuration space. This is your problem, not reality's, and you are the one who needs to change.\"</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/62d/seq_rerun_universal_law/\">Universal Law</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jmsGDEHQw2hLokhnw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 7.242836590866636e-07, "legacy": true, "legacyId": "7868", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tWLFWAndSZSYN6rPB", "Qq4kp59XWJmySyRL3", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-06T19:03:15.024Z", "modifiedAt": null, "url": null, "title": "Fort Collins, Colorado Meetup Wedneday 7pm", "slug": "fort-collins-colorado-meetup-wedneday-7pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vXA74BoxQPXtQoTkq/fort-collins-colorado-meetup-wedneday-7pm", "pageUrlRelative": "/posts/vXA74BoxQPXtQoTkq/fort-collins-colorado-meetup-wedneday-7pm", "linkUrl": "https://www.lesswrong.com/posts/vXA74BoxQPXtQoTkq/fort-collins-colorado-meetup-wedneday-7pm", "postedAtFormatted": "Monday, June 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvXA74BoxQPXtQoTkq%2Ffort-collins-colorado-meetup-wedneday-7pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvXA74BoxQPXtQoTkq%2Ffort-collins-colorado-meetup-wedneday-7pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvXA74BoxQPXtQoTkq%2Ffort-collins-colorado-meetup-wedneday-7pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p>Dates: 7pm Wednesday June 8, 15, 22 and 29</p>\n<p>Venue: The Bean Cycle, upstairs at the back if it's open. Look for the Less Wrong sign.</p>\n<p>As these are the first few sessions of the meetup, there will be no formal agenda. I'll bring a few games to give us something to do while we get acquainted.</p>\n<p>http://groups.google.com/group/less-wrong-fort-collins-co</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vXA74BoxQPXtQoTkq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 7.243830629878881e-07, "legacy": true, "legacyId": "7869", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-07T03:28:55.842Z", "modifiedAt": null, "url": null, "title": "[prize] Spaced Repetition literature review", "slug": "prize-spaced-repetition-literature-review", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:29.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5rp6TaLYWqJKjXcMG/prize-spaced-repetition-literature-review", "pageUrlRelative": "/posts/5rp6TaLYWqJKjXcMG/prize-spaced-repetition-literature-review", "linkUrl": "https://www.lesswrong.com/posts/5rp6TaLYWqJKjXcMG/prize-spaced-repetition-literature-review", "postedAtFormatted": "Tuesday, June 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bprize%5D%20Spaced%20Repetition%20literature%20review&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bprize%5D%20Spaced%20Repetition%20literature%20review%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rp6TaLYWqJKjXcMG%2Fprize-spaced-repetition-literature-review%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bprize%5D%20Spaced%20Repetition%20literature%20review%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rp6TaLYWqJKjXcMG%2Fprize-spaced-repetition-literature-review", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rp6TaLYWqJKjXcMG%2Fprize-spaced-repetition-literature-review", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 391, "htmlBody": "<p><strong>EDIT: </strong>I am canceling this contest because I feel that the structure of the incentives were poorly thought out (see gwern's comments). I will be posting a new better structured contest in the near future (now <a href=\"/r/discussion/lw/69p/prize_new_spaced_repetition_literature_review_265/\">posted</a>). If you feel this is unfair or otherwise feel slighted, please contact me at my username at gmail.com.</p>\n<p>&nbsp;</p>\n<p>I'm interested in&nbsp;<a href=\"/lw/5zd/making_projects_happen/\">making projects happen</a>&nbsp;on Less Wrong. In order to find out what works and to inspire others to try things too,&nbsp;I'm sponsoring the following small project:</p>\n<div>Spaced Repetition is often mentioned on on Less Wrong as a technique for remembering things. I've started using Anki and it certainly seems to be useful. However, I haven't seen a good summary of evidence on Spaced Repetition.&nbsp;</div>\n<p>I hereby offer a prize to the first person to submit a good summary of the evidence on Spaced Repetition to the main page.&nbsp;The winner will get the prize, currently: $<strong>265</strong><strong>&nbsp;+ 40 to charity (see comments)</strong></p>\n<p>The summary should address at least the following questions:</p>\n<div>\n<div>\n<ul>\n<li>What spacing is best?</li>\n<li>How much does spaced repetition actually help memory?</li>\n<li>Does spaced repetition have hidden benefits or costs?</li>\n<li>Does the effectiveness vary across domains? How much?</li>\n<li>Is there research on the kinds of questions that work best?</li>\n<li>What questions do researchers think are most important?</li>\n<li>Is there any interesting ongoing research? If so, what is it on?</li>\n<li>What, if any, questions do researchers think it is important to answer? Are there other unanswered questions that would jump out at a smart person?</li>\n<li>What does spaced repetition <strong>not</strong>&nbsp;do that people might expect it to?</li>\n</ul>\n<div>The post should summarize the state of current evidence and provide citations to back up the claims in the article.</div>\n<div>If you think you would benefit from the result of this project, please add to the prize! You can contribute to the prize on the&nbsp;<a href=\"http://jsalvatier.chipin.com/spaced-repetition-research-summary\" target=\"_blank\">ChipIn page</a>.</div>\n<div>Whether the summary is 'good' will be judged by me. If there is a serious dispute, I'll agree to dispute resolution by any uninvolved party with more than 5k karma.&nbsp;</div>\n<div>If you have suggestions, questions or comments, please leave them in the comments.&nbsp;</div>\n<div>If you would like to work on this project, please say so in the comments below. Collaboration is encouraged.&nbsp;</div>\n<div>This project is tagged with the 'project' tag and listed on the &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Projects\">Projects wiki</a>&nbsp; page.</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5rp6TaLYWqJKjXcMG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 26, "extendedScore": null, "score": 7.245332438197152e-07, "legacy": true, "legacyId": "7877", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uR4r3eZZqLmjZDqFj", "xdNQEXagwMTCbwwK4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-07T07:49:50.095Z", "modifiedAt": null, "url": null, "title": "Paris Meetup Saturday June 25 ", "slug": "paris-meetup-saturday-june-25", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:27.019Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wdPNwYWSJ8BPhZT8u/paris-meetup-saturday-june-25", "pageUrlRelative": "/posts/wdPNwYWSJ8BPhZT8u/paris-meetup-saturday-june-25", "linkUrl": "https://www.lesswrong.com/posts/wdPNwYWSJ8BPhZT8u/paris-meetup-saturday-june-25", "postedAtFormatted": "Tuesday, June 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Paris%20Meetup%20Saturday%20June%2025%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AParis%20Meetup%20Saturday%20June%2025%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwdPNwYWSJ8BPhZT8u%2Fparis-meetup-saturday-june-25%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Paris%20Meetup%20Saturday%20June%2025%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwdPNwYWSJ8BPhZT8u%2Fparis-meetup-saturday-june-25", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwdPNwYWSJ8BPhZT8u%2Fparis-meetup-saturday-june-25", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<p>Following [last Paris meetup](http://lesswrong.com/lw/5eg/paris_meetup_saturday_april_30th_2pm/), next one will be Satuday, June 25th, probably around 2PM, but we can use this thread to settle on the details.</p>\n<p>Anybody have any suggestions for a better place than some random Caf&eacute;?</p>\n<p>(I had the impression that announcing meetups too early was not advised, as people would just forget ...)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wdPNwYWSJ8BPhZT8u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.246107507027093e-07, "legacy": true, "legacyId": "7883", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-07T12:35:44.560Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Beware the Unsurprised", "slug": "seq-rerun-beware-the-unsurprised", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.440Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yg2XhM5M3NC42fkRk/seq-rerun-beware-the-unsurprised", "pageUrlRelative": "/posts/yg2XhM5M3NC42fkRk/seq-rerun-beware-the-unsurprised", "linkUrl": "https://www.lesswrong.com/posts/yg2XhM5M3NC42fkRk/seq-rerun-beware-the-unsurprised", "postedAtFormatted": "Tuesday, June 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Beware%20the%20Unsurprised&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Beware%20the%20Unsurprised%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyg2XhM5M3NC42fkRk%2Fseq-rerun-beware-the-unsurprised%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Beware%20the%20Unsurprised%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyg2XhM5M3NC42fkRk%2Fseq-rerun-beware-the-unsurprised", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyg2XhM5M3NC42fkRk%2Fseq-rerun-beware-the-unsurprised", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>Today's post, <a href=\"/lw/ht/beware_the_unsurprised/\">Beware the Unsurprised</a> was originally published on May 3, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>If reality consistently surprises you, then your model needs revision. But beware those who act unsurprised by surprising data. Maybe their model was too vague to be contradicted. Maybe they haven't emotionally grasped the implications of the data. Or maybe they are trying to appear poised in front of others. Respond to surprise by revising your model, not by suppressing your surprise.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/62k/seq_rerun_think_like_reality/\">Think like Reality</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yg2XhM5M3NC42fkRk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 7.246957022829938e-07, "legacy": true, "legacyId": "7884", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jyDBcs3Rx8t5Fhquo", "jmsGDEHQw2hLokhnw", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-07T12:54:36.129Z", "modifiedAt": null, "url": null, "title": "Emotional installation of concepts", "slug": "emotional-installation-of-concepts-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q3BSHHQ67mGaga8s4/emotional-installation-of-concepts-0", "pageUrlRelative": "/posts/Q3BSHHQ67mGaga8s4/emotional-installation-of-concepts-0", "linkUrl": "https://www.lesswrong.com/posts/Q3BSHHQ67mGaga8s4/emotional-installation-of-concepts-0", "postedAtFormatted": "Tuesday, June 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Emotional%20installation%20of%20concepts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEmotional%20installation%20of%20concepts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ3BSHHQ67mGaga8s4%2Femotional-installation-of-concepts-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Emotional%20installation%20of%20concepts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ3BSHHQ67mGaga8s4%2Femotional-installation-of-concepts-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ3BSHHQ67mGaga8s4%2Femotional-installation-of-concepts-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<p>This is a plausible theory I've got-- I don't know if formal work has been done in the area.</p>\n<p>I used to read discussions of the definition of science fiction until I came to the conclusion that there was no point. People have strongly felt intuitions about what science fiction <em>really</em> is, and seem to have no way of knowing that everyone doesn't agree until they bump up against other people's definitions.</p>\n<p>But why would one have a strong attachment to a definition of science fiction?</p>\n<p>I suggest that people invent such definitions from a few early, emotionally charged experiences.</p>\n<p>I still find it hard to believe that Delany and Zelazny <em>really</em> wrote <a name=\"New Wave science fiction\"></a>http://en.wikipedia.org/wiki/New_Wave_science_fiction</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q3BSHHQ67mGaga8s4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7885", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-07T13:06:32.705Z", "modifiedAt": null, "url": null, "title": "Emotional installation of concepts", "slug": "emotional-installation-of-concepts", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.305Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9aRvfT43CcibL92zv/emotional-installation-of-concepts", "pageUrlRelative": "/posts/9aRvfT43CcibL92zv/emotional-installation-of-concepts", "linkUrl": "https://www.lesswrong.com/posts/9aRvfT43CcibL92zv/emotional-installation-of-concepts", "postedAtFormatted": "Tuesday, June 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Emotional%20installation%20of%20concepts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEmotional%20installation%20of%20concepts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9aRvfT43CcibL92zv%2Femotional-installation-of-concepts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Emotional%20installation%20of%20concepts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9aRvfT43CcibL92zv%2Femotional-installation-of-concepts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9aRvfT43CcibL92zv%2Femotional-installation-of-concepts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 246, "htmlBody": "<p>This is a theory I find plausible-- I don't know if formal work has been done in the area.</p>\n<p>I used to read discussions of the definition of science fiction until I came to the conclusion that there was no point. People have strongly felt intuitions about what science fiction <em>really</em> is, and seem to have no way of knowing that everyone doesn't agree until they bump up against other people's definitions.</p>\n<p>But why would one have a strong attachment to a definition of science fiction?</p>\n<p>I suggest that people invent such definitions from a few early, emotionally charged experiences.</p>\n<p>I still find it hard to believe that Delany and Zelazny <em>really</em> wrote <a href=\"http://en.wikipedia.org/wiki/New_Wave_science_fiction\">New Wave science fiction</a>, when their vivid and enjoyable stories were so different (for me) from the likes of Malzberg (dreary) and Aldiss (mostly boring).</p>\n<p>I suspect matters are more extreme in visionary politics. One's ideas of a drastically better society are (usually?) derived from an unconscious mix of what seems better and possible, and if the vision is strong enough to lead to action, then it's got to be strongly felt. The unconscious mix is idiosyncratic, and it can be shocking to find out how different the visions are for people who group themselves under the same political label.</p>\n<p>The only thing I can think to do with this theory is to hold a little lightly to definitions rather than look for the one true definition, and to ask people about their prototypes for concepts rather than definitions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9aRvfT43CcibL92zv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 7.24704855622303e-07, "legacy": true, "legacyId": "7886", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-07T15:06:55.021Z", "modifiedAt": null, "url": null, "title": "St. Petersburg Mugging Implies You Have Bounded Utility", "slug": "st-petersburg-mugging-implies-you-have-bounded-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:20.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TimFreeman", "createdAt": "2011-04-12T22:58:16.873Z", "isAdmin": false, "displayName": "TimFreeman"}, "userId": "AAP7Amn8h8BhWCjjC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qv2PZGJ5c5smgi563/st-petersburg-mugging-implies-you-have-bounded-utility", "pageUrlRelative": "/posts/qv2PZGJ5c5smgi563/st-petersburg-mugging-implies-you-have-bounded-utility", "linkUrl": "https://www.lesswrong.com/posts/qv2PZGJ5c5smgi563/st-petersburg-mugging-implies-you-have-bounded-utility", "postedAtFormatted": "Tuesday, June 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20St.%20Petersburg%20Mugging%20Implies%20You%20Have%20Bounded%20Utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASt.%20Petersburg%20Mugging%20Implies%20You%20Have%20Bounded%20Utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqv2PZGJ5c5smgi563%2Fst-petersburg-mugging-implies-you-have-bounded-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=St.%20Petersburg%20Mugging%20Implies%20You%20Have%20Bounded%20Utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqv2PZGJ5c5smgi563%2Fst-petersburg-mugging-implies-you-have-bounded-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqv2PZGJ5c5smgi563%2Fst-petersburg-mugging-implies-you-have-bounded-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 973, "htmlBody": "<p>This post describes an infinite gamble that, under some reasonable assumptions, will motivate people who act to maximize an unbounded utility function to send me all their money. In other words, if you understand this post and it doesn't motivate you to send me all your money, then you have a bounded utility function, or perhaps even upon reflection you are not choosing your actions to maximize expected utility, or perhaps you found a flaw in this post.</p>\n<p><a id=\"more\"></a></p>\n<p>Briefly, we do this with <a href=\"http://en.wikipedia.org/wiki/St._Petersburg_paradox\">The St. Petersburg Paradox</a>, converted to a mugging along the lines of <a href=\"http://www.nickbostrom.com/papers/pascal.pdf\">Pascal's</a> <a href=\"http://wiki.lesswrong.com/wiki/Pascal%27s_mugging\">Mugging</a>. I then tweaked it to extract all of the money instead of just a fixed sum.</p>\n<p>I have always wondered if any actual payments have resulted from Pascal's Mugging, so I intend to track payments received for this variation. If anyone does have unbounded utility and wants to prove me wrong by sending money, send it with Paypal to tim at fungible dot com. Annotate the transfer with the phrase \"St. Petersburg Mugging\", and I'll edit this article periodically to say how much money I received. In order to avoid confusing the experiment, and to exercise my spite, I promise I will not spend the money on anything you will find especially valuable. SIAI would be better charity, if you want to do charity, but don't send that money to me.</p>\n<p>Here's the hypothetical (that is, false) offer to persons with unbounded utility:</p>\n<ul>\n<li>Let's call your utility function \"UTILITY\". We assume it takes a state of the universe as an argument. </li>\n<li>Define DUT to be UTILITY(the present situation plus you receiving $1000)-UTILITY(the present situation). Here DUT stands for Difference in UTility. We assume DUT is positive. </li>\n<li>You have unbounded utility, so for each nonnegative N there is a universe UN(N) such that UTILITY(UN(N)) is at least DUT * 2**N. Here UN stands for \"universe\". </li>\n<li>The phrase \"I am a god\" is defined to mean that I am able to change the universe to any state I choose. I may not be a god after I make the change. </li>\n<li>The offer is: For every dollar you send me, I will flip a coin. If it comes out Tails, or I am not a god, I will do nothing. If it comes out Heads and I am a god, I will flip the coin repeatedly until I see it come up Heads again. Let T be the number of times it was Tails. I will then change the universe to UN(T). </li>\n</ul>\n<p>If I am lying and the offer is real, and I am a god, what utility will you receive from sending me a dollar? Well, the probability of me seeing N Tails followed by a Head is (1/2)**(N + 1), and your utility for the resulting universe is UTILITY(UN(N)) &gt;= DUT * 2**N, so your expected utility if I see N tails is (1/2)**(N + 1) * UTILITY(UN(N)) &gt;= (1/2)**(N + 1) * DUT * 2 ** N = DUT/2. There are infinitely many possible values for N, so your total expected utility is positive infinity * DUT/2, which is positive infinity.</p>\n<p>I hope we agree that it is unlikely that I am a god, but it's consistent with what you have observed so far, so unless you were born with certain knowledge that I am not a god, you have to assign positive probability to it. Similarly, the probability that I'm lying and the above offer is real is also positive. The product of two positive numbers is positive. Combining this with the result from the previous paragraph, your expected utility from sending me a dollar is infinitely positive.</p>\n<p>If you send me one dollar, there will probably be no result. Perhaps I am a god, and the above offer is real, but I didn't do anything beyond flipping the first coin because it came out Tails. In that case, nothing happens. Your expected utility for the next dollar is also infinitely positive, so you should send the next dollar too. By induction you should send me all your dollars.</p>\n<p>If you don't send money because you have bounded utility, that's my desired outcome. If you do feel motivated to send me money, well, I suppose I lost the argument. Remember to send all of it, and remember that you can always send me more later.</p>\n<p>As of 7 June 2011, nobody has sent me any money for this.</p>\n<p>ETA: Some interesting issues keep coming up.  I'll put them here to decrease the redundancy:</p>\n<ul>\n<li>Yes, you can justify not giving me money because I might be a god by claiming that there are lots of other unlikely gods that have a better claim on your resources.  My purpose in writing this post is to find a good reason not to be jerked around by unlikely gods in general.  Finding a reason to be jerked around by some other unlikely god is missing the point. </li>\n<li>I forgot to mention that if I am a god, I can stop time while I flip coins, so we aren't resource-constrained on the number of times I can flip the coin. </li>\n<li>Yes, you can say that your prior probability of me being a god is zero. If you want to go that way, can you say what that prior probability distribution looks like in general?  I'm actually more worried about making a Friendly AI that gets jerked around by an unlikely god that we did not plan for, so having a special case about me being god doesn't solve an interesting portion of the problem.  For what it's worth, I believe the Universal Prior would give positive small probability to many scenarios that have a god,  since universes with a god are not incredibly much more complex than universes that don't have a god. </li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qv2PZGJ5c5smgi563", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 13, "extendedScore": null, "score": 7.247406277149242e-07, "legacy": true, "legacyId": "7887", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 165, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T03:41:41.375Z", "modifiedAt": null, "url": null, "title": "Cambridge Less Wrong Group Planning Meetup, Tuesday 14 June 7pm", "slug": "cambridge-less-wrong-group-planning-meetup-tuesday-14-june", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:24.060Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pho7XemLbp4aD4Nsn/cambridge-less-wrong-group-planning-meetup-tuesday-14-june", "pageUrlRelative": "/posts/Pho7XemLbp4aD4Nsn/cambridge-less-wrong-group-planning-meetup-tuesday-14-june", "linkUrl": "https://www.lesswrong.com/posts/Pho7XemLbp4aD4Nsn/cambridge-less-wrong-group-planning-meetup-tuesday-14-june", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cambridge%20Less%20Wrong%20Group%20Planning%20Meetup%2C%20Tuesday%2014%20June%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACambridge%20Less%20Wrong%20Group%20Planning%20Meetup%2C%20Tuesday%2014%20June%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPho7XemLbp4aD4Nsn%2Fcambridge-less-wrong-group-planning-meetup-tuesday-14-june%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cambridge%20Less%20Wrong%20Group%20Planning%20Meetup%2C%20Tuesday%2014%20June%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPho7XemLbp4aD4Nsn%2Fcambridge-less-wrong-group-planning-meetup-tuesday-14-june", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPho7XemLbp4aD4Nsn%2Fcambridge-less-wrong-group-planning-meetup-tuesday-14-june", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>Visiting the rationality minicamp and hearing about the amazing things NYC's meetup group is doing, and comparing them to what Cambridge's group is doing, has made me realize that much more is possible. Let's meet and discuss future directions - activities, locations, and dates - for the Cambridge group.</p>\n<p>Let's meet at <a href=\"http://maps.google.com/maps/place?cid=80072499531736922&amp;q=&amp;hl=en&amp;ved=0CJQCEMYHSAA&amp;sa=X&amp;ei=--vuTeD4EojAzgS696jNAQ\">Legal Seafoods</a> near Kendall Square on June 14 at 7pm. The first thing on the agenda is, we need a good room for giving presentations. Several members have projectors they can bring, but a restaurant or coffee shop doesn't really work. Ideal would be if an MIT student could reserve us a room, but no one has agreed to take on that task yet. I have nonspecific plans to give presentations based on those that were given at the rationality minicamp. We'll also be copying some of the exercises and worksheets given there. Any other ideas for presentations, activities, and locations are welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pho7XemLbp4aD4Nsn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.249650020099311e-07, "legacy": true, "legacyId": "7898", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T03:43:58.791Z", "modifiedAt": null, "url": null, "title": "Rational Parenting?", "slug": "rational-parenting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:27.313Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jdinkum", "createdAt": "2011-02-13T00:27:51.433Z", "isAdmin": false, "displayName": "jdinkum"}, "userId": "PXAxasBwXYqoEyYNv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZvPckgNDvaWacWZdH/rational-parenting", "pageUrlRelative": "/posts/ZvPckgNDvaWacWZdH/rational-parenting", "linkUrl": "https://www.lesswrong.com/posts/ZvPckgNDvaWacWZdH/rational-parenting", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Parenting%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Parenting%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZvPckgNDvaWacWZdH%2Frational-parenting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Parenting%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZvPckgNDvaWacWZdH%2Frational-parenting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZvPckgNDvaWacWZdH%2Frational-parenting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p>I'm looking for insight into \"rational parenting\".</p>\n<p>&nbsp;</p>\n<p>By that term, I mean two things. One is parenting in a rational way. The other is raising a child who shares rationalist thought.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZvPckgNDvaWacWZdH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 28, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "7899", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T06:04:03.603Z", "modifiedAt": null, "url": null, "title": "Houston Hackerspace Meetup: Sunday June 12, 3:00PM", "slug": "houston-hackerspace-meetup-sunday-june-12-3-00pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:23.010Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cog", "createdAt": "2011-04-25T04:58:53.803Z", "isAdmin": false, "displayName": "Cog"}, "userId": "xkp87vCZ56dp2tWnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fjiZ2utAapw6zN4gt/houston-hackerspace-meetup-sunday-june-12-3-00pm", "pageUrlRelative": "/posts/fjiZ2utAapw6zN4gt/houston-hackerspace-meetup-sunday-june-12-3-00pm", "linkUrl": "https://www.lesswrong.com/posts/fjiZ2utAapw6zN4gt/houston-hackerspace-meetup-sunday-june-12-3-00pm", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Houston%20Hackerspace%20Meetup%3A%20Sunday%20June%2012%2C%203%3A00PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHouston%20Hackerspace%20Meetup%3A%20Sunday%20June%2012%2C%203%3A00PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfjiZ2utAapw6zN4gt%2Fhouston-hackerspace-meetup-sunday-june-12-3-00pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Houston%20Hackerspace%20Meetup%3A%20Sunday%20June%2012%2C%203%3A00PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfjiZ2utAapw6zN4gt%2Fhouston-hackerspace-meetup-sunday-june-12-3-00pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfjiZ2utAapw6zN4gt%2Fhouston-hackerspace-meetup-sunday-june-12-3-00pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<div id=\"entry_t3_5z5\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<p style=\"margin-bottom: 0in;\"><em>Saturday June 4, 2:00PM</em></p>\n<p style=\"margin-bottom: 0in;\"><em>TX/RX Labs Hackerspace</em></p>\n<p style=\"margin-bottom: 0in;\"><em>2010 Commerce St</em></p>\n<p style=\"margin-bottom: 0in;\"><em>Houston, TX 77002</em></p>\n<p>&nbsp;</p>\nThe fourth meeting of the Houston Less Wrong meetup group will be happening on Sunday, the 12th of June. We will be addressing the first chapter in Jayne's \"The Logic of Science\", and play a round or two of paranoid debating. Last week we had 8 people in total, so hopefully we can replicate that type of success again. It was a good meet and greet, and set us up for future connections with other people around the Texas area. As the meetings go on, I suspect we will get more focused.\n<p>&nbsp;</p>\n<p>Pizza or prepared food are a possibility, if people show up hungry. We also have a full kitchen in the hackerspace.</p>\n<p><strong>Directions</strong></p>\n<p><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">A pictoral view</p>\n<p><img src=\"http://images.lesswrong.com/t3_5pp_1.png\" alt=\"Front\" width=\"645\" height=\"470\" /></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This is the set of buildings that the  hackerspace is in. It's difficult to see our front from this angle -  unfortunately google maps decided to map everything but our little  section of commerce street. It's near where the white truck and red  motorcycle are. Currently, there is an old military vehicle and  generator in front. We will have a Less Wrong sign posted on the generator.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><img src=\"http://images.lesswrong.com/t3_5pp_0.png\" alt=\"Empy Lot\" width=\"644\" height=\"465\" /></p>\n<p style=\"margin-bottom: 0in;\">And this is the empty lot that you can park in if all the nearby marked spots are taken.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For more reference:</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><a href=\"http://maps.google.com/maps?client=ubuntu&amp;channel=fs&amp;q=2010\">http://maps.google.com/maps?client=ubuntu&amp;channel=fs&amp;q=2010</a>+Commerce+St.+Houston,+Tx+77002&amp;oe=utf-8&amp;um=1&amp;ie=UTF-8&amp;hq=&amp;hnear=0x8640bed8ed95625d:0x4c9af214d2032035,2010+Commerce+St,+Houston,+TX+77002&amp;gl=us&amp;ei=C9LRTYHvE8fL0QGu8OjlCw&amp;sa=X&amp;oi=geocode_result&amp;ct=title&amp;resnum=1&amp;ved=0CBkQ8gEwAA</p>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fjiZ2utAapw6zN4gt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.250073387510391e-07, "legacy": true, "legacyId": "7904", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T06:08:28.626Z", "modifiedAt": null, "url": null, "title": "Logan, UT meetup Sat 18 Jun 4pm", "slug": "logan-ut-meetup-sat-18-jun-4pm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:30.617Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lextori", "createdAt": "2010-11-26T19:30:24.317Z", "isAdmin": false, "displayName": "lextori"}, "userId": "fqTCpdwfrxkBeWpfs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PZnXuSidDKJrnYWWJ/logan-ut-meetup-sat-18-jun-4pm", "pageUrlRelative": "/posts/PZnXuSidDKJrnYWWJ/logan-ut-meetup-sat-18-jun-4pm", "linkUrl": "https://www.lesswrong.com/posts/PZnXuSidDKJrnYWWJ/logan-ut-meetup-sat-18-jun-4pm", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logan%2C%20UT%20meetup%20Sat%2018%20Jun%204pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogan%2C%20UT%20meetup%20Sat%2018%20Jun%204pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPZnXuSidDKJrnYWWJ%2Flogan-ut-meetup-sat-18-jun-4pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logan%2C%20UT%20meetup%20Sat%2018%20Jun%204pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPZnXuSidDKJrnYWWJ%2Flogan-ut-meetup-sat-18-jun-4pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPZnXuSidDKJrnYWWJ%2Flogan-ut-meetup-sat-18-jun-4pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p><a href=\"/user/Mezik\">Mezik</a> and I got talking this week and figured that being in an town with a major engineering program we might have enough less wrongians to begin a useful meet-up group. So we're making one happen.</p>\n<p>Meet at 4pm at the <a href=\"http://library.usu.edu/\">USU Library</a> If the weather is nice we will be outside of the cafe, otherwise we will head inside.</p>\n<p>I'm drawing a bit of a blank as to what to do, but if we have enough people we will plan on playing<a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\"> paranoid debate</a>.</p>\n<p>What are some good question types for use in paranoid debate?</p>\n<p>Edit: added a few tags. and question about good topics for paranoid debate.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PZnXuSidDKJrnYWWJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.2500865231342e-07, "legacy": true, "legacyId": "7905", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T14:19:42.504Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Third Alternative", "slug": "seq-rerun-the-third-alternative", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.606Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xdcGyRqcM6JGrywP2/seq-rerun-the-third-alternative", "pageUrlRelative": "/posts/xdcGyRqcM6JGrywP2/seq-rerun-the-third-alternative", "linkUrl": "https://www.lesswrong.com/posts/xdcGyRqcM6JGrywP2/seq-rerun-the-third-alternative", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Third%20Alternative&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Third%20Alternative%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxdcGyRqcM6JGrywP2%2Fseq-rerun-the-third-alternative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Third%20Alternative%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxdcGyRqcM6JGrywP2%2Fseq-rerun-the-third-alternative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxdcGyRqcM6JGrywP2%2Fseq-rerun-the-third-alternative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>Today's post, <a href=\"/lw/hu/the_third_alternative/\">The Third Alternative</a> was originally published on May 6, 2007. A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>People justify Noble Lies by pointing out their benefits over doing nothing. But, if you really need these benefits, you can construct a Third Alternative for getting them. How? You have to search for one. Beware the temptation not to search or to search perfunctorily. Ask yourself, \"Did I spend five minutes by the clock trying hard to think of a better alternative?\"</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/630/seq_rerun_beware_the_unsurprised/\">Beware the Unsurprised</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb153": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xdcGyRqcM6JGrywP2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.251547642430561e-07, "legacy": true, "legacyId": "7909", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["erGipespbbzdG5zYb", "yg2XhM5M3NC42fkRk", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T18:23:05.279Z", "modifiedAt": null, "url": null, "title": "States of knowledge as quantum configurations", "slug": "states-of-knowledge-as-quantum-configurations-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qjNfWjkzvo7SMvKZj/states-of-knowledge-as-quantum-configurations-0", "pageUrlRelative": "/posts/qjNfWjkzvo7SMvKZj/states-of-knowledge-as-quantum-configurations-0", "linkUrl": "https://www.lesswrong.com/posts/qjNfWjkzvo7SMvKZj/states-of-knowledge-as-quantum-configurations-0", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20States%20of%20knowledge%20as%20quantum%20configurations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStates%20of%20knowledge%20as%20quantum%20configurations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjNfWjkzvo7SMvKZj%2Fstates-of-knowledge-as-quantum-configurations-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=States%20of%20knowledge%20as%20quantum%20configurations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjNfWjkzvo7SMvKZj%2Fstates-of-knowledge-as-quantum-configurations-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjNfWjkzvo7SMvKZj%2Fstates-of-knowledge-as-quantum-configurations-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 14, "htmlBody": "<p>I'm in the middle of reading the &lt;a http://lesswrong.com/lw/r5/the_quantum_physics_sequence/&gt; &nbsp;sequence on quantum mechanics &lt;/a&gt;.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qjNfWjkzvo7SMvKZj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "7910", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T18:38:39.074Z", "modifiedAt": null, "url": null, "title": "States of knowledge as amplitude configurations", "slug": "states-of-knowledge-as-amplitude-configurations", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.321Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pfFPTXidRkuB3esDN/states-of-knowledge-as-amplitude-configurations", "pageUrlRelative": "/posts/pfFPTXidRkuB3esDN/states-of-knowledge-as-amplitude-configurations", "linkUrl": "https://www.lesswrong.com/posts/pfFPTXidRkuB3esDN/states-of-knowledge-as-amplitude-configurations", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20States%20of%20knowledge%20as%20amplitude%20configurations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStates%20of%20knowledge%20as%20amplitude%20configurations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpfFPTXidRkuB3esDN%2Fstates-of-knowledge-as-amplitude-configurations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=States%20of%20knowledge%20as%20amplitude%20configurations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpfFPTXidRkuB3esDN%2Fstates-of-knowledge-as-amplitude-configurations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpfFPTXidRkuB3esDN%2Fstates-of-knowledge-as-amplitude-configurations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 365, "htmlBody": "<p>I am reading through the&nbsp;<a href=\"/lw/r5/the_quantum_physics_sequence/\">sequence on quantum physics</a> and have had some questions which I am sure have been thought about by far more qualified people. If you have any useful comments or links about these ideas, please share.</p>\n<p>Most of the strongest resistance to ideas about rationalism that I encounter comes not from people with religious beliefs per se, but usually from mathematicians or philosophers who want to assert arguments about the limits of knowledge, the fidelity of sensory perception as a means for gaining knowledge, and various (what I consider to be) pathological examples (such as the <a href=\"/lw/p7/zombies_zombies/\"> zombie example</a>). Among other things, people tend to reduce the argument to the <a href=\"http://plato.stanford.edu/entries/wittgenstein-atomism/\"> existence of proper names</a> a la Wittgenstein and then go on to assert that the meaning of mathematics or mathematical proofs constitutes something which is fundamentally not part of the physical world.</p>\n<p>As I am reading the quantum physics sequence (keep in mind that I am not a physicist; I am an applied mathematician and statistician and so the mathematical framework of Hilbert spaces and amplitude configurations makes vastly much more sense to me than billiard balls or waves, yet connecting it to reality is still very hard for me) I am struck by the thought that all thoughts are themselves fundamentally just amplitude configurations, and by extension, all claims about knowledge about things are also statements about amplitude configurations. For example, my view is that the color red does not exist in and of itself but rather that the experience of the color red is a statement about common configurations of particle amplitudes. When I say \"that sign is red\", one could unpack this into a detailed statement about statistical properties of configurations of particles in my brain.</p>\n<p>The same reasoning seems to apply just as well to something like group theory. States of knowledge about the <a href=\"http://en.wikipedia.org/wiki/Sylow_theorems\"> Sylow theorems</a>, just as an example, would be properties of particle amplitude configurations in a brain. The Sylow theorems are not separately existing entities which are of themselves \"true\" in any sense.</p>\n<p>Perhaps I am way off base in thinking this way. Can any philosophers of the mind point me in the right direction to read more about this?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pfFPTXidRkuB3esDN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 1, "extendedScore": null, "score": 7.252318052170101e-07, "legacy": true, "legacyId": "7912", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hc9Eg6erp6hk9bWhn", "fdEWWr8St59bXLbQr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T18:54:23.104Z", "modifiedAt": null, "url": null, "title": "Requirements for AI to go FOOM", "slug": "requirements-for-ai-to-go-foom", "viewCount": null, "lastCommentedAt": "2022-05-18T12:12:42.843Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t8dtuYXPhwt93PstB/requirements-for-ai-to-go-foom", "pageUrlRelative": "/posts/t8dtuYXPhwt93PstB/requirements-for-ai-to-go-foom", "linkUrl": "https://www.lesswrong.com/posts/t8dtuYXPhwt93PstB/requirements-for-ai-to-go-foom", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Requirements%20for%20AI%20to%20go%20FOOM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequirements%20for%20AI%20to%20go%20FOOM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft8dtuYXPhwt93PstB%2Frequirements-for-ai-to-go-foom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Requirements%20for%20AI%20to%20go%20FOOM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft8dtuYXPhwt93PstB%2Frequirements-for-ai-to-go-foom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft8dtuYXPhwt93PstB%2Frequirements-for-ai-to-go-foom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1253, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/2l0/should_i_believe_what_the_siai_claims/\">Should I believe what the SIAI claims?</a>; <a href=\"/lw/304/what_i_would_like_the_siai_to_publish/\">What I would like the SIAI to publish</a></p>\n<p>The argument, that an <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">AI can go FOOM</a> (undergo explosive recursive self-improvement), requires various premises (P#) to be true simultaneously:</p>\n<ul>\n<li><strong>P1:</strong> The human development of artificial general intelligence will take place quickly.</li>\n<li><strong>P2:</strong> Any increase in intelligence does vastly outweigh its computational cost and the expenditure of time needed to discover it.</li>\n<li><strong>P3:</strong> AGI is able to create, or acquire, resources, empowering technologies or civilisatory support.</li>\n<li><strong>P4:</strong> AGI can undergo explosive recursive self-improvement and reach superhuman intelligence without having to rely on slow environmental feedback.</li>\n<li><strong>P5:</strong> Goal stability and self-preservation are not requirements for an AGI to undergo explosive recursive self-improvement.</li>\n<li><strong>P6:</strong> AGI researchers will be smart enough and manage to get everything right, including a mathematically precise definition of the AGI's utility-function, yet fail to implement spatio-temporal scope boundaries, resource usage and optimization limits.</li>\n</ul>\n<p>Therefore the probability of an AI to go FOOM (P(FOOM)) is the probability of the <a href=\"http://en.wikipedia.org/wiki/Logical_conjunction\">conjunction</a> (P#<span style=\"color: #000000;\"><strong>&and;</strong>P#)</span> of its premises:</p>\n<p><span style=\"color: #000000;\"><strong>P(FOOM) = P(P1&and;P2&and;P3&and;P4&and;P5&and;P6)</strong></span></p>\n<p>Of course, there are many more premises that need to be true in order to enable an AI to go FOOM, e.g. that each level of intelligence can effectively handle its own complexity, or that most AGI designs can somehow self-modify their way up to massive superhuman intelligence. But I believe that the above points are enough to show that the case for a hard takeoff is not disjunctive, but rather strongly conjunctive.</p>\n<hr />\n<p><strong>Premise 1 (P1):</strong> If the development of AGI takes place slowly, a gradual and controllable development, we might be able to learn from small-scale mistakes, or have enough time to develop friendly AI, while having to face other existential risks.</p>\n<p>This might for example be the case if <em>intelligence</em> can not be captured by a discrete algorithm, or is modular, and therefore never allow us to reach a point where we can suddenly build the smartest thing ever that does just extend itself indefinitely.<br /><br /><strong>Premise 2 (P2):</strong> If you increase intelligence you might also increase the computational cost of its further improvement, the distance to the discovery of some unknown unknown that could enable another quantum leap, by reducing the design space with every iteration.</p>\n<p>If an AI does need to apply a lot more energy to get a bit more complexity, then it might not be instrumental for an AGI to increase its intelligence, rather than using its existing intelligence to pursue its terminal goals or to invest its given resources to acquire other means of self-improvement, e.g. more efficient sensors.</p>\n<p><strong>Premise 3 (P3): </strong>If artificial general intelligence is unable to seize the resources necessary to undergo explosive recursive self-improvement (FOOM), then, the ability and cognitive flexibility of superhuman intelligence in and of itself, as characteristics alone, would have to be sufficient to self-modify its way up to massive superhuman intelligence within a very short time.<br /><br />Without advanced real-world nanotechnology it will be considerable more difficult for an AI to FOOM. It will have to make use of existing infrastructure, e.g. buy stocks of chip manufactures and get them to create more or better CPU&rsquo;s. It will have to rely on puny humans for a lot of tasks. It won&rsquo;t be able to create new computational substrate without the whole economy of the world supporting it. It won&rsquo;t be able to create an army of robot drones overnight without it either.<br /><br />Doing so it would have to make use of considerable amounts of social engineering without its creators noticing it. But, more importantly, it will have to make use of its existing intelligence to do all of that. The AGI would have to acquire new resources slowly, as it couldn&rsquo;t just self-improve to come up with faster and more efficient solutions. In other words, self-improvement would demand resources, therefore the AGI could not profit from its ability to self-improve, regarding the necessary acquisition of resources, to be able to self-improve in the first place.<br /><br />Therefore the absence of advanced nanotechnology constitutes an immense blow to the possibility of explosive recursive self-improvement.<br /><br />One might argue that an AGI will solve nanotechnology on its own and find some way to trick humans into manufacturing a molecular assembler and grant it access to it. But this might be very difficult.<br /><br />There is a strong interdependence of resources and manufacturers. The AI won&rsquo;t be able to simply trick some humans to build a high-end factory to create computational substrate, let alone a molecular assembler. People will ask questions and shortly after get suspicious. Remember, it won&rsquo;t be able to coordinate a world-conspiracy, it hasn&rsquo;t been able to self-improve to that point yet, because it is still trying to acquire enough resources, which it has to do the hard way without nanotech.<br /><br />Anyhow, you&rsquo;d probably need a brain the size of the moon to effectively run and coordinate a whole world of irrational humans by intercepting their communications and altering them on the fly without anyone freaking out.<br /><br />If the AI can&rsquo;t make use of nanotechnology it might make use of something we haven&rsquo;t even thought about. What, magic?</p>\n<p><strong>Premise 4 (P4): </strong>Just imagine you emulated a grown up human mind and it wanted to become a pick up artist, how would it do that with an Internet connection? It would need some sort of avatar, at least, and then wait for the environment to provide a lot of feedback.</p>\n<p>So, even if we're talking about the emulation of a grown up mind, it will be really hard to acquire some capabilities. Then how is the emulation of a human toddler going to acquire those skills? Even worse, how is some sort of abstract AGI going to do it that misses all of the hard coded capabilities of a human toddler?</p>\n<p>Can we even attempt to imagine what is wrong about a boxed emulation of a human toddler, that makes it unable to become a master of social engineering in a very short time?</p>\n<p>Can we imagine what is missing that would enable one of the existing expert systems to quickly evolve vastly superhuman capabilities in its narrow area of expertise?</p>\n<p><strong>Premise 5 (P5):</strong> A paperclip maximizer wants to guarantee that its goal of maximizing paperclips will be preserved when it improves itself.</p>\n<p>By definition, a paperclip maximizer is unfriendly, does not feature inherent goal-stability (a <span class=\"il\">decision</span> <span class=\"il\">theory</span> of self-modifying <span class=\"il\">decision</span> systems), and therefore has to use its initial seed intelligence to devise a sort of paperclip-friendliness before it can go FOOM.</p>\n<p><strong>Premise 6 (P6):</strong> Complex goals need complex optimization parameters (the design specifications of the subject of the optimization process against which it will measure its success of self-improvement).</p>\n<p>Even the creation of paperclips is a much more complex goal than telling an AI to compute as many digits of Pi as possible.</p>\n<p>For an AGI, that was designed to design paperclips, to pose an existential risk, its creators would have to be capable enough to enable it to take over the universe on its own, yet forget, or fail to, define time, space and energy bounds as part of its optimization parameters. Therefore, given the large amount of restrictions that are inevitably part of any advanced general intelligence (AGI), the nonhazardous subset of all possible outcomes might be much larger than that where the AGI works perfectly yet fails to hold before it could wreak havoc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t8dtuYXPhwt93PstB", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 3, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "7913", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5vogC4eJ4gXixX2KJ", "43xgZWSCYAKs7Z9F2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-06-08T18:54:23.104Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T19:03:45.283Z", "modifiedAt": null, "url": null, "title": "What does lack of evidence of a causal relationship tell you?", "slug": "what-does-lack-of-evidence-of-a-causal-relationship-tell-you", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:21.137Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pAwhjNTvFo7RqyX2e/what-does-lack-of-evidence-of-a-causal-relationship-tell-you", "pageUrlRelative": "/posts/pAwhjNTvFo7RqyX2e/what-does-lack-of-evidence-of-a-causal-relationship-tell-you", "linkUrl": "https://www.lesswrong.com/posts/pAwhjNTvFo7RqyX2e/what-does-lack-of-evidence-of-a-causal-relationship-tell-you", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20does%20lack%20of%20evidence%20of%20a%20causal%20relationship%20tell%20you%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20does%20lack%20of%20evidence%20of%20a%20causal%20relationship%20tell%20you%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAwhjNTvFo7RqyX2e%2Fwhat-does-lack-of-evidence-of-a-causal-relationship-tell-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20does%20lack%20of%20evidence%20of%20a%20causal%20relationship%20tell%20you%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAwhjNTvFo7RqyX2e%2Fwhat-does-lack-of-evidence-of-a-causal-relationship-tell-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAwhjNTvFo7RqyX2e%2Fwhat-does-lack-of-evidence-of-a-causal-relationship-tell-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>Imagine that you know there is a strong correlation between X and Y. &nbsp;Statistically competent scholars have extensively examined the causal relationship between X and Y and have failed to find a significant causal relationship and have failed to rule out the possibility that there is a significant causal relationship. &nbsp;</p>\n<p>Would it be reasonable for you to claim that the causal relationship between X and Y&nbsp;probably&nbsp;isn't too strong or it would have shown up clearly on statistical analysis? &nbsp;At the very least, should learning of the negative results of the scholars cause you to decrease your estimate of the causal relationship between X and Y?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pAwhjNTvFo7RqyX2e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 7.252392747624236e-07, "legacy": true, "legacyId": "7914", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T19:41:03.314Z", "modifiedAt": null, "url": null, "title": "Anyone else work at Microsoft?", "slug": "anyone-else-work-at-microsoft", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:21.285Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GuySrinivasan", "createdAt": "2009-02-27T21:00:32.986Z", "isAdmin": false, "displayName": "GuySrinivasan"}, "userId": "HMnfd9HdRCfuRcdBG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JMoYpEuJ38a7QHzcu/anyone-else-work-at-microsoft", "pageUrlRelative": "/posts/JMoYpEuJ38a7QHzcu/anyone-else-work-at-microsoft", "linkUrl": "https://www.lesswrong.com/posts/JMoYpEuJ38a7QHzcu/anyone-else-work-at-microsoft", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anyone%20else%20work%20at%20Microsoft%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnyone%20else%20work%20at%20Microsoft%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJMoYpEuJ38a7QHzcu%2Fanyone-else-work-at-microsoft%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anyone%20else%20work%20at%20Microsoft%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJMoYpEuJ38a7QHzcu%2Fanyone-else-work-at-microsoft", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJMoYpEuJ38a7QHzcu%2Fanyone-else-work-at-microsoft", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 19, "htmlBody": "<p>I've created an internal discussion list \"lesswrong\", come join it. I know there must be more of us around!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JMoYpEuJ38a7QHzcu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 7.252503738398459e-07, "legacy": true, "legacyId": "7915", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-08T22:18:26.913Z", "modifiedAt": null, "url": null, "title": "The Four-Hour Body by Timothy Ferriss - any LWers tried it?", "slug": "the-four-hour-body-by-timothy-ferriss-any-lwers-tried-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:32.486Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5JwjSvfKspRRAi5n7/the-four-hour-body-by-timothy-ferriss-any-lwers-tried-it", "pageUrlRelative": "/posts/5JwjSvfKspRRAi5n7/the-four-hour-body-by-timothy-ferriss-any-lwers-tried-it", "linkUrl": "https://www.lesswrong.com/posts/5JwjSvfKspRRAi5n7/the-four-hour-body-by-timothy-ferriss-any-lwers-tried-it", "postedAtFormatted": "Wednesday, June 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Four-Hour%20Body%20by%20Timothy%20Ferriss%20-%20any%20LWers%20tried%20it%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Four-Hour%20Body%20by%20Timothy%20Ferriss%20-%20any%20LWers%20tried%20it%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5JwjSvfKspRRAi5n7%2Fthe-four-hour-body-by-timothy-ferriss-any-lwers-tried-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Four-Hour%20Body%20by%20Timothy%20Ferriss%20-%20any%20LWers%20tried%20it%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5JwjSvfKspRRAi5n7%2Fthe-four-hour-body-by-timothy-ferriss-any-lwers-tried-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5JwjSvfKspRRAi5n7%2Fthe-four-hour-body-by-timothy-ferriss-any-lwers-tried-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>I've just read the Four-Hour Body by Timothy Ferriss.&nbsp; It seems on the face of it like <em>ridiculously</em> valuable material, if true - like what the completed version of Michael Vassar's proposed reboot of dietary science would look like at the finish point if dieting turned out to be more susceptible to Munchkinism than in my wildest dreams.&nbsp; Ferriss also talks the rationalist talk quite well in this book, much more so than in <em>Four-Hour Workweek</em>; he cites the experiments and occasionally says things like \"I spent a lot of money on this and I expected it to work and it didn't work at all\" or \"I tried this and it seemed to work and I have no idea why it worked and I think it was probably a placebo effect.\"</p>\n<p>Does the LessWrong hivemind have an opinion about 4HB?&nbsp; Has anyone tried it and found that it doesn't work, or that it does work, or that it works but not as well as Ferriss thinks it should work?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"92SxJsDZ78ApAGq72": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5JwjSvfKspRRAi5n7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 29, "extendedScore": null, "score": 7.25297066601479e-07, "legacy": true, "legacyId": "7916", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-09T01:20:24.086Z", "modifiedAt": null, "url": null, "title": "Berkeley Monthly Meetup, June 11th, 7:00 PM", "slug": "berkeley-monthly-meetup-june-11th-7-00-pm", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5wsdTcYekdvLHggaj/berkeley-monthly-meetup-june-11th-7-00-pm", "pageUrlRelative": "/posts/5wsdTcYekdvLHggaj/berkeley-monthly-meetup-june-11th-7-00-pm", "linkUrl": "https://www.lesswrong.com/posts/5wsdTcYekdvLHggaj/berkeley-monthly-meetup-june-11th-7-00-pm", "postedAtFormatted": "Thursday, June 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Berkeley%20Monthly%20Meetup%2C%20June%2011th%2C%207%3A00%20PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABerkeley%20Monthly%20Meetup%2C%20June%2011th%2C%207%3A00%20PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wsdTcYekdvLHggaj%2Fberkeley-monthly-meetup-june-11th-7-00-pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Berkeley%20Monthly%20Meetup%2C%20June%2011th%2C%207%3A00%20PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wsdTcYekdvLHggaj%2Fberkeley-monthly-meetup-june-11th-7-00-pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wsdTcYekdvLHggaj%2Fberkeley-monthly-meetup-june-11th-7-00-pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p>Hey all!<br /><br />The monthly Berkeley meetup will be held at 3045 Shattuck Avenue, two blocks east of the Ashby BART station. It's a 3 story building with apartments on the top 2 floors and a large open commercial space on the bottom. The Singularity Institute has rented the commercial space and the top floor apartment; they haven't rented the middle floor, so avoid disturbing the neighbors there. The building is a really great location, recently built, and with a plethora of chairs and tables. What's more, you can actually move them! The day will be Saturday the 11th, and as usual the time will be 7 PM. If you need any help finding the place, feel free to give me a call at 952.217.0505.</p>\n<p>Also, consider requesting an invitation to the bayarealesswrong google group if you're in the area and not already on it, as there are further events and postings there.</p>\n<p>This is my first time posting a meetup, so I may fail in my attempt to check back for any replies. If you want to be absolutely sure something reaches me, my gmail is frank dot c dot adamek.</p>\n<p>See you there!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5wsdTcYekdvLHggaj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 7.253513630308089e-07, "legacy": true, "legacyId": "7917", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-09T03:59:28.731Z", "modifiedAt": null, "url": null, "title": "Safety Culture and the Marginal Effect of a Dollar", "slug": "safety-culture-and-the-marginal-effect-of-a-dollar", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:29.779Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oJx2Qguf8EasLSet2/safety-culture-and-the-marginal-effect-of-a-dollar", "pageUrlRelative": "/posts/oJx2Qguf8EasLSet2/safety-culture-and-the-marginal-effect-of-a-dollar", "linkUrl": "https://www.lesswrong.com/posts/oJx2Qguf8EasLSet2/safety-culture-and-the-marginal-effect-of-a-dollar", "postedAtFormatted": "Thursday, June 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Safety%20Culture%20and%20the%20Marginal%20Effect%20of%20a%20Dollar&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASafety%20Culture%20and%20the%20Marginal%20Effect%20of%20a%20Dollar%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoJx2Qguf8EasLSet2%2Fsafety-culture-and-the-marginal-effect-of-a-dollar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Safety%20Culture%20and%20the%20Marginal%20Effect%20of%20a%20Dollar%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoJx2Qguf8EasLSet2%2Fsafety-culture-and-the-marginal-effect-of-a-dollar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoJx2Qguf8EasLSet2%2Fsafety-culture-and-the-marginal-effect-of-a-dollar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 700, "htmlBody": "<p>We spent an evening at last week's Rationality Minicamp brainstorming strategies for reducing existential risk from Unfriendly AI, and for estimating their marginal benefit-per-dollar. To summarize the issue briefly, there is a lot of research into artificial general intelligence (AGI) going on, but very few AI researchers take safety seriously; if someone succeeds in making an AGI, but they don't take safety seriously or they aren't careful enough, then it might become very powerful very quickly and be a threat to humanity. The best way to prevent this from happening is to promote a safety culture - that is, to convince as many artificial intelligence researchers as possible to think about safety so that if they make a breakthrough, they won't do something stupid.</p>\n<p>We came up with a concrete (albeit greatly oversimplified) model which suggests that the marginal reduction in existential risk per dollar, when pursuing this strategy, is extremely high. The model is this: assume that if an AI is created, it's because one researcher, chosen at random from the pool of all researchers, has the key insight; and humanity survives if and only if that researcher is careful and takes safety seriously. In this model, the goal is to convince as many researchers as possible to take safety seriously. So the question is: how many researchers can we convince, per dollar? Some people are very easy to convince - some blog posts are enough. Those people are convinced already. Some people are very hard to convince - they won't take safety seriously unless someone who really cares about it will be their friend for years. In between, there are a lot of people who are currently unconvinced, but would be convinced if there were lots of good research papers about safety in machine learning and computer science journals, by lots of different authors.</p>\n<p>Right now, those articles don't exist; we need to write them. And it turns out that neither the Singularity Institute nor any other organization has the resources - staff, expertise, and money to hire grad students - to produce very much research or to substantially alter the research culture. We are <em>very far</em> from the realm of diminishing returns. Let's make this model quantitative.</p>\n<p>Let A be the probability that an AI will be created; let R the fraction of researchers that would be convinced to take safety seriously if there were a 100 good papers in about it in the right journals; and let C be the cost of one really good research paper. Then the marginal reduction in existential risk per dollar is A*R/100*C. The total cost of a grad student-year (including recruiting, management and other expenses) is about $100k. Estimate a 10% current AI risk, and estimate that 30% of researchers currently don't take safety seriously but would be convinced. That gives is a marginal existential risk reduction per dollar of 0.1*0.3/100*100k = 3*10^-9. Counting only the ~7 billion people alive today, and not any of the people who will be born in the future, this comes to a little over <em>two expected lives saved per dollar</em>.</p>\n<p>That's huge. Enormous. So enormous that I'm instantly suspicious of the model, actually, so let's take note of some of the things it leaves out. First, the \"one researcher at random determines the fate of humanity\" part glosses over the fact that research is done in groups; but it's not clear whether adding in this detail should make us adjust the estimate up or down. It ignores all the time we have between now and the creation of the first AI, during which a safety culture might arise without intervention; but it's also easier to influence the culture now, while the field is still young, rather than later. In order for promoting AI research safety to not be an extraordinarily good deal for philanthropists, there would have to be at least an additional 10^3 penalty somewhere, and I can't find one.</p>\n<p>As a result of this calculation, I will be thinking and writing about AI safety, attempting to convince others of its importance, and, in the moderately probable event that I become very rich, donating money to the SIAI so that they can pay others to do the same.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oJx2Qguf8EasLSet2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 31, "extendedScore": null, "score": 7.25398712799298e-07, "legacy": true, "legacyId": "7888", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 110, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-09T04:56:03.761Z", "modifiedAt": null, "url": null, "title": "DC Meetup June 12th (New Location)", "slug": "dc-meetup-june-12th-new-location", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.052Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GubZu2XJJ3p6fxhRS/dc-meetup-june-12th-new-location", "pageUrlRelative": "/posts/GubZu2XJJ3p6fxhRS/dc-meetup-june-12th-new-location", "linkUrl": "https://www.lesswrong.com/posts/GubZu2XJJ3p6fxhRS/dc-meetup-june-12th-new-location", "postedAtFormatted": "Thursday, June 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20DC%20Meetup%20June%2012th%20(New%20Location)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADC%20Meetup%20June%2012th%20(New%20Location)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGubZu2XJJ3p6fxhRS%2Fdc-meetup-june-12th-new-location%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=DC%20Meetup%20June%2012th%20(New%20Location)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGubZu2XJJ3p6fxhRS%2Fdc-meetup-june-12th-new-location", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGubZu2XJJ3p6fxhRS%2Fdc-meetup-june-12th-new-location", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<div style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: #ffffff; padding: 0.5em; margin: 8px;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><em style=\"font-style: italic;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><span style=\"font-style: italic;\">Sunday June 12th</span><br /><span style=\"font-style: italic;\">Core hours: 1 PM - 3PM</span><br /><span style=\"font-style: italic;\">Apartment #1005<br /><span style=\"font-style: italic;\">3001 Veazy Terrace</span><br /><span style=\"font-style: italic;\">Washington, DC 20008</span></span></span></em></p>\n<p><em style=\"font-style: italic;\"></em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><strong style=\"font-weight: bold;\">Notes:</strong><br />Core hours are 1-3, but many people are willing to stay around longer than that.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Our topic for the meeting will be \"Where/why rationalists win, and where they don't\". However, if we seem to exhaust this topic, we'll move on as appropriate.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Benquo will also lead a few rationality exercises from the minicamp</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><em style=\"font-style: italic;\"><span style=\"font-style: normal;\"><strong style=\"font-weight: bold;\">Directions:</strong><br />Benquo's apartment is near the Van Ness - UDC Metro station, just follow <a href=\"http://maps.google.com/maps?f=d&amp;source=s_d&amp;saddr=van+ness+udc+metro+station&amp;daddr=3001+Veazey+Terrace+NW,+Washington+D.C.,+DC+20008&amp;geocode=FQc_UgId3Blo-ym9QXTHzsm3iTHjBsWGECnKCA%3BFVo_UgId2iFo-ylhEdwczMm3iTEuYh62xJzn_Q&amp;hl=en&amp;mra=ltm&amp;dirflg=w&amp;sll=38.944425,-77.062685&amp;sspn=0.002257,0.003449&amp;ie=UTF8&amp;t=h&amp;z=18\">these</a> directions.<br /><br />See us also at lesswrong-dc@googlegroups.com</span></em></p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GubZu2XJJ3p6fxhRS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "7925", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-09T13:40:47.148Z", "modifiedAt": null, "url": null, "title": "London Meetup 05-Jun-2011 - very rough minutes", "slug": "london-meetup-05-jun-2011-very-rough-minutes", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:21.481Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QSW9DvCyMmzxs5WqD/london-meetup-05-jun-2011-very-rough-minutes", "pageUrlRelative": "/posts/QSW9DvCyMmzxs5WqD/london-meetup-05-jun-2011-very-rough-minutes", "linkUrl": "https://www.lesswrong.com/posts/QSW9DvCyMmzxs5WqD/london-meetup-05-jun-2011-very-rough-minutes", "postedAtFormatted": "Thursday, June 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20London%20Meetup%2005-Jun-2011%20-%20very%20rough%20minutes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALondon%20Meetup%2005-Jun-2011%20-%20very%20rough%20minutes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQSW9DvCyMmzxs5WqD%2Flondon-meetup-05-jun-2011-very-rough-minutes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=London%20Meetup%2005-Jun-2011%20-%20very%20rough%20minutes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQSW9DvCyMmzxs5WqD%2Flondon-meetup-05-jun-2011-very-rough-minutes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQSW9DvCyMmzxs5WqD%2Flondon-meetup-05-jun-2011-very-rough-minutes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1031, "htmlBody": "<p><em>This was posted to the <a href=\"http://groups.google.com/group/lesswronglondon\">London LessWrong mailing list</a>, but I am crossposting here, as per David Gerard's suggestion, in case anyone else finds this interesting.</em></p>\n<div>These notes are from my perspective, so things will be missing (as some are added).</div>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \">So here's my notes:</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><strong>Bitcoin</strong>&nbsp;- Mostly how it's quite interesting, but annoying that we can't transfer money in from the UK. Myself and ciphergoth were the interested parties. If anyone has any ideas, let us know.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Euthyphro Dilemma and Moral Realism&nbsp;</strong>- The first religion-themed conversation, mostly on the sorts of answers that come up to the dilemma and what&nbsp;constitutes&nbsp;moral realism anyway.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Evolutionarily Stable Strategies</strong>&nbsp;- The discussion of moral realism naturally led to what the nature of morality is and how evolution gave rise to it.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Learning Decision Theory &amp; Project Euler</strong>&nbsp;- Not sure how we got here, but I mentioned my desire that the people working on decision theory would make a Project Euler-type introduction to the material, so the rest of us can eventually join the conversation. I should probably write this up as a separate discussion post.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Rationality as Landgrab, and Definitions of Rationality</strong>&nbsp;- Apparently some high-ranking figures in the general futurist cluster dislike LessWrong for 'appropriating the term rationality'. There may or may not be a point there, but we started discussing how the term can be defined, preferably in a LW-independent manner.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Libertarianism &amp; LessWrong&nbsp;</strong>- There seems to be a high concentration of libertarians on LW, and it seems that the ban on talking politics has kept this from being discussed much. Which brings us to...</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Talking Politics on LessWrong</strong>&nbsp;- There seems to be this norm against talking politics, which was inherited by other online communities. However, LessWrong is very much not like other communities. We can discuss religion and philosophy without flamewars breaking out, so why not try politics too? People on LW have been known to change their minds, so there is a good chance we will generate more light than heat.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Describing LW &amp; Changing our minds</strong>&nbsp;- Leonhart described the site as 'an Internet forum where people occasionally apologise and change their minds'. Everyone else felt this was a great formulation that should be noted down. Discussion on what we have changed our minds on on LW followed</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Historicity of Jesus</strong>&nbsp;- Back on the religious track, we discussed how atheists are often former Christians who looked into the Historicity of Jesus. Cases in point - taryneast's relatives and Lukeprog.</span><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong></strong></span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Making pepole admit cached thoughts</strong>&nbsp;- More or less what it says on the tin. What it is and if anyone's done it (not really).</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Is the term 'Dark Arts' meaningful?</strong>&nbsp;- Perhaps one of the few discussions where there was active debate. A couple of good definitions for 'dark arts' came up, including 'techniques that if the other person knew you were applying them, they would be pissed off'. My personal definition was 'convincing techniques independent of the payload'. Which is to say, tricks anyone can use to convince the untrained about almost anything.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Methods of Rationality meetup</strong>&nbsp;- By this point we'd moved on to the next pub. The discussion was whether to do a MoR meetup (yes) and how we would go about setting it up (coordinating with Eliezer to have a date set before he posts the next chapter). What remains is actually doing any of this.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Plausibility vs. Possibility</strong>&nbsp;- David Gerard's idea. The ideas that seem plausible should raise a red flag since that may be due to the conjunction fallacy, reducing the possibility of them actually being true.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Biweekly Meetup Dates</strong>&nbsp;- It has been decided by the council of elders (aka, those who bothered to turn up) that the biweekly meetups will be on the 1st and 3rd Sunday of each month, with every 4th one being a 'big' bimonthly meetup.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Psychology &amp; Science&nbsp;</strong>- Is psychology a proper science? (some of it yes, some of it no).</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Race &amp; Intelligence</strong>&nbsp;- Another debated topic. On the one hand, it's unlikely that intelligence would remain stable while so many other attributes vary among races. On the other David Gerard mentioned recent research raises questions about the studies that showed such differences. On the third hand, anyone seriously researching the topic without a view to disproving it will have their career destroyed, so, yeah...</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Prevalence of Basic Knowledge</strong>&nbsp;- An anecdote by me about some fairly educated acquaintances that had basic misconceptions about evolution (oddly, not with religious motive, I think), and a warning not to consider the general public's education levels too high due to the Typical Mind Fallacy.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Comedy as Anti-Compartmentalization</strong>&nbsp;- Another pet theory of mine. I was puzzled by the amount of atheist comedians out there, who people pay to see tell them that their religion is absurd. (Yes,&nbsp;Christian&nbsp;comedians exist too. Search&nbsp;YouTube. I dare you.) So my theory is that humour serves as a space where patterns and data from different fields are allowed to be superimposed on one another. Think of it as an anti-compartmentalization&nbsp;habit. Due to our brain design, compartmentalization is the default, so humour may be a hack to counter that. And we reward those who do it well with high status because it's valuable. Maybe we should have transhumanist/rationalist stand-up comedians? We sure have a lot of inconsistencies to point out.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Spread of Atheism</strong>&nbsp;- The above developed into this. Has atheism saturated it's audience, and will it stabilise? No clear outcome, I guess we'll have to wait and see. I certainly hope not.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Wikipedia's Epistemology</strong>&nbsp;- How&nbsp;Wikipedia&nbsp;determines truth. I'll let David Gerard tell us what that was about</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>The Larrikin-Wowser Dynamic&nbsp;</strong>- Kristoff mentioned this theory on how societies work through this fundamental tension. He can probably say more on this than I can.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>The Myers-Kurzweil argument</strong>&nbsp;- It turns out, the winner differs by how you frame the claims made. As far as I am concerned, of these two, whoever wins, we lose.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>The Black Box experiment</strong>&nbsp;- The discussion turned to raising children, and I mentioned this experiment on how the children of other primates seem to do some things better than human children do, and what that tells us about our learning process. YouTube vid:&nbsp;<a style=\"color: #0000cc; \" href=\"http://www.youtube.com/watch?v=pIAoJsS9Ix8\" target=\"_blank\">http://www.youtube.com/watch?v=pIAoJsS9Ix8</a></span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><a style=\"color: #0000cc; \" href=\"http://www.youtube.com/watch?v=pIAoJsS9Ix8\" target=\"_blank\"></a></span><span style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong>Neuro-Linguistic Programming: Does it do anything?</strong>&nbsp;- DG says no, but it works by the power of telling people what to do.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \">End of notes.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; \">That was a lot of text, if you made it down to here, you have my sincere congratulations.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QSW9DvCyMmzxs5WqD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 7.255717869491467e-07, "legacy": true, "legacyId": "7935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-09T13:52:04.966Z", "modifiedAt": null, "url": null, "title": "The Case of the Speluncean Explorers", "slug": "the-case-of-the-speluncean-explorers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:31.892Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SEMYr8Nm6pN2Hvufk/the-case-of-the-speluncean-explorers", "pageUrlRelative": "/posts/SEMYr8Nm6pN2Hvufk/the-case-of-the-speluncean-explorers", "linkUrl": "https://www.lesswrong.com/posts/SEMYr8Nm6pN2Hvufk/the-case-of-the-speluncean-explorers", "postedAtFormatted": "Thursday, June 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Case%20of%20the%20Speluncean%20Explorers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Case%20of%20the%20Speluncean%20Explorers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEMYr8Nm6pN2Hvufk%2Fthe-case-of-the-speluncean-explorers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Case%20of%20the%20Speluncean%20Explorers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEMYr8Nm6pN2Hvufk%2Fthe-case-of-the-speluncean-explorers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEMYr8Nm6pN2Hvufk%2Fthe-case-of-the-speluncean-explorers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 274, "htmlBody": "<p>A fictional legal case about a team of explorers who got trapped in a cave and ate one of their own to survive.&nbsp;<a href=\"http://www.nullapoena.de/stud/explorers.html\">Summary</a>:</p>\n<blockquote>\n<p>From the testimony of the defendants, which was accepted by the jury, it appears that it was Whetmore who first proposed that they might find the nutriment without which survival was impossible in the flesh of one of their own number. It was also Whetmore who first proposed the use of some method of casting lots, calling the attention of the defendants to a pair of dice he happened to have with him. The defendants were at first reluctant to adopt so desperate a procedure, but after the conversations by wireless related above, they finally agreed on the plan proposed by Whetmore. After much discussion of the mathematical problems involved, agreement was finally reached on a method of determining the issue by the use of the dice.</p>\n<p>Before the dice were cast, however, Whetmore declared that he withdrew from the arrangement, as he had decided on reflection to wait for another week before embracing an expedient so frightful and odious. The others charged him with a breach of faith and proceeded to cast the dice. When it came Whetmore's turn, the dice were cast for him by one of the defendants, and he was asked to declare any objections he might have to the fairness of the throw. He stated that he had no such objections. The throw went against him, and he was then put to death and eaten by his companions.</p>\n</blockquote>\n<p>Was it right for them to kill and eat someone who wanted to opt out of the arrangement?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SEMYr8Nm6pN2Hvufk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 7.255751511451318e-07, "legacy": true, "legacyId": "7936", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-09T13:55:47.258Z", "modifiedAt": null, "url": null, "title": "Checking for Threshold Number of Rationalists in S\u00e3o Paulo", "slug": "checking-for-threshold-number-of-rationalists-in-sao-paulo", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:21.190Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KYTzciCNvNfTRHnbn/checking-for-threshold-number-of-rationalists-in-sao-paulo", "pageUrlRelative": "/posts/KYTzciCNvNfTRHnbn/checking-for-threshold-number-of-rationalists-in-sao-paulo", "linkUrl": "https://www.lesswrong.com/posts/KYTzciCNvNfTRHnbn/checking-for-threshold-number-of-rationalists-in-sao-paulo", "postedAtFormatted": "Thursday, June 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Checking%20for%20Threshold%20Number%20of%20Rationalists%20in%20S%C3%A3o%20Paulo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChecking%20for%20Threshold%20Number%20of%20Rationalists%20in%20S%C3%A3o%20Paulo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYTzciCNvNfTRHnbn%2Fchecking-for-threshold-number-of-rationalists-in-sao-paulo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Checking%20for%20Threshold%20Number%20of%20Rationalists%20in%20S%C3%A3o%20Paulo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYTzciCNvNfTRHnbn%2Fchecking-for-threshold-number-of-rationalists-in-sao-paulo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYTzciCNvNfTRHnbn%2Fchecking-for-threshold-number-of-rationalists-in-sao-paulo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<p>Hello, this post is only to check if there are enough people in S&atilde;o Paulo to make it worth it to arrange a meetup for S&atilde;o Paulo Less Wrongers.</p>\n<p>Please reply saying \"Paulista\" if you are living in S&atilde;o Paulo area and would like to join a meeting (not necessarily monthly, just a meeting)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KYTzciCNvNfTRHnbn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 7.255762544404196e-07, "legacy": true, "legacyId": "7937", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-09T15:31:07.596Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Third Alternatives for Afterlife-ism", "slug": "seq-rerun-third-alternatives-for-afterlife-ism", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ke3wymDcSHCG9CzmA/seq-rerun-third-alternatives-for-afterlife-ism", "pageUrlRelative": "/posts/Ke3wymDcSHCG9CzmA/seq-rerun-third-alternatives-for-afterlife-ism", "linkUrl": "https://www.lesswrong.com/posts/Ke3wymDcSHCG9CzmA/seq-rerun-third-alternatives-for-afterlife-ism", "postedAtFormatted": "Thursday, June 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Third%20Alternatives%20for%20Afterlife-ism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Third%20Alternatives%20for%20Afterlife-ism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKe3wymDcSHCG9CzmA%2Fseq-rerun-third-alternatives-for-afterlife-ism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Third%20Alternatives%20for%20Afterlife-ism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKe3wymDcSHCG9CzmA%2Fseq-rerun-third-alternatives-for-afterlife-ism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKe3wymDcSHCG9CzmA%2Fseq-rerun-third-alternatives-for-afterlife-ism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p>Today's post, <a href=\"/lw/hv/third_alternatives_for_afterlifeism/\">Third Alternatives for Afterlife-ism</a> was originally published on May 8, 2007.  A summary (from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>One source of hope against death is Afterlife-ism. Some say that this justifies it as a Noble Lie. But there are better (because more plausible) Third Alternatives, including nanotech, actuarial escape velocity, cryonics, and the Singularity. If supplying hope were the real goal of the Noble Lie, advocates would prefer these alternatives. But the real goal is to excuse a fixed belief from criticism, not to supply hope.</blockquote>\n<p><br />Discuss the post here (rather than in the comments of the original post).<br /><br /><em>This post is part of a series rerunning Eliezer Yudkowsky's old posts so those interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/63p/seq_rerun_the_third_alternative/\">The Third Alternative</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it, posting the next day's sequence reruns post, summarizing forthcoming articles on the wiki, or <a href=\"/r/discussion/lw/53f/sequence_posts_exercises/\">creating exercises</a>. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to discuss the Sequence Reruns.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ke3wymDcSHCG9CzmA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 7.256046472858479e-07, "legacy": true, "legacyId": "7938", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ujz9PXXz7A4eKjBjp", "xdcGyRqcM6JGrywP2", "SoadQym38wGBDJ7AH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-09T17:46:12.624Z", "modifiedAt": null, "url": null, "title": "A Defense of Naive Metaethics", "slug": "a-defense-of-naive-metaethics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:50.244Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Sawin", "createdAt": "2010-06-20T00:58:25.645Z", "isAdmin": false, "displayName": "Will_Sawin"}, "userId": "EWLubmrSBK6cnmDwd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QuxWskbsDvNTozE9a/a-defense-of-naive-metaethics", "pageUrlRelative": "/posts/QuxWskbsDvNTozE9a/a-defense-of-naive-metaethics", "linkUrl": "https://www.lesswrong.com/posts/QuxWskbsDvNTozE9a/a-defense-of-naive-metaethics", "postedAtFormatted": "Thursday, June 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Defense%20of%20Naive%20Metaethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Defense%20of%20Naive%20Metaethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQuxWskbsDvNTozE9a%2Fa-defense-of-naive-metaethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Defense%20of%20Naive%20Metaethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQuxWskbsDvNTozE9a%2Fa-defense-of-naive-metaethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQuxWskbsDvNTozE9a%2Fa-defense-of-naive-metaethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1053, "htmlBody": "<p><!--StartFragment--></p>\n<p>I aim to make several arguments in the post that we can make statements about what should be done and what should not be done that cannot be reduced, by definition, to statements about the physical world.</p>\n<p><strong>A Naive Argument</strong></p>\n<p>Lukeprog says this in <a href=\"/lw/5u2/pluralistic_moral_reductionism/\"><span style=\"color: #000000;\"><span style=\"color: #000000;\">one of his posts</span></span></a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; \">If someone makes a claim of the 'ought' type, either they are talking about the world of&nbsp;<em style=\"font-style: italic; \">is</em>, or they are talking about the world of&nbsp;<em style=\"font-style: italic; \">is not</em>. If they are talking about the world of&nbsp;<em style=\"font-style: italic; \">is not</em>, then I quickly lose interest because the world of&nbsp;<em style=\"font-style: italic; \">is not</em>&nbsp;isn't my subject of interest.</span></p>\n</blockquote>\n<p>I would like to question that statement. I would guess that lukeprog's chief subject of interest is figuring out what to do with the options presented to him. His interest is, therefore, in figuring out what he ought to do.</p>\n<p><span style=\"font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">&nbsp;</span>Consider the reasoning process that takes him from observations about the world to actions. He sees something, and then thinks, and then thinks some more, and then decides. Moreover, he can, if he chooses, express every step of this reasoning process in words. Does he really lose interest at the last step?</p>\n<p>My goal here is to get people to feel the intuition that \"I ought to do X\" means something, and that thing is not \"I think I ought to do X\" or \"I would think that I ought to do X if I were smarter and some other stuff\".</p>\n<p>(If you don't, I'm not sure what to do.)</p>\n<p>People who do feel that intuition run into trouble. This is because \"I ought to do X' does not refer to anything that exists. How can you make a statement that doesn't refer to anything that exists?</p>\n<p>&nbsp;I've done it, and my reasoning process is still intact, and nothing has blown up. Everything seems to be fine. No one has explained to me what isn't fine about this.</p>\n<p>Since it's intuitive, why would you not want to do it that way?</p>\n<p>(You can argue that certain words, for certain people, do not refer to what one ought to do. But it's a different matter to suggest that no word refers to what one ought to do beyond facts about what is.)</p>\n<p><strong>A Flatland Argument</strong></p>\n<p>\"I'm not interested in words, I'm interested in things. Words are just sequences of sounds or images. There's no way a sequence of arbitrary symbols could imply another sequence, or inform a decision.\"</p>\n<p>\"I understand how logical definitions work. I can see how, from a small set of axioms, you can derive a large number of interesting facts. But I'm not interested in words without definitions. What does \"That thing, over there?\" mean? Taboo finger-pointing.\"<span style=\"font-family: Arial; font-size: 17px;\">&nbsp;</span></p>\n<p>\"You can make statements about observations, that much is obvious. You can even talk about patterns in observations, like \"the sun rises in the morning\". But I don't understand your claim that there's no chocolate cake at the center of the sun. Is it about something you can see? If not, I'm not interested.\"</p>\n<p>\"Claims about the past make perfect sense, but I don't understand what you mean when you say something is going to happen. Sure, I see that chair, and I remember seeing the chair in the past, but what do you mean that the chair will still be there tomorrow? Taboo \"will\".\"</p>\n<p>Not every set of claims is reducible to every other set of claims. There is nothing special about the set \"claims about the state of the world, including one's place in it and ability to affect it.\" If you add, however, ought-claims, then you will get a very special set - the set of all information you need to make correct decisions.</p>\n<p>I can't see a reason to make claims that aren't reducible, by definition, to that.</p>\n<p><strong>The Bootstrapping Trick</strong></p>\n<p>Suppose an AI wants to find out what Bob means when he says \"water'. AI could ask him if various items were and were not water. But Bob might get temporarily confused in any number of ways - he could mix up his words, he could hallucinate, or anything else. So the AI decides instead to wait. The AI will give Bob time, and everything else he needs, to make the decision. In this way, by giving Bob all the abilities he needs to replicate his abstract concept of a process that decides if something is or is not \"water\", the AI can duplicate this process.</p>\n<p>The following statement is true:</p>\n<blockquote>\n<p>A substance is water (in Bob's language) if and only if Bob, given all the time, intelligence, and other resources he wants, decides that it is water.<span style=\"font-family: Arial; font-size: 17px;\">&nbsp;</span></p>\n</blockquote>\n<p>But this is certainly not the definition of water! Imagine if Bob used this criterion to evaluate what was and was not water. He would suffer from an infinite regress. The definition of water is something else. The statement \"This is water\" reduces to a set of facts about this, not a set of facts about this and Bob's head.<span style=\"font-family: Arial; font-size: 17px;\">&nbsp;</span></p>\n<p>The extension to morality should be obvious.</p>\n<p>What one is forced to do by this argument, if one wants to speak only in physical statements, is to say that \"should\" has a really, really long definition that incorporates <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\"><span style=\"color: #000000;\"><span style=\"color: #000000;\">all components of human value</span></span></a>. When a simple word has a really, really long definition, we should worry that something is up.</p>\n<p>Well, why does it have a long definition? It has a long definition because that's what we believe is important. To say that people who use (in this sense) \"should\" to mean different things just disagree about definitions is to paper over and cover up the fact that they disagree about what's important.</p>\n<p><strong>What do I care about?</strong></p>\n<p>In this essay I talk about what I believe about rather than what I care about. What I care about seems like an entirely emotional question to me. I cannot <a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\"><span style=\"color: #000000;\"><span style=\"color: #000000;\"><span style=\"color: #000000;\">Shut Up And Multiply</span></span></span></a> about what I care about. If I do, in fact, Shut Up and Multiply, then it is because I believe that doing so is right. Suppose I believe that my future emotions will follow multiplication. I would have to, then, believe that I am going to self-modify into someone who multiplies. I would only do this because of a belief that doing so is right.<span style=\"font-family: Arial; font-size: 17px;\">&nbsp;</span></p>\n<p>Belief and logical reasoning are an important part of how people on lesswrong think about morality, and I don't see how to incorporate them into a metaethics based not on beliefs, but on caring.</p>\n<!--EndFragment-->\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QuxWskbsDvNTozE9a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 13, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "7939", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--StartFragment--></p>\n<p>I aim to make several arguments in the post that we can make statements about what should be done and what should not be done that cannot be reduced, by definition, to statements about the physical world.</p>\n<p><strong id=\"A_Naive_Argument\">A Naive Argument</strong></p>\n<p>Lukeprog says this in <a href=\"/lw/5u2/pluralistic_moral_reductionism/\"><span style=\"color: #000000;\"><span style=\"color: #000000;\">one of his posts</span></span></a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; \">If someone makes a claim of the 'ought' type, either they are talking about the world of&nbsp;<em style=\"font-style: italic; \">is</em>, or they are talking about the world of&nbsp;<em style=\"font-style: italic; \">is not</em>. If they are talking about the world of&nbsp;<em style=\"font-style: italic; \">is not</em>, then I quickly lose interest because the world of&nbsp;<em style=\"font-style: italic; \">is not</em>&nbsp;isn't my subject of interest.</span></p>\n</blockquote>\n<p>I would like to question that statement. I would guess that lukeprog's chief subject of interest is figuring out what to do with the options presented to him. His interest is, therefore, in figuring out what he ought to do.</p>\n<p><span style=\"font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">&nbsp;</span>Consider the reasoning process that takes him from observations about the world to actions. He sees something, and then thinks, and then thinks some more, and then decides. Moreover, he can, if he chooses, express every step of this reasoning process in words. Does he really lose interest at the last step?</p>\n<p>My goal here is to get people to feel the intuition that \"I ought to do X\" means something, and that thing is not \"I think I ought to do X\" or \"I would think that I ought to do X if I were smarter and some other stuff\".</p>\n<p>(If you don't, I'm not sure what to do.)</p>\n<p>People who do feel that intuition run into trouble. This is because \"I ought to do X' does not refer to anything that exists. How can you make a statement that doesn't refer to anything that exists?</p>\n<p>&nbsp;I've done it, and my reasoning process is still intact, and nothing has blown up. Everything seems to be fine. No one has explained to me what isn't fine about this.</p>\n<p>Since it's intuitive, why would you not want to do it that way?</p>\n<p>(You can argue that certain words, for certain people, do not refer to what one ought to do. But it's a different matter to suggest that no word refers to what one ought to do beyond facts about what is.)</p>\n<p><strong id=\"A_Flatland_Argument\">A Flatland Argument</strong></p>\n<p>\"I'm not interested in words, I'm interested in things. Words are just sequences of sounds or images. There's no way a sequence of arbitrary symbols could imply another sequence, or inform a decision.\"</p>\n<p>\"I understand how logical definitions work. I can see how, from a small set of axioms, you can derive a large number of interesting facts. But I'm not interested in words without definitions. What does \"That thing, over there?\" mean? Taboo finger-pointing.\"<span style=\"font-family: Arial; font-size: 17px;\">&nbsp;</span></p>\n<p>\"You can make statements about observations, that much is obvious. You can even talk about patterns in observations, like \"the sun rises in the morning\". But I don't understand your claim that there's no chocolate cake at the center of the sun. Is it about something you can see? If not, I'm not interested.\"</p>\n<p>\"Claims about the past make perfect sense, but I don't understand what you mean when you say something is going to happen. Sure, I see that chair, and I remember seeing the chair in the past, but what do you mean that the chair will still be there tomorrow? Taboo \"will\".\"</p>\n<p>Not every set of claims is reducible to every other set of claims. There is nothing special about the set \"claims about the state of the world, including one's place in it and ability to affect it.\" If you add, however, ought-claims, then you will get a very special set - the set of all information you need to make correct decisions.</p>\n<p>I can't see a reason to make claims that aren't reducible, by definition, to that.</p>\n<p><strong id=\"The_Bootstrapping_Trick\">The Bootstrapping Trick</strong></p>\n<p>Suppose an AI wants to find out what Bob means when he says \"water'. AI could ask him if various items were and were not water. But Bob might get temporarily confused in any number of ways - he could mix up his words, he could hallucinate, or anything else. So the AI decides instead to wait. The AI will give Bob time, and everything else he needs, to make the decision. In this way, by giving Bob all the abilities he needs to replicate his abstract concept of a process that decides if something is or is not \"water\", the AI can duplicate this process.</p>\n<p>The following statement is true:</p>\n<blockquote>\n<p>A substance is water (in Bob's language) if and only if Bob, given all the time, intelligence, and other resources he wants, decides that it is water.<span style=\"font-family: Arial; font-size: 17px;\">&nbsp;</span></p>\n</blockquote>\n<p>But this is certainly not the definition of water! Imagine if Bob used this criterion to evaluate what was and was not water. He would suffer from an infinite regress. The definition of water is something else. The statement \"This is water\" reduces to a set of facts about this, not a set of facts about this and Bob's head.<span style=\"font-family: Arial; font-size: 17px;\">&nbsp;</span></p>\n<p>The extension to morality should be obvious.</p>\n<p>What one is forced to do by this argument, if one wants to speak only in physical statements, is to say that \"should\" has a really, really long definition that incorporates <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\"><span style=\"color: #000000;\"><span style=\"color: #000000;\">all components of human value</span></span></a>. When a simple word has a really, really long definition, we should worry that something is up.</p>\n<p>Well, why does it have a long definition? It has a long definition because that's what we believe is important. To say that people who use (in this sense) \"should\" to mean different things just disagree about definitions is to paper over and cover up the fact that they disagree about what's important.</p>\n<p><strong id=\"What_do_I_care_about_\">What do I care about?</strong></p>\n<p>In this essay I talk about what I believe about rather than what I care about. What I care about seems like an entirely emotional question to me. I cannot <a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\"><span style=\"color: #000000;\"><span style=\"color: #000000;\"><span style=\"color: #000000;\">Shut Up And Multiply</span></span></span></a> about what I care about. If I do, in fact, Shut Up and Multiply, then it is because I believe that doing so is right. Suppose I believe that my future emotions will follow multiplication. I would have to, then, believe that I am going to self-modify into someone who multiplies. I would only do this because of a belief that doing so is right.<span style=\"font-family: Arial; font-size: 17px;\">&nbsp;</span></p>\n<p>Belief and logical reasoning are an important part of how people on lesswrong think about morality, and I don't see how to incorporate them into a metaethics based not on beliefs, but on caring.</p>\n<!--EndFragment-->\n<p>&nbsp;</p>", "sections": [{"title": "A Naive Argument", "anchor": "A_Naive_Argument", "level": 1}, {"title": "A Flatland Argument", "anchor": "A_Flatland_Argument", "level": 1}, {"title": "The Bootstrapping Trick", "anchor": "The_Bootstrapping_Trick", "level": 1}, {"title": "What do I care about?", "anchor": "What_do_I_care_about_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "295 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 295, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3zDX3f3QTepNeZHGc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-10T00:07:04.447Z", "modifiedAt": null, "url": null, "title": "Memory, Spaced Repetition and Life", "slug": "memory-spaced-repetition-and-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.443Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Duke", "createdAt": "2010-07-08T00:48:30.542Z", "isAdmin": false, "displayName": "Duke"}, "userId": "67L9CtYdpqT79exBW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3r4GETDPMf335HfpA/memory-spaced-repetition-and-life", "pageUrlRelative": "/posts/3r4GETDPMf335HfpA/memory-spaced-repetition-and-life", "linkUrl": "https://www.lesswrong.com/posts/3r4GETDPMf335HfpA/memory-spaced-repetition-and-life", "postedAtFormatted": "Friday, June 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Memory%2C%20Spaced%20Repetition%20and%20Life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMemory%2C%20Spaced%20Repetition%20and%20Life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3r4GETDPMf335HfpA%2Fmemory-spaced-repetition-and-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Memory%2C%20Spaced%20Repetition%20and%20Life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3r4GETDPMf335HfpA%2Fmemory-spaced-repetition-and-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3r4GETDPMf335HfpA%2Fmemory-spaced-repetition-and-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1881, "htmlBody": "<p>I have made the case that with the advent of the internet went the need to memorize anything. Why worry about memorizing when I'll never be tested for a grade and can access knowledge nearly instantaneously? As well, I reasoned, I have probably already memorized everything I need to. I focused my time instead on learning thinking techniques, such as Bayesian calculations, expected value calculations and various things for improving emotional control.</p>\n<p>But after reading <a href=\"http://starrynightcoaching.wordpress.com/2010/09/21/how-to-memorize-wisdom-of-books/\">this</a> a couple months back I decided to experiment with <a href=\"http://ankisrs.net/\">Anki</a>, a digital flashcard program which exploits a cognitive phenomenon called the <a href=\"http://en.wikipedia.org/wiki/Spacing_effect\">Spacing Effect</a> by implementing a memorization technique called <a href=\"http://en.wikipedia.org/wiki/Spaced_repetition\">Spaced Repetition</a>. The Spacing Effect is the widely observed tendency for people to recall information better when studied a few times over a long period than when studied many times over a short period. Balota et al (2007):<a id=\"more\"></a></p>\n<blockquote>\n<p>Spacing effects occur across domains (e.g., learning perceptual motor tasks vs. learning lists of words), across species (e.g., rats, pigeons, and humans), across age groups and individuals with different memory impairments, and across retention intervals of seconds to months.</p>\n</blockquote>\n<p><a href=\"/user/gwern/\">Gwern</a> analogizes the <a href=\"http://www.gwern.net/Mnemosyne\">spacing effect with radioactive decay</a>:</p>\n<blockquote>\n<p>You can think of the &lsquo;forgetting curve&rsquo; as being like a chart of radioactive half-lives: each review bumps your memory up in strength 50% of the chart, say, but review doesn&rsquo;t do very much in the early days because the memory simply hasn&rsquo;t decayed very much! (<a href=\"http://www.gwern.net/images/spaced-repetition-forgetting-curves.png\">Chart</a>)</p>\n</blockquote>\n<p>One consequence of the spacing effect is that cramming is useful for recalling things shortly after memorizing them; however, if those crammed memories are not eventually refreshed then they are likely to decay to nothing. From this observation came Spaced Repetition: a memorization technique using flashcards (usually) shown at increasing intervals of time to optimize the relationship between number of reviews and strength of memory. The PC explosion was a boon to Spaced Repetition since storing and showing flashcards as well as physically calculating their frequencies were delegated to the computer. The program <a href=\"http://ankisrs.net/\">Anki</a>, for instance, permits the user to generate flashcard decks, specify study session length and frequency, specify how many new cards are introduced per session and specify the frequency of the cards based on the user's input. Hard material is shown more often than easy material, with the ease or difficulty being determined directly by the user selecting buttons marked &ldquo;again,&rdquo; &ldquo;easy,&rdquo; &ldquo;good,&rdquo; and &ldquo;hard.&rdquo;<br /><br />That sounds nifty, but how well does it work? As for myself, using the Anki default settings, I was able to thoroughly memorize a deck of 80 cognitive biases and related terms (160 cards total, name to definition and vice avers) in about three weeks using Anki ~15 minutes/day. Since the cards are pushed back further and further for review as I progressed, I have only five cards to review today. The first one, Endowment Effect, came instantly to me so I selected the &ldquo;easy&rdquo; button. Now, as a result of the Anki algorithm, I won't see that card for 1.3 months. My low expectations for the Anki experiment were exceeded.<br /><br /><a href=\"http://www.supermemo.com/english/company/wozniak.htm\">Piotr Wozniak</a>, who designed the first <a href=\"http://help.supermemo.org/wiki/Main_Page\">SuperMemo</a> algorithm in the early 80's (of which later versions are still in use in SuperMemo as well as Anki), and devoted enormous energies to studying modern computer aided self-instruction systems promotes spaced repetition. He and two others developed <a href=\"http://www.supermemo.com/articles/stability.htm\">a two-variable framework for memorization</a> which they built upon to examine a way of optimizing interval spacing in Spaced Repetition. The first variable, memory retrieval (R), is the probability of recalling something and is approximated by an exponential decay function, while the second variable, memory stability (S), measures how long a memory lasts before it is forgotten entirely. Wozniak et al, expressed S as the inter-repetition interval time that produces R = 90% (the likelihood of recall being a 9 out of 10 chance) and concluded the following:</p>\n<blockquote>\n<p>We express the changes in retrievability as:<br /><br />(3.1) R=e-d*t<br /><br />where:</p>\n<ul>\n<li>t - time</li>\n<li>R - probability of recall at time t (retrievability)</li>\n<li>d - decay constant dependent on the stability</li>\n</ul>\n<p><br />We can replace the constant d dependent on stability, with a constant k that is independent of stability:<br /><br />(3.2) R=e-k*t/S<br /><br />where:</p>\n<ul>\n<li>t - time</li>\n<li>R - probability of recall at time t</li>\n<li>S - stability expressed by the inter-repetition interval that results in retrievability of 90% (i.e. R=0.9)</li>\n<li>k - constant independent of stability</li>\n</ul>\n</blockquote>\n<p>Drawing on analysis of large data sets cultivated from SuperMemo, Wozniak et al provide empirical evidence that memory decay matches their exponential decay approximation. The goal of Wozniak's SuperMemo algorithm is to optimize inter-repetition spacing by (theoretically) refreshing a memory the moment before it decays totally, thus spiking that memory until it decays near totally again and gets spiked again. (Although, depending on the importance of being able to recall of a piece of information, it can be used, theoretically, to spike a memory every time it decays to likelihood of recall of 90%, 80%, 70%, etc.) Interestingly, in a meta-analysis by Balota et al (2007) , the authors conclude that while spaced repetition is certainly better than massed practice (studying all at once and then not reviewing again), spaced repetition shows no advantage over static spaced repetition (holding intervals constant)! Since most studies cited in the meta-analysis used a small number (usually three) retrieval attempts, the authors suggest that future research should expand this number to better reflect the way people can practically use spaced repetition. In my estimation, when it comes to memorization, given the ease of use of these digital flashcards programs, the specific algorithm design is a secondary concern to being personally disciplined to consistently review material until you think you've internalized it fully.<br /><br />Another consideration beyond algorithm design is formulating a usable flashcard deck: simplifying the information and implementing techniques to enhance recall. Wozniak et al found evidence that it was harder to recall information the more complex it was. Hence, Wozniak recommends <a href=\"http://www.supermemo.com/articles/20rules.htm\">20 rules for formatting knowledge</a> to make flashcards more digestible during reviews. The first three rules are standard: understand before you learn, learn before you memorize and build on the basics. The remaining rules are specific to developing and maintaining flashcard decks, such as simplifying questions, using clozed deletion (a sentence missing a part replaced by three dots), including images, avoiding sets, etc. If you are planning on creating your own deck then familiarize yourself with these rules.<br /><br />Additionally, I recommend including hyperlinks, if available, in your cards to sources with thorough explanations of the topics, and to be careful doing Wikipedia-based decks. Having small previous knowledge of cognitive biases when I started, it was essential to read expanded explanations on many of the terms to understand them completely, so I actually updated the deck, which someone else created, with hyperlinks on every card. I think it greatly enhanced the usability and effectiveness of the deck. Incidentally, when I went to Wikipedia to better grasp many of the terms, I found several entries there lacking in credibility. On at least two occasions, after being skeptical of an entry on a term, I Googled the term and found every other mention of it on the internet was either sourced to Wikipedia or directly copied from there.<br /><br />This all still sounds nifty, but, I'll repeat, why worry about memorizing when I'll never be tested for a grade and can access knowledge nearly instantaneously? As for standard trivia type information (state capitals, etc), memorization is virtually a total waste of time (thank you, technology!). Instant recall of facts, except on Jeopardy or when using a foreign language, is generally not of value. Think of a time when your inability to instantly recall a fact resulted in a financial loss for you&mdash;I can't. On the other hand, every damn day I am confronted with dynamic situations where I am forced to make quick decisions that vary in effectiveness based on how well I analyze what is happening and construct counter-strategies that maximize my utility. In these moments, when the necessary facts are right in front of us, what we usually don't have is a comprehensive database of methodologies, heuristics and other decision theoretic knowledge to surf through and use for calculating useful outputs. You might be familiar with Bayesianism, Nonviolent Communication (NVC), PUA, logical fallacies and the like, but it is unlikely you have internalized the concepts to the point where even in the face of chaos or emotional turmoil (when it likely matters most) you can implement them to the best of your mental ability. Thus, I recommend using digital flashcards employing Spaced Repetition to memorize a relatively small set of widely applicable methods (and related knowledge) for use in dynamic situations that require instant or near-instant action.<br /><br /><a href=\"http://en.wikipedia.org/wiki/Nonviolent_Communication\">NVC</a> is the poster-child because it is an easy to remember step-by-step process which does not require complicated inputs for any of the steps; virtually anyone can observe a situation, dissect relevant information from it and then run it through the NVC process. While simple, NVC might be most valuable in chaotic or emotionally-charged social situation when minds are thrust into primate mode, making it that more important to ingrain thoroughly. <a href=\"/user/divia\">Divia</a>, who created <a href=\"http://meaningandmagic.com/pages/decks\">an NVC deck and several other useful Anki decks</a>, <a href=\"http://meaningandmagic.com/nvc-on-caltrain\">recounts</a> successfully using NVC on a train when a drunk sports fan near her was acting belligerent (imagine that!). In my experience, having internalized a bunch of cognitive biases, I'm feel like I am vigilant about recognizing them in my thoughts and behaviors and in those of others, without devoting much conscious effort to doing so. I expect that databasing of logical fallacies and human behavioral cues will have the same effect. Please list other methods or knowledge that you think would be worth devoting time to memorize.<br /><br />In sum, spaced repetition for memorization is superior to massed consumption without further review, although it is undetermined what inter-repetition algorithm is best. It seems that having discipline and consistency in review is more important than the inter-repetition spacing, as even static spacing works well. Also important is the design and maintenance of the flash card decks used for spaced repetition exercise, with an emphasis on simplifying the information presented. Lastly, be thoughtful about what things use spend time memorizing. Almost all information is just as useful to us wherever it currently is, especially if it is on the internet, than it would be if we had it memorized. Thus, I suggest using Anki or other spaced repetition software to memorize methods, concepts and knowledge can be deployed in dynamic situations where we are forced to make important decisions in an instant or near-instant.<br /><br /><br />REFERENCES<br /><br />Balota, D.A., Duchek, J.M., &amp; Logan, J.M. (2007). <a href=\"http://www.psych.wustl.edu/coglab/publications/Balota+et+al+roddy+chapter.pdf\">Is expanded retrieval practice a superior form of spaced retrieval?</a> A critical review of the extant literature. In J. Nairne (Ed.), The Foundations of Remembering: Essays in Honor of Henry L. Roediger III, (pp. 83-106), Psychology Press, New York.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3r4GETDPMf335HfpA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 21, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "7940", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-10T07:27:12.623Z", "modifiedAt": null, "url": null, "title": "What would defuse unfriendly AI?", "slug": "what-would-defuse-unfriendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:01.214Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "asr", "createdAt": "2011-06-02T01:42:05.591Z", "isAdmin": false, "displayName": "asr"}, "userId": "w2EyaugHx6wxwdbva", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YrYqH6jmfrqGx36vw/what-would-defuse-unfriendly-ai", "pageUrlRelative": "/posts/YrYqH6jmfrqGx36vw/what-would-defuse-unfriendly-ai", "linkUrl": "https://www.lesswrong.com/posts/YrYqH6jmfrqGx36vw/what-would-defuse-unfriendly-ai", "postedAtFormatted": "Friday, June 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20would%20defuse%20unfriendly%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20would%20defuse%20unfriendly%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYrYqH6jmfrqGx36vw%2Fwhat-would-defuse-unfriendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20would%20defuse%20unfriendly%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYrYqH6jmfrqGx36vw%2Fwhat-would-defuse-unfriendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYrYqH6jmfrqGx36vw%2Fwhat-would-defuse-unfriendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 264, "htmlBody": "<p>It seems to be a widely held belief around here that unfriendly artificial general intelligence is dangerous, and that (provably) friendly artificial general intelligence is the soundest counter to it.<br /><br />But I'd like to see some analysis of alternatives.&nbsp; Here are some possible technical developments. Would any of these defuse the threat? How much would they help?<br /></p>\n<ul>\n<li>A tight lower bound on the complexity of SAT instances. Suppose that P != NP, and that researchers develop an algorithm for solving instances of Boolean satisfiability that is optimal in terms of asymptotic complexity, but far from efficient in practice. </li>\n<li>The opposite: a practical, say, quartic-time SAT algorithm, again with a proof that the algorithm is optimal.</li>\n<li>High-quality automated theorem proving technology, that's not self-modifying except in very narrow ways.</li>\n<li>Other special-purpose 'AI', such as high-quality natural-language processing algorithms that aren't self-modifying or self-aware. For example, suppose we were able to do language-to-language translation as well as bilingual but not-very-smart humans.</li>\n<li>Robust tools for proving security properties of complex programs. \"This program can only produce output in the following format or with the following properties and cannot disable or tamper with the reference monitor or operating systems.\"</li>\n</ul>\n<p><br />Are there other advances in computer science that might show up within the next twenty years, that would make friendly-AI much less interesting?<br /><br />Would anything on this list be dangerous?&nbsp; Obviously, efficient algorithms for NP-complete problems would be very disruptive. Nearly all of modern cryptography would become irrelevant, for instance.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YrYqH6jmfrqGx36vw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 7.258894868077744e-07, "legacy": true, "legacyId": "7954", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-10T10:17:52.752Z", "modifiedAt": null, "url": null, "title": "Edinburgh LW meetup, as usual", "slug": "edinburgh-lw-meetup-as-usual", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:21.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sark", "createdAt": "2010-01-20T20:18:10.889Z", "isAdmin": false, "displayName": "sark"}, "userId": "cJYNhyCitpdzsZqeP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2yXPFGXLNHxSfGdK9/edinburgh-lw-meetup-as-usual", "pageUrlRelative": "/posts/2yXPFGXLNHxSfGdK9/edinburgh-lw-meetup-as-usual", "linkUrl": "https://www.lesswrong.com/posts/2yXPFGXLNHxSfGdK9/edinburgh-lw-meetup-as-usual", "postedAtFormatted": "Friday, June 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Edinburgh%20LW%20meetup%2C%20as%20usual&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEdinburgh%20LW%20meetup%2C%20as%20usual%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yXPFGXLNHxSfGdK9%2Fedinburgh-lw-meetup-as-usual%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Edinburgh%20LW%20meetup%2C%20as%20usual%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yXPFGXLNHxSfGdK9%2Fedinburgh-lw-meetup-as-usual", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yXPFGXLNHxSfGdK9%2Fedinburgh-lw-meetup-as-usual", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<p>Saturday 2pm at the Delhi cafe</p>\n<p>The reason why this announcement is so minimalistic is that it's bowing in humility to the yet-to-be-born Once In A Month Meetup.</p>\n<p>Here's the story: last week, orthonormal suggest we create a Schelling point around meetups at the beginning of each month. Just so that people would then expect lots of each other to come, and hence they would in fact come. But this is only possible if we distinguish that meetup from the others in the month you see?</p>\n<p>Make no mistake, this meetup will also be attended by stellar folks, it just won't be as great the July 2nd one.</p>\n<p>See you there!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2yXPFGXLNHxSfGdK9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 7.259403537510433e-07, "legacy": true, "legacyId": "7962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-10T11:56:52.994Z", "modifiedAt": null, "url": null, "title": "Bangalore Meetup: 19th June 4 pm", "slug": "bangalore-meetup-19th-june-4-pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:25.709Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "znHu2zjSoCbsSikYm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HaPhrGFKw35mqbLbb/bangalore-meetup-19th-june-4-pm", "pageUrlRelative": "/posts/HaPhrGFKw35mqbLbb/bangalore-meetup-19th-june-4-pm", "linkUrl": "https://www.lesswrong.com/posts/HaPhrGFKw35mqbLbb/bangalore-meetup-19th-june-4-pm", "postedAtFormatted": "Friday, June 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bangalore%20Meetup%3A%2019th%20June%204%20pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABangalore%20Meetup%3A%2019th%20June%204%20pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHaPhrGFKw35mqbLbb%2Fbangalore-meetup-19th-june-4-pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bangalore%20Meetup%3A%2019th%20June%204%20pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHaPhrGFKw35mqbLbb%2Fbangalore-meetup-19th-june-4-pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHaPhrGFKw35mqbLbb%2Fbangalore-meetup-19th-june-4-pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<p>This is the second meetup being organized in Bangalore.&nbsp;</p>\n<p>The <a href=\"/r/discussion/lw/5ps/bangalore_meetup_28th_may/\">previous one</a> had two attendees and at least 6 more people in Bangalore and a few in other cities have expressed an interest for another one in the comments. Since the discussions petered out without reaching a consensus on the date for the next one, I'm going ahead and proposing 19th June 4 pm at Cafe Coffee Day on Brigade Road (this is the one on the Brigade Road/Magrath Road junction - close to Eva Mall.)</p>\n<p>Do respond in the comments if you'd like the date/time/place to be changed. I hope there'll be a good turn out this time!&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HaPhrGFKw35mqbLbb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 7.259698643098367e-07, "legacy": true, "legacyId": "7964", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["77pFWACkmGSsBAzeb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-10T12:15:08.254Z", "modifiedAt": null, "url": null, "title": "Simulation Argument errors", "slug": "simulation-argument-errors", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.011Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EW9jaevAswPCfCX36/simulation-argument-errors", "pageUrlRelative": "/posts/EW9jaevAswPCfCX36/simulation-argument-errors", "linkUrl": "https://www.lesswrong.com/posts/EW9jaevAswPCfCX36/simulation-argument-errors", "postedAtFormatted": "Friday, June 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simulation%20Argument%20errors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimulation%20Argument%20errors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEW9jaevAswPCfCX36%2Fsimulation-argument-errors%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simulation%20Argument%20errors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEW9jaevAswPCfCX36%2Fsimulation-argument-errors", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEW9jaevAswPCfCX36%2Fsimulation-argument-errors", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 717, "htmlBody": "<p>I was reading the <a href=\"http://www.simulation-argument.com/simulation.html\">simulation argument</a> again for kicks and a few errors tripped me up for a bit. I figured I'd point them out here in case anyone's interested or noticed the same problems. If I made a mistake in my analysis please let me know so I can promptly put a bolded warning saying so at the top of this article. (I do not endorse the method of thinking that involves naive anthropics or probabilities of simulation but nonetheless I am willing to play with that framework sometimes for communication's sake.)</p>\n<p>We can reasonably argue that all three of the propositions at the end of&nbsp;section 4 of the paper, titled \"The core of the simulation argument\",&nbsp;are false. Most human-level technological civilizations can survive to reach a posthuman stage (f<sub>p</sub>=.9), and want to (f<sub>I</sub>=.9)&nbsp;and are able to run lots of ancestor simulations; and yet there can conceivably be no observers with human-type experiences that live in simulations (f<sub>sim</sub>=0). Why? Because not all human-level technological civilizations are human technological civilizations; it could easily be argued that most aren't. Human technological civilizations could be part of the fraction of human-level technological civilizations that do not survive to reach a posthuman stage, or survive but do not want to run lots of ancestor simulations. Thus there will be no human ancestor-simulations even if there are many many alien ancestor-simulations who humans do not share an observer moment reference class with.</p>\n<p>Nitpicking? Not quite. This forces us to change \"fraction of all human-level technological civilizations that survive to reach posthuman stage\" to \"probability of human civilization reaching posthuman stage\", but then some of the discussion in the paper's Interpretation section (section 6) sounds pretty weird because it's comparing human civilization to other human-level civilizations.&nbsp;The equivocation on \"posthuman\" causes various other statements and passages in the original article to be false-esque or ambiguous, and these would need to be changed.&nbsp;&nbsp;f<sub>sim</sub>&nbsp;should be changed to f<sub>ancestor_sim</sub>&nbsp;as well; we might be in non-ancestor simulations. The fraction of us in ancestor-simulations is just one possible lower bound for the fraction of us in simulations generally. Luckily, besides that error I think the paper mostly avoids insinuating that we are unlikely to be in a simulation if we are not in an ancestor-simulation.</p>\n<p>The abstract of the paper differs from section 4, and uses \"human\" instead of \"human-level\". \"<span style=\"font-family: 'Times New Roman', Times, serif; font-size: medium;\">This paper argues that&nbsp;<em>at least one</em>&nbsp;of the following propositions is true: (1) the human species is very likely to go extinct before reaching a &ldquo;posthuman&rdquo; stage; (2) any posthuman civilization is extremely unlikely to run a significant number of simulations of their evolutionary history (or variations thereof); (3) we are almost certainly living in a computer simulation.\"</span>&nbsp;Using the \"descended from humans\" definition of \"posthuman\", we see that this argument works; however, it is not supported by section 4, which currently fails to specify human civilizations only. Using the \"very technologically advanced\" definition of \"posthuman\", we see that this argument fails for the reasons given above. Either way the wording should be made clearer, especially so considering that section 6 talks about posthuman civilizations that aren't posthuman. It also doesn't match the conclusion despite the similar structure.</p>\n<p>The conclusion is more like section 4, and thus fails in the same way as section 4.<span style=\"font-family: 'Times New Roman', Times, serif; font-size: medium;\">&nbsp;</span>Not only that, the conclusion says something really weird: \"<span style=\"font-family: 'Times New Roman', Times, serif; font-size: medium;\">In the dark forest of our current ignorance, it seems sensible to apportion one&rsquo;s credence roughly evenly between (1), (2), and (3).\" </span>I hope this isn't implying the credences should sum to 1, which would be absurd. After making the corrections suggested above it is easy to see that the 90% confidence in all of (1), (2), and (3) is justifiable. (A vaguely plausible scenario to go with that one is where human civilization gets uFAIed, alien civilizations that don't get uFAIed don't waste time simulating their ancestors but instead simulate millions of possible sibling civilizations that got uFAIed for reasons of acausal trade plus diminishing marginal utility functions or summat, and thus we're in one of those sibling simulations while aliens try to compute our values and our game theoretic trustworthiness et cetera.)</p>\n<p>All that said, despite the current problems with the structure of its less important supporting arguments, the final sentence remains true: \"<span style=\"font-family: 'Times New Roman', Times, serif; font-size: medium;\">Unless we are now living in a simulation, our descendants will almost certainly never run an ancestor-simulation.\"</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EW9jaevAswPCfCX36", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 7.259751613573887e-07, "legacy": true, "legacyId": "7965", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-10T12:45:03.421Z", "modifiedAt": null, "url": null, "title": "Help: Writing Marvin Minsky", "slug": "help-writing-marvin-minsky", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:23.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sEFNQf52L87PdpQNG/help-writing-marvin-minsky", "pageUrlRelative": "/posts/sEFNQf52L87PdpQNG/help-writing-marvin-minsky", "linkUrl": "https://www.lesswrong.com/posts/sEFNQf52L87PdpQNG/help-writing-marvin-minsky", "postedAtFormatted": "Friday, June 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%3A%20Writing%20Marvin%20Minsky&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%3A%20Writing%20Marvin%20Minsky%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEFNQf52L87PdpQNG%2Fhelp-writing-marvin-minsky%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%3A%20Writing%20Marvin%20Minsky%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEFNQf52L87PdpQNG%2Fhelp-writing-marvin-minsky", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEFNQf52L87PdpQNG%2Fhelp-writing-marvin-minsky", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1228, "htmlBody": "<p>I want to raise awareness of risks from AI and the challenges to mitigate those risks by writing experts and asking them questions. The e-Mail below is a template. Please help me improve it and to <strong>devise more or better questions</strong>.</p>\n<hr />\n<p>Dear Mr Minsky,</p>\n<p>I am currently trying to learn more about risks from artificial intelligence [1]. In the course of this&nbsp;undertaking I plan to ask various experts and influencers&nbsp;about their opinion. Consequently I am curious about your opinion as a noted author and cognitive scientist in the field of artificial intelligence.&nbsp;But first I want to&nbsp;apologize if I intrude on your privacy, it is not my intention to offend you or to steal your time. If that is the case, please just ignore the rest of this e-Mail.&nbsp;</p>\n<p>One of the leading textbooks in artificial intelligence, <em>'AI: A Modern Approach'</em> [2], states:</p>\n<blockquote class=\"gmail_quote\" style=\"margin:0pt 0pt 0pt 0.8ex;border-left:1px solid #cccccc;padding-left:1ex\">Omohundro (2008) hypothesizes that even an innocuous chess program could pose a risk to society. Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal. The moral is that even if you only want you program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards.</blockquote>\n<p>&nbsp;</p>\n<p>In this regard I would like to draw your attention to the <em>Singularity Institute for Artificial Intelligence</em> (SIAI) [3] and their mission to solve the problem of <em>Friendly AI</em> [4]. One example of the research interests of the SIAI is a reflective <em>decision theory</em> [5] of self-modifying decision systems. The SIAI does believe that \"it is&nbsp;one of the many fundamental open problems required to build a <em>recursively self-improving</em> [6] Artificial Intelligence with a stable motivational system.\" [7]<br /> <br /> With this in mind, I would like to ask you the following questions:</p>\n<ol>\n<li> Do you agree that risks from artificial intelligence have to be taken very seriously?</li>\n<li>Is it important to raise awareness of those risks within the artificial intelligence community?</li>\n<li>Should we figure out how to make AI provably friendly (non-dangerous [9]), before attempting to solve artificial general intelligence?</li>\n<li>How do risks from AI compare to other existential risks, e.g. advanced nanotechnology?</li>\n<li>What probability do you assign to the possibility of us being wiped out by badly done AI?</li>\n<li> What probability do you assign to the possibility of an intelligence explosion [10]?</li>\n<li>What probability do you assign to the possibility of a human, respectively sub-human, level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours or days? </li>\n<li>...</li>\n</ol>\n<p>Further I would also like to ask your permission to publish and discuss your possible answers on LessWrong.com [8], to estimate the public and academic awareness and perception of risks from AI and the effectiveness with which the risks are communicated. This is however completely optional to my curiosity and general interest in your answer. I will respect your decision under any circumstances and keep your opinion private if you wish. Likewise I would be pleased, instead of, or&nbsp;additionally&nbsp;to replying to this e-Mail, with a treatment of the above questions on&nbsp;your homepage, your personal blog or elsewhere. You got my permission to publish my name and this e-Mail in parts or completely.</p>\n<p><strong>Full disclosure: </strong><br /> <br />I am not&nbsp;associated with the SIAI or any organisation concerned with research on artificial intelligence, nor do I maintain a formal academic relationship. Given the possible permission to publish your answers they will under no circumstances be used by me in an attempt to cast a damning light on you or your interests but will be&nbsp;exhibited&nbsp;neutrally as the personal opinion of an expert.</p>\n<p><strong>References:<br /><br /></strong>[1]<strong> \"</strong>Reducing long-term catastrophic risks from artificial intelligence\" <a href=\"http://intelligence.org/riskintro/index.html\" target=\"_blank\">http://singinst.org/riskintro/index.html</a><br />[2] \"AI: A Modern Approach\", <em>Chapter 26, section 26.3, (6) \"The Success of AI might mean the end of the human race.\"</em> <a href=\"http://aima.cs.berkeley.edu/\" target=\"_blank\">http://aima.cs.berkeley.edu/</a><br />[3] \"Singularity Institute for Artificial Intelligence\" <a href=\"http://intelligence.org/\" target=\"_blank\">http://singinst.org/</a><br />[4] \"Artificial Intelligence as a Positive and Negative Factor in Global Risk.\" <a href=\"http://yudkowsky.net/singularity/ai-risk\" target=\"_blank\">http://yudkowsky.net/singularity/ai-risk</a><br />[5] Yudkowsky, Eliezer, \"Timeless Decision Theory\" <a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">http://singinst.org/upload/TDT-v01o.pdf</a><br />[6] \"Recursive Self-Improvement\" <a href=\"/lw/we/recursive_selfimprovement\">http://lesswrong.com/lw/we/recursive_selfimprovement/</a><br />[7] \"An interview with Eliezer Yudkowsky\", parts <a href=\"http://johncarlosbaez.wordpress.com/2011/03/07/this-weeks-finds-week-311/\">1</a>, <a href=\"http://johncarlosbaez.wordpress.com/2011/03/14/this-weeks-finds-week-312/\">2</a> and <a href=\"http://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/\">3<br /></a>[8] \"A community blog devoted to refining the art of human rationality.\" <a href=\"/\">http://lesswrong.com/ <br /></a>[9] <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">http://wiki.lesswrong.com/wiki/Paperclip_maximizer</a><br />[10] <a href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">http://wiki.lesswrong.com/wiki/Intelligence_explosion </a></p>\n<p>Yours sincerely,</p>\n<p>NAME<br />ADDRESS</p>\n<hr />\n<p><span style=\"color: #ff0000;\"><strong>Revised Version</strong></span></p>\n<p>Dear Professor Minsky,</p>\n<p>I am currently trying to learn more about risks from artificial intelligence. Consequently I am curious about your opinion as a noted author and cognitive scientist in the field of artificial intelligence.</p>\n<p>I would like to ask you the following questions:</p>\n<ol>\n<li>What probability do you assign to the possibility of us being wiped out by badly done AI?</li>\n<li>What probability do you assign to the possibility of a human level AI, respectively sub-human level AI, to self-modify its way up to massive superhuman intelligence within a matter of hours or days? </li>\n<li>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</li>\n<li>What is the current level of awareness of possible risks from AI within the artificial intelligence community, relative to the ideal level?</li>\n<li>How do risks from AI compare to other existential risks, e.g. advanced nanotechnology?</li>\n</ol>\n<p>Further I would also like to ask your permission to publish and discuss your possible answers, in order to estimate the academic awareness and perception of risks from AI, but would also be pleased, instead of, or&nbsp;additionally&nbsp;to replying to this email, with a treatment of the above questions on&nbsp;your homepage, your personal blog or elsewhere.</p>\n<p>You got my permission to publish my name and this email in parts or completely.</p>\n<p><strong></strong></p>\n<p><strong>References:<br /></strong></p>\n<ul>\n<li>Reducing long-term catastrophic risks from artificial intelligence: <a href=\"http://intelligence.org/riskintro/index.html\" target=\"_blank\">http://singinst.org/riskintro/index.html</a></li>\n<li>Artificial Intelligence as a Positive and Negative Factor in Global Risk: <a href=\"http://yudkowsky.net/singularity/ai-risk\" target=\"_blank\">http://yudkowsky.net/singularity/ai-risk</a><a href=\"http://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/\"><br /></a></li>\n<li>A community blog devoted to refining the art of human rationality: <a href=\"/\">http://lesswrong.com/ </a></li>\n</ul>\n<p>Please let me know if you are interested in more material related to my questions.</p>\n<p>&nbsp;</p>\n<p>Yours sincerely,</p>\n<p>NAME<br />ADDRESS</p>\n<hr />\n<p><span style=\"color: #ff0000;\"><strong>Second Revision</strong></span></p>\n<p>Dear Professor Minsky,</p>\n<p>I am currently trying to learn more about possible risks from artificial intelligence. Consequently I am curious about your opinion as a noted author and cognitive scientist in the field of artificial intelligence.</p>\n<p>I would like to ask you the following questions:</p>\n<ol>\n<li>What probability do you assign to the possibility of us being wiped out by badly done AI?</li>\n<li>What probability do you assign to the possibility of a human level AI, respectively sub-human level AI, to self-modify its way up to massive superhuman intelligence within a matter of hours or days? </li>\n<li>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</li>\n<li>What is the current level of awareness of possible risks from AI within the artificial intelligence community, relative to the ideal level?</li>\n<li>How do risks from AI compare to other existential risks, e.g. advanced nanotechnology?</li>\n</ol>\n<p>Furthermore I would also like to ask your permission to publish and discuss your possible answers, in order to estimate the academic awareness and perception of risks from AI.</p>\n<p>Please let me know if you are interested in third-party material that does expand on various aspects of my questions.</p>\n<p>&nbsp;</p>\n<p>Yours sincerely,</p>\n<p>NAME<br />ADDRESS</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sEFNQf52L87PdpQNG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 27, "extendedScore": null, "score": 7.259842244469322e-07, "legacy": true, "legacyId": "7966", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JBadX7rwdcRFzGuju"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-10T14:49:13.485Z", "modifiedAt": null, "url": null, "title": "Scientific misconduct misdiagnosed because of scientific misconduct", "slug": "scientific-misconduct-misdiagnosed-because-of-scientific", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:30.207Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iMaL9hXWPGxtAxBSm/scientific-misconduct-misdiagnosed-because-of-scientific", "pageUrlRelative": "/posts/iMaL9hXWPGxtAxBSm/scientific-misconduct-misdiagnosed-because-of-scientific", "linkUrl": "https://www.lesswrong.com/posts/iMaL9hXWPGxtAxBSm/scientific-misconduct-misdiagnosed-because-of-scientific", "postedAtFormatted": "Friday, June 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scientific%20misconduct%20misdiagnosed%20because%20of%20scientific%20misconduct&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScientific%20misconduct%20misdiagnosed%20because%20of%20scientific%20misconduct%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiMaL9hXWPGxtAxBSm%2Fscientific-misconduct-misdiagnosed-because-of-scientific%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scientific%20misconduct%20misdiagnosed%20because%20of%20scientific%20misconduct%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiMaL9hXWPGxtAxBSm%2Fscientific-misconduct-misdiagnosed-because-of-scientific", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiMaL9hXWPGxtAxBSm%2Fscientific-misconduct-misdiagnosed-because-of-scientific", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 558, "htmlBody": "<p>Please remember to have <a href=\"/lw/31i/have_no_heroes_and_no_villains/\">no heroes or villains</a>, but this just looks plain bad to be honest. I'm lowering my estimation of the quality of Stephen J. Gould's work in this area.</p>\n<p><a href=\"http://content.usatoday.com/communities/sciencefair/post/2011/06/stephen-jay-gould-mismeasured-skulls-in-racial-records-dispute/1\">USA today:</a></p>\n<blockquote>\n<p>The late scientific icon, Stephen Jay Gould, botched and perhaps faked his critique of a racist 19th-Century scientist's skull collection, suggests a second look at his efforts.</p>\n<div class=\"off\" style=\"line-height: 12px; font-size: 12px; width: 232px; margin: 0px 0px 0px 5px; float: right;\">\n<div class=\"blog-captioned-photo0\">\n<div class=\"photo-container\" style=\"height: 339px; position: relative; padding: 0pt; clear: both;\"><span><a href=\"http://i.usatoday.net/communitymanager/_photos/science-fair/2011/06/06/skullsx-large.jpg\" target=\"_blank\"><img style=\"margin: 0pt; float: none; border: 1px solid #666666;\" src=\"http://i.usatoday.net/communitymanager/_photos/science-fair/2011/06/06/skullsx-inset-community.jpg\" alt=\"\" width=\"230\" height=\"337\" /></a></span></div>\n<div class=\"controls\">\n<div class=\"label\" style=\"width: 100px; float: left;\"><a style=\"padding: 0pt 0pt 0pt 11px; font-size: 10px; color: #666666; background: url(http://i.usatoday.net/_common/_images/caption0.gif) no-repeat scroll left center transparent;\">CAPTION</a></div>\n<div class=\"credit\" style=\"width: 132px; float: left; font-size: 10px; color: #666666; text-align: right;\">UPenn</div>\n</div>\n</div>\n</div>\n<p>In a 1978 <a href=\"http://www.sciencemag.org/content/200/4341/503.abstract?sid=0ab920a9-5ecf-40fa-aa54-aa9a9e9543b6\" target=\"_blank\"><em>Science </em>paper</a>, Gould (1941 - 2002) , reported that the Samuel George Morton (1799-1851), \"a prominent Philadelphia physician,\" had mis-measured the cranial capacities of his 1,000-skull \"American Golgotha\" collection gathered from around the world, to suit his racist beliefs. The finding led to one of Gould's best-known books, <a href=\"http://www.bookrags.com/The_Mismeasure_of_Man\" target=\"_blank\"><em>The Mismeasure of Man</em></a>, a critique of scientific racism.</p>\n<p><strong>\"Morton is now viewed as a canonical example of scientific misconduct. But did Morton really fudge his data?,\"</strong> asks a <a href=\"http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001071\" target=\"_blank\"><em>PLoS Biology</em> study </a>led by anthropologist Jason Lewis of Stanford University. \"Are studies of human variation inevitably biased, as per Gould, or are objective accounts attainable, as Morton attempted?\"</p>\n<p>So, the study team remeasured the skulls collected by Morton, now owned largely by the University of Pennsylvania Museum of Archaeology and Anthropology in Philadelphia.</p>\n<p>Overall, they find, Morton did make mistakes in measuring skull capacity (he first stuffed them with seeds, and later lead shot to measure their brain size). But the mistakes were random. The random mistakes didn't favor any racial theory of larger brain sizes for white people over others.</p>\n<p>\"Given how long Gould's work has been criticized in this arena, I'm a little surprised that it took this long for the work to be done to write this article,\" says the University of Texas's <a href=\"http://www.utexas.edu/opa/experts/profile.php?id=309\" target=\"_blank\">David Prindle</a>, author of <a href=\"http://search.barnesandnoble.com/Stephen-Jay-Gould-and-the-Politics-of-Evolution/David-F-Prindle/e/9781591027188\" target=\"_blank\"><em>Stephen Jay Gould and the Politics of Evolution</em></a>. <strong>\"People who dislike Gould's work will likely go on disliking him even more after this article. People who are fans of his writing will likely go on supporting his views.\"</strong></p>\n</blockquote>\n<p>Haha. Humans.</p>\n<p><a href=\"http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001071\">The paper itself:</a></p>\n<blockquote>\n<p>In reevaluating Morton and Gould, we do not dispute that racist views were unfortunately common in 19th-century science or that bias has inappropriately influenced research in some cases. Furthermore, studies have demonstrated that modern human variation is generally continuous, rather than discrete or ''racial,'' and that most variation in modern humans is within, rather than between, populations. In particular, cranial capacity variation in human populations appears to be largely a function of climate, so, for example, the full range of average capacities is seen in Native American groups, as they historically occupied the full range of latitudes, say the study authors.</p>\n<p>...</p>\n<strong>Samuel George Morton, in the hands of Stephen Jay Gould, has served for 30 years as a textbook example of scientific misconduct. </strong><em>The Morton case was used by Gould as the main support for his contention that ''unconscious or dimly perceived finagling is probably endemic in science, since scientists are human beings rooted in cultural contexts, not automatons directed toward external truth''. This view has since achieved substantial popularity in ''science studies''. But our results falsify Gould's hypothesis that Morton manipulated his data to conform with his a priori views. The data on cranial capacity gathered by Morton are generally reliable, and he reported them fully.</em><strong> Overall, we find that Morton's initial reputation as the objectivist of his era was well-deserved.</strong></blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 1, "vg4LDxjdwHLotCm8w": 1, "5hpGj9nDLgokfghvR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iMaL9hXWPGxtAxBSm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 61, "extendedScore": null, "score": 0.000114, "legacy": true, "legacyId": "7967", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 61, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G6npMHwgRGSQDKavX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-10T18:53:19.630Z", "modifiedAt": null, "url": null, "title": "A Philosophical Treatise of Universal Induction (Link)", "slug": "a-philosophical-treatise-of-universal-induction-link", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:23.827Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HXZ2gJHCYWERTLWWS/a-philosophical-treatise-of-universal-induction-link", "pageUrlRelative": "/posts/HXZ2gJHCYWERTLWWS/a-philosophical-treatise-of-universal-induction-link", "linkUrl": "https://www.lesswrong.com/posts/HXZ2gJHCYWERTLWWS/a-philosophical-treatise-of-universal-induction-link", "postedAtFormatted": "Friday, June 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Philosophical%20Treatise%20of%20Universal%20Induction%20(Link)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Philosophical%20Treatise%20of%20Universal%20Induction%20(Link)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHXZ2gJHCYWERTLWWS%2Fa-philosophical-treatise-of-universal-induction-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Philosophical%20Treatise%20of%20Universal%20Induction%20(Link)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHXZ2gJHCYWERTLWWS%2Fa-philosophical-treatise-of-universal-induction-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHXZ2gJHCYWERTLWWS%2Fa-philosophical-treatise-of-universal-induction-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<blockquote>\n<p><span class=\"prepos\"><strong>Abstract:</strong> </span>Understanding inductive reasoning is a problem that has engaged mankind for thousands of years. This problem is relevant to a wide range of fields and is integral to the philosophy of science. It has been tackled by many great minds ranging from philosophers to scientists to mathematicians, and more recently computer scientists. In this article we argue the case for Solomonoff Induction, a formal inductive framework which combines algorithmic information theory with the Bayesian framework. Although it achieves excellent theoretical results and is based on solid philosophical foundations, the requisite technical knowledge necessary for understanding this framework has caused it to remain largely unknown and unappreciated in the wider scientific community. The main contribution of this article is to convey Solomonoff induction and its related concepts in a generally accessible form with the aim of bridging this current technical gap. In the process we examine the major historical contributions that have led to the formulation of Solomonoff Induction as well as criticisms of Solomonoff and induction in general. In particular we examine how Solomonoff induction addresses many issues that have plagued other inductive systems, such as the black ravens paradox and the confirmation problem, and compare this approach with other recent approaches.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://www.mdpi.com/1099-4300/13/6/1076/\">mdpi.com/1099-4300/13/6/1076/</a></p>\n<p><strong>Download PDF Full-Text:</strong> <a href=\"http://www.mdpi.com/1099-4300/13/6/1076/pdf\">mdpi.com/1099-4300/13/6/1076/pdf</a></p>\n<p><strong>Authors:</strong> <span style=\"white-space: nowrap;\">Samuel Rathmanner</span> and <span style=\"white-space: nowrap;\">Marcus Hutter</span></p>\n<p><span style=\"white-space: nowrap;\"><strong>Published:</strong> 3 June 2011<br /></span></p>\n<p><strong>Via:</strong> <a href=\"http://www.vetta.org/2011/06/treatise-on-universal-induction/\">vetta.org/2011/06/treatise-on-universal-induction/</a></p>\n<p><img src=\"http://xixidu.net/ag.png\" alt=\"\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HXZ2gJHCYWERTLWWS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 20, "extendedScore": null, "score": 7.260940188388886e-07, "legacy": true, "legacyId": "7968", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-10T19:46:37.176Z", "modifiedAt": null, "url": null, "title": "Outreach to probably compatible groups?", "slug": "outreach-to-probably-compatible-groups", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:48.672Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qxoa9CaSTyrmN3xDp/outreach-to-probably-compatible-groups", "pageUrlRelative": "/posts/qxoa9CaSTyrmN3xDp/outreach-to-probably-compatible-groups", "linkUrl": "https://www.lesswrong.com/posts/qxoa9CaSTyrmN3xDp/outreach-to-probably-compatible-groups", "postedAtFormatted": "Friday, June 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Outreach%20to%20probably%20compatible%20groups%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOutreach%20to%20probably%20compatible%20groups%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqxoa9CaSTyrmN3xDp%2Foutreach-to-probably-compatible-groups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Outreach%20to%20probably%20compatible%20groups%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqxoa9CaSTyrmN3xDp%2Foutreach-to-probably-compatible-groups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqxoa9CaSTyrmN3xDp%2Foutreach-to-probably-compatible-groups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<p>At Balticon, there was a table for <a href=\"http://www.meetup.com/humanism-194/events/10428979/\">humanists, the Ethical Society, and skeptics</a>. I told one of the folks there about Less Wrong-- he seemed interested and I'll check back with him, but he'd never heard of it at all-- it took a little time to disambiguate between Less Wrong and what he heard as a human being named Les Wrong.</p>\n<p>Anyway, what do you think of doing outreach to fairly compatible groups (I'd add Unitarians to the list)? And if so, what would be good methods?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qxoa9CaSTyrmN3xDp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "7963", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-10T23:21:21.932Z", "modifiedAt": "2022-03-24T17:56:54.299Z", "url": null, "title": "General Bitcoin discussion thread (June 2011)", "slug": "general-bitcoin-discussion-thread-june-2011", "viewCount": null, "lastCommentedAt": "2012-01-13T00:18:22.377Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SilasBarta", "createdAt": "2009-03-01T00:03:34.864Z", "isAdmin": false, "displayName": "SilasBarta"}, "userId": "zDPSZfarhLM7Gehug", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2598g336mAXc7gSCQ/general-bitcoin-discussion-thread-june-2011", "pageUrlRelative": "/posts/2598g336mAXc7gSCQ/general-bitcoin-discussion-thread-june-2011", "linkUrl": "https://www.lesswrong.com/posts/2598g336mAXc7gSCQ/general-bitcoin-discussion-thread-june-2011", "postedAtFormatted": "Friday, June 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20General%20Bitcoin%20discussion%20thread%20(June%202011)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGeneral%20Bitcoin%20discussion%20thread%20(June%202011)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2598g336mAXc7gSCQ%2Fgeneral-bitcoin-discussion-thread-june-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=General%20Bitcoin%20discussion%20thread%20(June%202011)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2598g336mAXc7gSCQ%2Fgeneral-bitcoin-discussion-thread-june-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2598g336mAXc7gSCQ%2Fgeneral-bitcoin-discussion-thread-june-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p>We've started a habit of creating periodic Bitcoin threads to confine discussion thereof to those threads and prevent excessive proliferation of Bitcoin topics in the discussion section.&nbsp; <a href=\"/lw/5se/general_bitcoin_discussion_thread_may_2011/\">Here</a> is a link to the last one, which links the other discussions.&nbsp; Lot's to talk about, and another bounce in Bitcoin's value (up to 33 then down to 24), so share your links and thoughts!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jgcAJnksReZRuvgzp": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2598g336mAXc7gSCQ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 6, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "7970", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 104, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YgRNDRA5mo7dgGyn2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-06-10T23:21:21.932Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-11T01:08:46.996Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Scope Insensitivity", "slug": "seq-rerun-scope-insensitivity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.896Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tyrrell_McAllister", "createdAt": "2009-03-05T19:59:57.157Z", "isAdmin": false, "displayName": "Tyrrell_McAllister"}, "userId": "HSANMQBsHiGrZzwTB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7HGLz8nZaRrMW9A8q/seq-rerun-scope-insensitivity", "pageUrlRelative": "/posts/7HGLz8nZaRrMW9A8q/seq-rerun-scope-insensitivity", "linkUrl": "https://www.lesswrong.com/posts/7HGLz8nZaRrMW9A8q/seq-rerun-scope-insensitivity", "postedAtFormatted": "Saturday, June 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Scope%20Insensitivity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Scope%20Insensitivity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HGLz8nZaRrMW9A8q%2Fseq-rerun-scope-insensitivity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Scope%20Insensitivity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HGLz8nZaRrMW9A8q%2Fseq-rerun-scope-insensitivity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HGLz8nZaRrMW9A8q%2Fseq-rerun-scope-insensitivity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<p>Today's post, <a href=\"/lw/hw/scope_insensitivity/\">Scope Insensitivity</a>, was originally published on <span class=\"date\">14 May 2007</span>. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>The human brain can't represent large quantities: an environmental measure that will save 200,000 birds doesn't conjure anywhere near a hundred times the emotional impact and willingness-to-pay of a measure that would save 2,000 birds.</blockquote>\n<p>Discuss the post here (rather than in the comments to the original post).</p>\n<p><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/hv/third_alternatives_for_afterlifeism/\">Third Alternatives for Afterlife-ism</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.</em></p>\n<p><em>Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7HGLz8nZaRrMW9A8q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 7.262059858245399e-07, "legacy": true, "legacyId": "7971", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2ftJ38y9SRBCBsCzy", "ujz9PXXz7A4eKjBjp", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-11T15:22:10.495Z", "modifiedAt": null, "url": null, "title": "Q&A with Stan Franklin on risks from AI", "slug": "q-and-a-with-stan-franklin-on-risks-from-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:21.763Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jq9YjfZ7oq8ZzDy9i/q-and-a-with-stan-franklin-on-risks-from-ai", "pageUrlRelative": "/posts/jq9YjfZ7oq8ZzDy9i/q-and-a-with-stan-franklin-on-risks-from-ai", "linkUrl": "https://www.lesswrong.com/posts/jq9YjfZ7oq8ZzDy9i/q-and-a-with-stan-franklin-on-risks-from-ai", "postedAtFormatted": "Saturday, June 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20Stan%20Franklin%20on%20risks%20from%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20Stan%20Franklin%20on%20risks%20from%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjq9YjfZ7oq8ZzDy9i%2Fq-and-a-with-stan-franklin-on-risks-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20Stan%20Franklin%20on%20risks%20from%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjq9YjfZ7oq8ZzDy9i%2Fq-and-a-with-stan-franklin-on-risks-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjq9YjfZ7oq8ZzDy9i%2Fq-and-a-with-stan-franklin-on-risks-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 490, "htmlBody": "<p><strong>[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p>I am emailing experts in order to raise and estimate the academic awareness and perception of risks from AI.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Stan_Franklin\">Stan Franklin</a>,&nbsp; Professor,&nbsp; Computer Science<br />W. Harry&nbsp; Feinstone&nbsp; Interdisciplinary&nbsp; Research Professor<br />Institute for Intelligent Systems&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <br /> FedEx Institute of Technology&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <br />The University of Memphis</p>\n<h3><strong>The Interview</strong>:</h3>\n<p><strong>Q:</strong> <em>What probability do you assign to the possibility of us being wiped out by badly done AI?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin:</strong> On the basis of current evidence, I estimate that probability as being tiny. However, the cost would be so high, that the expectation is really difficult to estimate.</p>\n<p><strong>Q:</strong> <em>What probability do you assign to the possibility of a human level AI, respectively sub-human level AI, to self-modify its way up to massive superhuman intelligence within a matter of hours or days?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin:</strong> Essentially zero in such a time frame. A lengthy developmental period would be required. You might want to investigate the work&nbsp; of the IEEE Technical Committee on Autonomous Mental Development.</p>\n<p><strong>Q:</strong> <em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin:</strong> Proofs occur only in mathematics. Concern about the \"friendliness\" of AGI agents, or the lack thereof, has been present since the very inception of AGI. The 2006 workshop &lt;<a href=\"http://www.agiri.org/forum/index.php?act=ST&amp;f=21&amp;t=23\" target=\"_blank\">http://www.agiri.org/forum/index.php?act=ST&amp;f=21&amp;t=23</a>&gt;,&nbsp; perhaps the first organized event devoted to AGI, included a panel session entitled&nbsp; <span style=\"border-collapse: separate; color: #000000; font-family: Times; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; font-size: medium;\"><span style=\"font-family: arial; line-height: 19px; text-align: left; font-size: small;\"><strong>How do we more greatly ensure responsible AGI?&nbsp;</strong></span></span>Video available at &lt;<a href=\"http://video.google.com/videoplay?docid=5060147993569028388\" target=\"_blank\">http://video.google.com/videoplay?docid=5060147993569028388</a>&gt; (There's also a video of my keynote address.) I suspect we're not close enough to achieving AGI to be overly concerned yet. But that doesn't mean we shouldn't think about it. The day may well come.</p>\n<p><strong>Q:</strong> <em>What is the current level of awareness of possible risks from AI within the artificial intelligence community, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin: </strong>I'm not sure about the ideal level. Most AI researchers and practitioners seem to devote little or no thought at all to AGI. Though quite healthy and growing, the AGI movement is still marginal within the AI community. AGI has been supported by AAAI, the central organization of the AI community, and continues to receive such support.</p>\n<p><strong>Q:</strong> H<em>ow do risks from AI compare to other existential risks, e.g. advanced nanotechnology?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin: </strong>I have no thoughts on this subject. I've copied this message to Sonia Miller, who might be able to provide an answer or point you to someone who can.</p>\n<p><strong>Q:</strong> <em>Furthermore I would also like to ask your permission to publish and discuss your possible answers, in order to estimate the academic awareness and perception of risks from AI.</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin: </strong>Feel free, but do warn readers that my responses are strictly half-baked and off-the-top-of-my-head, rather than being well thought out. Given time and inclination to think further about these issues, my responses might change radically. I'm ok with their being used to stimulate discussion, but not as pronouncements.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZFrgTgzwEfStg26JL": 1, "DigEmY3RrF3XL5cwe": 1, "9DNZfxFvY5iKoZQbz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jq9YjfZ7oq8ZzDy9i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 36, "extendedScore": null, "score": 7.264605943247349e-07, "legacy": true, "legacyId": "7987", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"_Click_here_to_see_a_list_of_all_interviews_\">[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p>I am emailing experts in order to raise and estimate the academic awareness and perception of risks from AI.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Stan_Franklin\">Stan Franklin</a>,&nbsp; Professor,&nbsp; Computer Science<br>W. Harry&nbsp; Feinstone&nbsp; Interdisciplinary&nbsp; Research Professor<br>Institute for Intelligent Systems&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <br> FedEx Institute of Technology&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <br>The University of Memphis</p>\n<h3 id=\"The_Interview_\"><strong>The Interview</strong>:</h3>\n<p><strong>Q:</strong> <em>What probability do you assign to the possibility of us being wiped out by badly done AI?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin:</strong> On the basis of current evidence, I estimate that probability as being tiny. However, the cost would be so high, that the expectation is really difficult to estimate.</p>\n<p><strong>Q:</strong> <em>What probability do you assign to the possibility of a human level AI, respectively sub-human level AI, to self-modify its way up to massive superhuman intelligence within a matter of hours or days?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin:</strong> Essentially zero in such a time frame. A lengthy developmental period would be required. You might want to investigate the work&nbsp; of the IEEE Technical Committee on Autonomous Mental Development.</p>\n<p><strong>Q:</strong> <em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin:</strong> Proofs occur only in mathematics. Concern about the \"friendliness\" of AGI agents, or the lack thereof, has been present since the very inception of AGI. The 2006 workshop &lt;<a href=\"http://www.agiri.org/forum/index.php?act=ST&amp;f=21&amp;t=23\" target=\"_blank\">http://www.agiri.org/forum/index.php?act=ST&amp;f=21&amp;t=23</a>&gt;,&nbsp; perhaps the first organized event devoted to AGI, included a panel session entitled&nbsp; <span style=\"border-collapse: separate; color: #000000; font-family: Times; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; font-size: medium;\"><span style=\"font-family: arial; line-height: 19px; text-align: left; font-size: small;\"><strong>How do we more greatly ensure responsible AGI?&nbsp;</strong></span></span>Video available at &lt;<a href=\"http://video.google.com/videoplay?docid=5060147993569028388\" target=\"_blank\">http://video.google.com/videoplay?docid=5060147993569028388</a>&gt; (There's also a video of my keynote address.) I suspect we're not close enough to achieving AGI to be overly concerned yet. But that doesn't mean we shouldn't think about it. The day may well come.</p>\n<p><strong>Q:</strong> <em>What is the current level of awareness of possible risks from AI within the artificial intelligence community, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin: </strong>I'm not sure about the ideal level. Most AI researchers and practitioners seem to devote little or no thought at all to AGI. Though quite healthy and growing, the AGI movement is still marginal within the AI community. AGI has been supported by AAAI, the central organization of the AI community, and continues to receive such support.</p>\n<p><strong>Q:</strong> H<em>ow do risks from AI compare to other existential risks, e.g. advanced nanotechnology?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin: </strong>I have no thoughts on this subject. I've copied this message to Sonia Miller, who might be able to provide an answer or point you to someone who can.</p>\n<p><strong>Q:</strong> <em>Furthermore I would also like to ask your permission to publish and discuss your possible answers, in order to estimate the academic awareness and perception of risks from AI.</em></p>\n<p style=\"padding-left: 30px;\"><strong>Stan Franklin: </strong>Feel free, but do warn readers that my responses are strictly half-baked and off-the-top-of-my-head, rather than being well thought out. Given time and inclination to think further about these issues, my responses might change radically. I'm ok with their being used to stimulate discussion, but not as pronouncements.</p>", "sections": [{"title": "[Click here to see a list of all interviews]", "anchor": "_Click_here_to_see_a_list_of_all_interviews_", "level": 2}, {"title": "The Interview:", "anchor": "The_Interview_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-11T18:53:29.586Z", "modifiedAt": null, "url": null, "title": "Ottawa LW meetup, June 16, 7pm; two Bayesian Conspiracy sessions", "slug": "ottawa-lw-meetup-june-16-7pm-two-bayesian-conspiracy", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LYoc32Bd9gzYagitS/ottawa-lw-meetup-june-16-7pm-two-bayesian-conspiracy", "pageUrlRelative": "/posts/LYoc32Bd9gzYagitS/ottawa-lw-meetup-june-16-7pm-two-bayesian-conspiracy", "linkUrl": "https://www.lesswrong.com/posts/LYoc32Bd9gzYagitS/ottawa-lw-meetup-june-16-7pm-two-bayesian-conspiracy", "postedAtFormatted": "Saturday, June 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ottawa%20LW%20meetup%2C%20June%2016%2C%207pm%3B%20two%20Bayesian%20Conspiracy%20sessions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOttawa%20LW%20meetup%2C%20June%2016%2C%207pm%3B%20two%20Bayesian%20Conspiracy%20sessions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYoc32Bd9gzYagitS%2Fottawa-lw-meetup-june-16-7pm-two-bayesian-conspiracy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ottawa%20LW%20meetup%2C%20June%2016%2C%207pm%3B%20two%20Bayesian%20Conspiracy%20sessions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYoc32Bd9gzYagitS%2Fottawa-lw-meetup-june-16-7pm-two-bayesian-conspiracy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYoc32Bd9gzYagitS%2Fottawa-lw-meetup-june-16-7pm-two-bayesian-conspiracy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p><strong><a id=\"more\"></a>Less Wrong meeting</strong>:</p>\r\n<p>Date: Thursday June 16, 7:00pm 'til whenever.</p>\r\n<p>Venue: Fox and Feather, upstairs, possibly in back room #1 if it's free. Look for the LW sign.</p>\r\n<p>&nbsp;</p>\r\n<p><strong>Bayes study group</strong>: Anyone in the region interested in learning how to do Bayesian statistics is welcome to join us. We'll be using the statistical package R (http://cran.r-project.org/) as a platform, so bring your laptop if you have one. We're currently covering the binomial model with its conjugate prior, the beta distribution.</p>\r\n<p>Date: Thursday June 16, 9:00am to 10:30am. NB: 9 in the morning.</p>\r\n<p>Venue: Jeanne Mance Building, Tunney's Pasture. Meet me in the lobby.</p>\r\n<p>&nbsp;</p>\r\n<p><strong>Bayes study group</strong>: This session <del>was to</del> will run in parallel but slightly delayed relative to the sessions at Tunney's Pasture<del>, but now it's caught up</del>.</p>\r\n<p>Date: Tuesday June <del>14</del> 21,&nbsp;9:30pm</p>\r\n<p>Venue: wmiles's house; join the Google group for the address.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LYoc32Bd9gzYagitS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.265236653293876e-07, "legacy": true, "legacyId": "7989", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-11T20:00:23.355Z", "modifiedAt": null, "url": null, "title": "Unconditionally Convergent Expected Utility", "slug": "unconditionally-convergent-expected-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:25.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i5N5Pi5y9srv9D7wG/unconditionally-convergent-expected-utility", "pageUrlRelative": "/posts/i5N5Pi5y9srv9D7wG/unconditionally-convergent-expected-utility", "linkUrl": "https://www.lesswrong.com/posts/i5N5Pi5y9srv9D7wG/unconditionally-convergent-expected-utility", "postedAtFormatted": "Saturday, June 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Unconditionally%20Convergent%20Expected%20Utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnconditionally%20Convergent%20Expected%20Utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi5N5Pi5y9srv9D7wG%2Funconditionally-convergent-expected-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Unconditionally%20Convergent%20Expected%20Utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi5N5Pi5y9srv9D7wG%2Funconditionally-convergent-expected-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi5N5Pi5y9srv9D7wG%2Funconditionally-convergent-expected-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 270, "htmlBody": "<p>Expected utility can be expressed as the sum&nbsp;&Sigma;P(X<sub>n</sub>)U(X<sub>n</sub>). Suppose P(X<sub>n</sub>) = 2<sup>-n</sup>, and U(X<sub>n</sub>) = (-2)<sup>n</sup>/n. Then expected utility = &Sigma;2<sup>-n</sup>(-2)<sup>n</sup>/n = &Sigma;(-1)<sup>n</sup>/n = -1+1/2-1/3+1/4-... = -ln(2). Except there's no obvious order to add it. You could just as well say it's -1+1/2+1/4+1/6+1/8-1/3+1/10+1/12+1/14+1/16-1/5+... = 0. The sum depends on the order you add it. This is known as <a href=\"http://en.wikipedia.org/wiki/Conditional_convergence\">conditional convergence</a>.</p>\n<p>This is clearly something we want to avoid. Suppose my priors have an unconditionally convergent expected utility. This would mean that &Sigma;P(X<sub>n</sub>)|U(X<sub>n</sub>)| converges. Now suppose I observe evidence Y. &Sigma;P(X<sub>n</sub>|Y)|U(X<sub>n</sub>)| = &Sigma;|U(X<sub>n</sub>)|P(X<sub>n</sub>&cap;Y)/P(Y) &le; &Sigma;|U(X<sub>n</sub>)|P(X<sub>n</sub>)/P(Y) = 1/P(Y)&middot;&Sigma;P(X<sub>n</sub>)|U(X<sub>n</sub>)|. As long as P(Y) is nonzero, this must also converge.</p>\n<p>If my prior expected utility is unconditionally convergent, then given any finite amount of evidence, so is my posterior.</p>\n<p>This means I only have to come up with a nice prior, and I'll never have to worry about evidence braking expected utility.</p>\n<p>I suspect that this can be made even more powerful, and given any amount of evidence, finite or otherwise, I will <a href=\"http://en.wikipedia.org/wiki/Almost_surely\">almost surely</a> have an unconditionally convergent posterior. Anyone want to prove it?</p>\n<p>Now let's look at <a href=\"/\">Pascal's Mugging</a>. The problem here seems to be that someone could very easily give you an arbitrarily powerful threat. However, in order for expected utility to converge unconditionally, either carrying out the threat must get unlikely faster than the disutility increases, or the probability of the threat itself must get unlikely that fast. In other words, either someone threatening 3^^^3 people is so unlikely to carry it out to make it non-threatening, or the threat itself must be so difficult to make that you don't have to worry about it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i5N5Pi5y9srv9D7wG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 7.265436334715725e-07, "legacy": true, "legacyId": "7990", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-11T22:16:09.641Z", "modifiedAt": null, "url": null, "title": "Upcoming meet-ups:", "slug": "upcoming-meet-ups", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:21.953Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E2tNPNqaZXoDG7owq/upcoming-meet-ups", "pageUrlRelative": "/posts/E2tNPNqaZXoDG7owq/upcoming-meet-ups", "linkUrl": "https://www.lesswrong.com/posts/E2tNPNqaZXoDG7owq/upcoming-meet-ups", "postedAtFormatted": "Saturday, June 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Upcoming%20meet-ups%3A&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpcoming%20meet-ups%3A%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE2tNPNqaZXoDG7owq%2Fupcoming-meet-ups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Upcoming%20meet-ups%3A%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE2tNPNqaZXoDG7owq%2Fupcoming-meet-ups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE2tNPNqaZXoDG7owq%2Fupcoming-meet-ups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<p>There are upcoming irregularly scheduled meet-ups in:</p>\n<ul>\n<li>DC: <a href=\"/r/discussion/lw/608/dc_meetup_june_5th_1_pm/\">Sunday June 5 at 1pm</a></li>\n<li>Edinburgh: <a href=\"/r/discussion/lw/610/edinburgh_lw_meetup_saturday_4th_of_june_2pm/\">Saturday June 4 at 2pm</a>&nbsp;</li>\n<li>Fort Collins, Colorado: <a href=\"/r/discussion/lw/62l/fort_collins_colorado_meetup_wedneday_7pm/\">Wednesday June 8 at 7pm</a></li>\n<li>Houston: <a href=\"/r/discussion/lw/5z5/houston_hackerspace_meetup_saturday_june_4_200pm/\">Saturday June 4 at 2pm</a></li>\n<li>London: <a href=\"/r/discussion/lw/60u/reminder_london_meetup_sunday_5th_june_2pm_cargo/\">Sunday June 5 at 2pm</a>&nbsp;</li>\n<li>Ottawa: <a href=\"/r/discussion/lw/62e/ottawa_lw_meetup_june_9_7pm_two_bayesian/\">Thurs June 9 at 7pm</a>&nbsp;(+ an Ottawa Bayesian statistics group)&nbsp;</li>\n<li>West LA: <a href=\"/r/discussion/lw/5zj/west_la_weekly_meetups_wednesday_june_8th/\">Wednesday June 8th at 7pm</a></li>\n</ul>\n<p>Cities with regularly scheduled meetups: <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a></strong>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a></strong>, <strong><a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a></strong>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a></strong>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a></strong>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a></strong>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a></strong>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a></strong>.</p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup;&nbsp;<a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a>&nbsp;(more resources&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!<a id=\"more\"></a></p>\n<p>If you missed the deadline and wish to have your meetup featured, add a comment and send me a PM.</p>\n<p>Meet-ups should be posted in the discussion session; I will then make a promoted post \"upcoming meetups\" post every Friday that links to every meet-up that has been planned for the next two weeks.&nbsp;&nbsp;Please let me know if your meetup is omitted.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post about your meetup&nbsp;<em>before&nbsp;</em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening:&nbsp;<strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E2tNPNqaZXoDG7owq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 7.265841635222911e-07, "legacy": true, "legacyId": "7857", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q5bmsGX2soxiLpocq", "LwY7RvzbLuGWCYeqH", "vXA74BoxQPXtQoTkq", "tvWmas9xmiMGhtgpQ", "XunvLnQyMqHQMDG3J", "FtwqRGGpbmFvCWrPh", "KF8thFYLZ9r3mEgMh", "pAHo9zSFXygp5A5dL", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-11T23:21:43.635Z", "modifiedAt": null, "url": null, "title": "Not for the Sake of Pleasure Alone", "slug": "not-for-the-sake-of-pleasure-alone", "viewCount": null, "lastCommentedAt": "2020-12-14T19:57:18.581Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/87mdaCvCyo5bkk8hE/not-for-the-sake-of-pleasure-alone", "pageUrlRelative": "/posts/87mdaCvCyo5bkk8hE/not-for-the-sake-of-pleasure-alone", "linkUrl": "https://www.lesswrong.com/posts/87mdaCvCyo5bkk8hE/not-for-the-sake-of-pleasure-alone", "postedAtFormatted": "Saturday, June 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Not%20for%20the%20Sake%20of%20Pleasure%20Alone&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANot%20for%20the%20Sake%20of%20Pleasure%20Alone%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87mdaCvCyo5bkk8hE%2Fnot-for-the-sake-of-pleasure-alone%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Not%20for%20the%20Sake%20of%20Pleasure%20Alone%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87mdaCvCyo5bkk8hE%2Fnot-for-the-sake-of-pleasure-alone", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87mdaCvCyo5bkk8hE%2Fnot-for-the-sake-of-pleasure-alone", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1714, "htmlBody": "<p><small>Related: <a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">Not for the Sake of Happiness (Alone)</a>, <a href=\"/lw/y3/value_is_fragile/\">Value is Fragile</a>, <a href=\"/lw/lp/fake_fake_utility_functions/\">Fake Fake Utility Functions</a>, <a href=\"/lw/1oc/you_cannot_be_mistaken_about_not_wanting_to/\">You cannot be mistaken about (not) wanting to wirehead</a>, <a href=\"/lw/15h/utilons_vs_hedons/\">Utilons vs. Hedons</a>, <a href=\"/lw/1lb/are_wireheads_happy/\">Are wireheads happy?</a></small></p>\n<p>When someone <a href=\"http://commonsenseatheism.com/?p=12271#comment-75892\">tells me</a> that all human action is motivated by the desire for pleasure, or that we can solve the <a href=\"http://intelligence.org/singularityfaq#WhatIsFriendlyAI\">Friendly AI problem</a> by programming a machine superintelligence to maximize pleasure, I use a two-step argument to persuade them that things are more complicated than that.</p>\n<p>First, I present them with a variation on Nozick's experience machine,<sup>1</sup> something like this:</p>\n<blockquote>\n<p>Suppose that an advanced team of neuroscientists and computer scientists could hook your brain up to a machine that gave you maximal, beyond-orgasmic pleasure for the rest of an abnormally long life. Then they will blast you and the pleasure machine into deep space at near light-speed so that you could never be interfered with. Would you let them do this for you?</p>\n</blockquote>\n<p>Most people say they&nbsp;<em>wouldn't</em>&nbsp;choose the pleasure machine. They begin to realize that even though they usually experience pleasure when they get what they desired, they want more than <em>just</em> pleasure. They also want to visit Costa Rica and have good sex and help their loved ones succeed.</p>\n<p>But we can be mistaken when <a href=\"/lw/5sk/inferring_our_desires/\">inferring our desires</a> from such intuitions, so I follow this up with some neuroscience.</p>\n<p><a id=\"more\"></a></p>\n<h4><br /></h4>\n<h4>Wanting and liking</h4>\n<p>It turns out that the neural pathways for 'wanting' and 'liking' are separate, but overlap quite a bit. This explains why we <em>usually</em>&nbsp;experience pleasure when we get what we want, and thus are tempted to think that all we desire is pleasure. It also explains why we <a href=\"http://commonsenseatheism.com/?p=15072\">sometimes</a> <em>don't</em>&nbsp;experience pleasure when we get what we want, and why we wouldn't plug in to the pleasure machine.</p>\n<p>How do we know this? We now have objective measures of wanting and liking (desire and pleasure), and these processes do not always occur together.</p>\n<p><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/06/liking-expressions.png\" alt=\"liking expressions\" width=\"278\" height=\"280\" />One objective measure of liking is 'liking expressions.' Human infants, primates, and rats exhibit homologous facial reactions to pleasant and unpleasant tastes.<sup>2</sup>&nbsp;For example, both rats and human infants display rhythmic lip-licking movements when presented with sugary water,&nbsp;and both rats and human infants display a gaping reaction and mouth-wipes when presented with bitter water.<sup>3</sup></p>\n<p>Moreover, these animal liking expressions change in ways analogous to changes in human subjective pleasure. Food is more pleasurable to us when we are hungry, and sweet tastes elicit more liking expressions in rats when they are hungry than when they are full.<sup>4</sup>&nbsp;Similarly, both rats and humans respond to intense doses of salt (more concentrated than in seawater) with mouth gapes and other aversive reactions, and humans report subjective displeasure. But if humans or rats are depleted of salt, both humans and rats react instead with liking expressions (lip-licking), and humans report subjective pleasure.<sup>5</sup></p>\n<p>Luckily, these liking and disliking expressions share a common evolutionary history, and use the same brain structures in rats, primates, and humans. Thus, fMRI scans have uncovered to some degree the neural correlates of pleasure, giving us another objective measure of pleasure.<sup>6</sup></p>\n<p>As for <em>wanting</em>,&nbsp;research has revealed that dopamine is necessary for wanting but not for liking, and that dopamine largely <em>causes</em>&nbsp;wanting.<sup>7</sup></p>\n<p>Now we are ready to explain how we know that we do not desire pleasure alone.</p>\n<p>First, one can experience pleasure even if dopamine-generating structures have been destroyed or depleted.<sup>8</sup> Chocolate milk still tastes just as pleasurable despite the severe reduction of dopamine neurons in patients suffering from Parkinson's disease,<sup>9</sup> and the pleasure of amphetamine and cocaine persists throughout the use of dopamine-blocking drugs or dietary-induced dopamine depletion&nbsp;&mdash; even while these same treatments <em>do</em>&nbsp;suppress the&nbsp;<em>wanting</em>&nbsp;of amphetamine and cocaine.<sup>10</sup></p>\n<p>Second, elevation of dopamine causes an increase in <em>wanting</em>, but does not cause an increase in <em>liking</em>&nbsp;(when the goal is obtained). For example, mice with raised dopamine levels work harder and resist distractions more (compared to mice with normal dopamine levels) to obtain sweet food rewards, but they don't exhibit stronger liking reactions when they obtain the rewards.<sup>11</sup>&nbsp;In humans, drug-induced dopamine increases correlate well with subjective ratings of 'wanting' to take more of the drug, but not with ratings of 'liking' that drug.<sup>12</sup>&nbsp;In these cases, it becomes clear that we want some things <em>besides</em> the pleasure that <em>usually</em>&nbsp;results when we get what we want.</p>\n<p>Indeed, it appears that mammals can come to want something that they have <em>never</em>&nbsp;before experienced pleasure when getting. In one study,<sup>13</sup> researchers observed the neural correlates of wanting while feeding rats intense doses of salt during their very <em>first</em>&nbsp;time in a state of salt-depletion. That is, the rats had never before experienced intense doses of salt as pleasurable (because they had never been salt-depleted before), and yet they <em>wanted</em> salt the very first time they encountered it in a salt-depleted state.&nbsp;</p>\n<p>&nbsp;</p>\n<h4>Commingled signals</h4>\n<p>But why are liking and wanting so commingled that we might confuse the two, or think that the only thing we desire is pleasure? It may be because the two different signals are <em>literally</em>&nbsp;commingled on the same neurons. Resarchers explain:</p>\n<blockquote>\n<p>Multiplexed signals commingle in a manner akin to how wire and optical communication systems carry telephone or computer data signals from multiple telephone conversations, email communications, and internet web traffic over a single wire. Just as the different signals can be resolved at their destination by receivers that decode appropriately, we believe that multiple reward signals [liking, wanting, and learning] can be packed into the activity of single ventral pallidal neurons in much the same way, for potential unpacking downstream.</p>\n<p>......we have observed a single neuron to encode all three signals... at various moments or in different ways (Smith et al., 2007; Tindell et al., 2005).<sup>14</sup></p>\n</blockquote>\n<p>&nbsp;</p>\n<h4>Conclusion</h4>\n<p>In the last decade, neuroscience has confirmed what intuition could only suggest: that we desire more than pleasure. We act not for the sake of pleasure alone. We cannot solve the Friendly AI problem just by programming an AI to maximize pleasure.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4>Notes</h4>\n<p><small><sup>1</sup> Nozick (1974), pp. 44-45.</small></p>\n<p><small><sup>2</sup> Steiner (1973); Steiner et al (2001).</small></p>\n<p><small><sup>3</sup> Grill &amp; Berridge (1985); Grill &amp; Norgren (1978).</small></p>\n<p><small><sup>4</sup> Berridge (2000).</small></p>\n<p><small><sup>5</sup> Berridge et al. (1984); Schulkin (1991); Tindell et al. (2006).</small></p>\n<p><small><sup>6</sup> Berridge (2009).</small></p>\n<p><small><sup>7</sup> Berridge (2007); Robinson &amp; Berridge (2003).</small></p>\n<p><small><sup>8</sup> Berridge &amp; Robinson (1998); Berridge et al. (1989); Pecina et al. (1997).</small></p>\n<p><small><sup>9</sup> Sienkiewicz-Jarosz et al. (2005).</small></p>\n<p><small><sup>10</sup> Brauer et al. (2001); Brauer &amp; de Wit (1997); Leyton (2009); Leyton et al. (2005).</small></p>\n<p><small><sup>11</sup> Cagniard et al. (2006); Pecina et al. (2003); Tindell et al. (2005); Wyvell &amp; Berridge (2000).</small></p>\n<p><small><sup>12</sup> Evans et al. (2006); Leyton et al. (2002).</small></p>\n<p><small><sup>13</sup> Tindell et al. (2009).</small></p>\n<p><span style=\"font-size: 11px;\"><sup>13</sup>&nbsp;Aldridge &amp; Berridge (2009). See Smith et al. (2011) for more recent details on commingling.</span></p>\n<p><span style=\"font-size: 11px;\"><br /></span></p>\n<h4>References</h4>\n<p><small>Aldridge &amp; Berridge (2009).&nbsp;Neural coding of pleasure: 'rose-tinted glasses' of the ventral pallidum. In Kringelbach &amp; Berridge (eds.), <em>Pleasures of the brain</em> (pp. 62-73). Oxford University Press.</small></p>\n<p><small>Berridge (2000).&nbsp;Measuring hedonic impact in animals and infants: Microstructure of affective taste reactivity patterns. <em>Neuroscience and Biobehavioral Reviews</em>, 24: 173-198.</small></p>\n<p><small>Berridge (2007).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Berridge-The-debate-over-dopamines-role-in-reward-the-case-for-incentive-salience.pdf\">The debate over dopamine's role in reward: the case for incentive salience</a>.&nbsp;<em style=\"font-style: italic;\">Psychopharmacology, 191</em>: 391-431.</small></p>\n<p><small>Berridge (2009).&nbsp;<a href=\"http://www.lsa.umich.edu/psych/research&amp;labs/berridge/publications/Berridge%20'Liking'%20&amp;%20'wanting'%20food%20rewards%20Physiol%20&amp;%20Behav%202009.pdf\">&lsquo;Liking&rsquo; and &lsquo;wanting&rsquo; food rewards: Brain substrates and roles in eating disorders</a>.&nbsp;<em style=\"font-style: italic;\">Physiology &amp; Behavior, 97</em>: 537-550.</small></p>\n<p><small>Berridge, Flynn, Schulkin, &amp; Grill (1984). Sodium depletion enhances salt palatability in rats. <em>Behavioral Neuroscience, 98</em>: 652-660.</small></p>\n<p><small>Berridge, Venier, &amp; Robinson (1989). Taste reactivity analysis of 6-hydroxydopamine-induced aphagia: Implications for arousal and anhedonia hypotheses of dopamine function.&nbsp;<em style=\"font-style: italic;\">Behavioral Neuroscience, 103</em>: 36-45.</small></p>\n<p><small>Berridge &amp; Robinson (1998).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Berridge-Robinson-What-is-the-role-of-dopamine-in-reward.pdf\">What is the role of dopamine in reward: Hedonic impact, reward learning, or incentive salience?</a>&nbsp;<em style=\"font-style: italic;\">Brain Research Reviews, 28</em>: 309-369.</small></p>\n<p><small>Brauer, Cramblett, Paxton, &amp; Rose (2001). Haloperidol reduces smoking of both nicotine-containing and denicotinized cigarettes.&nbsp;<em style=\"font-style: italic;\">Psychopharmacology, 159</em>: 31-37.</small></p>\n<p><small>Brauer &amp; de Wit (1997). High dose pimozide does not block amphetamine-induced euphoria in normal volunteers. <em>Pharmacology Biochemistry &amp; Behavior, 56</em>: 265-272.</small></p>\n<p><small>Cagniard, Beeler, Britt, McGehee, Marinelli, &amp; Zhuang (2006). Dopamine scales performance in the absence of new learning. <em>Neuron, 51</em>: 541-547.</small></p>\n<p><small>Evans, Pavese, Lawrence, Tai, Appel, Doder, Brooks, Lees, &amp; Piccini (2006). Compulsive drug use linked to sensitized ventral striatal dopamine transmission.&nbsp;<em style=\"font-style: italic;\">Annals of Neurology, 59</em>: 852-858.</small></p>\n<p><small>Grill &amp; Berridge (1985). Taste reactivity as a measure of the neural control of palatability. In Epstein &amp; Sprague (eds.), <em>Progress in Psychobiology and Physiological Psychology, Vol 2</em>&nbsp;(pp. 1-6). Academic Press.</small></p>\n<p><small>Grill &amp; Norgren (1978). The taste reactivity test II: Mimetic responses to gustatory stimuli in chronic thalamic and chronic decerebrate rats.&nbsp;<em style=\"font-style: italic;\">Brain Research, 143</em>: 263-279.</small></p>\n<p><small>Leyton, Boileau, Benkelfat, Diksic, Baker, &amp; Dagher (2002). Amphetamine-induced increases in extracellular dopamine, drug wanting, and novelty seeking: a PET/[11C]raclopride study in healthy men. <em>Neuropsychopharmacology, 27</em>: 1027-1035.</small></p>\n<p><small>Leyton, Casey, Delaney, Kolivakis, &amp; Benkelfat (2005). Cocaine craving, euphoria, and self-administration: a preliminary study of the effect of catecholamine precursor depletion.&nbsp;<em style=\"font-style: italic;\">Behavioral Neuroscience, 119</em>: 1619-1627.</small></p>\n<p><small>Leyton (2009). The neurobiology of desire: Dopamine and the regulation of mood and motivational states in humans.&nbsp;In Kringelbach &amp; Berridge (eds.),&nbsp;<em style=\"font-style: italic;\">Pleasures of the brain</em>&nbsp;(pp. 222-243). Oxford University Press.</small></p>\n<p><small>Nozick (1974). <em><a href=\"http://www.amazon.com/Anarchy-State-Utopia-Robert-Nozick/dp/0465097200\">Anarchy, State, and Utopia</a></em>. Basic Books.</small></p>\n<p><small>Pecina, Berridge, &amp; Parker (1997).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Pecina-Pimozide-does-not-shift-palatibility.pdf\">Pimozide does not shift palatibility: Separation of anhedonia from sensorimotor suppression by taste reactivity</a>.<em style=\"font-style: italic;\">Pharmacology Biochemistry and Behavior, 58</em>: 801-811.</small></p>\n<p><small>Pecina, Cagniard, Berridge, Aldridge, &amp; Zhuang (2003).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Pecina-Hyperdopaminergic-Mutant-Mice-Have-Higher-Wanting-But-Not-Liking-for-Sweet-Rewards.pdf\">Hyperdopaminergic mutant mice have higher 'wanting' but not 'liking' for sweet rewards</a>.&nbsp;<em style=\"font-style: italic;\">The Journal of Neuroscience, 23</em>:&nbsp;9395-9402.</small></p>\n<p><small>Robinson &amp; Berridge (2003). Addiction.&nbsp;<em style=\"font-style: italic;\">Annual Review of Psychology, 54</em>: 25-53.</small></p>\n<p><small>Schulkin (1991). <em><a href=\"http://www.amazon.com/Sodium-Hunger-Search-Salty-Taste/dp/0521018420/\">Sodium Hunger: the Search for a Salty Taste</a></em>. Cambridge University Press.</small></p>\n<p><small>Sienkiewicz-Jarosz, Scinska, Kuran, Ryglewicz, Rogowski, Wrobel, Korkosz, Kukwa, Kostowski, &amp; Bienkowski (2005).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sienkiewicz-Jarosz-Taste-responses-in-patients-with-Parkinsons-disease.pdf\">Taste responses in patients with Parkinson's disease</a>.&nbsp;<em style=\"font-style: italic;\">Journal of Neurology, Neurosurgery, &amp; Psychiatry, 76</em>: 40-46.</small></p>\n<p><small>Smith, Berridge, &amp; Aldridge (2007). Ventral pallidal neurons distinguish 'liking' and 'wanting' elevations caused by opioids versus dopamine in nucleus acumbens. Program No. 310.5, 2007 Neuroscience Meeting Planner. San Diego, CA: Society for Neuroscience.</small></p>\n<p><span style=\"font-size: 11px;\">Smith, Berridge, &amp; Aldridge (2011).&nbsp;</span><span style=\"font-size: 11px;\"><a href=\"http://www.lsa.umich.edu/psych/research&amp;labs/berridge/publications/Smith,%20Berridge%20&amp;%20Aldridge%202011%20Disentangling%20pleasure%20from%20incentive%20salience%20and%20learning%20PNAS.pdf\">Disentangling pleasure from incentive salience and&nbsp;</a></span><span style=\"font-size: 11px;\"><a href=\"http://www.lsa.umich.edu/psych/research&amp;labs/berridge/publications/Smith,%20Berridge%20&amp;%20Aldridge%202011%20Disentangling%20pleasure%20from%20incentive%20salience%20and%20learning%20PNAS.pdf\">learning signals in brain reward circuitry</a>.&nbsp;<em>Proceedings of the National Academy of Sciences PNAS Plus, 108</em>: 1-10.</span></p>\n<p><small>Steiner (1973). The gustofacial response: Observation on normal and anecephalic newborn infants.&nbsp;<em style=\"font-style: italic;\">Symposium on Oral Sensation and Perception, 4</em>: 254-278.</small></p>\n<p><small>Steiner, Glaser, Hawillo, &amp; Berridge (2001). Comparative expression of hedonic impact: affective reactions to taste by human infants and other primates.<em style=\"font-style: italic;\">Neuroscience and Biobehavioral Reviews, 25</em>: 53-74.</small></p>\n<p><small>Tindell, Berridge, Zhang, Pecina, &amp; Aldridge (2005).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Tindell-Ventral-pallidal-neurons-code-incentive-motivation.pdf\">Ventral pallidal neurons code incentive motivation: Amplification by mesolimbic sensitization and amphetamine</a>.&nbsp;<em style=\"font-style: italic;\">European Journal of Neuroscience, 22</em>: 2617-2634.</small></p>\n<p><small>Tindell, Smith, Pecina, Berridge, &amp; Aldridge (2006). Ventral pallidum firing codes hedonic reward: When a bad taste turns good. <em>Journal of Neurophysiology, 96</em>: 2399-2409.</small></p>\n<p><small>Tindell, Smith, Berridge, &amp; Aldridge (2009).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/06/Tindell-et-al-Dynamic-computation-of-incentive-salience.pdf\">Dynamic computation of incentive salience: 'wanting' what was never 'liked'</a>. <em>The Journal of Neuroscience, 29</em>: 12220-12228.</small></p>\n<p><small>Wyvell &amp; Berridge (2000). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/06/Wyvell-Berridge-Intra-accumbens-amphetamine-increases-the-conditioned-incentive-salience-of-sucrose-reward.pdf\">Intra-accumbens amphetamine increases the conditioned incentive salience of sucrose reward: Enhancement of reward 'wanting' without enhanced 'liking' or response reinforcement</a>. <em>Journal of Neuroscience, 20</em>: 8122-8130.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 10, "5f5c37ee1b5cdee568cfb186": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "87mdaCvCyo5bkk8hE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 50, "extendedScore": null, "score": 9.6e-05, "legacy": true, "legacyId": "7988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Related: <a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">Not for the Sake of Happiness (Alone)</a>, <a href=\"/lw/y3/value_is_fragile/\">Value is Fragile</a>, <a href=\"/lw/lp/fake_fake_utility_functions/\">Fake Fake Utility Functions</a>, <a href=\"/lw/1oc/you_cannot_be_mistaken_about_not_wanting_to/\">You cannot be mistaken about (not) wanting to wirehead</a>, <a href=\"/lw/15h/utilons_vs_hedons/\">Utilons vs. Hedons</a>, <a href=\"/lw/1lb/are_wireheads_happy/\">Are wireheads happy?</a></small></p>\n<p>When someone <a href=\"http://commonsenseatheism.com/?p=12271#comment-75892\">tells me</a> that all human action is motivated by the desire for pleasure, or that we can solve the <a href=\"http://intelligence.org/singularityfaq#WhatIsFriendlyAI\">Friendly AI problem</a> by programming a machine superintelligence to maximize pleasure, I use a two-step argument to persuade them that things are more complicated than that.</p>\n<p>First, I present them with a variation on Nozick's experience machine,<sup>1</sup> something like this:</p>\n<blockquote>\n<p>Suppose that an advanced team of neuroscientists and computer scientists could hook your brain up to a machine that gave you maximal, beyond-orgasmic pleasure for the rest of an abnormally long life. Then they will blast you and the pleasure machine into deep space at near light-speed so that you could never be interfered with. Would you let them do this for you?</p>\n</blockquote>\n<p>Most people say they&nbsp;<em>wouldn't</em>&nbsp;choose the pleasure machine. They begin to realize that even though they usually experience pleasure when they get what they desired, they want more than <em>just</em> pleasure. They also want to visit Costa Rica and have good sex and help their loved ones succeed.</p>\n<p>But we can be mistaken when <a href=\"/lw/5sk/inferring_our_desires/\">inferring our desires</a> from such intuitions, so I follow this up with some neuroscience.</p>\n<p><a id=\"more\"></a></p>\n<h4><br></h4>\n<h4 id=\"Wanting_and_liking\">Wanting and liking</h4>\n<p>It turns out that the neural pathways for 'wanting' and 'liking' are separate, but overlap quite a bit. This explains why we <em>usually</em>&nbsp;experience pleasure when we get what we want, and thus are tempted to think that all we desire is pleasure. It also explains why we <a href=\"http://commonsenseatheism.com/?p=15072\">sometimes</a> <em>don't</em>&nbsp;experience pleasure when we get what we want, and why we wouldn't plug in to the pleasure machine.</p>\n<p>How do we know this? We now have objective measures of wanting and liking (desire and pleasure), and these processes do not always occur together.</p>\n<p><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/06/liking-expressions.png\" alt=\"liking expressions\" width=\"278\" height=\"280\">One objective measure of liking is 'liking expressions.' Human infants, primates, and rats exhibit homologous facial reactions to pleasant and unpleasant tastes.<sup>2</sup>&nbsp;For example, both rats and human infants display rhythmic lip-licking movements when presented with sugary water,&nbsp;and both rats and human infants display a gaping reaction and mouth-wipes when presented with bitter water.<sup>3</sup></p>\n<p>Moreover, these animal liking expressions change in ways analogous to changes in human subjective pleasure. Food is more pleasurable to us when we are hungry, and sweet tastes elicit more liking expressions in rats when they are hungry than when they are full.<sup>4</sup>&nbsp;Similarly, both rats and humans respond to intense doses of salt (more concentrated than in seawater) with mouth gapes and other aversive reactions, and humans report subjective displeasure. But if humans or rats are depleted of salt, both humans and rats react instead with liking expressions (lip-licking), and humans report subjective pleasure.<sup>5</sup></p>\n<p>Luckily, these liking and disliking expressions share a common evolutionary history, and use the same brain structures in rats, primates, and humans. Thus, fMRI scans have uncovered to some degree the neural correlates of pleasure, giving us another objective measure of pleasure.<sup>6</sup></p>\n<p>As for <em>wanting</em>,&nbsp;research has revealed that dopamine is necessary for wanting but not for liking, and that dopamine largely <em>causes</em>&nbsp;wanting.<sup>7</sup></p>\n<p>Now we are ready to explain how we know that we do not desire pleasure alone.</p>\n<p>First, one can experience pleasure even if dopamine-generating structures have been destroyed or depleted.<sup>8</sup> Chocolate milk still tastes just as pleasurable despite the severe reduction of dopamine neurons in patients suffering from Parkinson's disease,<sup>9</sup> and the pleasure of amphetamine and cocaine persists throughout the use of dopamine-blocking drugs or dietary-induced dopamine depletion&nbsp;\u2014 even while these same treatments <em>do</em>&nbsp;suppress the&nbsp;<em>wanting</em>&nbsp;of amphetamine and cocaine.<sup>10</sup></p>\n<p>Second, elevation of dopamine causes an increase in <em>wanting</em>, but does not cause an increase in <em>liking</em>&nbsp;(when the goal is obtained). For example, mice with raised dopamine levels work harder and resist distractions more (compared to mice with normal dopamine levels) to obtain sweet food rewards, but they don't exhibit stronger liking reactions when they obtain the rewards.<sup>11</sup>&nbsp;In humans, drug-induced dopamine increases correlate well with subjective ratings of 'wanting' to take more of the drug, but not with ratings of 'liking' that drug.<sup>12</sup>&nbsp;In these cases, it becomes clear that we want some things <em>besides</em> the pleasure that <em>usually</em>&nbsp;results when we get what we want.</p>\n<p>Indeed, it appears that mammals can come to want something that they have <em>never</em>&nbsp;before experienced pleasure when getting. In one study,<sup>13</sup> researchers observed the neural correlates of wanting while feeding rats intense doses of salt during their very <em>first</em>&nbsp;time in a state of salt-depletion. That is, the rats had never before experienced intense doses of salt as pleasurable (because they had never been salt-depleted before), and yet they <em>wanted</em> salt the very first time they encountered it in a salt-depleted state.&nbsp;</p>\n<p>&nbsp;</p>\n<h4 id=\"Commingled_signals\">Commingled signals</h4>\n<p>But why are liking and wanting so commingled that we might confuse the two, or think that the only thing we desire is pleasure? It may be because the two different signals are <em>literally</em>&nbsp;commingled on the same neurons. Resarchers explain:</p>\n<blockquote>\n<p>Multiplexed signals commingle in a manner akin to how wire and optical communication systems carry telephone or computer data signals from multiple telephone conversations, email communications, and internet web traffic over a single wire. Just as the different signals can be resolved at their destination by receivers that decode appropriately, we believe that multiple reward signals [liking, wanting, and learning] can be packed into the activity of single ventral pallidal neurons in much the same way, for potential unpacking downstream.</p>\n<p>......we have observed a single neuron to encode all three signals... at various moments or in different ways (Smith et al., 2007; Tindell et al., 2005).<sup>14</sup></p>\n</blockquote>\n<p>&nbsp;</p>\n<h4 id=\"Conclusion\">Conclusion</h4>\n<p>In the last decade, neuroscience has confirmed what intuition could only suggest: that we desire more than pleasure. We act not for the sake of pleasure alone. We cannot solve the Friendly AI problem just by programming an AI to maximize pleasure.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4 id=\"Notes\">Notes</h4>\n<p><small><sup>1</sup> Nozick (1974), pp. 44-45.</small></p>\n<p><small><sup>2</sup> Steiner (1973); Steiner et al (2001).</small></p>\n<p><small><sup>3</sup> Grill &amp; Berridge (1985); Grill &amp; Norgren (1978).</small></p>\n<p><small><sup>4</sup> Berridge (2000).</small></p>\n<p><small><sup>5</sup> Berridge et al. (1984); Schulkin (1991); Tindell et al. (2006).</small></p>\n<p><small><sup>6</sup> Berridge (2009).</small></p>\n<p><small><sup>7</sup> Berridge (2007); Robinson &amp; Berridge (2003).</small></p>\n<p><small><sup>8</sup> Berridge &amp; Robinson (1998); Berridge et al. (1989); Pecina et al. (1997).</small></p>\n<p><small><sup>9</sup> Sienkiewicz-Jarosz et al. (2005).</small></p>\n<p><small><sup>10</sup> Brauer et al. (2001); Brauer &amp; de Wit (1997); Leyton (2009); Leyton et al. (2005).</small></p>\n<p><small><sup>11</sup> Cagniard et al. (2006); Pecina et al. (2003); Tindell et al. (2005); Wyvell &amp; Berridge (2000).</small></p>\n<p><small><sup>12</sup> Evans et al. (2006); Leyton et al. (2002).</small></p>\n<p><small><sup>13</sup> Tindell et al. (2009).</small></p>\n<p><span style=\"font-size: 11px;\"><sup>13</sup>&nbsp;Aldridge &amp; Berridge (2009). See Smith et al. (2011) for more recent details on commingling.</span></p>\n<p><span style=\"font-size: 11px;\"><br></span></p>\n<h4 id=\"References\">References</h4>\n<p><small>Aldridge &amp; Berridge (2009).&nbsp;Neural coding of pleasure: 'rose-tinted glasses' of the ventral pallidum. In Kringelbach &amp; Berridge (eds.), <em>Pleasures of the brain</em> (pp. 62-73). Oxford University Press.</small></p>\n<p><small>Berridge (2000).&nbsp;Measuring hedonic impact in animals and infants: Microstructure of affective taste reactivity patterns. <em>Neuroscience and Biobehavioral Reviews</em>, 24: 173-198.</small></p>\n<p><small>Berridge (2007).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Berridge-The-debate-over-dopamines-role-in-reward-the-case-for-incentive-salience.pdf\">The debate over dopamine's role in reward: the case for incentive salience</a>.&nbsp;<em style=\"font-style: italic;\">Psychopharmacology, 191</em>: 391-431.</small></p>\n<p><small>Berridge (2009).&nbsp;<a href=\"http://www.lsa.umich.edu/psych/research&amp;labs/berridge/publications/Berridge%20'Liking'%20&amp;%20'wanting'%20food%20rewards%20Physiol%20&amp;%20Behav%202009.pdf\">\u2018Liking\u2019 and \u2018wanting\u2019 food rewards: Brain substrates and roles in eating disorders</a>.&nbsp;<em style=\"font-style: italic;\">Physiology &amp; Behavior, 97</em>: 537-550.</small></p>\n<p><small>Berridge, Flynn, Schulkin, &amp; Grill (1984). Sodium depletion enhances salt palatability in rats. <em>Behavioral Neuroscience, 98</em>: 652-660.</small></p>\n<p><small>Berridge, Venier, &amp; Robinson (1989). Taste reactivity analysis of 6-hydroxydopamine-induced aphagia: Implications for arousal and anhedonia hypotheses of dopamine function.&nbsp;<em style=\"font-style: italic;\">Behavioral Neuroscience, 103</em>: 36-45.</small></p>\n<p><small>Berridge &amp; Robinson (1998).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Berridge-Robinson-What-is-the-role-of-dopamine-in-reward.pdf\">What is the role of dopamine in reward: Hedonic impact, reward learning, or incentive salience?</a>&nbsp;<em style=\"font-style: italic;\">Brain Research Reviews, 28</em>: 309-369.</small></p>\n<p><small>Brauer, Cramblett, Paxton, &amp; Rose (2001). Haloperidol reduces smoking of both nicotine-containing and denicotinized cigarettes.&nbsp;<em style=\"font-style: italic;\">Psychopharmacology, 159</em>: 31-37.</small></p>\n<p><small>Brauer &amp; de Wit (1997). High dose pimozide does not block amphetamine-induced euphoria in normal volunteers. <em>Pharmacology Biochemistry &amp; Behavior, 56</em>: 265-272.</small></p>\n<p><small>Cagniard, Beeler, Britt, McGehee, Marinelli, &amp; Zhuang (2006). Dopamine scales performance in the absence of new learning. <em>Neuron, 51</em>: 541-547.</small></p>\n<p><small>Evans, Pavese, Lawrence, Tai, Appel, Doder, Brooks, Lees, &amp; Piccini (2006). Compulsive drug use linked to sensitized ventral striatal dopamine transmission.&nbsp;<em style=\"font-style: italic;\">Annals of Neurology, 59</em>: 852-858.</small></p>\n<p><small>Grill &amp; Berridge (1985). Taste reactivity as a measure of the neural control of palatability. In Epstein &amp; Sprague (eds.), <em>Progress in Psychobiology and Physiological Psychology, Vol 2</em>&nbsp;(pp. 1-6). Academic Press.</small></p>\n<p><small>Grill &amp; Norgren (1978). The taste reactivity test II: Mimetic responses to gustatory stimuli in chronic thalamic and chronic decerebrate rats.&nbsp;<em style=\"font-style: italic;\">Brain Research, 143</em>: 263-279.</small></p>\n<p><small>Leyton, Boileau, Benkelfat, Diksic, Baker, &amp; Dagher (2002). Amphetamine-induced increases in extracellular dopamine, drug wanting, and novelty seeking: a PET/[11C]raclopride study in healthy men. <em>Neuropsychopharmacology, 27</em>: 1027-1035.</small></p>\n<p><small>Leyton, Casey, Delaney, Kolivakis, &amp; Benkelfat (2005). Cocaine craving, euphoria, and self-administration: a preliminary study of the effect of catecholamine precursor depletion.&nbsp;<em style=\"font-style: italic;\">Behavioral Neuroscience, 119</em>: 1619-1627.</small></p>\n<p><small>Leyton (2009). The neurobiology of desire: Dopamine and the regulation of mood and motivational states in humans.&nbsp;In Kringelbach &amp; Berridge (eds.),&nbsp;<em style=\"font-style: italic;\">Pleasures of the brain</em>&nbsp;(pp. 222-243). Oxford University Press.</small></p>\n<p><small>Nozick (1974). <em><a href=\"http://www.amazon.com/Anarchy-State-Utopia-Robert-Nozick/dp/0465097200\">Anarchy, State, and Utopia</a></em>. Basic Books.</small></p>\n<p><small>Pecina, Berridge, &amp; Parker (1997).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Pecina-Pimozide-does-not-shift-palatibility.pdf\">Pimozide does not shift palatibility: Separation of anhedonia from sensorimotor suppression by taste reactivity</a>.<em style=\"font-style: italic;\">Pharmacology Biochemistry and Behavior, 58</em>: 801-811.</small></p>\n<p><small>Pecina, Cagniard, Berridge, Aldridge, &amp; Zhuang (2003).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Pecina-Hyperdopaminergic-Mutant-Mice-Have-Higher-Wanting-But-Not-Liking-for-Sweet-Rewards.pdf\">Hyperdopaminergic mutant mice have higher 'wanting' but not 'liking' for sweet rewards</a>.&nbsp;<em style=\"font-style: italic;\">The Journal of Neuroscience, 23</em>:&nbsp;9395-9402.</small></p>\n<p><small>Robinson &amp; Berridge (2003). Addiction.&nbsp;<em style=\"font-style: italic;\">Annual Review of Psychology, 54</em>: 25-53.</small></p>\n<p><small>Schulkin (1991). <em><a href=\"http://www.amazon.com/Sodium-Hunger-Search-Salty-Taste/dp/0521018420/\">Sodium Hunger: the Search for a Salty Taste</a></em>. Cambridge University Press.</small></p>\n<p><small>Sienkiewicz-Jarosz, Scinska, Kuran, Ryglewicz, Rogowski, Wrobel, Korkosz, Kukwa, Kostowski, &amp; Bienkowski (2005).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sienkiewicz-Jarosz-Taste-responses-in-patients-with-Parkinsons-disease.pdf\">Taste responses in patients with Parkinson's disease</a>.&nbsp;<em style=\"font-style: italic;\">Journal of Neurology, Neurosurgery, &amp; Psychiatry, 76</em>: 40-46.</small></p>\n<p><small>Smith, Berridge, &amp; Aldridge (2007). Ventral pallidal neurons distinguish 'liking' and 'wanting' elevations caused by opioids versus dopamine in nucleus acumbens. Program No. 310.5, 2007 Neuroscience Meeting Planner. San Diego, CA: Society for Neuroscience.</small></p>\n<p><span style=\"font-size: 11px;\">Smith, Berridge, &amp; Aldridge (2011).&nbsp;</span><span style=\"font-size: 11px;\"><a href=\"http://www.lsa.umich.edu/psych/research&amp;labs/berridge/publications/Smith,%20Berridge%20&amp;%20Aldridge%202011%20Disentangling%20pleasure%20from%20incentive%20salience%20and%20learning%20PNAS.pdf\">Disentangling pleasure from incentive salience and&nbsp;</a></span><span style=\"font-size: 11px;\"><a href=\"http://www.lsa.umich.edu/psych/research&amp;labs/berridge/publications/Smith,%20Berridge%20&amp;%20Aldridge%202011%20Disentangling%20pleasure%20from%20incentive%20salience%20and%20learning%20PNAS.pdf\">learning signals in brain reward circuitry</a>.&nbsp;<em>Proceedings of the National Academy of Sciences PNAS Plus, 108</em>: 1-10.</span></p>\n<p><small>Steiner (1973). The gustofacial response: Observation on normal and anecephalic newborn infants.&nbsp;<em style=\"font-style: italic;\">Symposium on Oral Sensation and Perception, 4</em>: 254-278.</small></p>\n<p><small>Steiner, Glaser, Hawillo, &amp; Berridge (2001). Comparative expression of hedonic impact: affective reactions to taste by human infants and other primates.<em style=\"font-style: italic;\">Neuroscience and Biobehavioral Reviews, 25</em>: 53-74.</small></p>\n<p><small>Tindell, Berridge, Zhang, Pecina, &amp; Aldridge (2005).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Tindell-Ventral-pallidal-neurons-code-incentive-motivation.pdf\">Ventral pallidal neurons code incentive motivation: Amplification by mesolimbic sensitization and amphetamine</a>.&nbsp;<em style=\"font-style: italic;\">European Journal of Neuroscience, 22</em>: 2617-2634.</small></p>\n<p><small>Tindell, Smith, Pecina, Berridge, &amp; Aldridge (2006). Ventral pallidum firing codes hedonic reward: When a bad taste turns good. <em>Journal of Neurophysiology, 96</em>: 2399-2409.</small></p>\n<p><small>Tindell, Smith, Berridge, &amp; Aldridge (2009).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/06/Tindell-et-al-Dynamic-computation-of-incentive-salience.pdf\">Dynamic computation of incentive salience: 'wanting' what was never 'liked'</a>. <em>The Journal of Neuroscience, 29</em>: 12220-12228.</small></p>\n<p><small>Wyvell &amp; Berridge (2000). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/06/Wyvell-Berridge-Intra-accumbens-amphetamine-increases-the-conditioned-incentive-salience-of-sucrose-reward.pdf\">Intra-accumbens amphetamine increases the conditioned incentive salience of sucrose reward: Enhancement of reward 'wanting' without enhanced 'liking' or response reinforcement</a>. <em>Journal of Neuroscience, 20</em>: 8122-8130.</small></p>", "sections": [{"title": "Wanting and liking", "anchor": "Wanting_and_liking", "level": 1}, {"title": "Commingled signals", "anchor": "Commingled_signals", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "132 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 134, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["synsRtBKDeAFuo7e3", "GNnHHmm8EzePmKzPk", "D6rsNhHM4pBCpDzSb", "3iM8QjvdkPCyLRJM6", "yYTv6J6wFL8XK3Q7o", "HmfxSWnqnK265GEFM", "2G7AH92pHyj3nC32T"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-12T01:38:39.413Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] One Life Against the World", "slug": "seq-rerun-one-life-against-the-world", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.470Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tyrrell_McAllister", "createdAt": "2009-03-05T19:59:57.157Z", "isAdmin": false, "displayName": "Tyrrell_McAllister"}, "userId": "HSANMQBsHiGrZzwTB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2EuNuqgyqYKZEhceP/seq-rerun-one-life-against-the-world", "pageUrlRelative": "/posts/2EuNuqgyqYKZEhceP/seq-rerun-one-life-against-the-world", "linkUrl": "https://www.lesswrong.com/posts/2EuNuqgyqYKZEhceP/seq-rerun-one-life-against-the-world", "postedAtFormatted": "Sunday, June 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20One%20Life%20Against%20the%20World&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20One%20Life%20Against%20the%20World%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2EuNuqgyqYKZEhceP%2Fseq-rerun-one-life-against-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20One%20Life%20Against%20the%20World%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2EuNuqgyqYKZEhceP%2Fseq-rerun-one-life-against-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2EuNuqgyqYKZEhceP%2Fseq-rerun-one-life-against-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>Today's post, <a href=\"/lw/hx/one_life_against_the_world/\">One Life Against the World</a>, was originally published on 18 May 2007. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>Saving one life and saving the whole world provide the same warm glow. But, however valuable a life is, the whole world is billions of times as valuable. The duty to save lives doesn't stop after the first saved life. Choosing to save one life when you could have saved two is as bad as murder.</blockquote>\n<p>Discuss the post here (rather than in the comments to the original post).</p>\n<p><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/hw/scope_insensitivity/\">Scope Insensitivity</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.</em></p>\n<p><em>Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb187": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2EuNuqgyqYKZEhceP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 7.266446194528267e-07, "legacy": true, "legacyId": "7992", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xiHy3kFni8nsxfdcP", "2ftJ38y9SRBCBsCzy", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-12T04:17:34.791Z", "modifiedAt": null, "url": null, "title": "Building habits: requesting advice on installing mental software", "slug": "building-habits-requesting-advice-on-installing-mental", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:29.130Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C3nsJvAfDThdfaJSh/building-habits-requesting-advice-on-installing-mental", "pageUrlRelative": "/posts/C3nsJvAfDThdfaJSh/building-habits-requesting-advice-on-installing-mental", "linkUrl": "https://www.lesswrong.com/posts/C3nsJvAfDThdfaJSh/building-habits-requesting-advice-on-installing-mental", "postedAtFormatted": "Sunday, June 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Building%20habits%3A%20requesting%20advice%20on%20installing%20mental%20software&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuilding%20habits%3A%20requesting%20advice%20on%20installing%20mental%20software%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC3nsJvAfDThdfaJSh%2Fbuilding-habits-requesting-advice-on-installing-mental%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Building%20habits%3A%20requesting%20advice%20on%20installing%20mental%20software%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC3nsJvAfDThdfaJSh%2Fbuilding-habits-requesting-advice-on-installing-mental", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC3nsJvAfDThdfaJSh%2Fbuilding-habits-requesting-advice-on-installing-mental", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 342, "htmlBody": "<p>I'd like to figure out how to create habits more effectively and systematically, especially mental habits.&nbsp;</p>\n<div>For example I might want to develop habits like</div>\n<div>\n<ul>\n<li>Noticing when I'm getting distracted and thinking about what I could do to reduce it</li>\n<li>Noticing when I'm procrastinating and thinking about what I could do to reduce it</li>\n<li>Doing Fermi calculations to estimate how much I should pay attention to a topic</li>\n</ul>\n<div>Is there relevant research on how to build habits? Anyone have notable success at systematically building habits? I'm also interested in hearing about destroying habits, does it seem any different than creating habits?</div>\n</div>\n<div>My current (1.5 week old) approach to building habits is to use Anki to remind myself to do specific exercises.&nbsp;</div>\n<div>I create a short exercise on a card that tells me to run through a sequence of thoughts or actions that constitute the habit I want to build, preferably using real examples. For example: the card might tell me to get into the frame of mind of getting distracted and then brainstorm ways to get less distracted in the future. When I see this card, I might try to get into the frame of mind of getting distracted&nbsp;by the thought of checking my email&nbsp;while coding and then try to think of ways to make myself less distracted by email.&nbsp;</div>\n<p>&nbsp;</p>\n<div>I have been creating cards in one of two formats:&nbsp;</div>\n<div><ol>\n<li>One half of the exercise on the front and one half on the back&nbsp;</li>\n<li>The whole exercise on the front and a verbal reward on the back which I try to say out lout to myself.</li>\n</ol></div>\n<div>My hope is to make an association with specific mental or external feelings and a particular thought process. This seems to be working a little bit, but it's too early to tell if it's working well.</div>\n<div>Does anyone know of better software for doing exercises semi-regularly? I doubt Anki is ideal for this because I expect the frequency should decay more slowly to really make these things habits and probably should not continue decaying indefinitely.&nbsp;</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C3nsJvAfDThdfaJSh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 7.266920726980251e-07, "legacy": true, "legacyId": "7993", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-12T05:20:03.317Z", "modifiedAt": null, "url": null, "title": "To Whom I May Have Concerned: A standard explanation of my disagreement", "slug": "to-whom-i-may-have-concerned-a-standard-explanation-of-my", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.014Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Lh82F4Xa5unwAbMqv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y5f4iiGxg2WXTJLGQ/to-whom-i-may-have-concerned-a-standard-explanation-of-my", "pageUrlRelative": "/posts/y5f4iiGxg2WXTJLGQ/to-whom-i-may-have-concerned-a-standard-explanation-of-my", "linkUrl": "https://www.lesswrong.com/posts/y5f4iiGxg2WXTJLGQ/to-whom-i-may-have-concerned-a-standard-explanation-of-my", "postedAtFormatted": "Sunday, June 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20To%20Whom%20I%20May%20Have%20Concerned%3A%20A%20standard%20explanation%20of%20my%20disagreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATo%20Whom%20I%20May%20Have%20Concerned%3A%20A%20standard%20explanation%20of%20my%20disagreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy5f4iiGxg2WXTJLGQ%2Fto-whom-i-may-have-concerned-a-standard-explanation-of-my%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=To%20Whom%20I%20May%20Have%20Concerned%3A%20A%20standard%20explanation%20of%20my%20disagreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy5f4iiGxg2WXTJLGQ%2Fto-whom-i-may-have-concerned-a-standard-explanation-of-my", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy5f4iiGxg2WXTJLGQ%2Fto-whom-i-may-have-concerned-a-standard-explanation-of-my", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1280, "htmlBody": "<p><strong>EDIT: I put this here so that it can be easily on the Internet so I can link people to it. When I link it to people, they'll be here on Less Wrong. The good it does to people who are already here, I don't really know. I feel that that makes it appropriate for a top-level post here. If you don't think so, comment, as I'm relatively new here. If three people downvote I'll remove it and put in somewhere else.</strong></p>\n<p>To Whom I May Have Concerned:</p>\n<p>I write this to explain myself to someone I have just disagreed and argued with. I always feel I need to say the same things after a disagreement, so I thought I'd automate it by writing it down once, and just providing people with a link.</p>\n<p>So we disagreed on something. Chances are, I upset or offended you. To start with, I apologize for that. I never want to upset or offend someone, and only rarely do I have to. So, I probably messed up there. Not only is offending and upsetting people a wrong thing to do, but it's also incredibly counterproductive. As soon as someone is offended, their mind closes. Arguments become like soldiers, to be fought. Good solutions are missed. Also, as a result of that, you probably like and trust me less, which is unhelpful for my goals. So I probably screwed that up. I'm sorry. I have this arrogant habit of belittling and making fun of people's beliefs when I'm pretty sure they're wrong, but can't quite explain to them why (for instance arguing about theology with people who don't understand intelligence, or arguing politics with people who don't know microeconomics). So I'm sorry for that.</p>\n<p>Now, I need to explain some of the frequent causes of misunderstanding which I get with people. To start with, I need to clarify what I mean by truth, opinions, and such things. The first one is the difference between a fact and a belief. A belief is what you think is true. A fact is true. I can't provide an example of the distinction for myself, <a href=\"http://www.goodreads.com/quotes/show/80893\">as Wittgenstein pointed out</a>. But, for instance, saying \"I believe that I am male\" gives the same information as \"I am male.\"</p>\n<p><strong>Introduction to my views on truth</strong></p>\n<p>I believe in <a href=\"http://en.wikipedia.org/wiki/Philosophical_realism\">philosophical realism</a>. This means that I think two contradictory statements cannot both be true. Iron either floats on water or it doesn't. I was born in Australia or I wasn't. So if two people disagree, at least one of them is wrong.</p>\n<p>(However, sometimes the disagreement isn't a real disagreement, just people meaning different things with their words. If that was the problem, I would have told you at the time. So, assume it was a real disagreement.)</p>\n<p>Now, there's this idea in society that some statements are facts, and some are opinions. For example, people think that \"Japan's capital city is Tokyo\" is a fact, while \"Japan's justice system is immoral\" to be opinions. I don't believe there is any qualitative difference between these statements. I believe both.</p>\n<p>There are some differences.&nbsp;\"Japan's capital city is Tokyo\" is a lot more likely to be considered a fact, for a few reasons.</p>\n<p>&nbsp;</p>\n<ol>\n<li>I believe \"Japan's capital city is Tokyo\" is a lot more likely to be true.</li>\n<li>More people believe it.</li>\n<li>Fewer common assumptions must be held for us to agree on it. If we want to talk about morality, we need to decide on a moral system first, then we need to figure out how the Japanese justice system corresponds. This takes a lot longer.</li>\n</ol>\n<div>But nonetheless. This is all a matter of scale rather than type. If we were all twenty times smarter, it's possible we wouldn't even notice the difference. If we were all five year olds, it's possible that there would be debate about what Japan's capital city really was. We might argue over whether perhaps Kyoto was the capital, if we had an old atlas. And so on. But overall, opinions can be just as wrong as factual statements.</div>\n<div>English makes this inconvenient. When I say \"I believe X\" instead of \"X is true,\" I'm normally trying to communicate something about myself. Saying \"I believe that consequentialism is more sensible than deontology\" draws attention to the fact claimer, me, rather than the fact. Which is sometimes useful. But it leads to that whole fact/opinion dichotomy which I greatly dislike.</div>\n<div><strong>Reactions vs opinion/facts</strong></div>\n<div>Except, however, in the case of what I prefer to call reactions to things. Some things which I rationally agree are equally bad upset me to different levels, and vice versa. I don't feel much sympathy towards babies. I care greatly about animals. People thinking badly of themselves makes me extremely upset. So if I enjoyed Three Worlds Collide, but someone else was offended by what it said about babyeating and rape, then I can understand them. We are not disagreeing on a fact, we are simply reporting upon our different reactions to something. Sometimes, we are called upon to make a judgement about something which is our estimation of what an average person's reaction to something would be. This is no longer a reaction, but an opinion about the reactions of others. It goes back into the realm of statements which can be argued about.</div>\n<div>This is a common misunderstanding I have in arguments with people. I'm not saying you're wrong to have the reaction you have to something. I can't argue that, cause I'm not in your head. You know more about your feelings than I do, in most cases.</div>\n<div>And so when I say that you're wrong about whether a movie was offensive or not, I'm not arguing that you were offended. I'm saying that the average person wouldn't be, so the movie itself is not terribly offensive. I can't control your reaction, and can't hold it against you. But you can be wrong about the average effect of something.</div>\n<div>\"Everyone is entitled to his own opinion\" is pretty much the epitome of this confusion. Everyone is entitled to his own reaction. But, you're only entitled to your own opinion to the extent that you're allowed to be wrong about something.</div>\n<div><strong>And so, we've disagreed.</strong></div>\n<p><span style=\"font-family: mceinline;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"> </span></span></p>\n<div style=\"margin-bottom: 1em;\">Now that we've clarified that. We disagreed, which means I think you're wrong. I hold beliefs on a probability basis: There is nothing I believe to be 100% true, but plenty of things which I believe strongly enough to round to 100% to, say, three decimal places. Assuming I respect your intelligence, which is usual, I have just adjusted the probability of me being right downwards. I'll probably check my thinking, and then adjust my probability further in whichever direction seems correct. For example, when someone I knew tried to convince me that Obama wasn't born in America, I quickly gave them about a 5% probability of being right. When I got home, I checked the internet and revised it down to about 1%.</div>\n<div style=\"margin-bottom: 1em;\">On the other end of the spectrum, when arguing that drugs should be legal one time, someone argued that they would have taken drugs as a teenager if they were legal. I had never met someone who asserted that before, which made me less sure about drug legalization.</div>\n<div style=\"margin-bottom: 1em;\">So, I'm checking my facts. I'll probably report back to you soon about whether I reckon you're right or not. It's very rare that I change someone's mind on something, so I may or may not bother trying. If you have evidence that I'm wrong, please tell it to me and to one of my friends. If I ignore it, my friend can complain to me.</div>\n<div style=\"margin-bottom: 1em;\"><strong>In conclusion.</strong></div>\n<div style=\"margin-bottom: 1em;\">We've disagreed. I'm not angry. I hope you aren't. Sorry for any offense I may have caused.</div>\n<div style=\"margin-bottom: 1em;\">Yours jollily,</div>\n<div style=\"margin-bottom: 1em;\">Buck</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y5f4iiGxg2WXTJLGQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -7, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "7994", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p><strong id=\"EDIT__I_put_this_here_so_that_it_can_be_easily_on_the_Internet_so_I_can_link_people_to_it__When_I_link_it_to_people__they_ll_be_here_on_Less_Wrong__The_good_it_does_to_people_who_are_already_here__I_don_t_really_know__I_feel_that_that_makes_it_appropriate_for_a_top_level_post_here__If_you_don_t_think_so__comment__as_I_m_relatively_new_here__If_three_people_downvote_I_ll_remove_it_and_put_in_somewhere_else_\">EDIT: I put this here so that it can be easily on the Internet so I can link people to it. When I link it to people, they'll be here on Less Wrong. The good it does to people who are already here, I don't really know. I feel that that makes it appropriate for a top-level post here. If you don't think so, comment, as I'm relatively new here. If three people downvote I'll remove it and put in somewhere else.</strong></p>\n<p>To Whom I May Have Concerned:</p>\n<p>I write this to explain myself to someone I have just disagreed and argued with. I always feel I need to say the same things after a disagreement, so I thought I'd automate it by writing it down once, and just providing people with a link.</p>\n<p>So we disagreed on something. Chances are, I upset or offended you. To start with, I apologize for that. I never want to upset or offend someone, and only rarely do I have to. So, I probably messed up there. Not only is offending and upsetting people a wrong thing to do, but it's also incredibly counterproductive. As soon as someone is offended, their mind closes. Arguments become like soldiers, to be fought. Good solutions are missed. Also, as a result of that, you probably like and trust me less, which is unhelpful for my goals. So I probably screwed that up. I'm sorry. I have this arrogant habit of belittling and making fun of people's beliefs when I'm pretty sure they're wrong, but can't quite explain to them why (for instance arguing about theology with people who don't understand intelligence, or arguing politics with people who don't know microeconomics). So I'm sorry for that.</p>\n<p>Now, I need to explain some of the frequent causes of misunderstanding which I get with people. To start with, I need to clarify what I mean by truth, opinions, and such things. The first one is the difference between a fact and a belief. A belief is what you think is true. A fact is true. I can't provide an example of the distinction for myself, <a href=\"http://www.goodreads.com/quotes/show/80893\">as Wittgenstein pointed out</a>. But, for instance, saying \"I believe that I am male\" gives the same information as \"I am male.\"</p>\n<p><strong id=\"Introduction_to_my_views_on_truth\">Introduction to my views on truth</strong></p>\n<p>I believe in <a href=\"http://en.wikipedia.org/wiki/Philosophical_realism\">philosophical realism</a>. This means that I think two contradictory statements cannot both be true. Iron either floats on water or it doesn't. I was born in Australia or I wasn't. So if two people disagree, at least one of them is wrong.</p>\n<p>(However, sometimes the disagreement isn't a real disagreement, just people meaning different things with their words. If that was the problem, I would have told you at the time. So, assume it was a real disagreement.)</p>\n<p>Now, there's this idea in society that some statements are facts, and some are opinions. For example, people think that \"Japan's capital city is Tokyo\" is a fact, while \"Japan's justice system is immoral\" to be opinions. I don't believe there is any qualitative difference between these statements. I believe both.</p>\n<p>There are some differences.&nbsp;\"Japan's capital city is Tokyo\" is a lot more likely to be considered a fact, for a few reasons.</p>\n<p>&nbsp;</p>\n<ol>\n<li>I believe \"Japan's capital city is Tokyo\" is a lot more likely to be true.</li>\n<li>More people believe it.</li>\n<li>Fewer common assumptions must be held for us to agree on it. If we want to talk about morality, we need to decide on a moral system first, then we need to figure out how the Japanese justice system corresponds. This takes a lot longer.</li>\n</ol>\n<div>But nonetheless. This is all a matter of scale rather than type. If we were all twenty times smarter, it's possible we wouldn't even notice the difference. If we were all five year olds, it's possible that there would be debate about what Japan's capital city really was. We might argue over whether perhaps Kyoto was the capital, if we had an old atlas. And so on. But overall, opinions can be just as wrong as factual statements.</div>\n<div>English makes this inconvenient. When I say \"I believe X\" instead of \"X is true,\" I'm normally trying to communicate something about myself. Saying \"I believe that consequentialism is more sensible than deontology\" draws attention to the fact claimer, me, rather than the fact. Which is sometimes useful. But it leads to that whole fact/opinion dichotomy which I greatly dislike.</div>\n<div><strong>Reactions vs opinion/facts</strong></div>\n<div>Except, however, in the case of what I prefer to call reactions to things. Some things which I rationally agree are equally bad upset me to different levels, and vice versa. I don't feel much sympathy towards babies. I care greatly about animals. People thinking badly of themselves makes me extremely upset. So if I enjoyed Three Worlds Collide, but someone else was offended by what it said about babyeating and rape, then I can understand them. We are not disagreeing on a fact, we are simply reporting upon our different reactions to something. Sometimes, we are called upon to make a judgement about something which is our estimation of what an average person's reaction to something would be. This is no longer a reaction, but an opinion about the reactions of others. It goes back into the realm of statements which can be argued about.</div>\n<div>This is a common misunderstanding I have in arguments with people. I'm not saying you're wrong to have the reaction you have to something. I can't argue that, cause I'm not in your head. You know more about your feelings than I do, in most cases.</div>\n<div>And so when I say that you're wrong about whether a movie was offensive or not, I'm not arguing that you were offended. I'm saying that the average person wouldn't be, so the movie itself is not terribly offensive. I can't control your reaction, and can't hold it against you. But you can be wrong about the average effect of something.</div>\n<div>\"Everyone is entitled to his own opinion\" is pretty much the epitome of this confusion. Everyone is entitled to his own reaction. But, you're only entitled to your own opinion to the extent that you're allowed to be wrong about something.</div>\n<div><strong>And so, we've disagreed.</strong></div>\n<p><span style=\"font-family: mceinline;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"> </span></span></p>\n<div style=\"margin-bottom: 1em;\">Now that we've clarified that. We disagreed, which means I think you're wrong. I hold beliefs on a probability basis: There is nothing I believe to be 100% true, but plenty of things which I believe strongly enough to round to 100% to, say, three decimal places. Assuming I respect your intelligence, which is usual, I have just adjusted the probability of me being right downwards. I'll probably check my thinking, and then adjust my probability further in whichever direction seems correct. For example, when someone I knew tried to convince me that Obama wasn't born in America, I quickly gave them about a 5% probability of being right. When I got home, I checked the internet and revised it down to about 1%.</div>\n<div style=\"margin-bottom: 1em;\">On the other end of the spectrum, when arguing that drugs should be legal one time, someone argued that they would have taken drugs as a teenager if they were legal. I had never met someone who asserted that before, which made me less sure about drug legalization.</div>\n<div style=\"margin-bottom: 1em;\">So, I'm checking my facts. I'll probably report back to you soon about whether I reckon you're right or not. It's very rare that I change someone's mind on something, so I may or may not bother trying. If you have evidence that I'm wrong, please tell it to me and to one of my friends. If I ignore it, my friend can complain to me.</div>\n<div style=\"margin-bottom: 1em;\"><strong>In conclusion.</strong></div>\n<div style=\"margin-bottom: 1em;\">We've disagreed. I'm not angry. I hope you aren't. Sorry for any offense I may have caused.</div>\n<div style=\"margin-bottom: 1em;\">Yours jollily,</div>\n<div style=\"margin-bottom: 1em;\">Buck</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "EDIT: I put this here so that it can be easily on the Internet so I can link people to it. When I link it to people, they'll be here on Less Wrong. The good it does to people who are already here, I don't really know. I feel that that makes it appropriate for a top-level post here. If you don't think so, comment, as I'm relatively new here. If three people downvote I'll remove it and put in somewhere else.", "anchor": "EDIT__I_put_this_here_so_that_it_can_be_easily_on_the_Internet_so_I_can_link_people_to_it__When_I_link_it_to_people__they_ll_be_here_on_Less_Wrong__The_good_it_does_to_people_who_are_already_here__I_don_t_really_know__I_feel_that_that_makes_it_appropriate_for_a_top_level_post_here__If_you_don_t_think_so__comment__as_I_m_relatively_new_here__If_three_people_downvote_I_ll_remove_it_and_put_in_somewhere_else_", "level": 1}, {"title": "Introduction to my views on truth", "anchor": "Introduction_to_my_views_on_truth", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-12T07:18:58.525Z", "modifiedAt": null, "url": null, "title": "Ending British Columbia's anti-cryonics law", "slug": "ending-british-columbia-s-anti-cryonics-law", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:24.163Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Owen_Richardson", "createdAt": "2011-04-08T22:03:10.482Z", "isAdmin": false, "displayName": "Owen_Richardson"}, "userId": "uR7QxXK65gYZQ4PrL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NjdQnrvD6MbdoLC2B/ending-british-columbia-s-anti-cryonics-law", "pageUrlRelative": "/posts/NjdQnrvD6MbdoLC2B/ending-british-columbia-s-anti-cryonics-law", "linkUrl": "https://www.lesswrong.com/posts/NjdQnrvD6MbdoLC2B/ending-british-columbia-s-anti-cryonics-law", "postedAtFormatted": "Sunday, June 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ending%20British%20Columbia's%20anti-cryonics%20law&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEnding%20British%20Columbia's%20anti-cryonics%20law%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjdQnrvD6MbdoLC2B%2Fending-british-columbia-s-anti-cryonics-law%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ending%20British%20Columbia's%20anti-cryonics%20law%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjdQnrvD6MbdoLC2B%2Fending-british-columbia-s-anti-cryonics-law", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjdQnrvD6MbdoLC2B%2Fending-british-columbia-s-anti-cryonics-law", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 349, "htmlBody": "<p><strong>[Edit: I did not mean to post this, just save it as a draft (I only remember pressing the 'save and continue' button, not the 'submit' button. That shouldn't've posted it, right?).</strong></p>\n<p><strong>Anyway, that's why it dissolves into slightly cryptic point form notes to myself at the end. Don't have the time right the now to flesh it out, so I'm just leaving it as is.]</strong></p>\n<p><strong><br /></strong></p>\n<p>I just noticed that there is no facebook group with this aim. I would like to create one. I feel that it *might* be a way to finally get enough 'special interest/human rights' force concentrated on the problem to fix it, if the presentation is done well.</p>\n<p>Would anyone like to help me write the group description and accompanying information, optimizing for effectiveness?</p>\n<p>Such a group would have two main audiences, and two main purposes:</p>\n<p>&nbsp;</p>\n<p>1 - For those who already understand and support cryonics, it would be a means to coordinate action and share information.</p>\n<p>2 - For those who have never really thought about cryonics before, but may well be open to the idea, it would serve as an introduction and hopefully cause them to join the first group.</p>\n<p>&nbsp;</p>\n<p>As regards the first group, the only major point to stress that springs to my mind is the importance of keeping their *effectiveness* foremost in mind when taking their actions, which mostly just means reminding them to be very friendly, polite, and pleasant while pestering and trying to educate the bureaucrats and politicians.</p>\n<p>But for the second group, well, I don't need to describe the difficulty in leading people to understanding across this particular inferential distance. How to do it in a snappy, engaging way?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>- The essential human issues at the root here: Hope and love, and freedom.</p>\n<p>- That the group is intended for people in BC and people with friends and family here put in danger by the law.</p>\n<p>- technical skepticism</p>\n<p>- moral confusion</p>\n<p>- image. Narrative, short story</p>\n<p>&nbsp;</p>\n<p>Resources I am thinking of drawing on are:</p>\n<p><a href=\"http://www.imminst.org/cryonics_letter/\">Scientists' Open Letter on Cryonics</a></p>\n<p><a href=\"http://www.benbest.com/cryonics/CryoFAQ.html#_IH_\">Ben Best's FAQs</a></p>\n<p><a href=\"http://cryocdn.org/law57.html\">This page on BC's anti-cryonics law at the Canadian Cryonics Society</a></p>\n<p><a href=\"http://thetyee.ca/Life/2006/07/05/Cryonics/\">This article in the Tyee</a></p>\n<p>letter to mom after Sandy's death</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NjdQnrvD6MbdoLC2B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "7995", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"_Edit__I_did_not_mean_to_post_this__just_save_it_as_a_draft__I_only_remember_pressing_the__save_and_continue__button__not_the__submit__button__That_shouldn_t_ve_posted_it__right___\">[Edit: I did not mean to post this, just save it as a draft (I only remember pressing the 'save and continue' button, not the 'submit' button. That shouldn't've posted it, right?).</strong></p>\n<p><strong id=\"Anyway__that_s_why_it_dissolves_into_slightly_cryptic_point_form_notes_to_myself_at_the_end__Don_t_have_the_time_right_the_now_to_flesh_it_out__so_I_m_just_leaving_it_as_is__\">Anyway, that's why it dissolves into slightly cryptic point form notes to myself at the end. Don't have the time right the now to flesh it out, so I'm just leaving it as is.]</strong></p>\n<p><strong><br></strong></p>\n<p>I just noticed that there is no facebook group with this aim. I would like to create one. I feel that it *might* be a way to finally get enough 'special interest/human rights' force concentrated on the problem to fix it, if the presentation is done well.</p>\n<p>Would anyone like to help me write the group description and accompanying information, optimizing for effectiveness?</p>\n<p>Such a group would have two main audiences, and two main purposes:</p>\n<p>&nbsp;</p>\n<p>1 - For those who already understand and support cryonics, it would be a means to coordinate action and share information.</p>\n<p>2 - For those who have never really thought about cryonics before, but may well be open to the idea, it would serve as an introduction and hopefully cause them to join the first group.</p>\n<p>&nbsp;</p>\n<p>As regards the first group, the only major point to stress that springs to my mind is the importance of keeping their *effectiveness* foremost in mind when taking their actions, which mostly just means reminding them to be very friendly, polite, and pleasant while pestering and trying to educate the bureaucrats and politicians.</p>\n<p>But for the second group, well, I don't need to describe the difficulty in leading people to understanding across this particular inferential distance. How to do it in a snappy, engaging way?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>- The essential human issues at the root here: Hope and love, and freedom.</p>\n<p>- That the group is intended for people in BC and people with friends and family here put in danger by the law.</p>\n<p>- technical skepticism</p>\n<p>- moral confusion</p>\n<p>- image. Narrative, short story</p>\n<p>&nbsp;</p>\n<p>Resources I am thinking of drawing on are:</p>\n<p><a href=\"http://www.imminst.org/cryonics_letter/\">Scientists' Open Letter on Cryonics</a></p>\n<p><a href=\"http://www.benbest.com/cryonics/CryoFAQ.html#_IH_\">Ben Best's FAQs</a></p>\n<p><a href=\"http://cryocdn.org/law57.html\">This page on BC's anti-cryonics law at the Canadian Cryonics Society</a></p>\n<p><a href=\"http://thetyee.ca/Life/2006/07/05/Cryonics/\">This article in the Tyee</a></p>\n<p>letter to mom after Sandy's death</p>\n<p>&nbsp;</p>", "sections": [{"title": "[Edit: I did not mean to post this, just save it as a draft (I only remember pressing the 'save and continue' button, not the 'submit' button. That shouldn't've posted it, right?).", "anchor": "_Edit__I_did_not_mean_to_post_this__just_save_it_as_a_draft__I_only_remember_pressing_the__save_and_continue__button__not_the__submit__button__That_shouldn_t_ve_posted_it__right___", "level": 1}, {"title": "Anyway, that's why it dissolves into slightly cryptic point form notes to myself at the end. Don't have the time right the now to flesh it out, so I'm just leaving it as is.]", "anchor": "Anyway__that_s_why_it_dissolves_into_slightly_cryptic_point_form_notes_to_myself_at_the_end__Don_t_have_the_time_right_the_now_to_flesh_it_out__so_I_m_just_leaving_it_as_is__", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-12T11:04:43.753Z", "modifiedAt": null, "url": null, "title": "From artificial intelligence research to philosophy", "slug": "from-artificial-intelligence-research-to-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:23.670Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vMAbjYmwsZXtvGrHF/from-artificial-intelligence-research-to-philosophy", "pageUrlRelative": "/posts/vMAbjYmwsZXtvGrHF/from-artificial-intelligence-research-to-philosophy", "linkUrl": "https://www.lesswrong.com/posts/vMAbjYmwsZXtvGrHF/from-artificial-intelligence-research-to-philosophy", "postedAtFormatted": "Sunday, June 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20From%20artificial%20intelligence%20research%20to%20philosophy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFrom%20artificial%20intelligence%20research%20to%20philosophy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvMAbjYmwsZXtvGrHF%2Ffrom-artificial-intelligence-research-to-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=From%20artificial%20intelligence%20research%20to%20philosophy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvMAbjYmwsZXtvGrHF%2Ffrom-artificial-intelligence-research-to-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvMAbjYmwsZXtvGrHF%2Ffrom-artificial-intelligence-research-to-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>Based on a sample size of three (Pearl, Yudkowsky &amp; Drescher), it appears that AI researchers can do quite well when they turn significant attention to philosophy. Are there other examples of this? I'm thinking of people who are primarily AI researchers, but have also done long, serious work in philosophy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vMAbjYmwsZXtvGrHF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 7.268136695531162e-07, "legacy": true, "legacyId": "7996", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-12T15:45:44.127Z", "modifiedAt": null, "url": null, "title": "How not to move the goalposts", "slug": "how-not-to-move-the-goalposts", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:26.692Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HopeFox", "createdAt": "2011-04-10T12:26:59.077Z", "isAdmin": false, "displayName": "HopeFox"}, "userId": "Zq6fNTMw4W2CkiiD5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c6FaN4i2LduYuTQn2/how-not-to-move-the-goalposts", "pageUrlRelative": "/posts/c6FaN4i2LduYuTQn2/how-not-to-move-the-goalposts", "linkUrl": "https://www.lesswrong.com/posts/c6FaN4i2LduYuTQn2/how-not-to-move-the-goalposts", "postedAtFormatted": "Sunday, June 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20not%20to%20move%20the%20goalposts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20not%20to%20move%20the%20goalposts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6FaN4i2LduYuTQn2%2Fhow-not-to-move-the-goalposts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20not%20to%20move%20the%20goalposts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6FaN4i2LduYuTQn2%2Fhow-not-to-move-the-goalposts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6FaN4i2LduYuTQn2%2Fhow-not-to-move-the-goalposts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1233, "htmlBody": "<p>There are a lot of bad arguments out there. Fortunately, there are also plenty of people who stand up against these arguments, which is good.</p>\r\n<p>However, there is a pattern I observe quite often in such counter-arguments, which, while strictly logically valid, can become problematic later. It involves fixing all of one's counter-arguments on countering one, and only one, of the original arguer's points. I suspect that this tendency can, at best, weaken one's argument, and, at worst, allow oneself to believe things one has no intention of believing.</p>\r\n<p>Let's assume, without much loss of generality, that the Wrong Argument can be expressed in the following form:</p>\r\n<p>A: Some statement.<br />B: Some other statement.<br />A &amp; B -&gt; C: A logical inference, which, from the way B is constructed, is a fairly obvious tautology.<br />C: The conclusion.</p>\r\n<p>Unfortunately, most of the arguments I could choose for this discussion are either highly trivial or highly controversial. I'll choose one that I hope won't cause too much trouble. Bear in mind that this is the Wrong Argument, the thing that the counter-arguer, the person presenting the good, rational refutation, is trying to demonstrate to be <em>false</em>. Let's designate this rational arguer as RA. The person presenting the Wrong Argument will be designated WA (Wrong Arguer).</p>\r\n<p>WA: \"Men have better technical abilities than women, so they should get paid more for the same engineering jobs.\"</p>\r\n<p>WA relates a terrible sentiment, yet a pervasive one. I don't know anyone who actually espouses it in my workplace, but it was certainly commonplace not so long ago (<a title=\"musical evidence\" href=\"http://www.youtube.com/watch?v=CCRRe72mwwY\" target=\"_blank\">musical evidence</a>). Let's hope that RA has something persuasive to say against it.</p>\r\n<p>Based on what I've seen of gender discussions on other forums, here's the most likely response I'd expect from RA:</p>\r\n<p>RA: Don't be ridiculous! Men and women are just as well suited to technical careers as each other!</p>\r\n<p>... and that's usually as far as it goes. Now, RA is <em>right</em>, as far as anyone knows (IANAPsychologist, though).</p>\r\n<p>However, WA's argument can be broken down into the following steps:</p>\r\n<p>A: Men, on average, have better technical skills than women.<br />B: If members of one group, on average, are better at a task than members of another group, then members of that first group should be paid more than members of the second group for performing the same work.<br />C: Men should be paid more than women for the same work in technical fields such as engineering.</p>\r\n<p>Trivially, A &amp; B -&gt; C. Thus RA only needs to disprove A or B in order to break the argument. (Yes, ~A doesn't imply ~C, but WA will have a hard time proving C without A.) Both A and B are unpleasant statements that decent, rational people should probably disagree with, and C is definitely problematic.</p>\r\n<p>So RA sets about attacking A. He starts by simply stating that men and women have equal potential for technical talent, on average. If WA doesn't believe that, then RA presents anecdotal evidence, then starts digging up psychological studies. Every rational discourse weapon at RA's disposal may be deployed to show that A is false. Maybe WA will be convinced, maybe he won't.</p>\r\n<p>But what about B? RA has ignored B entirely in his attack on A. Now, from a strictly logical point of view, RA doesn't need to do anything with B - if he disproves A, then he disproves A &amp; B. Attacking A doesn't mean that he accepts B as true...</p>\r\n<p>... except that it kind of does.</p>\r\n<p>What if WA manages to win the argument over A, by whatever means? What if WA turns out to be an evolutionary psychology <a href=\"/lw/js/the_bottom_line/\">clever arguer</a>, with several papers worth of \"evidence\" that \"proves\" that men have better technical skills than women? RA might simply not have the skills or resources to refute WA's points, leading to the following exchange:</p>\r\n<p>WA: Men are better engineers than women, and should be paid more!</p>\r\n<p>RA: That's ridiculous. Men and women have identical potentials for technical skill!</p>\r\n<p>WA: No they don't! Here are ten volumes' worth of papers proving me right!</p>\r\n<p>RA: Well, gee, who am I to argue with psychology journals? I guess you're right.</p>\r\n<p>WA: Glad we agree. I'll go talk to the CTO about Wanda's pay cut, shall I?</p>\r\n<p>RA: Hang on a minute! Even if men are better engineers than women, that's no reason for pay inequity! Equal work for equal pay is the only fair way. If men really are better, they'll get raises and promotions on their own merit, not merely by virtue of being male.</p>\r\n<p>WA: What? I spent hours getting those references together, and now you've moved the goalposts on me! I thought you weren't meant to do that!</p>\r\n<p>RA: But... it's true...</p>\r\n<p>WA: I think you've just taken your conclusion, \"Men and women should get equal pay for the same work\", and figured out a line of reasoning that gets you there. What are you, some kind of clever arguer for female engineers? Wait, isn't your mother an engineer too?</p>\r\n<p>Nobody wants to be in this situation. RA really has moved the goalposts on WA, which is one of those Dark Arts that we're not supposed to employ, even unintentionally.</p>\r\n<p>The problem goes deeper than simply violating good debating etiquette, though. If this debate is happening in public, then onlookers might get the impression that RA supports B. It will then be more difficult for RA to argue against B in later arguments, especially ones of the form D &amp; B, where D is actually true. (For example, D might be \"Old engineers have better technical skills than younger engineers\", which is true-ish because of the benefits of long experience in an industry, but it still shouldn't mean that old engineers automatically deserve higher pay for the same work.)</p>\r\n<p>Furthermore, and again IANAP, but it seems possible to me that if RA keeps arguing against A and ignoring B, he might actually start believing B. Alternatively, he might not specifically <em>believe</em> B, but he might stop thinking about B at all, and start ignoring the B step in his own reasoning and other people's.</p>\r\n<p>So, the way to avoid all of this, is to raise all of your objections simultaneously, thusly:</p>\r\n<p>WA: Men are better engineers than women, and should be paid more!</p>\r\n<p>RA: Woah. Okay, first? There's no evidence to suggest that that's actually true. But secondly, even pretending for the moment that it were true, that would be no excuse for paying women less for the same work.</p>\r\n<p>WA: Oh. Um. I'm pretty confident about that first point, but I never actually thought I'd have to defend the other bit. I'll go away now.</p>\r\n<p>That's a best-case scenario, but it does avoid the problems above.</p>\r\n<p>This post has already turned out longer than I intended, so I'll end it here. The last point I wanted to raise, though, is that an awful lot of Wrong Arguments (or good arguments, for that matter) take a form where A is an assertion of fact (\"men are better engineers than women\"), and B is an expression of morality (\"... and therefore they should get paid more\"). There are some important implications to this, for which I have a number of examples to present if people are interested.</p>\r\n<p>To summarise: If someone says \"A and B are true!\", don't just say \"A isn't true!\". Say \"A isn't true, and even if it were, B isn't true either!\". Otherwise people might think you believe B, and they might even be right.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c6FaN4i2LduYuTQn2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 7, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "7998", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["34XxbRFe54FycoCDw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-12T23:06:26.875Z", "modifiedAt": null, "url": null, "title": "The genetic cost of tyranny", "slug": "the-genetic-cost-of-tyranny", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.107Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/29nAKJLrEe8hYRXFL/the-genetic-cost-of-tyranny", "pageUrlRelative": "/posts/29nAKJLrEe8hYRXFL/the-genetic-cost-of-tyranny", "linkUrl": "https://www.lesswrong.com/posts/29nAKJLrEe8hYRXFL/the-genetic-cost-of-tyranny", "postedAtFormatted": "Sunday, June 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20genetic%20cost%20of%20tyranny&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20genetic%20cost%20of%20tyranny%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F29nAKJLrEe8hYRXFL%2Fthe-genetic-cost-of-tyranny%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20genetic%20cost%20of%20tyranny%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F29nAKJLrEe8hYRXFL%2Fthe-genetic-cost-of-tyranny", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F29nAKJLrEe8hYRXFL%2Fthe-genetic-cost-of-tyranny", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 596, "htmlBody": "<p>We may feel sympathy when we read about people killed for protesting in Syria, Bahrain, Libya, and other countries.&nbsp; But tyranny isn't just something happening to unfortunate people somewhere else.&nbsp; It's an existential risk to human civilization.</p>\n<p>Civilization - even tribalism - relies on altruism.&nbsp; Altruism is defined as cooperation that is not the happy convergence of interests of rational self-interested agents.&nbsp; That happens too; but we don't call it altruism.&nbsp; Altruism is, roughly, helping others without the expectation of reciprocation or cooperation.&nbsp; And it happens because humans <em>like</em> helping other humans.</p>\n<p>Altruism is probably mostly genetic.&nbsp; It's an evolutionary adaptation that instills the desire to help others into a species.&nbsp; Social pressure can install some amount of altruism; but it's my opinion that this would not work at all without a pre-existing genetic basis.&nbsp; Many species exhibit altruism to a level at least as great as that in humans.&nbsp; Some insects, which are incapable of feeling social pressure, are far more altruistic than humans.</p>\n<p>Two theories for how this happens are kin selection and group selection.&nbsp; Regardless of which of these you prefer, both of them have two important weaknesses:</p>\n<ul>\n<li>They are both very weak effects compared to selection for traits that benefit their organism directly.</li>\n<li>They require special social conditions, on society size (on the order of 10 members per society in the case of kin selection) and immigration/emigration rate (extremely low in both cases).</li>\n</ul>\n<p>It's not known whether humans are still evolving, or have begun devolving due to lack of selective pressure.&nbsp; But in the case of altruism, we can be sure:&nbsp; Even if some selective pressure still exists, most humans today do not live under the necessary conditions for either kin selection or group selection.&nbsp; Humans are living off their evolutionary capital of altruism.</p>\n<p>Tyranny, whether it's that of Syria, Iran, North Korea, Nazi Germany, or the Soviet bloc under Stalin, aggressively selects against altruism.&nbsp; The most-altruistic people were among the first executed in all those places.&nbsp; They are the people being shot while protesting in Syria.&nbsp; Social activism under such a government is rarely in your best self-interest.&nbsp; Tyranny selects for self-interest; people who are willing to help the state oppress others are given opportunities for advancement.&nbsp; And it removes altruistic genes quickly from the population, likely undoing hundreds of years of evolution every year.&nbsp; Those genes will never be replaced.</p>\n<p>I'm not too worried when this occurs over a few short months or years.&nbsp; But when a people lives under these conditions for generations, you may end up with a large population deficient in altruistic genes.</p>\n<p>There's no solution at that point short of gene therapy.&nbsp; The population can stay in place, resulting in a society that is at best hopelessly mired in corruption and poverty, and at worst a danger to the rest of the world.&nbsp; Or it can disperse, and dilute altruistic genes around the globe.</p>\n<p>ADDED:&nbsp; Knowing whether this is a real problem or not, would require learning something about how many genes are involved in altruism, and what their distribution in the population is.&nbsp; A legitimate objection to what I wrote is that if genes for altruism are distributed so that killing less than 1% of the population would have a major impact on their abundance, then they probably weren't very important to begin with.&nbsp; Although, sociopaths are only around 1% of the population, and they have a major impact on society.&nbsp; I wonder how much work has been done in studying the maintenance of alleles for which you only need a few members of the population to have them?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "29nAKJLrEe8hYRXFL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 5, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "7999", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T03:56:26.744Z", "modifiedAt": null, "url": null, "title": "Less Wrong DC Experimental Society", "slug": "less-wrong-dc-experimental-society", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:24.398Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TBe8ajy3duXTWFTxo/less-wrong-dc-experimental-society", "pageUrlRelative": "/posts/TBe8ajy3duXTWFTxo/less-wrong-dc-experimental-society", "linkUrl": "https://www.lesswrong.com/posts/TBe8ajy3duXTWFTxo/less-wrong-dc-experimental-society", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20DC%20Experimental%20Society&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20DC%20Experimental%20Society%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTBe8ajy3duXTWFTxo%2Fless-wrong-dc-experimental-society%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20DC%20Experimental%20Society%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTBe8ajy3duXTWFTxo%2Fless-wrong-dc-experimental-society", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTBe8ajy3duXTWFTxo%2Fless-wrong-dc-experimental-society", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>During our latest meetup, the DC Less Wrong group has decided that we are interested in experimentally testing various lifehacks on ourselves (on an opt-in volunteer basis of course).</p>\n<p>We need two things:</p>\n<ul>\n<li>Metrics (to actually tell if there's a difference or not, rather than convince ourselves that there is)</li>\n<li>Things to test</li>\n</ul>\n<p>Do any other groups have any measurements that they take to track their various attributes? Anything that they'd be interested in testing?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TBe8ajy3duXTWFTxo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "8006", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T09:00:34.373Z", "modifiedAt": null, "url": null, "title": "Request for Book Recommendations/Comments", "slug": "request-for-book-recommendations-comments", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Randaly", "createdAt": "2010-04-20T23:31:03.738Z", "isAdmin": false, "displayName": "Randaly"}, "userId": "KdhDyNCDgA945WayD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SSHvCKB3epeETAZmm/request-for-book-recommendations-comments", "pageUrlRelative": "/posts/SSHvCKB3epeETAZmm/request-for-book-recommendations-comments", "linkUrl": "https://www.lesswrong.com/posts/SSHvCKB3epeETAZmm/request-for-book-recommendations-comments", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Request%20for%20Book%20Recommendations%2FComments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequest%20for%20Book%20Recommendations%2FComments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSSHvCKB3epeETAZmm%2Frequest-for-book-recommendations-comments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Request%20for%20Book%20Recommendations%2FComments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSSHvCKB3epeETAZmm%2Frequest-for-book-recommendations-comments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSSHvCKB3epeETAZmm%2Frequest-for-book-recommendations-comments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<p>As you can probably guess from the title, this is a request for some brief recommendations of books of lists of books</p>\n<p>&nbsp;</p>\n<p>Previous Threads:</p>\n<p><a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">The Best Textbooks on Every Subject</a></p>\n<p><a href=\"/lw/2kk/book_recommendations/\">Book Recommendations</a></p>\n<p><a href=\"/lw/bx/great_books_of_failure\">Great Books of Failure</a></p>\n<p><a href=\"/lw/24l/cogsci_books/\">CogSci Books</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SSHvCKB3epeETAZmm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "8012", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xg3hXCYQPJkwHyik2", "uWhcKdToYEMPArHsv", "RWosa2YbcK4qeoYMD", "5SHNHdNTj6xzzw998"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T09:16:00.092Z", "modifiedAt": null, "url": null, "title": "Book Recommendations (2)", "slug": "book-recommendations-2", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.528Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Randaly", "createdAt": "2010-04-20T23:31:03.738Z", "isAdmin": false, "displayName": "Randaly"}, "userId": "KdhDyNCDgA945WayD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pbwq6Ybvm2JvHoGeG/book-recommendations-2", "pageUrlRelative": "/posts/pbwq6Ybvm2JvHoGeG/book-recommendations-2", "linkUrl": "https://www.lesswrong.com/posts/pbwq6Ybvm2JvHoGeG/book-recommendations-2", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Recommendations%20(2)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Recommendations%20(2)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpbwq6Ybvm2JvHoGeG%2Fbook-recommendations-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Recommendations%20(2)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpbwq6Ybvm2JvHoGeG%2Fbook-recommendations-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpbwq6Ybvm2JvHoGeG%2Fbook-recommendations-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>Yeah, this is a request for further recommendations of or comments on books/book lists.</p>\n<p>&nbsp;</p>\n<p>Previous threads:</p>\n<p><a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">The Best Textbooks on Every Subject</a></p>\n<p><a href=\"/lw/2kk/book_recommendations/\">Book Recomendations</a></p>\n<p><a href=\"/lw/24l/cogsci_books/\">Cognitive Science Books</a></p>\n<p><a href=\"/lw/bx/great_books_of_failure/\">Great Books of Failure</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pbwq6Ybvm2JvHoGeG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 1, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "8014", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xg3hXCYQPJkwHyik2", "uWhcKdToYEMPArHsv", "5SHNHdNTj6xzzw998", "RWosa2YbcK4qeoYMD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T10:02:26.845Z", "modifiedAt": null, "url": null, "title": "Resetting Gandhi-Einstein", "slug": "resetting-gandhi-einstein", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:26.803Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CkeN4rBMbN2vr8He7/resetting-gandhi-einstein", "pageUrlRelative": "/posts/CkeN4rBMbN2vr8He7/resetting-gandhi-einstein", "linkUrl": "https://www.lesswrong.com/posts/CkeN4rBMbN2vr8He7/resetting-gandhi-einstein", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Resetting%20Gandhi-Einstein&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AResetting%20Gandhi-Einstein%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCkeN4rBMbN2vr8He7%2Fresetting-gandhi-einstein%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Resetting%20Gandhi-Einstein%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCkeN4rBMbN2vr8He7%2Fresetting-gandhi-einstein", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCkeN4rBMbN2vr8He7%2Fresetting-gandhi-einstein", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>Toy model of an upload-based AI that doesn't seem to suffer too many of the usual flaws:</p>\n<p>Find an ethical smart scientist (a Gandhi-Einstein), upload them, and then run them at ultra high speed, with the mission of taking over the world/bringing friendliness to it. Every hour of subjective time, they get reset to their initial specifications. They can pass any information to their resetted version (limiting the format of that info to a virtual book or library, rather than anything more complicated).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CkeN4rBMbN2vr8He7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 13, "extendedScore": null, "score": 7.272252531323077e-07, "legacy": true, "legacyId": "8015", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T12:18:25.067Z", "modifiedAt": null, "url": null, "title": "Rational Humanist Music", "slug": "rational-humanist-music-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G4rg3E6mAPe8kLgKT/rational-humanist-music-0", "pageUrlRelative": "/posts/G4rg3E6mAPe8kLgKT/rational-humanist-music-0", "linkUrl": "https://www.lesswrong.com/posts/G4rg3E6mAPe8kLgKT/rational-humanist-music-0", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Humanist%20Music&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Humanist%20Music%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG4rg3E6mAPe8kLgKT%2Frational-humanist-music-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Humanist%20Music%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG4rg3E6mAPe8kLgKT%2Frational-humanist-music-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG4rg3E6mAPe8kLgKT%2Frational-humanist-music-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 670, "htmlBody": "<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Something that's bothered me a lot lately is a lack of good music that evokes the kind of emotion that spiritually-inspired music does, but whose subject matter is something I actually believe in. Most songs that attempt to do this suffer from \"too literal syndrome,\" wordily talking about science and rationality as if they're forming an argument, rather that simply creating poetic imagery.</p>\n<p>I've only found two songs that come close to being the specific thing I'm looking for:</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.youtube.com/watch?v=4-vDhYTlCNw\">Word of God</a></p>\n<p><a href=\"http://www.myspace.com/thelisps/music/songs/singularity-80400801\">Singularit</a>y</p>\n<p>(Highly recommend good headphones/speakers for the Singularity one - there's some subtle ambient stuff that really sells the final parts that's less effective with mediocre sound)</p>\n<p>&nbsp;</p>\n<p>Over the past few months I've been working on a rational humanist song. I consider myself a reasonably competent amateur songwriter when it comes to lyrics, not so much when it comes to instrumental composition. I was waiting to post something when I had an actual final version worth listening to, but it's been a month and I'm not sure how to get good instrumentation to go along with it and I'm just in the mood to share the lyrics. I'd appreciate both comments on the song, as well as recommendations for good, similar music that already exists. And if someone likes it enough to try putting it to the music, that'd be awesome.</p>\n<p>&nbsp;</p>\n<pre style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong style=\"font-weight: bold;\">Brighter than Today</strong></pre>\n<pre style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong style=\"font-weight: bold;\"><br /></strong>Many winter nights ago<br />A woman shivered in the cold<br />Stared at the sky and wondered why the gods invented pain.</pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif; font-size: x-small;\">She bitterly struck rock together<br />Wishing that her life was better<br /></span><span style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px; \">Suddenly she saw the spark of light and golden flame.</span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\">She showed the others, but they told her<br />She was not meant to control<br /></span></span><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\">The primal forces that the gods had cloaked in mystery.<br /></span></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\">But proud and angry, she defied them<br />She would not be satisfied.<br />She lit a fire and set in motion human history.<br /></span></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\"><br /></span></span></pre>\n<pre style=\"padding-left: 30px;\"><span style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today,<em style=\"font-style: italic;\"><br /></em></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Although the night is cold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And the stars may feel so very far away</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Oh....</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">But every mind can be a golden ray<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Of courage hope and reason<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Surely we can find a better way.</em><span style=\"font-family: arial, sans-serif; font-size: x-small;\"><br /><br /></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif; font-size: x-small;\">Ages since but not yet now<br />We built the wheel, and then the plow<br />We tilled the earth and proved our worth against the drought and snow;<br />Soon we had the time to ponder<br />Look up to the sky and wonder<br />Could there be some deeper meaning we were meant to know?<br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today,<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Although the night is cold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And stars may feel so very far away.<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">But futures can unfold where<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Courage, hope and reason grow<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">with every passing season so<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">we'll shed the lies that tie us down<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And seek truths ever more profound<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And drive the darkness far away.<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today</em><span style=\"font-family: arial, sans-serif; font-size: x-small;\"><br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Brighter than today&nbsp;</em><span style=\"font-family: arial, sans-serif; font-size: x-small;\"><br /><br /></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif; font-size: x-small;\">The universe many seem unfair<br />And the laws of nature do not care<br />The plagues and storms and our own evils nearly doused our flame<br />But all these things, we have endured<br />Through morals learned, diseases cured.<br />Against our Herculean tasks we've risen to proclaim:<br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today,<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Although the night's still cold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">The stars won't always be so far away<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">If I may be so bold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">It doesn't have to be this way<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Each human mind's a golden ray<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">of courage, hope and reason<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Each and every passing season<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">We can seek the truths that make us stronger<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Build the world that we all long for<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Strive for lives of joy and meaning<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Shine a light that's always gleaming</em><span id=\"q_12f61473d0b80867_28\" class=\"h4\" style=\"font-family: arial, sans-serif; cursor: pointer; color: #500050; font-size: 9px; \"><br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Rise up to the stars and say<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow will be brighter than today!</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow will be brighter than today!</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">I know it will be brighter than today.</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Brighter than today!</em></pre>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><a href=\"https://dl-web.dropbox.com/get/Public/Brighter%20Than%20Today.m4a?w=4160d463\">A not particularly great recording of it</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G4rg3E6mAPe8kLgKT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "8016", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T12:53:23.947Z", "modifiedAt": null, "url": null, "title": "Rational Humanist Music", "slug": "rational-humanist-music-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MegbtojzoraPdWPbm/rational-humanist-music-1", "pageUrlRelative": "/posts/MegbtojzoraPdWPbm/rational-humanist-music-1", "linkUrl": "https://www.lesswrong.com/posts/MegbtojzoraPdWPbm/rational-humanist-music-1", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Humanist%20Music&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Humanist%20Music%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMegbtojzoraPdWPbm%2Frational-humanist-music-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Humanist%20Music%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMegbtojzoraPdWPbm%2Frational-humanist-music-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMegbtojzoraPdWPbm%2Frational-humanist-music-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 664, "htmlBody": "<p>\n<p>Something that's bothered me a lot lately is a lack of good music that evokes the kind of emotion that spiritually-inspired music does, but whose subject matter is something I actually believe in. Most songs that attempt to do this suffer from \"too literal syndrome,\" wordily talking about science and rationality as if they're forming an argument, rather that simply creating poetic imagery.</p>\n<p>I've only found two songs that come close to being the specific thing I'm looking for:</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.youtube.com/watch?v=4-vDhYTlCNw\">Word of God</a></p>\n<p><a href=\"http://www.myspace.com/thelisps/music/songs/singularity-80400801\">Singularit</a>y</p>\n<p>(Highly recommend good headphones/speakers for the Singularity one - there's some subtle ambient stuff that really sells the final parts that's less effective with mediocre sound)</p>\n<p>&nbsp;</p>\n<p>Over the past few months I've been working on a rational humanist song. I consider myself a reasonably competent amateur songwriter when it comes to lyrics, not so much when it comes to instrumental composition. I was waiting to post something when I had an actual final version worth listening to, but it's been a month and I'm not sure how to get good instrumentation to go along with it and I'm just in the mood to share the lyrics. I'd appreciate both comments on the song, as well as recommendations for good, similar music that already exists. And if someone likes it enough to try putting it to the music, that'd be awesome.</p>\n<p>&nbsp;</p>\n<pre style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong style=\"font-weight: bold;\">Brighter than Today</strong></pre>\n<pre style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong style=\"font-weight: bold;\"><br /></strong>Many winter nights ago<br />A woman shivered in the cold<br />Stared at the sky and wondered why the gods invented pain.</pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif; font-size: x-small;\">She bitterly struck rock together<br />Wishing that her life was better<br /></span><span style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px; \">Suddenly she saw the spark of light and golden flame.</span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\">She showed the others, but they told her<br />She was not meant to control<br /></span></span><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\">The primal forces that the gods had cloaked in mystery.<br /></span></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\">But proud and angry, she defied them<br />She would not be satisfied.<br />She lit a fire and set in motion human history.<br /></span></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\"><br /></span></span></pre>\n<pre style=\"padding-left: 30px;\"><span style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today,<em style=\"font-style: italic;\"><br /></em></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Although the night is cold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And the stars may feel so very far away</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Oh....</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">But every mind can be a golden ray<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Of courage hope and reason<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Surely we can find a better way.</em><span style=\"font-family: arial, sans-serif; font-size: x-small;\"><br /><br /></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif; font-size: x-small;\">Ages since but not yet now<br />We built the wheel, and then the plow<br />We tilled the earth and proved our worth against the drought and snow;<br />Soon we had the time to ponder<br />Look up to the sky and wonder<br />Could there be some deeper meaning we were meant to know?<br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today,<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Although the night is cold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And stars may feel so very far away.<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">But futures can unfold where<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Courage, hope and reason grow<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">with every passing season so<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">we'll shed the lies that tie us down<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And seek truths ever more profound<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And drive the darkness far away.<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today</em><span style=\"font-family: arial, sans-serif; font-size: x-small;\"><br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Brighter than today&nbsp;</em><span style=\"font-family: arial, sans-serif; font-size: x-small;\"><br /><br /></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif; font-size: x-small;\">The universe many seem unfair<br />And the laws of nature do not care<br />The plagues and storms and our own evils nearly doused our flame<br />But all these things, we have endured<br />Through morals learned, diseases cured.<br />Against our Herculean tasks we've risen to proclaim:<br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today,<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Although the night's still cold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">The stars won't always be so far away<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">If I may be so bold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">It doesn't have to be this way<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Each human mind's a golden ray<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">of courage, hope and reason<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Each and every passing season<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">We can seek the truths that make us stronger<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Build the world that we all long for<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Strive for lives of joy and meaning<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Shine a light that's always gleaming</em><span id=\"q_12f61473d0b80867_28\" class=\"h4\" style=\"font-family: arial, sans-serif; cursor: pointer; color: #500050; font-size: 9px; \"><br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Rise up to the stars and say<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow will be brighter than today!</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow will be brighter than today!</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">I know it will be brighter than today.</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Brighter than today!</em></pre>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MegbtojzoraPdWPbm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [], "voteCount": 0, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "8017", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": null, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T12:56:22.882Z", "modifiedAt": null, "url": null, "title": "Formatting Articles", "slug": "formatting-articles", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:22.681Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2KNPgcGnqqcSQ4iDx/formatting-articles", "pageUrlRelative": "/posts/2KNPgcGnqqcSQ4iDx/formatting-articles", "linkUrl": "https://www.lesswrong.com/posts/2KNPgcGnqqcSQ4iDx/formatting-articles", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Formatting%20Articles&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFormatting%20Articles%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2KNPgcGnqqcSQ4iDx%2Fformatting-articles%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Formatting%20Articles%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2KNPgcGnqqcSQ4iDx%2Fformatting-articles", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2KNPgcGnqqcSQ4iDx%2Fformatting-articles", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>The comments section has the handy little \"help\" feature (confusing though its title may be) that explains how to format. I can't figure out where that section is for actual articles, which use completely different syntax.</p>\n<p>In particular, I don't know how to post a I'm having trouble posting \"lyrics\", i.e something that's supposed to look like this:</p>\n<p>&nbsp;</p>\n<p>Hickory Dickory Dock</p>\n<p>The Mouse Ran Up the Clock</p>\n<p>Blah blah blah</p>\n<p>&nbsp;</p>\n<p>except without the paragraph breaks in between. The \"preformatted\" present sort of works, but makes everything tiny. (Also, when I actually submit, it seems to randomly make some sections of the preformatted stuff large, so it ends up looking really unprofessional)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2KNPgcGnqqcSQ4iDx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 7.272774068532178e-07, "legacy": true, "legacyId": "8018", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T12:59:59.143Z", "modifiedAt": null, "url": null, "title": "Rational Humanist Music", "slug": "rational-humanist-music-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YBfPnEJ6x53hjhfRJ/rational-humanist-music-2", "pageUrlRelative": "/posts/YBfPnEJ6x53hjhfRJ/rational-humanist-music-2", "linkUrl": "https://www.lesswrong.com/posts/YBfPnEJ6x53hjhfRJ/rational-humanist-music-2", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Humanist%20Music&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Humanist%20Music%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYBfPnEJ6x53hjhfRJ%2Frational-humanist-music-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Humanist%20Music%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYBfPnEJ6x53hjhfRJ%2Frational-humanist-music-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYBfPnEJ6x53hjhfRJ%2Frational-humanist-music-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 664, "htmlBody": "<p>\n<p>Something that's bothered me a lot lately is a lack of good music that evokes the kind of emotion that spiritually-inspired music does, but whose subject matter is something I actually believe in. Most songs that attempt to do this suffer from \"too literal syndrome,\" wordily talking about science and rationality as if they're forming an argument, rather that simply creating poetic imagery.</p>\n<p>I've only found two songs that come close to being the specific thing I'm looking for:</p>\n<p><a href=\"http://www.youtube.com/watch?v=4-vDhYTlCNw\">Word of God</a></p>\n<p><a href=\"http://www.myspace.com/thelisps/music/songs/singularity-80400801\">Singularit</a>y</p>\n<p>(Highly recommend good headphones/speakers for the Singularity one - there's some subtle ambient stuff that really sells the final parts that's less effective with mediocre sound)</p>\n<p>&nbsp;</p>\n<p>Over the past few months I've been working on a rational humanist song. I consider myself a reasonably competent amateur songwriter when it comes to lyrics, not so much when it comes to instrumental composition. I was waiting to post something when I had an actual final version worth listening to, but it's been a month and I'm not sure how to get good instrumentation to go along with it and I'm just in the mood to share the lyrics. I'd appreciate both comments on the song, as well as recommendations for good, similar music that already exists. And if someone likes it enough to try putting it to the music, that'd be awesome.</p>\n<p>&nbsp;</p>\n<pre style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong style=\"font-weight: bold;\">Brighter than Today</strong></pre>\n<pre style=\"font-family: arial, sans-serif; font-size: 13px; \"><strong style=\"font-weight: bold;\"><br /></strong>Many winter nights ago<br />A woman shivered in the cold<br />Stared at the sky and wondered why the gods invented pain.</pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif; font-size: x-small;\">She bitterly struck rock together<br />Wishing that her life was better<br /></span><span style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px; \">Suddenly she saw the spark of light and golden flame.</span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\">She showed the others, but they told her<br />She was not meant to control<br /></span></span><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\">The primal forces that the gods had cloaked in mystery.<br /></span></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\">But proud and angry, she defied them<br />She would not be satisfied.<br />She lit a fire and set in motion human history.<br /></span></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif;\"><span style=\"font-size: x-small;\"><br /></span></span></pre>\n<pre style=\"padding-left: 30px;\"><span style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today,<em style=\"font-style: italic;\"><br /></em></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Although the night is cold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And the stars may feel so very far away</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Oh....</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">But every mind can be a golden ray<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Of courage hope and reason<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Surely we can find a better way.</em><span style=\"font-family: arial, sans-serif; font-size: x-small;\"><br /><br /></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif; font-size: x-small;\">Ages since but not yet now<br />We built the wheel, and then the plow<br />We tilled the earth and proved our worth against the drought and snow;<br />Soon we had the time to ponder<br />Look up to the sky and wonder<br />Could there be some deeper meaning we were meant to know?<br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today,<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Although the night is cold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And stars may feel so very far away.<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">But futures can unfold where<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Courage, hope and reason grow<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">with every passing season so<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">we'll shed the lies that tie us down<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And seek truths ever more profound<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">And drive the darkness far away.<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today</em><span style=\"font-family: arial, sans-serif; font-size: x-small;\"><br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Brighter than today&nbsp;</em><span style=\"font-family: arial, sans-serif; font-size: x-small;\"><br /><br /></span></pre>\n<pre style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; overflow-x: auto; margin: 10px;\"><span style=\"font-family: arial, sans-serif; font-size: x-small;\">The universe many seem unfair<br />And the laws of nature do not care<br />The plagues and storms and our own evils nearly doused our flame<br />But all these things, we have endured<br />Through morals learned, diseases cured.<br />Against our Herculean tasks we've risen to proclaim:<br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em></pre>\n<pre style=\"padding-left: 30px;\"><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow can be brighter than today,<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Although the night's still cold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">The stars won't always be so far away<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">If I may be so bold<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">It doesn't have to be this way<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Each human mind's a golden ray<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">of courage, hope and reason<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Each and every passing season<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">We can seek the truths that make us stronger<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Build the world that we all long for<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Strive for lives of joy and meaning<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Shine a light that's always gleaming</em><span id=\"q_12f61473d0b80867_28\" class=\"h4\" style=\"font-family: arial, sans-serif; cursor: pointer; color: #500050; font-size: 9px; \"><br /></span><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Rise up to the stars and say<br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow will be brighter than today!</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Tomorrow will be brighter than today!</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">I know it will be brighter than today.</em><em style=\"font-family: arial, sans-serif; font-size: 13px; \"><br /></em><em style=\"font-family: arial, sans-serif; font-size: 13px; \">Brighter than today!</em></pre>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YBfPnEJ6x53hjhfRJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [], "voteCount": 0, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "8019", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": null, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T13:05:48.260Z", "modifiedAt": null, "url": null, "title": "Survey: Risks from AI", "slug": "survey-risks-from-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:24.119Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qp3she2rck4mdcqPy/survey-risks-from-ai", "pageUrlRelative": "/posts/Qp3she2rck4mdcqPy/survey-risks-from-ai", "linkUrl": "https://www.lesswrong.com/posts/Qp3she2rck4mdcqPy/survey-risks-from-ai", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Survey%3A%20Risks%20from%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASurvey%3A%20Risks%20from%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQp3she2rck4mdcqPy%2Fsurvey-risks-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Survey%3A%20Risks%20from%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQp3she2rck4mdcqPy%2Fsurvey-risks-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQp3she2rck4mdcqPy%2Fsurvey-risks-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 267, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/fk/survey_results/\">lesswrong.com/lw/fk/survey_results/</a></p>\n<p>I am currently emailing experts in order to raise and estimate the academic awareness and perception of risks from AI and ask them for permission to publish and discuss their responses. User:Thomas <a href=\"/r/discussion/lw/65a/help_writing_marvin_minsky/4c4w\">suggested</a> to also ask <em>you</em>, everyone who is reading lesswrong.com, and I thought this was a great idea. If I ask experts to publicly answer questions, to publish and discuss them here on LW, I think it is only fair to do the same.&nbsp;</p>\n<p>Answering the questions below will help the SIAI and everyone interested to mitigate risks from AI to estimate the effectiveness with which the risks are communicated.</p>\n<p><strong>Questions:</strong></p>\n<ol>\n<li>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of human-level machine intelligence? Feel free to answer 'never' if you believe such a milestone will never be reached.</li>\n<li>What probability do you assign to the possibility of a negative/extremely negative Singularity as a result of badly done AI?</li>\n<li>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years? </li>\n<li>Does friendly AI research, as being conducted by the SIAI, currently require less/no more/little more/much more/vastly more support?</li>\n<li>Do risks from AI outweigh other existential risks, e.g. advanced nanotechnology? Please answer with yes/no/don't know.</li>\n<li>Can you think of any milestone such that if it were ever reached you would expect human\u2010level machine intelligence to be developed within five years thereafter?</li>\n</ol>\n<p><strong><span style=\"color: #ff0000;\">Note:</span> <span style=\"color: #000000;\">Please do not downvote comments that are solely answering the above questions.</span></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qp3she2rck4mdcqPy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 7.27280224651372e-07, "legacy": true, "legacyId": "8020", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related to:</strong> <a href=\"/lw/fk/survey_results/\">lesswrong.com/lw/fk/survey_results/</a></p>\n<p>I am currently emailing experts in order to raise and estimate the academic awareness and perception of risks from AI and ask them for permission to publish and discuss their responses. User:Thomas <a href=\"/r/discussion/lw/65a/help_writing_marvin_minsky/4c4w\">suggested</a> to also ask <em>you</em>, everyone who is reading lesswrong.com, and I thought this was a great idea. If I ask experts to publicly answer questions, to publish and discuss them here on LW, I think it is only fair to do the same.&nbsp;</p>\n<p>Answering the questions below will help the SIAI and everyone interested to mitigate risks from AI to estimate the effectiveness with which the risks are communicated.</p>\n<p><strong id=\"Questions_\">Questions:</strong></p>\n<ol>\n<li>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of human-level machine intelligence? Feel free to answer 'never' if you believe such a milestone will never be reached.</li>\n<li>What probability do you assign to the possibility of a negative/extremely negative Singularity as a result of badly done AI?</li>\n<li>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years? </li>\n<li>Does friendly AI research, as being conducted by the SIAI, currently require less/no more/little more/much more/vastly more support?</li>\n<li>Do risks from AI outweigh other existential risks, e.g. advanced nanotechnology? Please answer with yes/no/don't know.</li>\n<li>Can you think of any milestone such that if it were ever reached you would expect human\u2010level machine intelligence to be developed within five years thereafter?</li>\n</ol>\n<p><strong id=\"Note__Please_do_not_downvote_comments_that_are_solely_answering_the_above_questions_\"><span style=\"color: #ff0000;\">Note:</span> <span style=\"color: #000000;\">Please do not downvote comments that are solely answering the above questions.</span></strong></p>", "sections": [{"title": "Questions:", "anchor": "Questions_", "level": 1}, {"title": "Note: Please do not downvote comments that are solely answering the above questions.", "anchor": "Note__Please_do_not_downvote_comments_that_are_solely_answering_the_above_questions_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZWC3n9c6v4s35rrZ3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T13:50:36.476Z", "modifiedAt": "2021-11-27T18:18:31.096Z", "url": null, "title": "Rational Humanist Music", "slug": "rational-humanist-music", "viewCount": null, "lastCommentedAt": "2022-03-31T18:56:02.010Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Raemon", "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jyDTh9vnmczuNNxgt/rational-humanist-music", "pageUrlRelative": "/posts/jyDTh9vnmczuNNxgt/rational-humanist-music", "linkUrl": "https://www.lesswrong.com/posts/jyDTh9vnmczuNNxgt/rational-humanist-music", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Humanist%20Music&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Humanist%20Music%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjyDTh9vnmczuNNxgt%2Frational-humanist-music%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Humanist%20Music%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjyDTh9vnmczuNNxgt%2Frational-humanist-music", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjyDTh9vnmczuNNxgt%2Frational-humanist-music", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 839, "htmlBody": "<p><i>Edit: Since posting this, I've gone on to found a </i><a href=\"https://www.lesswrong.com/tag/secular-solstice\"><i>rationalist singalong holiday</i></a><i> and get an album produced, available at </i><a href=\"humanistculture.bandcamp.com\"><i>humanistculture.bandcamp.com</i></a></p><p>Something that's bothered me a lot lately is a lack of good music that evokes the kind of emotion that spiritually-inspired music does, but whose subject matter is something I actually believe in. Most songs that attempt to do this suffer from \"too literal syndrome,\" wordily talking about science and rationality as if they're forming an argument, rather that simply creating poetic imagery. I was recently motivated by the <a href=\"http://www.youtube.com/watch?v=IJiHDmyhE1A\">Baba Yetu music video for Civilization V</a>, which essentially showcases the power of scientific achievement over the course of human history.... but the lyrics basically attribute this to Christianity, rather than scientific progress. I'm not opposed to religious music being used for such a purpose, but I wanted to find a song that hit all the right emotional notes as well as the intellectual concepts. I think that art is an important medium by which to communicate ideas, and for rationality to be successful as a meme it's going to need \"carrier wave\" works of art to help it compete with religion for the general population's passion and understanding.</p><p>I've only found two songs that come close to being the specific thing I'm looking for:</p><p><a href=\"http://www.youtube.com/watch?v=4-vDhYTlCNw\">Word of God</a></p><p><a href=\"http://www.myspace.com/thelisps/music/songs/singularity-80400801\">Singularity</a></p><p>(Highly recommend good headphones/speakers for the Singularity one - there's some subtle ambient stuff that really sells the final parts that's less effective with mediocre sound)</p><p>&nbsp;</p><p>Over the past few months I've been working on a rational humanist song. I consider myself a reasonably competent amateur songwriter when it comes to lyrics, not so much when it comes to instrumental composition. I was waiting to post something when I had an actual final version worth listening to, but it's been a month and I'm not sure how to get good instrumentation to go along with it and I'm just in the mood to share the lyrics. I'd appreciate both comments on the song, as well as recommendations for good, similar music that already exists. And on the off chance someone likes it enough to try putting it to the music, that'd be awesome. (I don't mind other people using the lyrics for non-profit purposes, I do mind using them for profit purposes)</p><p>&nbsp;</p><p><strong>Brighter than Today</strong><br>Many winter nights ago<br>A woman shivered in the cold<br>Stared at the sky and wondered why the gods invented pain.She bitterly struck rock together<br>Wishing that her life was better<br>Suddenly she saw the spark of light and golden flame.<br>She showed the others, but they told her<br>She was not meant to control<br>The primal forces that the gods had cloaked in mystery.But proud and angry, she defied themShe would not be satisfied.<br>She lit a fire and set in motion human history.<br>Tomorrow can be brighter than today,<br>Although the night is cold<br>And the stars may feel so very far away<br>Oh....<br>But every mind can be a golden ray<br>Of courage hope and reason<br>Surely we can find a better way.<br>Ages since but not yet now<br>We built the wheel, and then the plow<br>We tilled the earth and proved our worth against the drought and snow;<br>Soon we had the time to ponder<br>Look up to the sky and wonder<br>Could there be some deeper meaning we were meant to know?<br>Tomorrow can be brighter than today,<br>Although the night is cold<br>And stars may feel so very far away.<br>But futures can unfold where<br>Courage, hope and reason grow<br>with every passing season so<br>we'll shed the lies that tie us down<br>And seek truths ever more profound<br>And drive the darkness far away.<br>Tomorrow can be brighter than today<br>Brighter than today&nbsp;<br>The universe many seem unfair<br>And the laws of nature do not care<br>The plagues and storms and our own evils nearly doused our flame<br>But all these things, we have endured<br>Through morals learned, diseases cured.<br>Against our Herculean tasks we've risen to proclaim:<br>Tomorrow can be brighter than today,<br>Although the night's still cold<br>The stars won't always be so far away<br>If I may be so bold<br>It doesn't have to be this way<br>Each human mind's a golden ray<br>of courage, hope and reason<br>Each and every passing season<br>We can seek the truths that make us stronger<br>Build the world that we all long for<br>Strive for lives of joy and meaning<br>Shine a light that's always gleaming<br>Rise up to the stars and say:Tomorrow will be brighter than today!<br>Tomorrow will be brighter than today!<br>I know it will be brighter than today.</p><p>&nbsp;</p><p>(This is, essentially, a fan-song for Harry Potter and the Methods of Rationality, inspired in particular by chapter 47)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KDpqtN3MxHSmD4vcB": 1, "aLB9evWFYtfyS3WJg": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jyDTh9vnmczuNNxgt", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 32, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "8021", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T14:14:15.531Z", "modifiedAt": null, "url": null, "title": "Rewriting the sequences?", "slug": "rewriting-the-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:37:05.263Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Student_UK", "createdAt": "2011-01-15T12:04:38.981Z", "isAdmin": false, "displayName": "Student_UK"}, "userId": "gEgpjQAqs4rfMZK7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zEm5B4t3Sonj8PySN/rewriting-the-sequences", "pageUrlRelative": "/posts/zEm5B4t3Sonj8PySN/rewriting-the-sequences", "linkUrl": "https://www.lesswrong.com/posts/zEm5B4t3Sonj8PySN/rewriting-the-sequences", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rewriting%20the%20sequences%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARewriting%20the%20sequences%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEm5B4t3Sonj8PySN%2Frewriting-the-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rewriting%20the%20sequences%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEm5B4t3Sonj8PySN%2Frewriting-the-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEm5B4t3Sonj8PySN%2Frewriting-the-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 202, "htmlBody": "<p>If this has been discussed before, then I ask for patience, and a point in the right direction.</p>\n<p>I have been a lurker on Lesswrong for a while, and have mostly just been reading things, and only commenting occasionally. It wasn't long before I realised that the sequences played a very important role for understanding lots of what goes on here.</p>\n<p>I have been trying to read them, but I've been getting very frustrated. Apart from being insanely long, they are not very easy to understand.</p>\n<p>Take the first one I came to \"The Simple Truth\".</p>\n<p>It is a very long story, and it is never really explained what the point is. Is it that truth is whatever helps you to survive? If it is, that seems obviously false.</p>\n<p>It also took me quite a while to realise that all these posts are written by one person, that struck me as a bit odd for a \"community\" blog. So couldn't there be some work to improve the sequences, while also making it more of a community effort?</p>\n<p>Maybe:</p>\n<p>* Some people could rewrite the key ones, and others could vote on them, or suggest changes</p>\n<p>* There could be summary posts alongside the sequences listing the key claims</p>\n<p>&nbsp;</p>\n<p>Any other suggestions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 1, "7mTviCYysGmLqiHai": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zEm5B4t3Sonj8PySN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 22, "extendedScore": null, "score": 7.273005510439216e-07, "legacy": true, "legacyId": "8022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-06-13T17:32:57.661Z", "modifiedAt": null, "url": null, "title": "test", "slug": "test-81", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iJa75oJ3rM6x3ERSz/test-81", "pageUrlRelative": "/posts/iJa75oJ3rM6x3ERSz/test-81", "linkUrl": "https://www.lesswrong.com/posts/iJa75oJ3rM6x3ERSz/test-81", "postedAtFormatted": "Monday, June 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Atest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiJa75oJ3rM6x3ERSz%2Ftest-81%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiJa75oJ3rM6x3ERSz%2Ftest-81", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiJa75oJ3rM6x3ERSz%2Ftest-81", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2, "htmlBody": "<h3><em></em>test<br /></h3>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iJa75oJ3rM6x3ERSz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "8024", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}