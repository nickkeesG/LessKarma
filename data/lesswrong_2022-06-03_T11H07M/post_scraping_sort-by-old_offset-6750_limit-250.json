{"results": [{"createdAt": null, "postedAt": "2012-06-06T18:57:27.904Z", "modifiedAt": null, "url": null, "title": "Building toward a Friendly AI team", "slug": "building-toward-a-friendly-ai-team", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:09.443Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p4Gd8pRcbnKo46hus/building-toward-a-friendly-ai-team", "pageUrlRelative": "/posts/p4Gd8pRcbnKo46hus/building-toward-a-friendly-ai-team", "linkUrl": "https://www.lesswrong.com/posts/p4Gd8pRcbnKo46hus/building-toward-a-friendly-ai-team", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Building%20toward%20a%20Friendly%20AI%20team&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuilding%20toward%20a%20Friendly%20AI%20team%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp4Gd8pRcbnKo46hus%2Fbuilding-toward-a-friendly-ai-team%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Building%20toward%20a%20Friendly%20AI%20team%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp4Gd8pRcbnKo46hus%2Fbuilding-toward-a-friendly-ai-team", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp4Gd8pRcbnKo46hus%2Fbuilding-toward-a-friendly-ai-team", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 897, "htmlBody": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p>A key part of SI's strategy for AI risk reduction is to build toward hosting a Friendly AI development team at the Singularity Institute.</p>\n<p>I don't take it to be <em>obvious</em>&nbsp;that an SI-hosted FAI team is the correct path toward the endgame of humanity \"winning.\" That is a matter for much strategic research and debate.</p>\n<p>Either way, I think that building <em>toward</em>&nbsp;an FAI team is good for AI risk reduction, even if we decide (later) that an SI-hosted FAI team is <em>not</em>&nbsp;the best thing to do. Why is this so?</p>\n<p>Building <em>toward</em>&nbsp;an SI-hosted FAI team means:</p>\n<ol>\n<li>Growing SI into a tighter, larger, and more effective organization in general.</li>\n<li>Attracting and creating people who are trustworthy, altruistic, hard-working, highly capable, extremely intelligent, and deeply concerned about AI risk. (We'll call these people \"superhero mathematicians.\")</li>\n</ol>\n<p>Both (1) and (2) are useful for AI risk reduction <em>even if an SI-hosted FAI team turns out not to be the best strategy</em>.</p>\n<p>This is because: Achieving part (1) would make SI more effective at <em>whatever</em>&nbsp;it is doing to reduce AI risk, and achieving part (2) would bring great human resources to the cause of AI risk reduction, which will be useful to a wide range of purposes (FAI team or otherwise).</p>\n<p>So, how do we accomplish both these things?</p>\n<p>&nbsp;</p>\n<h3>Growing SI into a better organization</h3>\n<p>Like many (most?) non-profits with less than $1m/yr in funding, SI has had <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">difficulty</a> attracting the top-level executive talent often required to build a highly efficient and effective organization. Luckily, we have made <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">rapid progress</a> on this front in the past 9 months. For example we now have (1) a comprehensive donor database, (2) a strategic plan, (3) a team of remote contractors used to more efficiently complete large and varied projects requiring many different skillsets, (4) an increasingly \"best practices\" implementation of central management, (5) an office we actually use to work together on projects, and many other improvements.</p>\n<p>What else can SI do to become a tighter, larger, and more effective organization?</p>\n<ol>\n<li>Hire a professional bookkeeper, implement additional bookkeeping and accounting best practices. (Currently underway.)</li>\n<li>Create a more navigable and up-to-date website. (Currently underway.)</li>\n<li>Improve our fundraising strategy, e.g. by creating a deck of slides for major donors which explains what we're doing and what we can do with more funding. (Currently underway.)</li>\n<li>Create standard policy documents that lower our risk of being distracted by an IRS audit. (Currently underway.)</li>\n<li>Shift the Singularity Summit toward being more directly useful for AI risk reduction, and also toward greater profitability&mdash;so that we have at least one funding source that is not donations. (Currently underway.)</li>\n<li>Spin off the Center for Applied Rationality so that SI is more solely focused on AI safety. (Currently underway.)</li>\n<li>Build a fundraising/investment-focused Board of Trustees (ala <a href=\"http://www.ias.edu/people/trustees\">IAS</a> or <a href=\"http://singularityu.org/community/board-of-trustees/\">SU</a>) in addition to our Board of Directors and Board of Advisors.</li>\n<li>Create an endowment to ensure ongoing funding for core researchers.</li>\n<li>Consult with the most relevant university department heads and experienced principal investigators (e.g. at&nbsp;<a href=\"http://www.ias.edu/\">IAS</a>&nbsp;and <a href=\"http://www.santafe.edu/\">Santa Fe</a>) about how to start and run an effective team for advanced technical research.</li>\n<li>Do the things recommended by these experts (that are relevant to SI's mission).</li>\n</ol>\n<p>They key point, of course, is that <em>all these things cost money</em>. They may be \"boring,\" but they are <em>incredibly important</em>.</p>\n<p>&nbsp;</p>\n<h3>Attracting and creating superhero mathematicians</h3>\n<p>The kind of people we'd need for an FAI team are:</p>\n<ol>\n<li>Highly intelligent, and especially skilled in maths, probably at the IMO medal-winning level. (FAI team members will need to create lots of new math during the course of the FAI research initiative.)</li>\n<li>Trustworthy. (Most FAI work is not \"Friendliness theory\" but instead AI architectures work that could be made more dangerous if released to a wider community that is less concerned with AI safety.)</li>\n<li>Altruistic. (Since the fate of humanity may be in their hands, they need to be robustly altruistic.)</li>\n<li>Hard-working, determined. (FAI is a very difficult research problem and will require lots of hard work and also an attitude of \"<a href=\"/lw/up/shut_up_and_do_the_impossible/\">shut up and do the impossible</a>.\")</li>\n<li>Deeply committed to AI risk reduction. (It would be risky to have people who could be pulled off the team&mdash;with all their potentially dangerous knowledge&mdash;by offers from hedge funds or Google.)</li>\n<li>Unusually rational. (To avoid philosophical confusions, to promote general effectiveness and group cohesion, and more.)</li>\n</ol>\n<p>There are other criteria, too, but those are some of the biggest.</p>\n<p>We can attract some of the people meeting these criteria by using the methods described in <a href=\"/r/discussion/lw/cs7/reaching_young_mathcompsci_talent/\">Reaching young math/compsci talent</a>. The trouble is that the number of people on Earth who qualify may be <em>very</em>&nbsp;close to 0 (especially given the \"committed to AI risk reduction\" criterion).</p>\n<p>Thus, we'll need to <em>create</em>&nbsp;some superhero mathematicians.</p>\n<p>Math ability seems to be even more \"fixed\" than the other criteria, so a (very rough) strategy for creating superhero mathematicians might look like this:</p>\n<ol>\n<li>Find people with the required level of math ability.</li>\n<li>Train them on AI risk and rationality.</li>\n<li>Focus on the few who become deeply committed to AI risk reduction and rationality.</li>\n<li>Select from among those people the ones who are most altruistic, trustworthy, hard-working, and determined. (Some training may be possible for these features, too.)</li>\n<li>Try them out for 3 months and select the best few candidates for the FAI team.</li>\n</ol>\n<p>All these steps, too, <em>cost money</em>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p4Gd8pRcbnKo46hus", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 37, "extendedScore": null, "score": 9.174367952438049e-07, "legacy": true, "legacyId": "16677", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p>A key part of SI's strategy for AI risk reduction is to build toward hosting a Friendly AI development team at the Singularity Institute.</p>\n<p>I don't take it to be <em>obvious</em>&nbsp;that an SI-hosted FAI team is the correct path toward the endgame of humanity \"winning.\" That is a matter for much strategic research and debate.</p>\n<p>Either way, I think that building <em>toward</em>&nbsp;an FAI team is good for AI risk reduction, even if we decide (later) that an SI-hosted FAI team is <em>not</em>&nbsp;the best thing to do. Why is this so?</p>\n<p>Building <em>toward</em>&nbsp;an SI-hosted FAI team means:</p>\n<ol>\n<li>Growing SI into a tighter, larger, and more effective organization in general.</li>\n<li>Attracting and creating people who are trustworthy, altruistic, hard-working, highly capable, extremely intelligent, and deeply concerned about AI risk. (We'll call these people \"superhero mathematicians.\")</li>\n</ol>\n<p>Both (1) and (2) are useful for AI risk reduction <em>even if an SI-hosted FAI team turns out not to be the best strategy</em>.</p>\n<p>This is because: Achieving part (1) would make SI more effective at <em>whatever</em>&nbsp;it is doing to reduce AI risk, and achieving part (2) would bring great human resources to the cause of AI risk reduction, which will be useful to a wide range of purposes (FAI team or otherwise).</p>\n<p>So, how do we accomplish both these things?</p>\n<p>&nbsp;</p>\n<h3 id=\"Growing_SI_into_a_better_organization\">Growing SI into a better organization</h3>\n<p>Like many (most?) non-profits with less than $1m/yr in funding, SI has had <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">difficulty</a> attracting the top-level executive talent often required to build a highly efficient and effective organization. Luckily, we have made <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">rapid progress</a> on this front in the past 9 months. For example we now have (1) a comprehensive donor database, (2) a strategic plan, (3) a team of remote contractors used to more efficiently complete large and varied projects requiring many different skillsets, (4) an increasingly \"best practices\" implementation of central management, (5) an office we actually use to work together on projects, and many other improvements.</p>\n<p>What else can SI do to become a tighter, larger, and more effective organization?</p>\n<ol>\n<li>Hire a professional bookkeeper, implement additional bookkeeping and accounting best practices. (Currently underway.)</li>\n<li>Create a more navigable and up-to-date website. (Currently underway.)</li>\n<li>Improve our fundraising strategy, e.g. by creating a deck of slides for major donors which explains what we're doing and what we can do with more funding. (Currently underway.)</li>\n<li>Create standard policy documents that lower our risk of being distracted by an IRS audit. (Currently underway.)</li>\n<li>Shift the Singularity Summit toward being more directly useful for AI risk reduction, and also toward greater profitability\u2014so that we have at least one funding source that is not donations. (Currently underway.)</li>\n<li>Spin off the Center for Applied Rationality so that SI is more solely focused on AI safety. (Currently underway.)</li>\n<li>Build a fundraising/investment-focused Board of Trustees (ala <a href=\"http://www.ias.edu/people/trustees\">IAS</a> or <a href=\"http://singularityu.org/community/board-of-trustees/\">SU</a>) in addition to our Board of Directors and Board of Advisors.</li>\n<li>Create an endowment to ensure ongoing funding for core researchers.</li>\n<li>Consult with the most relevant university department heads and experienced principal investigators (e.g. at&nbsp;<a href=\"http://www.ias.edu/\">IAS</a>&nbsp;and <a href=\"http://www.santafe.edu/\">Santa Fe</a>) about how to start and run an effective team for advanced technical research.</li>\n<li>Do the things recommended by these experts (that are relevant to SI's mission).</li>\n</ol>\n<p>They key point, of course, is that <em>all these things cost money</em>. They may be \"boring,\" but they are <em>incredibly important</em>.</p>\n<p>&nbsp;</p>\n<h3 id=\"Attracting_and_creating_superhero_mathematicians\">Attracting and creating superhero mathematicians</h3>\n<p>The kind of people we'd need for an FAI team are:</p>\n<ol>\n<li>Highly intelligent, and especially skilled in maths, probably at the IMO medal-winning level. (FAI team members will need to create lots of new math during the course of the FAI research initiative.)</li>\n<li>Trustworthy. (Most FAI work is not \"Friendliness theory\" but instead AI architectures work that could be made more dangerous if released to a wider community that is less concerned with AI safety.)</li>\n<li>Altruistic. (Since the fate of humanity may be in their hands, they need to be robustly altruistic.)</li>\n<li>Hard-working, determined. (FAI is a very difficult research problem and will require lots of hard work and also an attitude of \"<a href=\"/lw/up/shut_up_and_do_the_impossible/\">shut up and do the impossible</a>.\")</li>\n<li>Deeply committed to AI risk reduction. (It would be risky to have people who could be pulled off the team\u2014with all their potentially dangerous knowledge\u2014by offers from hedge funds or Google.)</li>\n<li>Unusually rational. (To avoid philosophical confusions, to promote general effectiveness and group cohesion, and more.)</li>\n</ol>\n<p>There are other criteria, too, but those are some of the biggest.</p>\n<p>We can attract some of the people meeting these criteria by using the methods described in <a href=\"/r/discussion/lw/cs7/reaching_young_mathcompsci_talent/\">Reaching young math/compsci talent</a>. The trouble is that the number of people on Earth who qualify may be <em>very</em>&nbsp;close to 0 (especially given the \"committed to AI risk reduction\" criterion).</p>\n<p>Thus, we'll need to <em>create</em>&nbsp;some superhero mathematicians.</p>\n<p>Math ability seems to be even more \"fixed\" than the other criteria, so a (very rough) strategy for creating superhero mathematicians might look like this:</p>\n<ol>\n<li>Find people with the required level of math ability.</li>\n<li>Train them on AI risk and rationality.</li>\n<li>Focus on the few who become deeply committed to AI risk reduction and rationality.</li>\n<li>Select from among those people the ones who are most altruistic, trustworthy, hard-working, and determined. (Some training may be possible for these features, too.)</li>\n<li>Try them out for 3 months and select the best few candidates for the FAI team.</li>\n</ol>\n<p>All these steps, too, <em>cost money</em>.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Growing SI into a better organization", "anchor": "Growing_SI_into_a_better_organization", "level": 1}, {"title": "Attracting and creating superhero mathematicians", "anchor": "Attracting_and_creating_superhero_mathematicians", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "96 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LrrHdt8HSvZFKmRb", "nCvvhFBaayaXyuBiD", "4dFzBkpjx6aHZpjqL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-06T19:57:50.836Z", "modifiedAt": null, "url": null, "title": "Poly marriage?", "slug": "poly-marriage", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:04.855Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "h-H", "createdAt": "2010-01-20T16:15:08.891Z", "isAdmin": false, "displayName": "h-H"}, "userId": "gtYfE7wvJFDrTYutu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5YATvuE5HbPJt3ADp/poly-marriage", "pageUrlRelative": "/posts/5YATvuE5HbPJt3ADp/poly-marriage", "linkUrl": "https://www.lesswrong.com/posts/5YATvuE5HbPJt3ADp/poly-marriage", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Poly%20marriage%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APoly%20marriage%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5YATvuE5HbPJt3ADp%2Fpoly-marriage%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Poly%20marriage%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5YATvuE5HbPJt3ADp%2Fpoly-marriage", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5YATvuE5HbPJt3ADp%2Fpoly-marriage", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<p>A thought occurred to me today as I skimmed an article in a rationality forum where the subject of gay marriage cropped up; seeing as the issue has been hotly contested in various public fora and especially the courts, what about poly? After all, many if not all the arguments for gay marriage apply to poly marriage as well.</p>\n<p>Questions for LWers who are currently in a such a relationship, or have an opinion to share:</p>\n<p>Do polies want to marry each other or do such relationships not lend themselves to permanence above a threshold of partners? Should polies campaign for the right for a civil union anyway? what are the up and down sides of this? etc</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5YATvuE5HbPJt3ADp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": -16, "extendedScore": null, "score": -3.3e-05, "legacy": true, "legacyId": "16731", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 128, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-06T23:27:38.654Z", "modifiedAt": null, "url": null, "title": "Mailing List for Digitized Belief Network Discussion", "slug": "mailing-list-for-digitized-belief-network-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "avichapman", "createdAt": "2012-04-30T05:38:19.163Z", "isAdmin": false, "displayName": "avichapman"}, "userId": "AkCycCQcKgvoCiDgu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wQrroyCgh5NCCMqHp/mailing-list-for-digitized-belief-network-discussion", "pageUrlRelative": "/posts/wQrroyCgh5NCCMqHp/mailing-list-for-digitized-belief-network-discussion", "linkUrl": "https://www.lesswrong.com/posts/wQrroyCgh5NCCMqHp/mailing-list-for-digitized-belief-network-discussion", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mailing%20List%20for%20Digitized%20Belief%20Network%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMailing%20List%20for%20Digitized%20Belief%20Network%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwQrroyCgh5NCCMqHp%2Fmailing-list-for-digitized-belief-network-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mailing%20List%20for%20Digitized%20Belief%20Network%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwQrroyCgh5NCCMqHp%2Fmailing-list-for-digitized-belief-network-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwQrroyCgh5NCCMqHp%2Fmailing-list-for-digitized-belief-network-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>Hi all,</p>\n<p>This is a follow-up to a previous post of mine - '<a href=\"/lw/cmg/a_digitized_belief_network/\">A digitized belief network?</a>'.</p>\n<p>I have now created a discussion group for anyone who wants to discuss the problems involved in creating a digital representation of a human's beliefs. Anyone who is interested in joining us can sign up <a href=\"http://www.secularcommunity.org.au/mailman/listinfo/rationality_secularcommunity.org.au\">here</a>.</p>\n<p>See you all around the list,<br />Avi</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wQrroyCgh5NCCMqHp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 9.175586658306135e-07, "legacy": true, "legacyId": "16732", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PopyaEqjBsFsgmGb7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-07T02:43:19.652Z", "modifiedAt": null, "url": null, "title": "Free Kindle Textbook: The Cerebellum: Brain for an Implicit Self (FT Press Science)", "slug": "free-kindle-textbook-the-cerebellum-brain-for-an-implicit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:05.794Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "buybuydandavis", "createdAt": "2011-09-04T00:02:35.971Z", "isAdmin": false, "displayName": "buybuydandavis"}, "userId": "sQqaueY24d7xa3PXD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YSQoKJyuhp5NXQKpT/free-kindle-textbook-the-cerebellum-brain-for-an-implicit", "pageUrlRelative": "/posts/YSQoKJyuhp5NXQKpT/free-kindle-textbook-the-cerebellum-brain-for-an-implicit", "linkUrl": "https://www.lesswrong.com/posts/YSQoKJyuhp5NXQKpT/free-kindle-textbook-the-cerebellum-brain-for-an-implicit", "postedAtFormatted": "Thursday, June 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Free%20Kindle%20Textbook%3A%20The%20Cerebellum%3A%20Brain%20for%20an%20Implicit%20Self%20(FT%20Press%20Science)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFree%20Kindle%20Textbook%3A%20The%20Cerebellum%3A%20Brain%20for%20an%20Implicit%20Self%20(FT%20Press%20Science)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSQoKJyuhp5NXQKpT%2Ffree-kindle-textbook-the-cerebellum-brain-for-an-implicit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Free%20Kindle%20Textbook%3A%20The%20Cerebellum%3A%20Brain%20for%20an%20Implicit%20Self%20(FT%20Press%20Science)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSQoKJyuhp5NXQKpT%2Ffree-kindle-textbook-the-cerebellum-brain-for-an-implicit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSQoKJyuhp5NXQKpT%2Ffree-kindle-textbook-the-cerebellum-brain-for-an-implicit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<p>**** DEAL OVER: As of 20120611.</p>\n<p>Another free kindle I thought some might have interest in. I haven't read it, but the first review was glowing and looked relevant.</p>\n<p>First Amazon Review:</p>\n<p>&gt; Five Star Final; Excellent; A \"must read\" for any \"student\" of brain-behavior relationships</p>\n<p>http://www.amazon.com/gp/product/B005DKQQG4/</p>\n<p>UPDATE: Still free at the US amazon at 2pm eastern time. Reports that it is not free at the UK site, which I verified. Since I can log in to the UK site from the US and see the price, I assume people in the UK could sign into the US site and buy it. If anyone gives that a try, let me know and I'll further update the top level.</p>\n<p>UPDATE: &nbsp;Free at amazon.fr. Can buy at the US site from the Netherlands. Can't buy from FR or US sites from UK.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YSQoKJyuhp5NXQKpT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 9.176469511186121e-07, "legacy": true, "legacyId": "16733", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-07T02:55:23.584Z", "modifiedAt": null, "url": null, "title": "Is there math for interplanetary travel vs existential risk?", "slug": "is-there-math-for-interplanetary-travel-vs-existential-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:04.049Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qzfTKZesQ9sDcPoBb/is-there-math-for-interplanetary-travel-vs-existential-risk", "pageUrlRelative": "/posts/qzfTKZesQ9sDcPoBb/is-there-math-for-interplanetary-travel-vs-existential-risk", "linkUrl": "https://www.lesswrong.com/posts/qzfTKZesQ9sDcPoBb/is-there-math-for-interplanetary-travel-vs-existential-risk", "postedAtFormatted": "Thursday, June 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20there%20math%20for%20interplanetary%20travel%20vs%20existential%20risk%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20there%20math%20for%20interplanetary%20travel%20vs%20existential%20risk%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqzfTKZesQ9sDcPoBb%2Fis-there-math-for-interplanetary-travel-vs-existential-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20there%20math%20for%20interplanetary%20travel%20vs%20existential%20risk%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqzfTKZesQ9sDcPoBb%2Fis-there-math-for-interplanetary-travel-vs-existential-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqzfTKZesQ9sDcPoBb%2Fis-there-math-for-interplanetary-travel-vs-existential-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<p>Can anyone recommend some resources, or provide actual examples, of any relevant evolutionary-biology-style equations which could describe what factors would most likely predominate to allow for the survival of sapience, given such factors as the ease which any given individual can acquire the means of killing large numbers of people, the scale of those means, the willingness o people to use those means, the ability of small groups to travel away from other groups, and so on?</p>\n<p>&nbsp;</p>\n<p>Or, put another way - is there any way I can quantitatively check my intuition that a valuable way to avoid certain existential risks is to flee Earth?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qzfTKZesQ9sDcPoBb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 9.17652395137945e-07, "legacy": true, "legacyId": "16739", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-07T04:51:33.768Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Ghosts in the Machine", "slug": "seq-rerun-ghosts-in-the-machine", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:03.600Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ee2Hkewt5W6PR79xB/seq-rerun-ghosts-in-the-machine", "pageUrlRelative": "/posts/Ee2Hkewt5W6PR79xB/seq-rerun-ghosts-in-the-machine", "linkUrl": "https://www.lesswrong.com/posts/Ee2Hkewt5W6PR79xB/seq-rerun-ghosts-in-the-machine", "postedAtFormatted": "Thursday, June 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Ghosts%20in%20the%20Machine&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Ghosts%20in%20the%20Machine%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEe2Hkewt5W6PR79xB%2Fseq-rerun-ghosts-in-the-machine%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Ghosts%20in%20the%20Machine%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEe2Hkewt5W6PR79xB%2Fseq-rerun-ghosts-in-the-machine", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEe2Hkewt5W6PR79xB%2Fseq-rerun-ghosts-in-the-machine", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<p>Today's post, <a href=\"/lw/rf/ghosts_in_the_machine/\">Ghosts in the Machine</a> was originally published on 17 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Ghosts_in_the_Machine\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There is a way of thinking about programming a computer that conforms well to human intuitions: telling the computer what to do. The problem is that the computer isn't going to understand you, unless you program the computer to understand. If you are programming an AI, you are not giving instructions to a ghost in the machine; you are creating the ghost.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cvl/seq_rerun_grasping_slippery_things/\">Grasping Slippery Things</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ee2Hkewt5W6PR79xB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.177048143880736e-07, "legacy": true, "legacyId": "16745", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cnYHFNBF3kZEyx24v", "5cQjafoPJneoswLT3", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-07T07:48:26.973Z", "modifiedAt": null, "url": null, "title": "[Link] Nerds are nuts", "slug": "link-nerds-are-nuts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:05.725Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ahfcr7nCzDhuELzL2/link-nerds-are-nuts", "pageUrlRelative": "/posts/ahfcr7nCzDhuELzL2/link-nerds-are-nuts", "linkUrl": "https://www.lesswrong.com/posts/ahfcr7nCzDhuELzL2/link-nerds-are-nuts", "postedAtFormatted": "Thursday, June 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Nerds%20are%20nuts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Nerds%20are%20nuts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fahfcr7nCzDhuELzL2%2Flink-nerds-are-nuts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Nerds%20are%20nuts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fahfcr7nCzDhuELzL2%2Flink-nerds-are-nuts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fahfcr7nCzDhuELzL2%2Flink-nerds-are-nuts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2503, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/18b/reason_as_memetic_immune_disorder/\">Reason as memetic immune disorder</a>, <a href=\"/lw/18b/reason_as_memetic_immune_disorder/14ac\">Commentary on compartmentalization </a></p>\n<p>On the old <a href=\"http://www.gnxp.com/\">old gnxp site</a> site Razib Khan wrote an interesting piece on a failure mode of nerds. This is I think something very important to keep in mind because for better or worse LessWrong is nerdspace. It deals with how the systematizing tendencies coupled with a lack of common sense can lead to troublesome failure modes and identifies some religious fundamentalism as symptomatic of such minds. At the end of both the original article as well as in the text I quote here is a quick list summary of the contents, if you aren't sure about the <a href=\"/lw/85x/value_of_information_four_examples/\">VOI</a> consider reading that point by point summary first to help you judge it. The introduction provides interesting information very useful in context but isn't absolutely necessary.</p>\n<p><a href=\"http://www.gnxp.com/blog/2007/04/nerds-are-nuts.php\">Link to original article.</a></p>\n<h2>Introduction<br /></h2>\n<blockquote>\n<p>Reading <a href=\"http://www.amazon.com/Spite-Gods-Strange-Modern-India/dp/0385514743/ref=pd_bbs_sr_1/002-9954258-8060061?ie=UTF8&amp;s=books&amp;qid=1175666384&amp;sr=8-1\">In Spite of the Gods: The Strange Rise of Modern India</a>, I stumbled upon this passage on page 151:</p>\n</blockquote>\n<blockquote>\n<p>\"...Whereas the Congress Party was dominated by lawyers and journalists, <span style=\"font-weight: bold;\">the RSS was dominated by people from a scientific background</span>. Both groups were almost exclusively Brahmin in their formative years...three out of four of Hedegwar's [the founder, who was a doctor -Razib] successors were also from scientific backgrounds: M.S. Golwalker...was a zoologist...Rajendra Singh was a physicist; and K.S. Sudarshan...is an engineer....\"</p>\n</blockquote>\n<blockquote>\n<p><span class=\"fullpost\">Some quick \"background.\" The <a href=\"http://en.wikipedia.org/wiki/Rashtriya_Swayamsevak_Sangh\">RSS</a> is a prominent member of the <a href=\"http://en.wikipedia.org/wiki/Hindutva\">Hindutva</a> movement, roughly, Hindu nationalism. Some people have termed them \"Hindu fundamentalists,\" suggesting an equivalence with reactionary religious movements the world over. There is a problem with such a broad brush term: some proponents and adherents of Hindutva are not themselves particularly religious and make no effort to pretend that they are. Rather, they are individuals who are attracted to the movement for racial-nationalist reasons, they view \"Hindus\" as a people as much, or more than, a religion. One could make an argument that the \"Christian Right\" or \"Islamism\" are not at the root concerned or driven by religious motives, but, members of both these movements would assert at least a pretense toward religiosity almost universally.<br /><br />With that preamble out of the way, <span style=\"font-weight: bold;\">I was not surprised that the RSS had a core cadre of scientifically oriented leaders. </span> This is a common tendency amongst <span style=\"font-style: italic;\">faux </span>reactionary movements with a religious element. I say <span style=\"font-style: italic;\">faux </span>because these movements tend to be extremely innovative and progressive in the process of attempting to recreate a mythic golden past. The militancy of some of the organizations in the Hindutva movement, like the VHP and RSS, has been asserted by some Hindu intellectuals as being...un-Hindu. Some of the early intellectuals in the movement admitted that they were attempting to fight back against Islam and Christianity by co-opting some of the modalities of these two religions. The question becomes at what point does pragmatic methodology suborn the ultimate ends? I won't offer an answer because I have little interest in that topic, at least in this post. Rather, I want to move back to the point about scientists and their involvement in \"fundamentalist\" religious movements. Scientifically trained individuals are over represented within Islam in the <a href=\"http://www.gnxp.com/blog/2005/07/profile-of-salafi-jihadists.php\">Salafist Terror Network</a>. As a child the fundamentalist engineer was a cut-out stereotype amongst the circle of graduate students in the natural sciences from Muslim backgrounds that my parents socialized amongst. Ethnological research confirms that Islamist movements are highly concentrated within departments of engineering at universities. Engineers are also very prominent in the Creationist movement in the United States. If civilizations can be analogized to organisms, then a particular subset of technically minded folk get very strange when interfacing with the world around us...and turn into fundamentalists.<br /><br /><br />So why the tendency for technical people to be so prominent in these groups? First, <span style=\"font-weight: bold;\">let me clarify that just because technical folk are heavily over represented amongst religious radicals does not mean that religious radicals are necessarily a large demographic among technical folk</span>. Rather, amongst the set of religious radicals the technicians seem to rise up to positions of power and provide excellent recruits.<br /><br />There is I think a socioeconomic angle on this. Years back I was curious as to the class origin of different scientific professions. I didn't find much, but the data I did gather implied that engineers are generally more likely to be from less affluent backgrounds than more abstract and less practical fields like botany or astronomy. This makes sense, engineering is one of the best tickets to a middle class livelihood, and it might necessitate fewer social graces (acquired through \"breeding\") than medicine or law. As it happens, oftentimes fundamentalist movements draw much of their strength from upwardly mobile groups who are striving to ascend up from lower to lower-middle-class status. Though the Hindutva movement in India is mostly upper caste, it is not concentrated amongst the English speaking super elite who are quite Westernized, but rather its strength lay amongst the non-Western sub-elites (e.g., merchants in small to mid-sized cities) or the <span style=\"font-style: italic;\">petite bourgeois</span>. Islamism in much of the world can be traced to the anomie generated by the transformation of \"traditional\" societies through urbanization and other assorted dislocations, and as peasants enter the modern world Islamic orthodoxy is a way to moor themselves within the new urban matrix and the world of wage labor. Similarly, the rise of the Christian Right can be tied in part to the entrance of evangelicals into the broad middle class as the Old South became the New South and air conditioning led to the blossoming of the Sun Belt.<br /></span></p>\n</blockquote>\n<h2>Nerd Failure Mode</h2>\n<p>This section is the part most relevant to LessWrong:&nbsp;</p>\n<blockquote>\n<p><span class=\"fullpost\"><span style=\"font-weight: bold;\">But there are likely other factors at play which are not sociological or cultural, but individual</span>. Fundamentalists tend to be \"literalists,\" and have a tendency to look at their religious texts as divine manuals which describe and prescribe every aspect of the world. In some ways this is a new tendency in our species, at least as a mass movement. One can definitely trace scriptural fundamentalism to the Protestant Reformation with the call to <span style=\"font-style: italic;\">sola scriptura</span>, but in the West its contemporary origin can be found in the reaction in the late 19th century and early 20th century to textual analysis of the Bible by modernists. The assault on the historicity of the Bible, combined with both mass literacy and a democratic culture in the United States, led inevitably to a crass literalism that birthed the peculiarities which we see before us in the form of Creationism and its sisters. A literal reading of the Bible leads to ludicrous conclusions, but if one perceives that the game is all or nothing, then perhaps one must assert the truth value of Genesis as if it was a scientific treatise. Religious professionals have often been skeptical of literalism because a deep knowledge of languages and the translation process highlights various ambiguities and gray shades, but for those whom the text is plain and unadorned by deeper knowledge its meaning is \"clear\" and must be take at its word. Scientists and engineers live in a world of axioms, laws and theories, which though rough and ready, <span style=\"font-weight: bold;\">must be taken as truths for predictions and models to be valid</span>. You make assumptions, you construct a model, and you project a range of values bounded by errors. Once science is established you take it is as a given and don't engage in excessive philosophical reflection. This is \"<a href=\"http://en.wikipedia.org/wiki/Normal_science\">normal science</a>.\" The axioms are validated by their utility in an instrumental fashion in engineering and model building. Obviously religious truths are different. Plainly, the direct material benefits of religion, magic, is easily falsifiable. The indirect benefits, the afterlife, etc., are often beyond verification. A critical examination of the Hebrew Bible shows all sorts of fallacious assumptions. For example, there is an implication that the world is flat and that the sun revolves around the earth. Though these contentions are not defensible, there are a host of other assertions which are less plainly incorrect, or at least seem to be refuted only by a more complex suite of contingent facts (e.g., the historical sciences in the form of geology and evolutionary biology falsify the creation account, but these are complex stories which require acceptance of a chain of inferences). Obviously many religious people have a deep emotional attachment to their faith. If one is told that one's religion is based on a book, and that book <span style=\"font-style: italic;\">plainly</span> seems to imply ludicrous assertions, how to square this circle? Many a scientific mind simply accepts the ludicrous axioms and starts to generate inferences. Consider the<a href=\"http://www.godandscience.org/youngearth/canopy.html\"> Water Canopy Theory</a>. Or, the Hindutva ideology that Aryans originated in India, spread to the rest of the world, and so brought civilization (the gift of the Indians). Or that Hindu mythology records the ancient use of nuclear weapons and spaceships. There are even books like <a href=\"http://www.amazon.com/Human-Devolution-alternative-Darwins-theory/dp/0892133341/ref=pd_bbs_sr_1/002-9954258-8060061?ie=UTF8&amp;s=books&amp;qid=1175674970&amp;sr=1-1\">Human Devolution: a Vedic alternative to Darwin's theory</a>. Strictly speaking much of this work is not irrational, <span style=\"font-weight: bold;\">insofar as it exhibits internal logical coherency</span>. The axioms are simply ludicrous.<br /><br />Which gets me back to the way scientists think: though some scientists are very philosophical, the way in which science is taught is often not particularly focused on the nature and reasoning beyond the axioms given. PV = nRT. Why? There are quick primers in regards to the root of the Ideal Gas Law, but the key is to take this law and <span style=\"font-weight: bold;\">utilize it to solve problems</span>. But what if PV = nRT is subjective, a misinterpretation. Perhaps a cultural mix-up resulted in a transcription error which introduced the gas constant, R. This is an idiotic question to ask in science. If you're taking a course on the kinetics of gases you don't have long discussions lingering upon the nature of motion and gas particles, <span style=\"font-weight: bold;\">those are assumed</span>. In contrast in softer disciplines the very concept of \"motion\" an \"particles\" are subject to critique because the objects of study are far more slippery. Is it the \"Red Sea\" or \"Sea of Reeds\"? Does the Bible refer to Mary as a virgin or an unmarried woman? Does the color coding of the Aryans and Dasas in the Vedas refer to literal differences in complexion, or are they narrative conventions? Language lacks the interpersonal precision of mathematics, and while <a href=\"http://en.wikipedia.org/wiki/Uniformitarianism_%28science%29\">uniformitarianism</a> has served us admirably in the natural sciences, the dynamic nature of idiom, phrase and speech within shifting context means that teasing apart meaning from the records of the past can be a difficult feat which requires care, erudition and common sense.<br /><br />Up until this point I have focused on the way scientists work, and the necessity of background assumptions, and the relative short shrift they often give to the \"meta\" analysis of background concepts. Though I don't want to push this line of thought too far, I will offer the following illustrations of behaviors which I think are not totally unlike the manner in which some fundamentalists behave. Someone tells a child to \"pull the door behind\" them. He proceeds to unscrew the hinges and drag the front door across to the street to his house. Siblings are told that there is life after death by their parent. They proceed to plan the death of one so that some confirmation of this possibility can be ascertained. These two instances are real examples of individuals who exhibit Autism/Asperger's Syndrome. <strong>Anyone who would behave in this way lacks common social sense. </strong>I believe that a disproportionate number of those who are attracted to fundamentalism tend to lack the same perspective and contextualizing capacity in regards to their religious beliefs. If they can do some matrix algebra too, they're nerds. On a mass scale, consider that both Salafis among Muslims and Puritans among Calvinists debated whether all that was not mentioned within their Holy Texts as permissible were therefore impermissible. I suspect that for most people common sense might persuade one to the conclusion that these sort of debates imply a lack of a sense of proportion, frankly, of normalcy.<br /></span></p>\n</blockquote>\n<h2><span class=\"fullpost\"><span style=\"font-weight: bold;\">In sum:</span></span></h2>\n<blockquote>\n<h3></h3>\n<ul>\n<li>Hard core religious fundamentalists are somewhat atypical psychologically</li>\n<li>Scientists and engineers are also atypical psychologically</li>\n<li>Some of the traits modal within these two sets intersect</li>\n<li>Resulting in a disproportionate number of scientists amongst fundamentalists</li>\n<li>Science converges upon rock solid truths, which become the axioms for the next set of projections and investigations. Fundamentalism presents itself as axioms and clear and distinct inferences from those axioms. Both are fundamentally elegant and simple cognitive processes, but, the content is so radically different that the outcomes in regards to truth value are very different</li>\n<li><strong>Mass literacy and mass society, as well as the decentralization of authority and power, likely made fundamentalism inevitable as the basal level of individuals with susceptible psychological profiles could now have direct access to the axioms in question (texts)</strong></li>\n<li>Just as some scientists tend to take ideas to their \"logical extremes\" (e.g., the \"paradoxes\" of physics) no matter the dictates of common sense, so some fundamentalists take the logical conclusion of their religious texts to extremes</li>\n<li>No matter the religion it seems that modernity will produce <span style=\"font-style: italic;\">faux </span>reactionary fundamentalism because of the nature of normal human variation combined with universal inputs (e.g., the rise of normative consumerism, urbanization, etc.).</li>\n</ul>\n</blockquote>\n<p>I bolded the note on mass literacy and participation because of the interesting historical conclusion that in the United Stated mass participation in democracy inevitably made the influence of religion on policy greater. It goes against a deep assumption shared by most educated people that \"democratic elections\" necessarily produce \"liberal\" or \"secular\" results. It was particularly evident among pundits and particularly <a href=\"http://blogs.discovermagazine.com/gnxp/2011/02/culture-differences-matter-even-within-islam/\">easy to see</a> <a href=\"http://secularright.org/SR/wordpress/2012/01/22/72-percent-of-egypts-parliament-is-islamist/\">as foolish</a> with the recent upheavals in the Middle East. &nbsp;</p>\n<blockquote>\n<p><strong>Note:</strong> <strong>Much of what I said above applies to non-religious domains.</strong> After all, many scientists were once Communists and Nazis.</p>\n</blockquote>\n<p>This last rather minor seeming note is perhaps the most relevant part of the article for aspiring rationalist. Not only is it particularly salient for those us inclined to <a href=\"/lw/8w8/link_belief_in_religion_considered_harmful/\">questioning the usefulness of the category \"religion\"</a> in certain context, but because <a href=\"/lw/8p4/2011_survey_results/\">nearly all of us are not religious</a>. Our bad axioms seem unlikely to originate directly from something like a religious texts, though obviously it is plausible many of our axioms <em>ultimately</em> originate from such sources.Not many of us are Communists either, but we are attracted to highly consistent ideologies. <strong>We seem likely to be particularly vulnerable to bad axioms in a way most minds aren't.</strong></p>\n<p>So if after some thought and examination you notice that a widely respected and universally endorsed axiom in your society has clear and hard to deny implications that are in practice ignored or even denounced by most people,<strong> you should be more willing to dump such axioms than is comfortable. </strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LNsEBXoFdAy8yzvbw": 1, "NSMKfa8emSbGNXRKD": 1, "9YFoDPFwMoWthzgkY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ahfcr7nCzDhuELzL2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 35, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "16756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related to:</strong> <a href=\"/lw/18b/reason_as_memetic_immune_disorder/\">Reason as memetic immune disorder</a>, <a href=\"/lw/18b/reason_as_memetic_immune_disorder/14ac\">Commentary on compartmentalization </a></p>\n<p>On the old <a href=\"http://www.gnxp.com/\">old gnxp site</a> site Razib Khan wrote an interesting piece on a failure mode of nerds. This is I think something very important to keep in mind because for better or worse LessWrong is nerdspace. It deals with how the systematizing tendencies coupled with a lack of common sense can lead to troublesome failure modes and identifies some religious fundamentalism as symptomatic of such minds. At the end of both the original article as well as in the text I quote here is a quick list summary of the contents, if you aren't sure about the <a href=\"/lw/85x/value_of_information_four_examples/\">VOI</a> consider reading that point by point summary first to help you judge it. The introduction provides interesting information very useful in context but isn't absolutely necessary.</p>\n<p><a href=\"http://www.gnxp.com/blog/2007/04/nerds-are-nuts.php\">Link to original article.</a></p>\n<h2 id=\"Introduction\">Introduction<br></h2>\n<blockquote>\n<p>Reading <a href=\"http://www.amazon.com/Spite-Gods-Strange-Modern-India/dp/0385514743/ref=pd_bbs_sr_1/002-9954258-8060061?ie=UTF8&amp;s=books&amp;qid=1175666384&amp;sr=8-1\">In Spite of the Gods: The Strange Rise of Modern India</a>, I stumbled upon this passage on page 151:</p>\n</blockquote>\n<blockquote>\n<p>\"...Whereas the Congress Party was dominated by lawyers and journalists, <span style=\"font-weight: bold;\">the RSS was dominated by people from a scientific background</span>. Both groups were almost exclusively Brahmin in their formative years...three out of four of Hedegwar's [the founder, who was a doctor -Razib] successors were also from scientific backgrounds: M.S. Golwalker...was a zoologist...Rajendra Singh was a physicist; and K.S. Sudarshan...is an engineer....\"</p>\n</blockquote>\n<blockquote>\n<p><span class=\"fullpost\">Some quick \"background.\" The <a href=\"http://en.wikipedia.org/wiki/Rashtriya_Swayamsevak_Sangh\">RSS</a> is a prominent member of the <a href=\"http://en.wikipedia.org/wiki/Hindutva\">Hindutva</a> movement, roughly, Hindu nationalism. Some people have termed them \"Hindu fundamentalists,\" suggesting an equivalence with reactionary religious movements the world over. There is a problem with such a broad brush term: some proponents and adherents of Hindutva are not themselves particularly religious and make no effort to pretend that they are. Rather, they are individuals who are attracted to the movement for racial-nationalist reasons, they view \"Hindus\" as a people as much, or more than, a religion. One could make an argument that the \"Christian Right\" or \"Islamism\" are not at the root concerned or driven by religious motives, but, members of both these movements would assert at least a pretense toward religiosity almost universally.<br><br>With that preamble out of the way, <span style=\"font-weight: bold;\">I was not surprised that the RSS had a core cadre of scientifically oriented leaders. </span> This is a common tendency amongst <span style=\"font-style: italic;\">faux </span>reactionary movements with a religious element. I say <span style=\"font-style: italic;\">faux </span>because these movements tend to be extremely innovative and progressive in the process of attempting to recreate a mythic golden past. The militancy of some of the organizations in the Hindutva movement, like the VHP and RSS, has been asserted by some Hindu intellectuals as being...un-Hindu. Some of the early intellectuals in the movement admitted that they were attempting to fight back against Islam and Christianity by co-opting some of the modalities of these two religions. The question becomes at what point does pragmatic methodology suborn the ultimate ends? I won't offer an answer because I have little interest in that topic, at least in this post. Rather, I want to move back to the point about scientists and their involvement in \"fundamentalist\" religious movements. Scientifically trained individuals are over represented within Islam in the <a href=\"http://www.gnxp.com/blog/2005/07/profile-of-salafi-jihadists.php\">Salafist Terror Network</a>. As a child the fundamentalist engineer was a cut-out stereotype amongst the circle of graduate students in the natural sciences from Muslim backgrounds that my parents socialized amongst. Ethnological research confirms that Islamist movements are highly concentrated within departments of engineering at universities. Engineers are also very prominent in the Creationist movement in the United States. If civilizations can be analogized to organisms, then a particular subset of technically minded folk get very strange when interfacing with the world around us...and turn into fundamentalists.<br><br><br>So why the tendency for technical people to be so prominent in these groups? First, <span style=\"font-weight: bold;\">let me clarify that just because technical folk are heavily over represented amongst religious radicals does not mean that religious radicals are necessarily a large demographic among technical folk</span>. Rather, amongst the set of religious radicals the technicians seem to rise up to positions of power and provide excellent recruits.<br><br>There is I think a socioeconomic angle on this. Years back I was curious as to the class origin of different scientific professions. I didn't find much, but the data I did gather implied that engineers are generally more likely to be from less affluent backgrounds than more abstract and less practical fields like botany or astronomy. This makes sense, engineering is one of the best tickets to a middle class livelihood, and it might necessitate fewer social graces (acquired through \"breeding\") than medicine or law. As it happens, oftentimes fundamentalist movements draw much of their strength from upwardly mobile groups who are striving to ascend up from lower to lower-middle-class status. Though the Hindutva movement in India is mostly upper caste, it is not concentrated amongst the English speaking super elite who are quite Westernized, but rather its strength lay amongst the non-Western sub-elites (e.g., merchants in small to mid-sized cities) or the <span style=\"font-style: italic;\">petite bourgeois</span>. Islamism in much of the world can be traced to the anomie generated by the transformation of \"traditional\" societies through urbanization and other assorted dislocations, and as peasants enter the modern world Islamic orthodoxy is a way to moor themselves within the new urban matrix and the world of wage labor. Similarly, the rise of the Christian Right can be tied in part to the entrance of evangelicals into the broad middle class as the Old South became the New South and air conditioning led to the blossoming of the Sun Belt.<br></span></p>\n</blockquote>\n<h2 id=\"Nerd_Failure_Mode\">Nerd Failure Mode</h2>\n<p>This section is the part most relevant to LessWrong:&nbsp;</p>\n<blockquote>\n<p><span class=\"fullpost\"><span style=\"font-weight: bold;\">But there are likely other factors at play which are not sociological or cultural, but individual</span>. Fundamentalists tend to be \"literalists,\" and have a tendency to look at their religious texts as divine manuals which describe and prescribe every aspect of the world. In some ways this is a new tendency in our species, at least as a mass movement. One can definitely trace scriptural fundamentalism to the Protestant Reformation with the call to <span style=\"font-style: italic;\">sola scriptura</span>, but in the West its contemporary origin can be found in the reaction in the late 19th century and early 20th century to textual analysis of the Bible by modernists. The assault on the historicity of the Bible, combined with both mass literacy and a democratic culture in the United States, led inevitably to a crass literalism that birthed the peculiarities which we see before us in the form of Creationism and its sisters. A literal reading of the Bible leads to ludicrous conclusions, but if one perceives that the game is all or nothing, then perhaps one must assert the truth value of Genesis as if it was a scientific treatise. Religious professionals have often been skeptical of literalism because a deep knowledge of languages and the translation process highlights various ambiguities and gray shades, but for those whom the text is plain and unadorned by deeper knowledge its meaning is \"clear\" and must be take at its word. Scientists and engineers live in a world of axioms, laws and theories, which though rough and ready, <span style=\"font-weight: bold;\">must be taken as truths for predictions and models to be valid</span>. You make assumptions, you construct a model, and you project a range of values bounded by errors. Once science is established you take it is as a given and don't engage in excessive philosophical reflection. This is \"<a href=\"http://en.wikipedia.org/wiki/Normal_science\">normal science</a>.\" The axioms are validated by their utility in an instrumental fashion in engineering and model building. Obviously religious truths are different. Plainly, the direct material benefits of religion, magic, is easily falsifiable. The indirect benefits, the afterlife, etc., are often beyond verification. A critical examination of the Hebrew Bible shows all sorts of fallacious assumptions. For example, there is an implication that the world is flat and that the sun revolves around the earth. Though these contentions are not defensible, there are a host of other assertions which are less plainly incorrect, or at least seem to be refuted only by a more complex suite of contingent facts (e.g., the historical sciences in the form of geology and evolutionary biology falsify the creation account, but these are complex stories which require acceptance of a chain of inferences). Obviously many religious people have a deep emotional attachment to their faith. If one is told that one's religion is based on a book, and that book <span style=\"font-style: italic;\">plainly</span> seems to imply ludicrous assertions, how to square this circle? Many a scientific mind simply accepts the ludicrous axioms and starts to generate inferences. Consider the<a href=\"http://www.godandscience.org/youngearth/canopy.html\"> Water Canopy Theory</a>. Or, the Hindutva ideology that Aryans originated in India, spread to the rest of the world, and so brought civilization (the gift of the Indians). Or that Hindu mythology records the ancient use of nuclear weapons and spaceships. There are even books like <a href=\"http://www.amazon.com/Human-Devolution-alternative-Darwins-theory/dp/0892133341/ref=pd_bbs_sr_1/002-9954258-8060061?ie=UTF8&amp;s=books&amp;qid=1175674970&amp;sr=1-1\">Human Devolution: a Vedic alternative to Darwin's theory</a>. Strictly speaking much of this work is not irrational, <span style=\"font-weight: bold;\">insofar as it exhibits internal logical coherency</span>. The axioms are simply ludicrous.<br><br>Which gets me back to the way scientists think: though some scientists are very philosophical, the way in which science is taught is often not particularly focused on the nature and reasoning beyond the axioms given. PV = nRT. Why? There are quick primers in regards to the root of the Ideal Gas Law, but the key is to take this law and <span style=\"font-weight: bold;\">utilize it to solve problems</span>. But what if PV = nRT is subjective, a misinterpretation. Perhaps a cultural mix-up resulted in a transcription error which introduced the gas constant, R. This is an idiotic question to ask in science. If you're taking a course on the kinetics of gases you don't have long discussions lingering upon the nature of motion and gas particles, <span style=\"font-weight: bold;\">those are assumed</span>. In contrast in softer disciplines the very concept of \"motion\" an \"particles\" are subject to critique because the objects of study are far more slippery. Is it the \"Red Sea\" or \"Sea of Reeds\"? Does the Bible refer to Mary as a virgin or an unmarried woman? Does the color coding of the Aryans and Dasas in the Vedas refer to literal differences in complexion, or are they narrative conventions? Language lacks the interpersonal precision of mathematics, and while <a href=\"http://en.wikipedia.org/wiki/Uniformitarianism_%28science%29\">uniformitarianism</a> has served us admirably in the natural sciences, the dynamic nature of idiom, phrase and speech within shifting context means that teasing apart meaning from the records of the past can be a difficult feat which requires care, erudition and common sense.<br><br>Up until this point I have focused on the way scientists work, and the necessity of background assumptions, and the relative short shrift they often give to the \"meta\" analysis of background concepts. Though I don't want to push this line of thought too far, I will offer the following illustrations of behaviors which I think are not totally unlike the manner in which some fundamentalists behave. Someone tells a child to \"pull the door behind\" them. He proceeds to unscrew the hinges and drag the front door across to the street to his house. Siblings are told that there is life after death by their parent. They proceed to plan the death of one so that some confirmation of this possibility can be ascertained. These two instances are real examples of individuals who exhibit Autism/Asperger's Syndrome. <strong>Anyone who would behave in this way lacks common social sense. </strong>I believe that a disproportionate number of those who are attracted to fundamentalism tend to lack the same perspective and contextualizing capacity in regards to their religious beliefs. If they can do some matrix algebra too, they're nerds. On a mass scale, consider that both Salafis among Muslims and Puritans among Calvinists debated whether all that was not mentioned within their Holy Texts as permissible were therefore impermissible. I suspect that for most people common sense might persuade one to the conclusion that these sort of debates imply a lack of a sense of proportion, frankly, of normalcy.<br></span></p>\n</blockquote>\n<h2 id=\"In_sum_\"><span class=\"fullpost\"><span style=\"font-weight: bold;\">In sum:</span></span></h2>\n<blockquote>\n<h3></h3>\n<ul>\n<li>Hard core religious fundamentalists are somewhat atypical psychologically</li>\n<li>Scientists and engineers are also atypical psychologically</li>\n<li>Some of the traits modal within these two sets intersect</li>\n<li>Resulting in a disproportionate number of scientists amongst fundamentalists</li>\n<li>Science converges upon rock solid truths, which become the axioms for the next set of projections and investigations. Fundamentalism presents itself as axioms and clear and distinct inferences from those axioms. Both are fundamentally elegant and simple cognitive processes, but, the content is so radically different that the outcomes in regards to truth value are very different</li>\n<li><strong>Mass literacy and mass society, as well as the decentralization of authority and power, likely made fundamentalism inevitable as the basal level of individuals with susceptible psychological profiles could now have direct access to the axioms in question (texts)</strong></li>\n<li>Just as some scientists tend to take ideas to their \"logical extremes\" (e.g., the \"paradoxes\" of physics) no matter the dictates of common sense, so some fundamentalists take the logical conclusion of their religious texts to extremes</li>\n<li>No matter the religion it seems that modernity will produce <span style=\"font-style: italic;\">faux </span>reactionary fundamentalism because of the nature of normal human variation combined with universal inputs (e.g., the rise of normative consumerism, urbanization, etc.).</li>\n</ul>\n</blockquote>\n<p>I bolded the note on mass literacy and participation because of the interesting historical conclusion that in the United Stated mass participation in democracy inevitably made the influence of religion on policy greater. It goes against a deep assumption shared by most educated people that \"democratic elections\" necessarily produce \"liberal\" or \"secular\" results. It was particularly evident among pundits and particularly <a href=\"http://blogs.discovermagazine.com/gnxp/2011/02/culture-differences-matter-even-within-islam/\">easy to see</a> <a href=\"http://secularright.org/SR/wordpress/2012/01/22/72-percent-of-egypts-parliament-is-islamist/\">as foolish</a> with the recent upheavals in the Middle East. &nbsp;</p>\n<blockquote>\n<p><strong>Note:</strong> <strong>Much of what I said above applies to non-religious domains.</strong> After all, many scientists were once Communists and Nazis.</p>\n</blockquote>\n<p>This last rather minor seeming note is perhaps the most relevant part of the article for aspiring rationalist. Not only is it particularly salient for those us inclined to <a href=\"/lw/8w8/link_belief_in_religion_considered_harmful/\">questioning the usefulness of the category \"religion\"</a> in certain context, but because <a href=\"/lw/8p4/2011_survey_results/\">nearly all of us are not religious</a>. Our bad axioms seem unlikely to originate directly from something like a religious texts, though obviously it is plausible many of our axioms <em>ultimately</em> originate from such sources.Not many of us are Communists either, but we are attracted to highly consistent ideologies. <strong>We seem likely to be particularly vulnerable to bad axioms in a way most minds aren't.</strong></p>\n<p>So if after some thought and examination you notice that a widely respected and universally endorsed axiom in your society has clear and hard to deny implications that are in practice ignored or even denounced by most people,<strong> you should be more willing to dump such axioms than is comfortable. </strong></p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Nerd Failure Mode", "anchor": "Nerd_Failure_Mode", "level": 1}, {"title": "In sum:", "anchor": "In_sum_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "44 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aHaqgTNnFzD7NGLMx", "vADtvr9iDeYsCDfxd", "fMHq4djhTRBFedQyq", "HAEPbGaMygJq8L59k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-07T12:27:48.075Z", "modifiedAt": null, "url": null, "title": "Peter Thiel's AGI discussion in his startups class @ Stanford [link]", "slug": "peter-thiel-s-agi-discussion-in-his-startups-class-stanford", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:05.452Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K5fWkTkYCiAN86PMJ/peter-thiel-s-agi-discussion-in-his-startups-class-stanford", "pageUrlRelative": "/posts/K5fWkTkYCiAN86PMJ/peter-thiel-s-agi-discussion-in-his-startups-class-stanford", "linkUrl": "https://www.lesswrong.com/posts/K5fWkTkYCiAN86PMJ/peter-thiel-s-agi-discussion-in-his-startups-class-stanford", "postedAtFormatted": "Thursday, June 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Peter%20Thiel's%20AGI%20discussion%20in%20his%20startups%20class%20%40%20Stanford%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APeter%20Thiel's%20AGI%20discussion%20in%20his%20startups%20class%20%40%20Stanford%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK5fWkTkYCiAN86PMJ%2Fpeter-thiel-s-agi-discussion-in-his-startups-class-stanford%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Peter%20Thiel's%20AGI%20discussion%20in%20his%20startups%20class%20%40%20Stanford%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK5fWkTkYCiAN86PMJ%2Fpeter-thiel-s-agi-discussion-in-his-startups-class-stanford", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK5fWkTkYCiAN86PMJ%2Fpeter-thiel-s-agi-discussion-in-his-startups-class-stanford", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 24, "htmlBody": "<p><a href=\"http://blakemasters.tumblr.com/post/24464587112/peter-thiels-cs183-startup-class-17-deep-thought\">http://blakemasters.tumblr.com/post/24464587112/peter-thiels-cs183-startup-class-17-deep-thought</a></p>\n<p>Some perspectives on AI risk that might be interesting. Peter is (the primary?) donor for SI, and an investor in AGI startup Vicarious Systems.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K5fWkTkYCiAN86PMJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 19, "extendedScore": null, "score": 9.179107342331085e-07, "legacy": true, "legacyId": "16758", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-07T13:30:01.258Z", "modifiedAt": null, "url": null, "title": "Debate between 80,000 hours and a socialist", "slug": "debate-between-80-000-hours-and-a-socialist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WnGZo6aMxwLjTwTmw/debate-between-80-000-hours-and-a-socialist", "pageUrlRelative": "/posts/WnGZo6aMxwLjTwTmw/debate-between-80-000-hours-and-a-socialist", "linkUrl": "https://www.lesswrong.com/posts/WnGZo6aMxwLjTwTmw/debate-between-80-000-hours-and-a-socialist", "postedAtFormatted": "Thursday, June 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Debate%20between%2080%2C000%20hours%20and%20a%20socialist&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADebate%20between%2080%2C000%20hours%20and%20a%20socialist%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWnGZo6aMxwLjTwTmw%2Fdebate-between-80-000-hours-and-a-socialist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Debate%20between%2080%2C000%20hours%20and%20a%20socialist%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWnGZo6aMxwLjTwTmw%2Fdebate-between-80-000-hours-and-a-socialist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWnGZo6aMxwLjTwTmw%2Fdebate-between-80-000-hours-and-a-socialist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 846, "htmlBody": "<p>The current issue of the <a href=\"http://oxfordleftreview.wordpress.com/\">Oxford Left Review</a> has a debate between socialist Pete Mills and two <a href=\"http://80000hours.org/\">80,000 hours</a> people, <a href=\"http://80000hours.org/members/benjamin-todd\">Ben Todd</a> and <a href=\"http://80000hours.org/members/sebastian-farquhar\">Sebastian Farquhar</a>: <a href=\"http://oxfordleftreview.files.wordpress.com/2012/06/olr7_web.pdf\">The Ethical Careers Debate</a>, p4-9. I'm interested in it because I want to understand why people object to the ideas of 80,000 hours. A paraphrasing:</p>\n<blockquote><dl> <dt>Todd and Farquhar</dt><dd> Choose your career to most improve the world. Focus on the <a href=\"http://www.jefftk.com/news/2011-11-08.html\">consequences</a> of your decisions. </dd><dt>Mills</dt><dd> 80,000 hours says you must work a high paying but world-destroying career so you can give more money away. Then \"your interests are aligned with the interests of capital\" and you can't use political means to improve the world because that endangers your career. </dd><dt>Todd and Farquhar</dt><dd> <a href=\"http://www.utilitarian-essays.com/make-money.html\">Professional philanthropy</a> is one option, but so are <a href=\"http://80000hours.org/career-profiles#Scientist\">research</a> and <a href=\"http://80000hours.org/career-profiles#Campaigner\">advocacy</a>. Even if you go the high paying career route, you could <a href=\"http://80000hours.org/career-profiles#Doctor\">be a doctor</a>. Even if you take a people-harming but well paying job you're just <a href=\"http://www.jefftk.com/news/2012-04-03.html\">replacing</a> someone else who would do it instead. <a href=\"http://en.wikipedia.org/wiki/Friedrich_Engels\">Engels</a> was a professional philanthropist, funding <a href=\"http://en.wikipedia.org/wiki/Karl_Marx\">Marx</a>'s reasearch. </dd><dt>Mills</dt><dd> \"80k makes much of replaceability: 'the job will exist whatever you do.' This is stronger than the claim that someone else will become a banker; rather, it states that there will always be bankers, that there will always be exploitation.\" Engles took on too much by trying to be both a professional philanthropist and an activist which drove him to depression and illness. </dd><dt>Todd and Farquhar</dt><dd> Campaigning might be better than professional philanthropy, though you should consider whether you do better to get a well-paying job and fund multiple people to campaign. Replaceability means that a given job will exist whether you take it or not, but \"there might be some things you could do that would cause the job to cease to exist; for instance, by campaigning against banking\". \"Even if you believe capitalism is one of the world's greatest problems, you shouldn't make the seductive inference that you should devote your energies to fighting it. Rather, you should work on the cause that enables you to make the biggest difference. There may be other very big problems which are more tractable.\" </dd><dt>Mills</dt><dd> \"The language of probability will always fail to capture the possibility of system change. What was the expected value of the civil rights movement, or the campaign for universal suffrage, or anticolonial struggles for independence? As we have seen most recently with the Arab Spring, every revolution is impossible, until it is inevitable.\" I don't like that 80,000 hours uses calculations in their attempt to estimate the good you could do through various potential careers. Stop focusing on the individual when the system is the problem. [Other stuff that doesn't make sense to me.] </dd></dl></blockquote>\n<p>As a socialist, Mills really doesn't like the argument that the best way to help the world's poor is probably to work in heavily capitalist industries. He seems to be avoiding engaging with Todd and Farquhar's arguments, especially replaceability. He also really doesn't like looking at things in terms of numbers, I think because numbers suggest certainty. When I calculate that in 50 years of giving away $40K a year you save 1000 lives at $2K each, that's not saying the number is exactly 1000. It's saying 1000 is my best guess, and unless I can come up with a better guess it's the estimate <a href=\"http://80000hours.org/blog/4-estimation-is-the-best-we-have\">I should use</a> when choosing between this career path and other ones. He also doesn't seem to understand prediction and probability: \"every revolution is impossible, until it is inevitable\" may be how it feels for those living under an oppressive regime but it's not our best probability estimate. [1]</p>\n<p>In a previous discussion a friend <a href=\"http://www.jefftk.com/news/2011-10-15.html\">also was mislead calculations</a>. When I said \"one can avert infant deaths for about $500 each\" their response was \"What do they do with the 500 dollars? That doesn't seem to make sense. Do they give the infant a $500 anti-death pill? How do you know it really takes a constant stream of $500 for each infant?\". Have other people run into this? Bad calculations also tend to be distributed widely, with people saying things like \"<a href=\"http://www.americasblood.org/go.cfm?do=page.view&amp;pid=12\">one pint of blood can save up to three lives</a>\" when the expected marginal lives saved is actually tiny. Maybe we should focus less on estimates of effectiveness in smart-giving advocacy? Is there a way to show the huge difference in effect between the best charities and most charities without using these?</p>\n<p>Maybe I should have <em>way</em> more of these discussions, enough that I can collect statistics on what arguments and examples work and which don't.</p>\n<p><em>(I also posted this <a href=\"http://www.jefftk.com/news/2012-06-07.html\">on my blog</a>)</em></p>\n<p><br /> [1] Which is not to say you can't have big jumps in probability estimates. I could put the chance of revolution at 5% somewhere based on historical data but then hear some new information about how one has just started and sounds really promising which bumps my estimate up to 70%. But expected value calculations for jobs can work with numbers like these, it's just \"impossible\" and \"inevitable\" that break estimates.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"se3XDuQ4xbeWvu4eF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WnGZo6aMxwLjTwTmw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 6, "extendedScore": null, "score": 9.179388229854013e-07, "legacy": true, "legacyId": "16759", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-08T04:33:33.855Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] LA-602 vs RHIC Review", "slug": "seq-rerun-la-602-vs-rhic-review", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:02.271Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sXv8sQrXJ9XyQBPmf/seq-rerun-la-602-vs-rhic-review", "pageUrlRelative": "/posts/sXv8sQrXJ9XyQBPmf/seq-rerun-la-602-vs-rhic-review", "linkUrl": "https://www.lesswrong.com/posts/sXv8sQrXJ9XyQBPmf/seq-rerun-la-602-vs-rhic-review", "postedAtFormatted": "Friday, June 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20LA-602%20vs%20RHIC%20Review&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20LA-602%20vs%20RHIC%20Review%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXv8sQrXJ9XyQBPmf%2Fseq-rerun-la-602-vs-rhic-review%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20LA-602%20vs%20RHIC%20Review%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXv8sQrXJ9XyQBPmf%2Fseq-rerun-la-602-vs-rhic-review", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXv8sQrXJ9XyQBPmf%2Fseq-rerun-la-602-vs-rhic-review", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<p>Today's post, <a href=\"/lw/rg/la602_vs_rhic_review/\">LA-602 vs. RHIC Review</a> was originally published on 19 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#LA-602_vs._RHIC_Review\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A comparison of LA-602, the classified report investigating the possibility of a nuclear bomb igniting the atmosphere and killing everyone, and RHIC, the document explaining why the LHC is not going to destroy the world. There is a key difference between these documents: one of them is a genuine discussion of the risks, taking them seriously, and the other is a work of public relations. Work on existential risk needs to be more like the former.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cx5/seq_rerun_ghosts_in_the_machine/\">Ghosts in the Machine</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sXv8sQrXJ9XyQBPmf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 9.183468940586009e-07, "legacy": true, "legacyId": "16777", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["f3W7QbLBA2B7hk84y", "Ee2Hkewt5W6PR79xB", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-08T09:44:21.126Z", "modifiedAt": null, "url": null, "title": "Thoughts and problems with Eliezer's measure of optimization power", "slug": "thoughts-and-problems-with-eliezer-s-measure-of-optimization", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:35.606Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iJNK7rE5jphMSJJCa/thoughts-and-problems-with-eliezer-s-measure-of-optimization", "pageUrlRelative": "/posts/iJNK7rE5jphMSJJCa/thoughts-and-problems-with-eliezer-s-measure-of-optimization", "linkUrl": "https://www.lesswrong.com/posts/iJNK7rE5jphMSJJCa/thoughts-and-problems-with-eliezer-s-measure-of-optimization", "postedAtFormatted": "Friday, June 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20and%20problems%20with%20Eliezer's%20measure%20of%20optimization%20power&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20and%20problems%20with%20Eliezer's%20measure%20of%20optimization%20power%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiJNK7rE5jphMSJJCa%2Fthoughts-and-problems-with-eliezer-s-measure-of-optimization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20and%20problems%20with%20Eliezer's%20measure%20of%20optimization%20power%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiJNK7rE5jphMSJJCa%2Fthoughts-and-problems-with-eliezer-s-measure-of-optimization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiJNK7rE5jphMSJJCa%2Fthoughts-and-problems-with-eliezer-s-measure-of-optimization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1551, "htmlBody": "<p>Back in the day, Eliezer <a href=\"/lw/va/measuring_optimization_power/\">proposed</a> a method for measuring the optimization power (OP) of a system S. The idea is to get a measure of small a target the system can hit:</p>\n<blockquote>\n<p>You can quantify this, at least in theory, supposing you have (A) the agent or optimization process's preference ordering, and (B) a measure of the space of outcomes - which, for discrete outcomes in a finite space of possibilities, could just consist of counting them - then you can quantify how small a target is being hit, within how large a greater region.</p>\n<p>Then we count the total number of states with equal or greater rank in the preference ordering to the outcome achieved, or integrate over the measure of states with equal or greater rank. &nbsp;Dividing this by the total size of the space gives you the relative smallness of the target - did you hit an outcome that was one in a million? &nbsp;One in a trillion?</p>\n<p>Actually, most optimization processes produce \"surprises\" that are exponentially more improbable than this - you'd need to try far more than a trillion random reorderings of the letters in a book, to produce a play of quality equalling or exceeding Shakespeare. &nbsp;So we take the log base two of the reciprocal of the improbability, and that gives us optimization power in bits.</p>\n</blockquote>\n<p>For example, assume there were eight equally likely possible states {<strong>X<sub>0</sub></strong>, <strong>X<sub>1</sub></strong>, ... , <strong>X<sub>7</sub></strong>}, and S gives them utilities {0, 1, ... , 7}. Then if S can make <strong>X<sub>6</sub></strong>&nbsp;happen, there are two states better or equal to its achievement (<strong>X<sub>6</sub></strong> and <strong>X<sub>7</sub></strong>), hence it has hit a target filling 1/4 of the total space. Hence its OP is log<sub>2</sub> 4 = 2. If the best S could manage is <strong>X<sub>4</sub></strong>, then it has only hit half the total space, and has an OP of only log<sub>2</sub> 2 = 1. Conversely, if S reached the perfect <strong>X<sub>7</sub></strong>, 1/8 of the total space, then it would have an OP of log<sub>2</sub> 8 = 3.</p>\n<p><a id=\"more\"></a></p>\n<h2>The system, the whole system, and everything else in the universe</h2>\n<p>Notice that OP is defined in terms of the state that S achieved (for the moment this will be a pure world, but later we'll allow probabilistically mixed worlds to be S's \"achievement\"). So it give us a measure of how powerful S is in practice in our model, not some platonic measure of how good S is in general situations. So an idiot king has more OP than a brilliant peasant; a naive search algorithm distributed&nbsp;across&nbsp;the internet has more OP than a much better program running on <a href=\"http://en.wikipedia.org/wiki/Colossus_computer\">Colossus</a>. This does not seem a drawback to OP: after all, we want to measure how powerful a system actually is, not how powerful it could be in other circumstances.</p>\n<p>Similarly, OP measures the system's ability to achieve its very top goals, not how hard these goals are. A system that wants to compose a brilliant sonnet has more OP than exactly the same system that wants to compose a brilliant sonnet <em>while embodied in the&nbsp;Andromeda&nbsp;galaxy</em>. Even though the second is&nbsp;plausibly&nbsp;more dangerous. So OP is a very imperfect measure of how powerful a system is.</p>\n<p>We could maybe extend this to some sort of \"opposed OP\": what is the optimization power of S, given that humans want to stop it from achieving its goals? But even there, a highly powerful system with nearly un-achievable goals will still have a very low opposed OP. Maybe the difference between the opposed OP and the standard OP is a better measure of power.</p>\n<p>As&nbsp;<a href=\"/r/discussion/lw/cxk/thoughts_and_problems_with_eliezers_measure_of/6s62\">pointed out</a>&nbsp;by Tim Tyler, OP can also increase if we change the size of the solution space. Imagine an agent that has to print out a non-negative integer N, and whose utility is -N. The agent will obviously print 0, but if the printer is limited to ten digit numbers, its OP is smaller than if the printer is limited to twenty digit numbers: though the solution is just as easy and obvious, the number of ways it \"could have been worse\" is increased, increasing OP.</p>\n<p>&nbsp;</p>\n<h2>Is it OP an entropy? Is it defined for mixed states?</h2>\n<p>In his post Eliezer makes a comparison between OP and entropy. And OP does have some of the properties of entropy: for instance if S is optimizing two&nbsp;separate&nbsp;independent processes (and its own utility treats them as independent), then its OP is the sum of the OP for each process. If for instance S hit an area of 1/4 in the first process (OP 2) and 1/8 in the second (OP 3), then it hits an area of 1/(4*8)=1/32 for the joint processes, for an OP of 5. This property, incidentally, is what allows us to talk about \"the\" entropy of an isolated system, without worrying about the rest of the universe.</p>\n<p>But now imagine that our S in the first example can't be sure to hit a pure state, but has 50% chance of hitting <strong>X<sub>7</sub></strong> and 50% of hitting <strong>X<sub>4</sub></strong>. If OP were an entropy, then we'd simply do a weighted sum 1/2(OP(<strong>X<sub>4</sub></strong>)+OP(<strong>X<sub>7</sub></strong>))=1/2(1+3)=2, and then add one extra bit of entropy to represent our (binary) uncertainty as to what state we were in, giving a total OP of 3. But this is the same OP as <strong>X<sub>7</sub></strong> itself! And obviously a 50% of <strong>X<sub>7</sub></strong> and 50% of something inferior cannot be as good as a certainty of the best possible state. So unlike entropy, mere uncertainty cannot increase OP.</p>\n<p>So how should OP extend to mixed states? Can we write a simple distributive law:</p>\n<p style=\"padding-left: 30px;\">OP(1/2 <strong>X<sub>4</sub></strong>&nbsp;+ 1/2 <strong>X<sub>7</sub></strong>) = 1/2(OP(<strong>X<sub>4</sub></strong>) + OP(<strong>X<sub>7</sub></strong>)) = 2?</p>\n<p>It turns out we can't. Imagine that, without changing anything else, the utility of <strong>X<sub>7</sub></strong> is suddenly set to ten trillion, rather than 7. The OP of <strong>X<sub>7</sub></strong> is still 3 - it's still the best option, still with probability 1/8. And yet&nbsp;1/2&nbsp;<strong>X<sub>4</sub></strong>&nbsp;+ 1/2&nbsp;<strong>X</strong><sub style=\"font-weight: bold; \">7</sub>&nbsp;is now obviously much, much better than <strong>X<sub>6</sub></strong>, which has an OP of 2. But now let's reset <strong>X<sub>6</sub></strong> to being ten trillion minus 1. Then it still has a OP of 2, and yet is now much much better than&nbsp;1/2&nbsp;<strong>X<sub>4</sub></strong>&nbsp;+ 1/2&nbsp;<strong>X</strong><sub style=\"font-weight: bold; \">7</sub>.</p>\n<p>But I may have been unfair in those examples. After all, we're looking at mixed states, and <strong>X<sub>6</sub></strong> need not have a fixed OP of 2 in the space of mixed states. Maybe if we looked at the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Simplex\">simplex</a>&nbsp;formed by all mixed states made up of&nbsp;{<strong>X<sub>0</sub></strong>,&nbsp;<strong>X<sub>1</sub></strong>, ... ,&nbsp;<strong>X<sub>7</sub></strong>}, we could get these results to work? Since all <strong>X<sub>i</sub></strong> are equally likely, we'd simply put a uniform measure on that simplex. But now we run into another problem: the OP of <strong>X<sub>7</sub></strong> has suddenly shot up to infinity! After all, <strong>X<sub>7</sub></strong> is now an event of probability zero, better than any other outcome; the log<sub>2</sub> of the inverse of its probability is infinity. Even if we just restrict to a tiny, non-zero area, around <strong>X<sub>7</sub></strong>, we get&nbsp;arbitrarily&nbsp;high OP - it's not a fluke or a calculation error. Which means that if we followed the distributive law, <strong>Q</strong>=(1-10<sup>-1000</sup>) <strong>X<sub>0</sub></strong> +&nbsp; 10<sup>-1000</sup>&nbsp;<strong>X<sub>7</sub></strong> must have a much larger OP than <strong>X<sub>6</sub></strong>&nbsp;- despite the fact that nearly every possible outcome is better than <strong>Q</strong>.</p>\n<p>So it seems that unlike entropy, OP cannot have anything resembling a distributive law. The set of possible outcomes that you started with - including any possible mixed outcomes that S could cause - is what you're going to have to use. This sits uncomfortably with the whole Bayesian philosophy - after all, there mixed states shouldn't represent anything but uncertainty between pure states. They shouldn't be listed as separate outcomes.</p>\n<p>&nbsp;</p>\n<h2>Measures and coarse-graining</h2>\n<p>In the previous section, we moved from using a finite set of equally likely outcomes, to a measure over a simplex of mixed outcomes. This is the natural generalisation of OP: simply compute the probability measure of the states better than what S achieves, and use the log<sub>2</sub> of the inverse of this measure as OP.</p>\n<p>Some of you may have spotted the massive elephant in the room, whose mass twists space and underlines and undermines the definition of OP. What does this probability measure actually&nbsp;represent? Eliezer <a href=\"/lw/va/measuring_optimization_power/\">saw</a>&nbsp;it in his original post:</p>\n<blockquote>\n<p>The quantity we're measuring tells us&nbsp;<em>how improbable this event is, in the absence of optimization, relative to some prior measure that describes the unoptimized probabilities</em>.</p>\n</blockquote>\n<p>Or how could I write \"there were eight equally likely possible states\" <em>and</em>&nbsp;\"S can make&nbsp;<strong>X<sub>6</sub></strong>&nbsp;happen\"? Well, obviously, what I meant was that if S didn't exist, then it would be equally likely that&nbsp;<strong>X</strong><sub style=\"font-weight: bold;\">7</sub>&nbsp;and&nbsp;<strong>X<sub>6</sub></strong> and <strong>X<sub>5</sub></strong>&nbsp;and&nbsp;<strong>X<sub>4</sub></strong>&nbsp;and...</p>\n<p>But wait! These <strong>X<sub>i</sub></strong>'s are final states of the world - so they include the information as to whether S existed in them or not. So what I'm actually saying is that&nbsp;{<strong>X<sub>0</sub></strong>(&not;S),&nbsp;<strong>X<sub>1</sub></strong>(&not;S), ... ,&nbsp;<strong>X<sub>7</sub></strong>(&not;S)} (the worlds with no S) are equally likely, whereas <strong>X<sub>i</sub></strong>(S) (the worlds with S) are impossible for i&ne;6. But what has allowed me to identify&nbsp;<strong>X<sub>0</sub></strong>(&not;S) with&nbsp;<strong>X<sub>0</sub></strong>(S)? I'm claiming they're the same world \"apart from S\" but what does this mean? After all, S can have huge impacts, and&nbsp;<strong>X<sub>0</sub></strong>(S) is actually an impossible world! So I'm saying that \"there two worlds are strictly the same, apart that S exists in one of them, but them again, S would never allow that world to happen if it did exist, so, hum...\"</p>\n<p>Thus it seems that we need to use some sort of coarse-graining to identify <strong>X<sub>i</sub></strong>(&not;S) with <strong>X<sub>i</sub></strong>(S), similar to those I speculated on in the reduced impact <a href=\"/lw/a39/the_mathematics_of_reduced_impact_help_needed/\">post</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nvKzwpiranwy29HFJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iJNK7rE5jphMSJJCa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 35, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "16760", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Back in the day, Eliezer <a href=\"/lw/va/measuring_optimization_power/\">proposed</a> a method for measuring the optimization power (OP) of a system S. The idea is to get a measure of small a target the system can hit:</p>\n<blockquote>\n<p>You can quantify this, at least in theory, supposing you have (A) the agent or optimization process's preference ordering, and (B) a measure of the space of outcomes - which, for discrete outcomes in a finite space of possibilities, could just consist of counting them - then you can quantify how small a target is being hit, within how large a greater region.</p>\n<p>Then we count the total number of states with equal or greater rank in the preference ordering to the outcome achieved, or integrate over the measure of states with equal or greater rank. &nbsp;Dividing this by the total size of the space gives you the relative smallness of the target - did you hit an outcome that was one in a million? &nbsp;One in a trillion?</p>\n<p>Actually, most optimization processes produce \"surprises\" that are exponentially more improbable than this - you'd need to try far more than a trillion random reorderings of the letters in a book, to produce a play of quality equalling or exceeding Shakespeare. &nbsp;So we take the log base two of the reciprocal of the improbability, and that gives us optimization power in bits.</p>\n</blockquote>\n<p>For example, assume there were eight equally likely possible states {<strong>X<sub>0</sub></strong>, <strong>X<sub>1</sub></strong>, ... , <strong>X<sub>7</sub></strong>}, and S gives them utilities {0, 1, ... , 7}. Then if S can make <strong>X<sub>6</sub></strong>&nbsp;happen, there are two states better or equal to its achievement (<strong>X<sub>6</sub></strong> and <strong>X<sub>7</sub></strong>), hence it has hit a target filling 1/4 of the total space. Hence its OP is log<sub>2</sub> 4 = 2. If the best S could manage is <strong>X<sub>4</sub></strong>, then it has only hit half the total space, and has an OP of only log<sub>2</sub> 2 = 1. Conversely, if S reached the perfect <strong>X<sub>7</sub></strong>, 1/8 of the total space, then it would have an OP of log<sub>2</sub> 8 = 3.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"The_system__the_whole_system__and_everything_else_in_the_universe\">The system, the whole system, and everything else in the universe</h2>\n<p>Notice that OP is defined in terms of the state that S achieved (for the moment this will be a pure world, but later we'll allow probabilistically mixed worlds to be S's \"achievement\"). So it give us a measure of how powerful S is in practice in our model, not some platonic measure of how good S is in general situations. So an idiot king has more OP than a brilliant peasant; a naive search algorithm distributed&nbsp;across&nbsp;the internet has more OP than a much better program running on <a href=\"http://en.wikipedia.org/wiki/Colossus_computer\">Colossus</a>. This does not seem a drawback to OP: after all, we want to measure how powerful a system actually is, not how powerful it could be in other circumstances.</p>\n<p>Similarly, OP measures the system's ability to achieve its very top goals, not how hard these goals are. A system that wants to compose a brilliant sonnet has more OP than exactly the same system that wants to compose a brilliant sonnet <em>while embodied in the&nbsp;Andromeda&nbsp;galaxy</em>. Even though the second is&nbsp;plausibly&nbsp;more dangerous. So OP is a very imperfect measure of how powerful a system is.</p>\n<p>We could maybe extend this to some sort of \"opposed OP\": what is the optimization power of S, given that humans want to stop it from achieving its goals? But even there, a highly powerful system with nearly un-achievable goals will still have a very low opposed OP. Maybe the difference between the opposed OP and the standard OP is a better measure of power.</p>\n<p>As&nbsp;<a href=\"/r/discussion/lw/cxk/thoughts_and_problems_with_eliezers_measure_of/6s62\">pointed out</a>&nbsp;by Tim Tyler, OP can also increase if we change the size of the solution space. Imagine an agent that has to print out a non-negative integer N, and whose utility is -N. The agent will obviously print 0, but if the printer is limited to ten digit numbers, its OP is smaller than if the printer is limited to twenty digit numbers: though the solution is just as easy and obvious, the number of ways it \"could have been worse\" is increased, increasing OP.</p>\n<p>&nbsp;</p>\n<h2 id=\"Is_it_OP_an_entropy__Is_it_defined_for_mixed_states_\">Is it OP an entropy? Is it defined for mixed states?</h2>\n<p>In his post Eliezer makes a comparison between OP and entropy. And OP does have some of the properties of entropy: for instance if S is optimizing two&nbsp;separate&nbsp;independent processes (and its own utility treats them as independent), then its OP is the sum of the OP for each process. If for instance S hit an area of 1/4 in the first process (OP 2) and 1/8 in the second (OP 3), then it hits an area of 1/(4*8)=1/32 for the joint processes, for an OP of 5. This property, incidentally, is what allows us to talk about \"the\" entropy of an isolated system, without worrying about the rest of the universe.</p>\n<p>But now imagine that our S in the first example can't be sure to hit a pure state, but has 50% chance of hitting <strong>X<sub>7</sub></strong> and 50% of hitting <strong>X<sub>4</sub></strong>. If OP were an entropy, then we'd simply do a weighted sum 1/2(OP(<strong>X<sub>4</sub></strong>)+OP(<strong>X<sub>7</sub></strong>))=1/2(1+3)=2, and then add one extra bit of entropy to represent our (binary) uncertainty as to what state we were in, giving a total OP of 3. But this is the same OP as <strong>X<sub>7</sub></strong> itself! And obviously a 50% of <strong>X<sub>7</sub></strong> and 50% of something inferior cannot be as good as a certainty of the best possible state. So unlike entropy, mere uncertainty cannot increase OP.</p>\n<p>So how should OP extend to mixed states? Can we write a simple distributive law:</p>\n<p style=\"padding-left: 30px;\">OP(1/2 <strong>X<sub>4</sub></strong>&nbsp;+ 1/2 <strong>X<sub>7</sub></strong>) = 1/2(OP(<strong>X<sub>4</sub></strong>) + OP(<strong>X<sub>7</sub></strong>)) = 2?</p>\n<p>It turns out we can't. Imagine that, without changing anything else, the utility of <strong>X<sub>7</sub></strong> is suddenly set to ten trillion, rather than 7. The OP of <strong>X<sub>7</sub></strong> is still 3 - it's still the best option, still with probability 1/8. And yet&nbsp;1/2&nbsp;<strong>X<sub>4</sub></strong>&nbsp;+ 1/2&nbsp;<strong>X</strong><sub style=\"font-weight: bold; \">7</sub>&nbsp;is now obviously much, much better than <strong>X<sub>6</sub></strong>, which has an OP of 2. But now let's reset <strong>X<sub>6</sub></strong> to being ten trillion minus 1. Then it still has a OP of 2, and yet is now much much better than&nbsp;1/2&nbsp;<strong>X<sub>4</sub></strong>&nbsp;+ 1/2&nbsp;<strong>X</strong><sub style=\"font-weight: bold; \">7</sub>.</p>\n<p>But I may have been unfair in those examples. After all, we're looking at mixed states, and <strong>X<sub>6</sub></strong> need not have a fixed OP of 2 in the space of mixed states. Maybe if we looked at the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Simplex\">simplex</a>&nbsp;formed by all mixed states made up of&nbsp;{<strong>X<sub>0</sub></strong>,&nbsp;<strong>X<sub>1</sub></strong>, ... ,&nbsp;<strong>X<sub>7</sub></strong>}, we could get these results to work? Since all <strong>X<sub>i</sub></strong> are equally likely, we'd simply put a uniform measure on that simplex. But now we run into another problem: the OP of <strong>X<sub>7</sub></strong> has suddenly shot up to infinity! After all, <strong>X<sub>7</sub></strong> is now an event of probability zero, better than any other outcome; the log<sub>2</sub> of the inverse of its probability is infinity. Even if we just restrict to a tiny, non-zero area, around <strong>X<sub>7</sub></strong>, we get&nbsp;arbitrarily&nbsp;high OP - it's not a fluke or a calculation error. Which means that if we followed the distributive law, <strong>Q</strong>=(1-10<sup>-1000</sup>) <strong>X<sub>0</sub></strong> +&nbsp; 10<sup>-1000</sup>&nbsp;<strong>X<sub>7</sub></strong> must have a much larger OP than <strong>X<sub>6</sub></strong>&nbsp;- despite the fact that nearly every possible outcome is better than <strong>Q</strong>.</p>\n<p>So it seems that unlike entropy, OP cannot have anything resembling a distributive law. The set of possible outcomes that you started with - including any possible mixed outcomes that S could cause - is what you're going to have to use. This sits uncomfortably with the whole Bayesian philosophy - after all, there mixed states shouldn't represent anything but uncertainty between pure states. They shouldn't be listed as separate outcomes.</p>\n<p>&nbsp;</p>\n<h2 id=\"Measures_and_coarse_graining\">Measures and coarse-graining</h2>\n<p>In the previous section, we moved from using a finite set of equally likely outcomes, to a measure over a simplex of mixed outcomes. This is the natural generalisation of OP: simply compute the probability measure of the states better than what S achieves, and use the log<sub>2</sub> of the inverse of this measure as OP.</p>\n<p>Some of you may have spotted the massive elephant in the room, whose mass twists space and underlines and undermines the definition of OP. What does this probability measure actually&nbsp;represent? Eliezer <a href=\"/lw/va/measuring_optimization_power/\">saw</a>&nbsp;it in his original post:</p>\n<blockquote>\n<p>The quantity we're measuring tells us&nbsp;<em>how improbable this event is, in the absence of optimization, relative to some prior measure that describes the unoptimized probabilities</em>.</p>\n</blockquote>\n<p>Or how could I write \"there were eight equally likely possible states\" <em>and</em>&nbsp;\"S can make&nbsp;<strong>X<sub>6</sub></strong>&nbsp;happen\"? Well, obviously, what I meant was that if S didn't exist, then it would be equally likely that&nbsp;<strong>X</strong><sub style=\"font-weight: bold;\">7</sub>&nbsp;and&nbsp;<strong>X<sub>6</sub></strong> and <strong>X<sub>5</sub></strong>&nbsp;and&nbsp;<strong>X<sub>4</sub></strong>&nbsp;and...</p>\n<p>But wait! These <strong>X<sub>i</sub></strong>'s are final states of the world - so they include the information as to whether S existed in them or not. So what I'm actually saying is that&nbsp;{<strong>X<sub>0</sub></strong>(\u00acS),&nbsp;<strong>X<sub>1</sub></strong>(\u00acS), ... ,&nbsp;<strong>X<sub>7</sub></strong>(\u00acS)} (the worlds with no S) are equally likely, whereas <strong>X<sub>i</sub></strong>(S) (the worlds with S) are impossible for i\u22606. But what has allowed me to identify&nbsp;<strong>X<sub>0</sub></strong>(\u00acS) with&nbsp;<strong>X<sub>0</sub></strong>(S)? I'm claiming they're the same world \"apart from S\" but what does this mean? After all, S can have huge impacts, and&nbsp;<strong>X<sub>0</sub></strong>(S) is actually an impossible world! So I'm saying that \"there two worlds are strictly the same, apart that S exists in one of them, but them again, S would never allow that world to happen if it did exist, so, hum...\"</p>\n<p>Thus it seems that we need to use some sort of coarse-graining to identify <strong>X<sub>i</sub></strong>(\u00acS) with <strong>X<sub>i</sub></strong>(S), similar to those I speculated on in the reduced impact <a href=\"/lw/a39/the_mathematics_of_reduced_impact_help_needed/\">post</a>.</p>", "sections": [{"title": "The system, the whole system, and everything else in the universe", "anchor": "The_system__the_whole_system__and_everything_else_in_the_universe", "level": 1}, {"title": "Is it OP an entropy? Is it defined for mixed states?", "anchor": "Is_it_OP_an_entropy__Is_it_defined_for_mixed_states_", "level": 1}, {"title": "Measures and coarse-graining", "anchor": "Measures_and_coarse_graining", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q4hLMDrFd8fbteeZ8", "8Nwg7kqAfCM46tuHq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-08T14:17:10.725Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Berlin, Buenos Aires, Cambridge MA, Pittsburgh", "slug": "weekly-lw-meetups-berlin-buenos-aires-cambridge-ma", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.409Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/arTM5678ZrBpceuZr/weekly-lw-meetups-berlin-buenos-aires-cambridge-ma", "pageUrlRelative": "/posts/arTM5678ZrBpceuZr/weekly-lw-meetups-berlin-buenos-aires-cambridge-ma", "linkUrl": "https://www.lesswrong.com/posts/arTM5678ZrBpceuZr/weekly-lw-meetups-berlin-buenos-aires-cambridge-ma", "postedAtFormatted": "Friday, June 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Berlin%2C%20Buenos%20Aires%2C%20Cambridge%20MA%2C%20Pittsburgh&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Berlin%2C%20Buenos%20Aires%2C%20Cambridge%20MA%2C%20Pittsburgh%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FarTM5678ZrBpceuZr%2Fweekly-lw-meetups-berlin-buenos-aires-cambridge-ma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Berlin%2C%20Buenos%20Aires%2C%20Cambridge%20MA%2C%20Pittsburgh%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FarTM5678ZrBpceuZr%2Fweekly-lw-meetups-berlin-buenos-aires-cambridge-ma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FarTM5678ZrBpceuZr%2Fweekly-lw-meetups-berlin-buenos-aires-cambridge-ma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 400, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/ai\">Pittsburgh: Making Beliefs Pay Rent:&nbsp;<span class=\"date\">01 June 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/ak\">Third Buenos Aires Meetup: 2 June 2012 4:00PM:&nbsp;<span class=\"date\">02 June 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/9s\">First Berlin meetup:&nbsp;<span class=\"date\">05 June 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/9p\">Phoenix, Arizona:&nbsp;<span class=\"date\">15 June 2012 07:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/ac\">Less Wrong Cambridge (MA) first-Sundays meetup:&nbsp;<span class=\"date\">03 June 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/ad\">Less Wrong Cambridge (MA) third-Sundays meetup:&nbsp;<span class=\"date\">17 June 2012 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "arTM5678ZrBpceuZr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.186106450096853e-07, "legacy": true, "legacyId": "16598", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-08T19:12:16.599Z", "modifiedAt": null, "url": null, "title": "Near Concerns About Life Extension", "slug": "near-concerns-about-life-extension", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:03.461Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bart119", "createdAt": "2012-04-19T14:46:10.499Z", "isAdmin": false, "displayName": "Bart119"}, "userId": "JQyd5QFQyrRxdo9G4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sLifoy74WEMmRHZSH/near-concerns-about-life-extension", "pageUrlRelative": "/posts/sLifoy74WEMmRHZSH/near-concerns-about-life-extension", "linkUrl": "https://www.lesswrong.com/posts/sLifoy74WEMmRHZSH/near-concerns-about-life-extension", "postedAtFormatted": "Friday, June 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Near%20Concerns%20About%20Life%20Extension&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANear%20Concerns%20About%20Life%20Extension%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsLifoy74WEMmRHZSH%2Fnear-concerns-about-life-extension%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Near%20Concerns%20About%20Life%20Extension%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsLifoy74WEMmRHZSH%2Fnear-concerns-about-life-extension", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsLifoy74WEMmRHZSH%2Fnear-concerns-about-life-extension", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 628, "htmlBody": "<p class=\"MsoNormal\"><span style=\"font-family: &quot;Tahoma&quot;,&quot;sans-serif&quot;;\">There's near thinking and far thinking. While LWers debate far questions, near questions remain. To take a few examples, we in the US still spend large sums on special interests through subsidies and tax breaks, and jockeying for partisan advantage makes pursuit of sound policies very difficult.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Tahoma, sans-serif;\">Cryonic revival, FAIs, UFAIs colonizing other star systems -- they all seem pretty far out in the future to me and a lot of other people. So a tiny band of LWers and like-minded people work on what they have a passion for, which is as it should be.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Tahoma, sans-serif;\">But there is one area where progress really seems feasible within a few years: radical life extension. The right combination of drugs affecting gene expression just might do it.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Tahoma, sans-serif; background-color: white;\">Eliezer, in <a href=\"/lw/rr/the_moral_void/\">The Moral Void</a></span><span style=\"font-family: Tahoma, sans-serif;\"> and elsewhere, makes clear a passionate if not <a href=\"/lw/1mc/normal_cryonics/\">fanatical</a></span><a href=\"/lw/1mc/normal_cryonics/\"></a><span style=\"font-family: Tahoma, sans-serif;\"> commitment to longer life, and it seems the common LW view. Yet surely economics must come into play. If we can give a 60-year-old another day of high-quality life for $10, no one would question that we should. But suppose the cost of each extra day doubles. At 20 days the cost is on the order of $10,000,000 a day. Although we might try to couch it in kinder terms, at some point we end up saying to this person, \"Sorry, you die today because we can't afford what it costs to keep you alive until tomorrow.\" Harsh, but inevitable.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Tahoma, sans-serif;\">I assume exponential costs get through to thinkers even in far mode. I see economics downplayed throughout LW thinking, on the assumption that radical improvements are possible. They might happen in the longer term, but not within the course of a few years. In the next few years, mere linear increases in costs are very relevant.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Tahoma, sans-serif;\">The LW thinking on life extension assumes that (1) it is vigorous, healthy life we will extend, not decrepit, depressed old age, and (2) it will be affordable to all. When thinking to a far future, it's easy to assume such conditions will be met. But when a technology is right in front of us, timing issues can be extremely important.</span></p>\n<p class=\"MsoNormal\">It is a good guess that if life-extension technology comes to the market, the demand will be intense and immediate. It's a good guess that it will be expensive (the cheaper it is, the less drug companies will be motivated to develop it). And it's also a good guess that the first people to sign up in the face of risks and side effects will be the old, who have little to lose.</p>\n<p class=\"MsoNormal\">We face the prospect of sucking up larger and larger portions of our economy extending the lives of decrepit, demoralized old people. (We already face this trend, but the life-extension technologies that are in the offing would make it much worse).</p>\n<p class=\"MsoNormal\">From a distance, thinkers will see an unsustainable pattern. Yet faced with the prospect of immediate extinction, these old people and their loved ones will demand the therapies. They would be upset to think that the drugs are in the closet right down the hall but they aren't eligible to get them. They'd be less upset to know that the drugs are not approved yet, and still less upset to know that there is no clear evidence that they work in humans, and so on.</p>\n<p class=\"MsoNormal\">My plea is to keep life-extension therapies far from the market until all the conditions are in place to solve the problems of cost and making sure that the path is clear to extending healthy life, not decrepitude. This should include an enthusiastic, positive attitude towards life instead of weariness and depression.</p>\n<p class=\"MsoNormal\">There are other reasons why we should be wary of such new technologies, but these short-term, practical ones seem like a clear case that is largely independent of one's utility function.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sLifoy74WEMmRHZSH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -15, "extendedScore": null, "score": 9.187440587064274e-07, "legacy": true, "legacyId": "16789", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K9JSM7d7bLJguMxEp", "hiDkhLyN5S2MEjrSE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-08T19:22:58.835Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh - Past Wrongs", "slug": "meetup-pittsburgh-past-wrongs", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tG4bgbrPzyRF9GEJ9/meetup-pittsburgh-past-wrongs", "pageUrlRelative": "/posts/tG4bgbrPzyRF9GEJ9/meetup-pittsburgh-past-wrongs", "linkUrl": "https://www.lesswrong.com/posts/tG4bgbrPzyRF9GEJ9/meetup-pittsburgh-past-wrongs", "postedAtFormatted": "Friday, June 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%20-%20Past%20Wrongs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%20-%20Past%20Wrongs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtG4bgbrPzyRF9GEJ9%2Fmeetup-pittsburgh-past-wrongs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%20-%20Past%20Wrongs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtG4bgbrPzyRF9GEJ9%2Fmeetup-pittsburgh-past-wrongs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtG4bgbrPzyRF9GEJ9%2Fmeetup-pittsburgh-past-wrongs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ax'>Pittsburgh - Past Wrongs</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 June 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Eatunique, S. Craig street, Pittsburgh</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>What have you been most wrong about in the past? When have you changed your mind? Why were you wrong? How did you become right?</p>\n\n<p>Bring a tale of giant belief updating (or else thoughts on why this has never happened to you!)</p>\n\n<p>Eatunique serves salads, wraps, sandwiches, drinks, and that kind of thing. Call 412-304-6258 if you can't find us.</p>\n\n<p>Our mailing list is at https://groups.google.com/group/lw-pgh?pli=1</p>\n\n<p>NOTE THAT THIS IS ON THURSDAY, AN UNUSUAL DAY</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ax'>Pittsburgh - Past Wrongs</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tG4bgbrPzyRF9GEJ9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.187488985745863e-07, "legacy": true, "legacyId": "16790", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh___Past_Wrongs\">Discussion article for the meetup : <a href=\"/meetups/ax\">Pittsburgh - Past Wrongs</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 June 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Eatunique, S. Craig street, Pittsburgh</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>What have you been most wrong about in the past? When have you changed your mind? Why were you wrong? How did you become right?</p>\n\n<p>Bring a tale of giant belief updating (or else thoughts on why this has never happened to you!)</p>\n\n<p>Eatunique serves salads, wraps, sandwiches, drinks, and that kind of thing. Call 412-304-6258 if you can't find us.</p>\n\n<p>Our mailing list is at https://groups.google.com/group/lw-pgh?pli=1</p>\n\n<p>NOTE THAT THIS IS ON THURSDAY, AN UNUSUAL DAY</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh___Past_Wrongs1\">Discussion article for the meetup : <a href=\"/meetups/ax\">Pittsburgh - Past Wrongs</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh - Past Wrongs", "anchor": "Discussion_article_for_the_meetup___Pittsburgh___Past_Wrongs", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh - Past Wrongs", "anchor": "Discussion_article_for_the_meetup___Pittsburgh___Past_Wrongs1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-08T20:18:35.085Z", "modifiedAt": null, "url": null, "title": "Teaching Bayesianism", "slug": "teaching-bayesianism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.511Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JQuinton", "createdAt": "2011-04-29T16:29:16.319Z", "isAdmin": false, "displayName": "JQuinton"}, "userId": "edHNo35soNo2w6ZwN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zYFhc2dRCs77fv7Bh/teaching-bayesianism", "pageUrlRelative": "/posts/zYFhc2dRCs77fv7Bh/teaching-bayesianism", "linkUrl": "https://www.lesswrong.com/posts/zYFhc2dRCs77fv7Bh/teaching-bayesianism", "postedAtFormatted": "Friday, June 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Teaching%20Bayesianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATeaching%20Bayesianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzYFhc2dRCs77fv7Bh%2Fteaching-bayesianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Teaching%20Bayesianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzYFhc2dRCs77fv7Bh%2Fteaching-bayesianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzYFhc2dRCs77fv7Bh%2Fteaching-bayesianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 278, "htmlBody": "<p>I've had a bit of success with getting people to understand Bayesianism at parties and such, and I'm posting this thought experiment that I came up with to see if it can be improved or if&nbsp;an entirely different thought experiment&nbsp;would be grasped more intuitively in that context:</p>\r\n<blockquote>\r\n<p>Say there is a jar that is filled with dice. There are two types of dice in the jar: One is an 8-sided die with the numbers 1 - 8 and the other is a trick die that has a&nbsp;3 on all faces. The jar has an even distribution between the 8-sided die and the trick die. If a friend of yours grabbed a die from the jar at random and rolled it and told you that the number that landed was a 3, is it more likely that the person grabbed the 8-sided die or the trick die?</p>\r\n</blockquote>\r\n<p>I originally came up with this idea to explain falsifiability which is why I didn't go with say the example in the <a href=\"/lw/1to/what_is_bayesianism/\">better article on Bayesianism</a> (i.e. any other number besides a&nbsp;3 rolled refutes the possibility that the trick die was picked) and having a hypothesis that explains too much contradictory data, so eventually I increase the sides that the die has (like a hypothetical 50-sided die), the different types of die in the jar (100-sided, 6-sided, trick die), and different distributions of die in the jar (90% of the die are 200-sided but a&nbsp;3 is rolled, etc.). Again, I've been discussing this at parties where alcohol is flowing and cognition is impaired yet people understand it, so I figure if it works there then it can be understood intuitively by many people.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zYFhc2dRCs77fv7Bh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 9.187740412742759e-07, "legacy": true, "legacyId": "16791", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AN2cBr6xKWCB8dRQG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-08T23:43:03.288Z", "modifiedAt": null, "url": null, "title": "Ask an experimental physicist", "slug": "ask-an-experimental-physicist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:01.587Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y7e6K22jdb4qEK5Nf/ask-an-experimental-physicist", "pageUrlRelative": "/posts/y7e6K22jdb4qEK5Nf/ask-an-experimental-physicist", "linkUrl": "https://www.lesswrong.com/posts/y7e6K22jdb4qEK5Nf/ask-an-experimental-physicist", "postedAtFormatted": "Friday, June 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ask%20an%20experimental%20physicist&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsk%20an%20experimental%20physicist%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7e6K22jdb4qEK5Nf%2Fask-an-experimental-physicist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ask%20an%20experimental%20physicist%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7e6K22jdb4qEK5Nf%2Fask-an-experimental-physicist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7e6K22jdb4qEK5Nf%2Fask-an-experimental-physicist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p>In response to falenas108's \"Ask an X\" thread. I have a PhD in experimental particle physics; I'm currently working as a postdoc at the University of Cincinnati. Ask me anything, as the saying goes.</p>\n<p>This is an experiment. There's nothing I like better than talking about what I do; but I usually find that even quite well-informed people don't know enough to ask questions sufficiently specific that I can answer any better than the next guy. What goes through most people's heads when they hear \"particle physics\" is, judging by experience, string theory. Well, I dunno nuffin' about string theory - at least not any more than the average layman who has read Brian Greene's book. (Admittedly, neither do string theorists.) I'm equally ignorant about quantum gravity, dark energy, quantum computing, and the Higgs boson - in other words, the big theory stuff that shows up in popular-science articles. For that sort of thing you want a theorist, and not just any theorist at that, but one who works specifically on that problem. On the other hand I'm reasonably well informed about production, decay, and mixing of the charm quark and charmed mesons, but who has heard of that? (Well, now you have.) I know a little about CP violation, a bit about detectors, something about reconstructing and simulating events, a fair amount about how we extract signal from background, and quite a lot about fitting distributions in multiple dimensions.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"csMv9MvvjYJyeHqoo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y7e6K22jdb4qEK5Nf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 49, "extendedScore": null, "score": 0.0005384403465089948, "legacy": true, "legacyId": "16730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 294, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-09T01:31:59.979Z", "modifiedAt": null, "url": null, "title": "Combining causality with algorithmic information theory", "slug": "combining-causality-with-algorithmic-information-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:08.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "rgSakPrcDAcn9X8ae", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hxzgoZp9CiQLPB9Le/combining-causality-with-algorithmic-information-theory", "pageUrlRelative": "/posts/hxzgoZp9CiQLPB9Le/combining-causality-with-algorithmic-information-theory", "linkUrl": "https://www.lesswrong.com/posts/hxzgoZp9CiQLPB9Le/combining-causality-with-algorithmic-information-theory", "postedAtFormatted": "Saturday, June 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Combining%20causality%20with%20algorithmic%20information%20theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACombining%20causality%20with%20algorithmic%20information%20theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxzgoZp9CiQLPB9Le%2Fcombining-causality-with-algorithmic-information-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Combining%20causality%20with%20algorithmic%20information%20theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxzgoZp9CiQLPB9Le%2Fcombining-causality-with-algorithmic-information-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxzgoZp9CiQLPB9Le%2Fcombining-causality-with-algorithmic-information-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 309, "htmlBody": "<p>Warning: maths.</p>\n<p><a href=\"http://arxiv.org/abs/0804.3678\">Causal inference using the algorithmic Markov condition</a> (Janzing and Sch&ouml;lkopf, 2008) replaces conditional independences between random variables, which define the structure of causal graphs, with algorithmic conditional independences between bit strings.</p>\n<p>Conditional probabilities between variables become conditional complexities between strings, i.e. K(x|y) is the length of the shortest program that can generate the string x from y. Similarly, algorithmic mutual information I(x:y) is the amount of information that can be omitted in defining a string y given a shortest compressor for string x, I(x:y) = K(y) - K(y|x*). K(x,y) is the complexity of the concatenation of two strings x and y. These lead naturally to a definition of algorithmic conditional independence as I(x:y|z) = K(x|z) + K(y|z) - K(x,y|z) = 0 , where equality is defined up to the standard additive constant.</p>\n<p>Then a lot of sexy, confusing proofs happen. When the dust settles, it looks like if you take some strings describing observations, interpret them as nodes in a graph, and \"factor\" so that a certain algorithmic Markov condition holds (every node string should be algorithmically independent of its non-descendant node strings given the optimal compressor of its parents' node strings), then every node can be computed by an O(1) program run on a Turing machine, with the node's parents and a noise term as input (with each node's noise string being jointly independent of the others).&nbsp;</p>\n<p>Notably, this means that if we make two observations which were \"generated from their parents by the same complex rule\", then we can \"postulate another causal link between the nodes that explains the similarity of mechanisms\". They say \"complex rule\" because the mutual algorithmic information between simple information strings, like some digits of pi, will be swallowed up by additive constants. Which all seems very close to rediscovering TDT.</p>\n<p>There's more to the paper, but that's the tasty bit, so the summary ends here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DCN2zNscbMZp5aatL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hxzgoZp9CiQLPB9Le", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 21, "extendedScore": null, "score": 9.18915781780743e-07, "legacy": true, "legacyId": "16793", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-09T05:41:45.225Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Heading Toward Morality", "slug": "seq-rerun-heading-toward-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.198Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZHmiuFJQJsf8WJCfa/seq-rerun-heading-toward-morality", "pageUrlRelative": "/posts/ZHmiuFJQJsf8WJCfa/seq-rerun-heading-toward-morality", "linkUrl": "https://www.lesswrong.com/posts/ZHmiuFJQJsf8WJCfa/seq-rerun-heading-toward-morality", "postedAtFormatted": "Saturday, June 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Heading%20Toward%20Morality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Heading%20Toward%20Morality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZHmiuFJQJsf8WJCfa%2Fseq-rerun-heading-toward-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Heading%20Toward%20Morality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZHmiuFJQJsf8WJCfa%2Fseq-rerun-heading-toward-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZHmiuFJQJsf8WJCfa%2Fseq-rerun-heading-toward-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"/lw/rh/heading_toward_morality/\">Heading Toward Morality</a> was originally published on 20 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Heading_Toward_Morality\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A description of the last several months of sequence posts, that identifies the topic that Eliezer actually wants to explain: morality.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cy1/seq_rerun_la602_vs_rhic_review/\">LA-602 vs RHIC Review</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZHmiuFJQJsf8WJCfa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.1902875953415e-07, "legacy": true, "legacyId": "16804", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RFnkagDaJSBLDXEHs", "sXv8sQrXJ9XyQBPmf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-09T10:37:02.223Z", "modifiedAt": null, "url": null, "title": "Are the bacteria/parasites in your gut affecting your thinking?", "slug": "are-the-bacteria-parasites-in-your-gut-affecting-your", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:36.929Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "witzvo", "createdAt": "2012-05-10T07:45:38.575Z", "isAdmin": false, "displayName": "witzvo"}, "userId": "efsbsXkkuRESNruDe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5GhusoM5BSQtbH7ia/are-the-bacteria-parasites-in-your-gut-affecting-your", "pageUrlRelative": "/posts/5GhusoM5BSQtbH7ia/are-the-bacteria-parasites-in-your-gut-affecting-your", "linkUrl": "https://www.lesswrong.com/posts/5GhusoM5BSQtbH7ia/are-the-bacteria-parasites-in-your-gut-affecting-your", "postedAtFormatted": "Saturday, June 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20the%20bacteria%2Fparasites%20in%20your%20gut%20affecting%20your%20thinking%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20the%20bacteria%2Fparasites%20in%20your%20gut%20affecting%20your%20thinking%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GhusoM5BSQtbH7ia%2Fare-the-bacteria-parasites-in-your-gut-affecting-your%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20the%20bacteria%2Fparasites%20in%20your%20gut%20affecting%20your%20thinking%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GhusoM5BSQtbH7ia%2Fare-the-bacteria-parasites-in-your-gut-affecting-your", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GhusoM5BSQtbH7ia%2Fare-the-bacteria-parasites-in-your-gut-affecting-your", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1110, "htmlBody": "<p>There is a variety of science going on, mostly in mice, about how intestinal microbiota (bacteria/parasites/fungi?) can affect behavior, emotion, and mental development (<a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2982.2010.01664.x/full\">Review paper: The micro-biome gut-brain axis, 2011</a>. <a href=\"http://www.nature.com/nrgastro/journal/v6/n5/abs/nrgastro.2009.35.html\">Review paper 2009)</a>. Does any of it have relevance to those of us who'd like to be better rationalists? I'm just beginning to look into this and I'm not a biologist, so I could use help. I'll reference some interesting papers here and give interesting excerpts.</p>\n<p>To start off with something easy to read, here is an excerpt from a&nbsp;<a href=\"http://fhs.mcmaster.ca/main/news/news_2011/gut_anxiety_link_study.html\">press release</a>&nbsp;for this <a href=\"http://www.gastrojournal.org/article/S0016-5085(11)00607-X/abstract\">2011 paper</a>&nbsp;about effects on mice:</p>\n<p style=\"font-size: 13px; margin: 0px 0px 10px; padding: 0px 0px 0px 30px; border: none; text-align: left; \">Working with healthy adult mice, the researchers showed that disrupting the normal bacterial content of the gut with antibiotics produced changes in behaviour; the mice <em>became less cautious or anxious</em>. This change was accompanied by an increase in&nbsp;brain derived neurotrophic factor (BDNF), which has been linked, to depression and anxiety.</p>\n<p style=\"font-size: 13px; margin: 0px 0px 10px; padding: 0px 0px 0px 30px; border: none; text-align: left; \">When oral antibiotics were discontinued, bacteria in the gut returned to normal.</p>\n<p style=\"font-size: 13px; margin: 0px 0px 10px; padding: 0px 0px 0px 30px; border: none; text-align: left; \">\"This was accompanied by <em>restoration of normal behaviour</em> and brain chemistry,\" Collins said.</p>\n<p style=\"font-size: 13px; margin: 0px 0px 10px; padding: 0px 0px 0px 30px; border: none; text-align: left; \">To confirm that bacteria can influence behaviour, the researchers colonized germ-free mice with <em>bacteria taken from mice with a different behavioural pattern</em>. They found that when germ-free mice with a genetic background associated with passive behaviour were colonized with bacteria from mice with higher exploratory behaviour, <em>they became more active and daring</em>. Similarly, normally active mice <em>became more passive</em> after receiving bacteria from mice whose genetic background is associated with passive behaviour.</p>\n<p style=\"font-size: 13px; margin: 0px 0px 10px; padding: 0px 0px 0px 30px; border: none; text-align: left; \">While previous research has focused on the role bacteria play in brain development early in life, Collins said this latest research indicates that while many factors determine behaviour, the nature and stability of bacteria in the gut appear to influence behaviour and any disruption, from antibiotics or infection, might produce changes in behaviour.</p>\n<p>The writing in the abstract is a lot more abstruse, but perhaps substantially more accurate too. I'd be happy to know if anyone can translate it more accurately than this press release. (E.g. they didn't just transfer bacteria)</p>\n<p>Naturally, the experimenters are doing things to the mice they'd never get away with on people, and so less is known about the relevance to humans. For example, normal humans may already have reasonably healthy bacteria. College students, for example, did not have a significant reduction in exam-related anxiety by consuming yogurt rather than placebo [<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20216564?dopt=Abstract\">citation</a>]. Nor did healthy 1 to 3 year olds miss less school because by consuming a Gerber product with yogurt [<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20216564?dopt=Abstract\">citation</a>].</p>\n<p>However, humans with chronic fatigue syndrome who consumed a potent pro-biotic had reduced anxiety (but not reduced depression): [<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2664325/?tool=pubmed\">citation</a>]</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #f7f7f8; \">\n<p style=\"margin: 0px 0px 1em; \">Chronic fatigue syndrome (CFS) is complex illness of unknown etiology. Among the broad range of symptoms, many patients report disturbances in the emotional realm, the most frequent of which is anxiety. Research shows that patients with CFS and other so-called functional somatic disorders have alterations in the intestinal microbial flora. Emerging studies have suggested that pathogenic and non-pathogenic gut bacteria might influence mood-related symptoms and even behavior in animals and humans. In this pilot study, 39 CFS patients were randomized to receive either 24 billion colony forming units of Lactobacillus casei strain Shirota (LcS) or a placebo daily for two months. Patients provided stool samples and completed the Beck Depression and Beck Anxiety Inventories before and after the intervention. We found a significant rise in both Lactobacillus and Bifidobacteria in those taking the LcS, and there was also a`&nbsp;<em>significant decrease in anxiety symptoms among those taking the probiotic vs controls</em>&nbsp;(p = 0.01). These results lend <em>further support to the presence of a gut-brain interface</em>, one that may be mediated by microbes that reside or pass through the intestinal tract.</p>\n</blockquote>\n<p>Interestingly, the ongoing <a href=\"http://clinicaltrials.gov/ct2/results?term=chronic+fatigue+syndrome&amp;type=Intr&amp;rcv_s=01%2F01%2F2009&amp;rcv_e=01%2F01%2F2013&amp;pg=2\">clinical trials</a>&nbsp;don't seem to be following up on this line of treatment, instead focusing mainly on drugs or alternative therapy. The jury seems to still be out in <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/21992953\">inflammatory bowel disease</a>.</p>\n<p>Late-onset autism, is associated with bacterial irregularities [<a href=\"http://www.de-poort.be/cgi-bin/Document.pl?id=33\">citation</a>] and can respond to oral vancomycin (an antibiotic) [<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/21524713\">citation</a>].</p>\n<p>Going back to mice, these findings indicate that the bacteria that are present during development have a lasting effect:</p>\n<p style=\"padding-left: 30px; \"><a style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; color: #8a8a8b; \" rel=\"nofollow\" href=\"http://www.pnas.org/content/108/7/3047.full.pdf\">Hejitz et.al.: Normal gut microbiota modulates brain development and behavior, 2011</a><span style=\"background-color: #f7f7f8; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #f7f7f8; padding-left: 30px; \">Here, we report that colonization by <em>gut microbiota impacts mammalian brain development and subsequent adult behavio</em>r. Using measures of motor activity and anxiety-like behavior, we demonstrate that germ free (GF) mice display increased motor activity and reduced anxiety, compared with specific pathogen free (SPF) mice with a normal gut microbiota. ... Hence, our results suggest that the microbial colonization process initiates signaling mechanisms that affect neuronal circuits involved in motor control and anxiety behavior.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #f7f7f8; \">They found that if they \"conventionalized\" the germ free mice early enough (i.e. if they infected them with the usual bacteria) that they'd be pretty much normal. However, if they tried to do this on adults, it was too late, so the absence of the bacteria during development had an enduring effect on behavior.</p>\n<p style=\"margin: 0px 0px 1em; padding-left: 30px; \"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Encouraged by the observation that early colonization of GF mice&nbsp;</span></span><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \">could normalize several behavioral patterns of GF mice, we explored whether there is a sensitive/critical period for the effects of&nbsp;</span><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \">the normal gut microbiota on behavior. We therefore conventionalized adult GF mice and studied their behavior in open \ufb01eld&nbsp;</span><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \">test as described above. Notably, conventionalization of adult mice&nbsp;</span><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \">failed to normalize the behavior of GF mice (Fig. 1F and Fig. S4).</span></p>\n<p><span style=\"background-color: #f7f7f8; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Conversely, if the animal is subjected to stress, this can change the composition of microbiota. [</span><a style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"http://farncombe.mcmaster.ca/documents/CollinsBercikGastroenterology200913662003-14.pdf\">citation</a><span style=\"background-color: #f7f7f8; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">]</span></p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #f7f7f8; \">\n<p style=\"margin: 0px 0px 1em; \">Although many people are aware of the communication that occurs between the gastrointestinal (GI) tract and the central nervous system, fewer know about the ability of the central nervous system to influence the microbiota or of the microbiota's influence on the brain and behavior. Within the GI tract, the microbiota have a mutually beneficial relationship with their host that maintains normal mucosal immune function, epithelial barrier integrity, motility, and nutrient absorption. Disruption of this relationship alters GI function and disease susceptibility. Animal studies suggest that <em>perturbations of behavior, such as stress, can change the composition of the microbiota</em>; these changes are associated with increased vulnerability to inflammatory stimuli in the GI tract. The mechanisms that underlie these alterations are likely to involve stress-induced changes in GI physiology that alter the habitat of enteric bacteria. Furthermore, <em>experimental perturbation of the microbiota can alter behavior, and the behavior of germ-free mice differs from that of colonized mice</em>. Gaining a better understanding of the relationship between behavior and the microbiota could provide insight into the pathogenesis of functional and inflammatory bowel disorders.</p>\n</blockquote>\n<p>Whew. That's a lot of material. And there's still so much more to find out. Let me know what you find, or what you think the next experiment should be.</p>\n<p>DISCLAIMER: I am not an expert in medicine. Nothing here should be construed as medical advice. See your doctor.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5GhusoM5BSQtbH7ia", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 19, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "16818", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-09T15:15:42.409Z", "modifiedAt": null, "url": null, "title": "Conspiracy Theories as Agency Fictions", "slug": "conspiracy-theories-as-agency-fictions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.835Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Peo8jAyjGL9kWoYAH/conspiracy-theories-as-agency-fictions", "pageUrlRelative": "/posts/Peo8jAyjGL9kWoYAH/conspiracy-theories-as-agency-fictions", "linkUrl": "https://www.lesswrong.com/posts/Peo8jAyjGL9kWoYAH/conspiracy-theories-as-agency-fictions", "postedAtFormatted": "Saturday, June 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Conspiracy%20Theories%20as%20Agency%20Fictions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConspiracy%20Theories%20as%20Agency%20Fictions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPeo8jAyjGL9kWoYAH%2Fconspiracy-theories-as-agency-fictions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Conspiracy%20Theories%20as%20Agency%20Fictions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPeo8jAyjGL9kWoYAH%2Fconspiracy-theories-as-agency-fictions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPeo8jAyjGL9kWoYAH%2Fconspiracy-theories-as-agency-fictions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1283, "htmlBody": "<p><strong>Related to:</strong> <a href=\"http://www.overcomingbias.com/2011/04/consider-conspiracies.html\">Consider Conspiracies</a>, <a href=\"/lw/5ku/what_causes_people_to_believe_in_conspiracy/\">What causes people to believe in conspiracy theories?</a></p>\n<p><em>Here I consider in some detail a failure mode that classical rationality often recognizes. Unfortunately nearly all heuristics normally used to detect it seem remarkably vulnerable to misfiring or being <a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">exploited</a> by others. I advocate an approach where we try our best to account for the key bias, seeing agency where there is none, while trying to minimize the risk of being tricked into dismissing claims because of <a href=\"http://wiki.lesswrong.com/wiki/Applause_light\">boo lights</a>. &nbsp; </em></p>\n<h3>What does calling something a \"conspiracy theory\" tell us? <br /></h3>\n<p>What is a conspiracy theory? Explanations that invoke plots orchestrated by covert groups are easily called or thought of as such. In a more legal sense conspiracy is an agreement between persons to mislead or defraud others. This simple story gets complicated because people aren't very clear on what they consider a conspiracy.</p>\n<p>To give an example, is explicit negotiation or agreement really necessary to call something a conspiracy? Does <em>silent</em> cooperation on <a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_dilemma\">Prisoner's Dilemma</a> count? What if the players are deceiving themselves that they are really following a different goal and the resulting cooperation is just a side effect? How could we tell the difference and would it matter? The latter is especially interesting if one applies the <a href=\"http://en.wikipedia.org/wiki/Anthropic_principle\">anthropic principle</a> to social attitudes and norms.</p>\n<p>The phrase is also <strong>a convenient tool</strong> to mark an opponent's tale as low status and unworthy of further investigation. A <a href=\"/lw/jb/applause_lights/\">boo light</a> easily applied to anything that has people acting in something that can be framed as self-interest and happens to be few <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential jumps</a> away from the audience. Not only is its use in this way <a href=\"http://en.wiktionary.org/wiki/conspiracy_theory\">well known</a>, this is arguably the <em>primary meaning</em> of calling an argument a conspiracy theory.</p>\n<p>We have <em>plenty</em> of <a href=\"/r/lesswrong/lw/cz7/conspiracy_theories_as_agency_fictions/6sl6\">historical examples</a> of <span>high-stakes conspiracies</span> so we know <strong>they <em>can</em> be the right answer</strong>. Noting this and putting aside the misuse of the label, people <em>do</em> engage in crafting conspiracy theories when they just aren't needed. Entire communities can fixate on them or fail to call such bad thinking out. Why does this happen? Humans being the social animals that we are, the group dynamics at work probably need an article or sequence of their own. It should suffice for now to point to <a href=\"http://wiki.lesswrong.com/wiki/Belief_as_attire\">belief as attire</a>, the <a href=\"http://en.wikipedia.org/wiki/Bandwagon_effect\">bandwagon effect</a> and Robin Hanson's take on status. <strong>Let's rather consider the question of why individuals may be biased towards such explanations. </strong><a href=\"/lw/19m/privileging_the_hypothesis/\">Why do they privilege the hypothesis?</a><em><br /></em></p>\n<h3>When do they seem more likely than they are?<br /></h3>\n<p>First off we have a hard time understanding that <a href=\"http://www.overcomingbias.com/2010/02/coordination-is-hard.html\">coordination is hard</a>. Seeing a large pay off available and thinking it easily in reach if \"we could just get along\" seems like a classical failing. Our pro-social sentiments lead us to downplay such barriers in our future plans. Motivated cognition on behalf of assessing the threat potential of perceived <em>enemies</em> or <em>strangers</em> likely shares this problem. Even if we avoid this, we may still be lost since the second big relevant thing is our tendency for anthropomorphizing things that better not be. Ours is a paranoid brain seeing agency in every shadow or strange sound. The cost of false positives was once reasonably low, while the cost of a false negative very high.</p>\n<p>Our minds are also just plain lazy. We are pretty good at modelling other human minds and considering just how hard the task really is, we do a pretty remarkable job of it. If you are stuck in relative ignorance on a subject, say the weather, dancing to appease the sky spirits makes sense. After all the weather is pretty capricious and angry sky spirits is a model that makes as much or more sense as any other model you know. Unlike some other models this one is at least cheap to run on your brain! The modern world is remarkably complex. Do we see ghosts in it?</p>\n<p>Our <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Dunbar%27s_number\">Dunbarian</a> minds probably just plain can't <em>get</em> how a society can be that complex and unpredictable without it being \"planned\" by a cabal of Satan or Heterosexual White Males or the Illuminati (but I repeat myself twice) scheming to make weird things happen in our oblivious small stone age tribe. <strong>Learning about useful models helps people escape anthropomorphizing human society or the economy or government</strong>. The latter is particularly salient. I think most people slip up occasionally in assuming that say something like the United States government can be successfully modelled as a single agent to explain most of its \"actions\". To make matters worse it is a common literary device used by pundits.</p>\n<p><span>A mysterious malignant agency or someone keeping a secret playing the role of the villain makes a good story.</span> <em><a href=\"http://wiki.lesswrong.com/wiki/Narrative_fallacy\">Humans love stories</a></em>. Its <em>fun</em> to think in stories. Any real conspiracy revealed will probably be widely publicized. <span class=\"reference-text\">Peter Knight in his <a href=\"http://www.amazon.com/Conspiracy-Theories-American-History-Encyclopedia/dp/1576078124\">2003 book</a> </span>cites historians who have put forward the idea, that the United States is something of a home for popular conspiracy theories <em>because</em> <em>so many high-level ones have been undertaken and uncovered since the 1960s</em><span class=\"reference-text\">. </span><span>We are more likely to <em>hear</em> about real confirmed conspiracies today than ever before.</span></p>\n<p>Wishful thinking also plays a role. <strong>A universe where bad things happen because bad people make them to is appealing.</strong> Getting rid of bad people, even very bad people, is easy compared to all the different things one has to do to make sure bad things don't happen in a universe that doesn't care about us and where <a href=\"/lw/uk/beyond_the_reach_of_god/\">really bad things</a> are allowed to happen. Finding bad people whether there <a href=\"http://en.wikipedia.org/wiki/Scapegoating\">are or aren't</a> is a problematic tendency. The sad thing is that this may also be how we often <a href=\"http://blakemasters.tumblr.com/post/24578683805/peter-thiels-cs183-startup-class-18-notes-essay\">manage to coordinate</a>. Do all theories of <a href=\"http://en.wikipedia.org/wiki/Legitimacy_%28political%29\">legitimacy</a> also perhaps rest on the <a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2589\">same cognitive failings</a> that conspiracy theories do? The difference between a shadowy cabal we need to get rid of and an institution worthy of respect may be just some bad luck.</p>\n<h3><span>How this misleads us</span></h3>\n<p>Putting aside such wild speculation, what should we take away from this? When do conspiracy theories seem more likely than they are?</p>\n<ul>\n<li>The phenomena is unpredictable or can't be modelled very well </li>\n<li>Models used by others are hard to understand or are very counter-intuitive</li>\n<li>Thinking about the subject significantly strains cognitive resources</li>\n<li>The theory explains why bad things happen or why something went wrong</li>\n<li>The theory requires coordination </li>\n</ul>\n<p>When you see these features you probably find the theory more plausible than it is.&nbsp;</p>\n<p>But how many here are likely to accept \"conspiracy theories\"? To do so with stuff that actually gets called a conspiracy theory doesn't fit our <a href=\"/lw/i7/belief_as_attire/\">tribal attire</a>. <a href=\"http://wiki.lesswrong.com/wiki/Reversed_stupidity_is_not_intelligence\">Reverse stupidity</a> may be particularly problematic for us on this topic. <em>Being open to thinking conspiracy is recommended.</em> Just remember to compare how probable it is in relation to other explanations. <strong>It is important to call out people who misuse the tag for rhetorical gain.</strong></p>\n<p>This applies to debunking as well. <a href=\"/tag/majoritarianism/\">Don't go wildly contrarian</a>. But remember that even things that are tagged conspiracy theories are surprisingly popular. How popular might false theories that avoid that tag be? History shows us we don't have the luxury of hoping that kind of thing just doesn't happen in human societies. <strong>When assessing an explanation sharing the key features that make conspiracy theories seem more plausible than they are, compensate as you would with a conspiracy theory.</strong><strong>&nbsp;</strong></p>\n<p>But don't listen to me, I'm talking conspiracy theories.&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p><strong>Note: </strong>This article started out as a <a href=\"/lw/c4f/open_thread_may_115_2012/6hi3\">public draft</a>, feedback to <a href=\"/lw/ecf/open_thread_september_115_2012/7bl3\">other such drafts</a> is always welcomed.&nbsp; Special thanks to user <strong><a href=\"/user/Viliam_Bur/\">Villiam_Bur</a></strong> for his <a href=\"/lw/c4f/open_thread_may_115_2012/6hul\">commentary</a> and user <strong><a href=\"/user/copt\">copt</a></strong> for proofreading and suggestions. Also thanks to the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">LessWrong IRC chatroom</a> for last minute corrections and stylistic tips.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 2, "5f5c37ee1b5cdee568cfb1b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Peo8jAyjGL9kWoYAH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 43, "extendedScore": null, "score": 0.000145, "legacy": true, "legacyId": "16819", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related to:</strong> <a href=\"http://www.overcomingbias.com/2011/04/consider-conspiracies.html\">Consider Conspiracies</a>, <a href=\"/lw/5ku/what_causes_people_to_believe_in_conspiracy/\">What causes people to believe in conspiracy theories?</a></p>\n<p><em>Here I consider in some detail a failure mode that classical rationality often recognizes. Unfortunately nearly all heuristics normally used to detect it seem remarkably vulnerable to misfiring or being <a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">exploited</a> by others. I advocate an approach where we try our best to account for the key bias, seeing agency where there is none, while trying to minimize the risk of being tricked into dismissing claims because of <a href=\"http://wiki.lesswrong.com/wiki/Applause_light\">boo lights</a>. &nbsp; </em></p>\n<h3 id=\"What_does_calling_something_a__conspiracy_theory__tell_us__\">What does calling something a \"conspiracy theory\" tell us? <br></h3>\n<p>What is a conspiracy theory? Explanations that invoke plots orchestrated by covert groups are easily called or thought of as such. In a more legal sense conspiracy is an agreement between persons to mislead or defraud others. This simple story gets complicated because people aren't very clear on what they consider a conspiracy.</p>\n<p>To give an example, is explicit negotiation or agreement really necessary to call something a conspiracy? Does <em>silent</em> cooperation on <a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_dilemma\">Prisoner's Dilemma</a> count? What if the players are deceiving themselves that they are really following a different goal and the resulting cooperation is just a side effect? How could we tell the difference and would it matter? The latter is especially interesting if one applies the <a href=\"http://en.wikipedia.org/wiki/Anthropic_principle\">anthropic principle</a> to social attitudes and norms.</p>\n<p>The phrase is also <strong>a convenient tool</strong> to mark an opponent's tale as low status and unworthy of further investigation. A <a href=\"/lw/jb/applause_lights/\">boo light</a> easily applied to anything that has people acting in something that can be framed as self-interest and happens to be few <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential jumps</a> away from the audience. Not only is its use in this way <a href=\"http://en.wiktionary.org/wiki/conspiracy_theory\">well known</a>, this is arguably the <em>primary meaning</em> of calling an argument a conspiracy theory.</p>\n<p>We have <em>plenty</em> of <a href=\"/r/lesswrong/lw/cz7/conspiracy_theories_as_agency_fictions/6sl6\">historical examples</a> of <span>high-stakes conspiracies</span> so we know <strong>they <em>can</em> be the right answer</strong>. Noting this and putting aside the misuse of the label, people <em>do</em> engage in crafting conspiracy theories when they just aren't needed. Entire communities can fixate on them or fail to call such bad thinking out. Why does this happen? Humans being the social animals that we are, the group dynamics at work probably need an article or sequence of their own. It should suffice for now to point to <a href=\"http://wiki.lesswrong.com/wiki/Belief_as_attire\">belief as attire</a>, the <a href=\"http://en.wikipedia.org/wiki/Bandwagon_effect\">bandwagon effect</a> and Robin Hanson's take on status. <strong>Let's rather consider the question of why individuals may be biased towards such explanations. </strong><a href=\"/lw/19m/privileging_the_hypothesis/\">Why do they privilege the hypothesis?</a><em><br></em></p>\n<h3 id=\"When_do_they_seem_more_likely_than_they_are_\">When do they seem more likely than they are?<br></h3>\n<p>First off we have a hard time understanding that <a href=\"http://www.overcomingbias.com/2010/02/coordination-is-hard.html\">coordination is hard</a>. Seeing a large pay off available and thinking it easily in reach if \"we could just get along\" seems like a classical failing. Our pro-social sentiments lead us to downplay such barriers in our future plans. Motivated cognition on behalf of assessing the threat potential of perceived <em>enemies</em> or <em>strangers</em> likely shares this problem. Even if we avoid this, we may still be lost since the second big relevant thing is our tendency for anthropomorphizing things that better not be. Ours is a paranoid brain seeing agency in every shadow or strange sound. The cost of false positives was once reasonably low, while the cost of a false negative very high.</p>\n<p>Our minds are also just plain lazy. We are pretty good at modelling other human minds and considering just how hard the task really is, we do a pretty remarkable job of it. If you are stuck in relative ignorance on a subject, say the weather, dancing to appease the sky spirits makes sense. After all the weather is pretty capricious and angry sky spirits is a model that makes as much or more sense as any other model you know. Unlike some other models this one is at least cheap to run on your brain! The modern world is remarkably complex. Do we see ghosts in it?</p>\n<p>Our <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Dunbar%27s_number\">Dunbarian</a> minds probably just plain can't <em>get</em> how a society can be that complex and unpredictable without it being \"planned\" by a cabal of Satan or Heterosexual White Males or the Illuminati (but I repeat myself twice) scheming to make weird things happen in our oblivious small stone age tribe. <strong>Learning about useful models helps people escape anthropomorphizing human society or the economy or government</strong>. The latter is particularly salient. I think most people slip up occasionally in assuming that say something like the United States government can be successfully modelled as a single agent to explain most of its \"actions\". To make matters worse it is a common literary device used by pundits.</p>\n<p><span>A mysterious malignant agency or someone keeping a secret playing the role of the villain makes a good story.</span> <em><a href=\"http://wiki.lesswrong.com/wiki/Narrative_fallacy\">Humans love stories</a></em>. Its <em>fun</em> to think in stories. Any real conspiracy revealed will probably be widely publicized. <span class=\"reference-text\">Peter Knight in his <a href=\"http://www.amazon.com/Conspiracy-Theories-American-History-Encyclopedia/dp/1576078124\">2003 book</a> </span>cites historians who have put forward the idea, that the United States is something of a home for popular conspiracy theories <em>because</em> <em>so many high-level ones have been undertaken and uncovered since the 1960s</em><span class=\"reference-text\">. </span><span>We are more likely to <em>hear</em> about real confirmed conspiracies today than ever before.</span></p>\n<p>Wishful thinking also plays a role. <strong>A universe where bad things happen because bad people make them to is appealing.</strong> Getting rid of bad people, even very bad people, is easy compared to all the different things one has to do to make sure bad things don't happen in a universe that doesn't care about us and where <a href=\"/lw/uk/beyond_the_reach_of_god/\">really bad things</a> are allowed to happen. Finding bad people whether there <a href=\"http://en.wikipedia.org/wiki/Scapegoating\">are or aren't</a> is a problematic tendency. The sad thing is that this may also be how we often <a href=\"http://blakemasters.tumblr.com/post/24578683805/peter-thiels-cs183-startup-class-18-notes-essay\">manage to coordinate</a>. Do all theories of <a href=\"http://en.wikipedia.org/wiki/Legitimacy_%28political%29\">legitimacy</a> also perhaps rest on the <a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2589\">same cognitive failings</a> that conspiracy theories do? The difference between a shadowy cabal we need to get rid of and an institution worthy of respect may be just some bad luck.</p>\n<h3 id=\"How_this_misleads_us\"><span>How this misleads us</span></h3>\n<p>Putting aside such wild speculation, what should we take away from this? When do conspiracy theories seem more likely than they are?</p>\n<ul>\n<li>The phenomena is unpredictable or can't be modelled very well </li>\n<li>Models used by others are hard to understand or are very counter-intuitive</li>\n<li>Thinking about the subject significantly strains cognitive resources</li>\n<li>The theory explains why bad things happen or why something went wrong</li>\n<li>The theory requires coordination </li>\n</ul>\n<p>When you see these features you probably find the theory more plausible than it is.&nbsp;</p>\n<p>But how many here are likely to accept \"conspiracy theories\"? To do so with stuff that actually gets called a conspiracy theory doesn't fit our <a href=\"/lw/i7/belief_as_attire/\">tribal attire</a>. <a href=\"http://wiki.lesswrong.com/wiki/Reversed_stupidity_is_not_intelligence\">Reverse stupidity</a> may be particularly problematic for us on this topic. <em>Being open to thinking conspiracy is recommended.</em> Just remember to compare how probable it is in relation to other explanations. <strong>It is important to call out people who misuse the tag for rhetorical gain.</strong></p>\n<p>This applies to debunking as well. <a href=\"/tag/majoritarianism/\">Don't go wildly contrarian</a>. But remember that even things that are tagged conspiracy theories are surprisingly popular. How popular might false theories that avoid that tag be? History shows us we don't have the luxury of hoping that kind of thing just doesn't happen in human societies. <strong>When assessing an explanation sharing the key features that make conspiracy theories seem more plausible than they are, compensate as you would with a conspiracy theory.</strong><strong>&nbsp;</strong></p>\n<p>But don't listen to me, I'm talking conspiracy theories.&nbsp;</p>\n<p>&nbsp;</p>\n<hr>\n<p><strong>Note: </strong>This article started out as a <a href=\"/lw/c4f/open_thread_may_115_2012/6hi3\">public draft</a>, feedback to <a href=\"/lw/ecf/open_thread_september_115_2012/7bl3\">other such drafts</a> is always welcomed.&nbsp; Special thanks to user <strong><a href=\"/user/Viliam_Bur/\">Villiam_Bur</a></strong> for his <a href=\"/lw/c4f/open_thread_may_115_2012/6hul\">commentary</a> and user <strong><a href=\"/user/copt\">copt</a></strong> for proofreading and suggestions. Also thanks to the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">LessWrong IRC chatroom</a> for last minute corrections and stylistic tips.</p>", "sections": [{"title": "What does calling something a \"conspiracy theory\" tell us? ", "anchor": "What_does_calling_something_a__conspiracy_theory__tell_us__", "level": 1}, {"title": "When do they seem more likely than they are?", "anchor": "When_do_they_seem_more_likely_than_they_are_", "level": 1}, {"title": "How this misleads us", "anchor": "How_this_misleads_us", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "117 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KG9xzRiQQm8tYXfkQ", "dLbkrPu5STNCBLRjr", "X2AD2LgtKgkRNPj2a", "sYgv4eYH82JEsTD34", "nYkMLFpx77Rz3uo9c"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-09T23:29:09.625Z", "modifiedAt": null, "url": null, "title": "Proposal: Show up and down votes separately", "slug": "proposal-show-up-and-down-votes-separately", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.817Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "witzvo", "createdAt": "2012-05-10T07:45:38.575Z", "isAdmin": false, "displayName": "witzvo"}, "userId": "efsbsXkkuRESNruDe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WTxYChzB5twRjdmXN/proposal-show-up-and-down-votes-separately", "pageUrlRelative": "/posts/WTxYChzB5twRjdmXN/proposal-show-up-and-down-votes-separately", "linkUrl": "https://www.lesswrong.com/posts/WTxYChzB5twRjdmXN/proposal-show-up-and-down-votes-separately", "postedAtFormatted": "Saturday, June 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%3A%20Show%20up%20and%20down%20votes%20separately&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%3A%20Show%20up%20and%20down%20votes%20separately%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWTxYChzB5twRjdmXN%2Fproposal-show-up-and-down-votes-separately%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%3A%20Show%20up%20and%20down%20votes%20separately%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWTxYChzB5twRjdmXN%2Fproposal-show-up-and-down-votes-separately", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWTxYChzB5twRjdmXN%2Fproposal-show-up-and-down-votes-separately", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 586, "htmlBody": "<p>One of the most interesting things about this site is the karma scoring, and that it reflects (to a greater degree than you see elsewhere) an objective assessment of the merits of an argument.</p>\n<p>[Edit^6: the proposal in this post is related to the <a href=\"/lw/1s/lesswrong_antikibitzer_hides_comment_authors_and/\">Kibitzer system</a>, but this post discusses adding information, while that system concentrates on taking information away. Special thanks for&nbsp;<a href=\"/lw/cz8/proposal_show_up_and_down_votes_separately/6szc\">matt's comment</a>&nbsp;and&nbsp;to&nbsp;<a href=\"/lw/cz8/proposal_show_up_and_down_votes_separately/6sit\">Vincentyu</a>&nbsp;for being the first to point to prior discussion. &nbsp;A related issue is discussed&nbsp;<a href=\"/lw/z/information_cascades/\">here</a>&nbsp;(2009) with reference to a <a href=\"http://en.wikipedia.org/wiki/Information_cascade\">wikipedia</a>, and on which Eliezer said \"<span style=\"background-color: #f7f7f8; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">I may end up linking this from the About page when it comes time to explain suggested voting policies\"</span>). <em>Data: It took me ~2 days of effort to obtain get linked to this information </em>(09 June 2012 11:29PM -&gt; 11 June 2012 10:28:26PM).]</p>\n<p>Suppose a controversial post/comment has six up votes and three down votes. Right now we only see the net result: 3 points, but when the voting is mixed we're losing important information. <strong>If it's reasonably easy to implement, could we please show up and down tallies separately?&nbsp;</strong>E.g show \"3 points (+6,-3)\", at least when the voting is mixed? I think the negative votes are the single most important thing. In particular, I want to know about negative votes I receive and where I receive them, because those are the posts where I need to think carefully.</p>\n<p>Example: here's a&nbsp;<a href=\"/lw/90l/welcome_to_less_wrong_2012/60wy\">welcome post by syzygy</a>, which relates to Eliezer's post about&nbsp;<a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics as the Mind Killer</a>. I know that it's controversial, because I can sort by controversial and it shows up high on the welcome post thread (neat feature!), but I can't tell how many down votes it has. Does syzygy commit a fallacy? (I don't mean to pick on you, sorry about that; I liked your post.)</p>\n<p>Of course this change wouldn't fix everything. If a post has \"-1 points (+0,-1)\", that doesn't mean only one person read it and disapproved; maybe 100s read it and thought it was bad, but saw that it already had -1 net and considered that sufficiently&nbsp;punitive. This is pretty good; we don't want to spend all our time fiddling with scores.</p>\n<p>I mean if we wanted to get fancy and use Bayesian inspired scoring, we could let everyone who wishes assign a score (say from -5 to 5) and report posterior summaries of the scores. Or, more importantly if we value objective scoring, we could identify posts that are controversial and we could have the system randomly select users with respectable karma, and <em>assign them</em> to give their score on the post. Such a score would be valid in a way that the current \"convenience\" scores are not. Additionally, posts could be scored on multiple axes: soundness of argument, potential impact, innovation, whether we agree with the normative basis of a judgement, etc....</p>\n<p>But I'm not arguing for a complicated change, just a simple <em>less wrong one</em>.</p>\n<p>Other than feasibility concerns, or maybe aesthetics, the strongest argument I can see against this proposal is that we might&nbsp;embarrass&nbsp;or shame users. Can any one give an example where that might be a concern? I figure that since we already show negative scores, users have gotten over most of that inhibition, but I'm new here.</p>\n<p>Another possible criticism is that it's a non-issue: almost all posts are all plus or all minus, so it's not worth the effort. I disagree with this one because I think the posts where we have mixed judgements are the most important ones to get right.</p>\n<p>EDIT: Wouldn't it be nice to know how many down votes this post has?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WTxYChzB5twRjdmXN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 24, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "16820", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XbfdLQrAWTRfpggRM", "DNQw596nPCX4x7xT9", "9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-09T23:57:21.851Z", "modifiedAt": null, "url": null, "title": "What was your biggest recent surprise?", "slug": "what-was-your-biggest-recent-surprise", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:05.450Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/raq9gCoBcwvvZok7K/what-was-your-biggest-recent-surprise", "pageUrlRelative": "/posts/raq9gCoBcwvvZok7K/what-was-your-biggest-recent-surprise", "linkUrl": "https://www.lesswrong.com/posts/raq9gCoBcwvvZok7K/what-was-your-biggest-recent-surprise", "postedAtFormatted": "Saturday, June 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20was%20your%20biggest%20recent%20surprise%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20was%20your%20biggest%20recent%20surprise%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fraq9gCoBcwvvZok7K%2Fwhat-was-your-biggest-recent-surprise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20was%20your%20biggest%20recent%20surprise%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fraq9gCoBcwvvZok7K%2Fwhat-was-your-biggest-recent-surprise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fraq9gCoBcwvvZok7K%2Fwhat-was-your-biggest-recent-surprise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>I recently flipped through the \"Cartoon Guide to Physics\", expecting an easy-to-understand rehash of ideas I was long familiar with; and that's what I got - right up to the last few pages, where I was presented with a fairly fundamental concept that's been absent from the popular science media I've enjoyed over the years. (Specifically, that the uncertainty principle, when expressed as linking energy and time, explains what electromagnetic fields actually /are/, as the propensity for virtual photons of various strengths to happen.) I find myself happy to try to integrate this new understanding - and at least mildly disturbed that I'd been missing it for so long, and with an increased curiosity about how I might find any other such gaps in my understanding of how the universe works.</p>\n<p>&nbsp;</p>\n<p>So: what's the biggest, or most surprising, or most interesting concept /you/ have learned of, after you'd already gotten a handle on the basics?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "raq9gCoBcwvvZok7K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 9.195246560123377e-07, "legacy": true, "legacyId": "16821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-10T04:03:56.435Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Big Public Meetup", "slug": "meetup-vancouver-big-public-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:07.065Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WZkRZ6u7BantzAXNF/meetup-vancouver-big-public-meetup-0", "pageUrlRelative": "/posts/WZkRZ6u7BantzAXNF/meetup-vancouver-big-public-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/WZkRZ6u7BantzAXNF/meetup-vancouver-big-public-meetup-0", "postedAtFormatted": "Sunday, June 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Big%20Public%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Big%20Public%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWZkRZ6u7BantzAXNF%2Fmeetup-vancouver-big-public-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Big%20Public%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWZkRZ6u7BantzAXNF%2Fmeetup-vancouver-big-public-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWZkRZ6u7BantzAXNF%2Fmeetup-vancouver-big-public-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ay'>Vancouver Big Public Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 June 2012 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Benny's Bagels 2505 W Broadway Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello everybody</p>\n\n<p>We're having our monthly big meetup where we invite everybody and give you plausible reasons why you (yes, even <em>you</em>) should come and hang out with us. If you live in or near vancouver, or will be around next thursday, you should come out.</p>\n\n<p>One of our members is going to present the <a href=\"http://wiki.lesswrong.com/wiki/Seeing_with_Fresh_Eyes\">seeing with fresh eyes</a> sequence, but we are going to let tangents run wild and generally have a fun discussion about topics LWers are interested in. Feel free to come prepared or unprepared.</p>\n\n<p>If you haven't been coming out regularly, or you were looking for an exceptional excuse to start coming, or are otherwise lurking in the woodwork wondering whether we are cool or not, this is your chance! Get it while it's hot; next big meetup will be in a whole month!</p>\n\n<p>As usual, come see us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ay'>Vancouver Big Public Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WZkRZ6u7BantzAXNF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.196363271058873e-07, "legacy": true, "legacyId": "16822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Big_Public_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ay\">Vancouver Big Public Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 June 2012 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Benny's Bagels 2505 W Broadway Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello everybody</p>\n\n<p>We're having our monthly big meetup where we invite everybody and give you plausible reasons why you (yes, even <em>you</em>) should come and hang out with us. If you live in or near vancouver, or will be around next thursday, you should come out.</p>\n\n<p>One of our members is going to present the <a href=\"http://wiki.lesswrong.com/wiki/Seeing_with_Fresh_Eyes\">seeing with fresh eyes</a> sequence, but we are going to let tangents run wild and generally have a fun discussion about topics LWers are interested in. Feel free to come prepared or unprepared.</p>\n\n<p>If you haven't been coming out regularly, or you were looking for an exceptional excuse to start coming, or are otherwise lurking in the woodwork wondering whether we are cool or not, this is your chance! Get it while it's hot; next big meetup will be in a whole month!</p>\n\n<p>As usual, come see us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Big_Public_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ay\">Vancouver Big Public Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Big Public Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Big_Public_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Big Public Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Big_Public_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-10T05:03:36.625Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Outside View's Domain", "slug": "seq-rerun-the-outside-view-s-domain", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sNLGPEnyk7Xf2mMho/seq-rerun-the-outside-view-s-domain", "pageUrlRelative": "/posts/sNLGPEnyk7Xf2mMho/seq-rerun-the-outside-view-s-domain", "linkUrl": "https://www.lesswrong.com/posts/sNLGPEnyk7Xf2mMho/seq-rerun-the-outside-view-s-domain", "postedAtFormatted": "Sunday, June 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Outside%20View's%20Domain&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Outside%20View's%20Domain%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsNLGPEnyk7Xf2mMho%2Fseq-rerun-the-outside-view-s-domain%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Outside%20View's%20Domain%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsNLGPEnyk7Xf2mMho%2Fseq-rerun-the-outside-view-s-domain", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsNLGPEnyk7Xf2mMho%2Fseq-rerun-the-outside-view-s-domain", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/ri/the_outside_views_domain/\">The Outside View's Domain</a> was originally published on 21 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Outside_View.27s_Domain\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A dialogue on the proper application of the inside and outside views.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/cys/seq_rerun_heading_toward_morality/\">Heading Toward Morality</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sNLGPEnyk7Xf2mMho", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.19663354364522e-07, "legacy": true, "legacyId": "16823", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pqoxE3AGMbse68dvb", "ZHmiuFJQJsf8WJCfa", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-10T14:05:06.238Z", "modifiedAt": null, "url": null, "title": "Meetup : Punt Trip", "slug": "meetup-punt-trip", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:07.627Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MmTSAq8SHA5sAXEwq/meetup-punt-trip", "pageUrlRelative": "/posts/MmTSAq8SHA5sAXEwq/meetup-punt-trip", "linkUrl": "https://www.lesswrong.com/posts/MmTSAq8SHA5sAXEwq/meetup-punt-trip", "postedAtFormatted": "Sunday, June 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Punt%20Trip&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Punt%20Trip%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmTSAq8SHA5sAXEwq%2Fmeetup-punt-trip%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Punt%20Trip%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmTSAq8SHA5sAXEwq%2Fmeetup-punt-trip", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmTSAq8SHA5sAXEwq%2Fmeetup-punt-trip", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/az'>Punt Trip</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 June 2012 12:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Cambridge_UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join the Cambridge LessWrong group for a free punt trip up the beautiful river Cam.  A Sm\u00f6rg\u00e5sbord of discussion, fun, food and learning to punt.</p>\n\n<p>Meet at the Great Gate of Trinity College, CB2 1TQ, at 11:50 for 12 noon departure.  For those coming in by car, the nearest multistory carpark to Trinity is on Park Street, CB5 8AS</p>\n\n<p>There's no need to book, but it will help us judge numbers if you send an email to cambridgelesswrong@googlegroups.com if you think there's more than a 25% chance that you'll be coming.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/az'>Punt Trip</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MmTSAq8SHA5sAXEwq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.199086864395017e-07, "legacy": true, "legacyId": "16824", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Punt_Trip\">Discussion article for the meetup : <a href=\"/meetups/az\">Punt Trip</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 June 2012 12:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Cambridge_UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join the Cambridge LessWrong group for a free punt trip up the beautiful river Cam.  A Sm\u00f6rg\u00e5sbord of discussion, fun, food and learning to punt.</p>\n\n<p>Meet at the Great Gate of Trinity College, CB2 1TQ, at 11:50 for 12 noon departure.  For those coming in by car, the nearest multistory carpark to Trinity is on Park Street, CB5 8AS</p>\n\n<p>There's no need to book, but it will help us judge numbers if you send an email to cambridgelesswrong@googlegroups.com if you think there's more than a 25% chance that you'll be coming.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Punt_Trip1\">Discussion article for the meetup : <a href=\"/meetups/az\">Punt Trip</a></h2>", "sections": [{"title": "Discussion article for the meetup : Punt Trip", "anchor": "Discussion_article_for_the_meetup___Punt_Trip", "level": 1}, {"title": "Discussion article for the meetup : Punt Trip", "anchor": "Discussion_article_for_the_meetup___Punt_Trip1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-10T14:06:40.174Z", "modifiedAt": null, "url": null, "title": "I think I've found the source of what's been bugging me about \"Friendly AI\"", "slug": "i-think-i-ve-found-the-source-of-what-s-been-bugging-me", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:06.688Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vYznKkPJncjZzQGS2/i-think-i-ve-found-the-source-of-what-s-been-bugging-me", "pageUrlRelative": "/posts/vYznKkPJncjZzQGS2/i-think-i-ve-found-the-source-of-what-s-been-bugging-me", "linkUrl": "https://www.lesswrong.com/posts/vYznKkPJncjZzQGS2/i-think-i-ve-found-the-source-of-what-s-been-bugging-me", "postedAtFormatted": "Sunday, June 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20think%20I've%20found%20the%20source%20of%20what's%20been%20bugging%20me%20about%20%22Friendly%20AI%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20think%20I've%20found%20the%20source%20of%20what's%20been%20bugging%20me%20about%20%22Friendly%20AI%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvYznKkPJncjZzQGS2%2Fi-think-i-ve-found-the-source-of-what-s-been-bugging-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20think%20I've%20found%20the%20source%20of%20what's%20been%20bugging%20me%20about%20%22Friendly%20AI%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvYznKkPJncjZzQGS2%2Fi-think-i-ve-found-the-source-of-what-s-been-bugging-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvYznKkPJncjZzQGS2%2Fi-think-i-ve-found-the-source-of-what-s-been-bugging-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 473, "htmlBody": "<p>In the comments on <a href=\"/lw/cjx/how_likely_the_ai_that_knows_its_evil_or_is_a/\">this post</a> (which in retrospect I feel was not very clearly written), someone linked me to a post Eliezer wrote five years ago, <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">\"The Hidden Complexity of Wishes.\"</a> After reading it, I think I've figured out why the term <a href=\"/lw/cib/work_harder_on_tabooing_friendly_ai/\">\"Friendly AI\" is used so inconsistently.</a></p>\n<p>This post explicitly lays out a view that&nbsp;seems to be&nbsp;implicit in, but not entirely clear from, many of of Eliezer's other writings. That view is this:</p>\n<blockquote>\n<p>There are three kinds of genies:&nbsp; Genies to whom you can safely say \"I wish for you to do what I should wish for\"; genies for which <em>no</em> wish is safe; and <a href=\"/lw/l8/conjuring_an_evolution_to_serve_you/\"><span style=\"color: #8a8a8b;\">genies that aren't very powerful or intelligent</span></a>.</p>\n</blockquote>\n<p>Even if Eliezer is right about that, I think that view of his has led to confusing usage of the term \"Friendly AI.\" If you accept Eliezer's view, it may seem to make sense to not worry to much about whether by \"Friendly AI\" you mean:</p>\n<ol>\n<li>\n<p>A utopia-making machine (the AI \"to whom you can safely say, 'I wish for you to do what I should wish for.'\") Or:</p>\n</li>\n<li>\n<p>A non-doomsday machine (a doomsday machine being the&nbsp;AI \"for which <em>no </em>wish is safe.\")</p>\n</li>\n</ol>\n<p>And it would make sense not to worry too much about that distinction, if you were talking only to people who also believe those two concepts are very nearly co-extensive for powerful AI. But failing to make that distinction is obviously going to be confusing when you're talking to people who don't think that. It will make it harder to communicate both your ideas and your reasons for holding those ideas to them.</p>\n<p>One solution would be to more frequently link people back to \"The Hidden Complexity of Wishes\" (or other writing by Eliezer that makes similar points--what else would be suitable?) But while it's a good post and Eliezer makes some very good points with the \"Outcome Pump\" thought-experiment, the argument isn't entirely convincing.</p>\n<p>As Eliezer himself has <a href=\"/lw/lp/fake_fake_utility_functions/\">argued at great length,</a> (see also section 6.1 of <a href=\"/singinst.org/upload/artificial-intelligence-risk.pdf\">this paper</a>) humans' own understanding of our values is far from perfect. None of us are, right now, qualified to design a utopia. But we do have some understanding of our own values; we can identify some things that would be improvements over our current situation while marking other scenarios as \"this would be a disaster.\" It seems like there might be a point in the future where we can design an AI whose understanding of human values is similarly serviceable but no better than that.</p>\n<p>Maybe I'm wrong about that. But if I am, until there's a better easy to read explanation of why I'm wrong for everybody to link to, it would be helpful to have different terms for (1) and (2) above. Perhaps call them \"utopia AI\" and \"safe AI,\" respectively?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vYznKkPJncjZzQGS2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "16326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GqSZiHL89EaBrdTba", "4ARaTpNX62uaL86j6", "5pYfLADQ8gvk6Sd33", "KE8wPzGiX5QPotyS8", "D6rsNhHM4pBCpDzSb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-11T04:44:01.150Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Surface Analogies and Deep Causes", "slug": "seq-rerun-surface-analogies-and-deep-causes", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qar8jkDbpa9xfMgHL/seq-rerun-surface-analogies-and-deep-causes", "pageUrlRelative": "/posts/Qar8jkDbpa9xfMgHL/seq-rerun-surface-analogies-and-deep-causes", "linkUrl": "https://www.lesswrong.com/posts/Qar8jkDbpa9xfMgHL/seq-rerun-surface-analogies-and-deep-causes", "postedAtFormatted": "Monday, June 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Surface%20Analogies%20and%20Deep%20Causes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Surface%20Analogies%20and%20Deep%20Causes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQar8jkDbpa9xfMgHL%2Fseq-rerun-surface-analogies-and-deep-causes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Surface%20Analogies%20and%20Deep%20Causes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQar8jkDbpa9xfMgHL%2Fseq-rerun-surface-analogies-and-deep-causes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQar8jkDbpa9xfMgHL%2Fseq-rerun-surface-analogies-and-deep-causes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<p>Today's post, <a href=\"/lw/rj/surface_analogies_and_deep_causes/\">Surface Analogies and Deep Causes</a> was originally published on 22 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Surface_Analogies_and_Deep_Causes\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Just because two things share surface similarities doesn't mean that they work the same way, or can be expected to be similar in other respects. If you want to understand what something does, it typically doesn't help you to understand something else. That type of reasoning only works if the two things are especially similar, on a deep level.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/czb/seq_rerun_the_outside_views_domain/\">The Outside View's Domain</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qar8jkDbpa9xfMgHL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 9.203071389388237e-07, "legacy": true, "legacyId": "16834", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ByPxcGDhmx74gPSm", "sNLGPEnyk7Xf2mMho", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-11T04:44:19.981Z", "modifiedAt": null, "url": null, "title": "[link] Aubrey de Grey answers Reddit AMA in video", "slug": "link-aubrey-de-grey-answers-reddit-ama-in-video", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Filipe", "createdAt": "2011-11-30T22:27:14.259Z", "isAdmin": false, "displayName": "Filipe"}, "userId": "eXxJNrGGCYkRKHThQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GNiR3Q7tQm5fQQevM/link-aubrey-de-grey-answers-reddit-ama-in-video", "pageUrlRelative": "/posts/GNiR3Q7tQm5fQQevM/link-aubrey-de-grey-answers-reddit-ama-in-video", "linkUrl": "https://www.lesswrong.com/posts/GNiR3Q7tQm5fQQevM/link-aubrey-de-grey-answers-reddit-ama-in-video", "postedAtFormatted": "Monday, June 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Aubrey%20de%20Grey%20answers%20Reddit%20AMA%20in%20video&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Aubrey%20de%20Grey%20answers%20Reddit%20AMA%20in%20video%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGNiR3Q7tQm5fQQevM%2Flink-aubrey-de-grey-answers-reddit-ama-in-video%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Aubrey%20de%20Grey%20answers%20Reddit%20AMA%20in%20video%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGNiR3Q7tQm5fQQevM%2Flink-aubrey-de-grey-answers-reddit-ama-in-video", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGNiR3Q7tQm5fQQevM%2Flink-aubrey-de-grey-answers-reddit-ama-in-video", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p><a title=\"Aubrey de Grey\" href=\"http://en.wikipedia.org/wiki/Aubrey_de_Grey\">Aubrey de Grey</a>,&nbsp;Chief Science Officer of the SENS Foundation, has posted a video with answers to some of the questions posed at him in a recent <a title=\"Reddit AMA\" href=\"http://www.reddit.com/r/IAmA/comments/to32n/ask_aubrey_de_grey_chief_science_officer_sens/\">Reddit AMA</a>.</p>\n<p>Link: http://www.youtube.com/watch?v=6eet44YacRg</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YgizoZqa7LEb3LEJn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GNiR3Q7tQm5fQQevM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 9.203072812684794e-07, "legacy": true, "legacyId": "16833", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-11T05:06:42.267Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Meetup", "slug": "meetup-washington-dc-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:04.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CNjf7FTupXMqzaspe/meetup-washington-dc-meetup-0", "pageUrlRelative": "/posts/CNjf7FTupXMqzaspe/meetup-washington-dc-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/CNjf7FTupXMqzaspe/meetup-washington-dc-meetup-0", "postedAtFormatted": "Monday, June 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNjf7FTupXMqzaspe%2Fmeetup-washington-dc-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNjf7FTupXMqzaspe%2Fmeetup-washington-dc-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNjf7FTupXMqzaspe%2Fmeetup-washington-dc-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/b0\">Washington DC Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">17 June 2012 03:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Portrait Gallery</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Topic is EY's response to the Givewell critique, although additional suggestions are solicited. (Also, I'll be there)</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/b0\">Washington DC Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CNjf7FTupXMqzaspe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.203174272434776e-07, "legacy": true, "legacyId": "16835", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Meetup\">Discussion article for the meetup : <a href=\"/meetups/b0\">Washington DC Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">17 June 2012 03:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Portrait Gallery</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Topic is EY's response to the Givewell critique, although additional suggestions are solicited. (Also, I'll be there)</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/b0\">Washington DC Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-11T12:23:20.875Z", "modifiedAt": null, "url": null, "title": "Seeking information relevant to deciding whether to try to become an AI researcher and, if so, how.", "slug": "seeking-information-relevant-to-deciding-whether-to-try-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.439Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XqhAyKfkzhnJpmuw8/seeking-information-relevant-to-deciding-whether-to-try-to", "pageUrlRelative": "/posts/XqhAyKfkzhnJpmuw8/seeking-information-relevant-to-deciding-whether-to-try-to", "linkUrl": "https://www.lesswrong.com/posts/XqhAyKfkzhnJpmuw8/seeking-information-relevant-to-deciding-whether-to-try-to", "postedAtFormatted": "Monday, June 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeking%20information%20relevant%20to%20deciding%20whether%20to%20try%20to%20become%20an%20AI%20researcher%20and%2C%20if%20so%2C%20how.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeking%20information%20relevant%20to%20deciding%20whether%20to%20try%20to%20become%20an%20AI%20researcher%20and%2C%20if%20so%2C%20how.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqhAyKfkzhnJpmuw8%2Fseeking-information-relevant-to-deciding-whether-to-try-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeking%20information%20relevant%20to%20deciding%20whether%20to%20try%20to%20become%20an%20AI%20researcher%20and%2C%20if%20so%2C%20how.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqhAyKfkzhnJpmuw8%2Fseeking-information-relevant-to-deciding-whether-to-try-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqhAyKfkzhnJpmuw8%2Fseeking-information-relevant-to-deciding-whether-to-try-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1048, "htmlBody": "<p>I was initially going to title this post, \"try to become an AI researcher and, if so, how?\" but see <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">Hold Off On Proposing Solutions.</a> So instead, I'm going to ask people to give me as much relevant information as possible. The rest of this post will be a dump of what I've figured out so far, so people can read it and try to figure out what I might be missing.</p>\n<p>If you yourself are trying to make this decision, some of what I say about myself may apply to you. Hopefully, some of the comments on this post will also be generally applicable.</p>\n<p>Oh, and if you can think of any bias-avoiding advice that's relevant here, along the lines of holding off on proposing solutions, that would be most helpful.</p>\n<p>Though I'm really hard to offend in general, I've made a conscious decision to operate by <a href=\"http://www.sl4.org/crocker.html\">Crocker's Rules</a> in this thread.</p>\n<p>One possibility that's crossed my mind for getting involved is going back go graduate school in philosophy to study under Bostrom or Chalmers. But I really have no idea what the other possible routes for me are, and ought to know about them before making a decision.</p>\n<p>~~~~~</p>\n<p>Now for the big dump of background info. Feel free to skip and just respond based on what's above the squigglies.</p>\n<p>I seem to be good at a lot of different things (not necessarily everything), but I'm especially good at math. SAT was 800 math, 790 verbal, GRE was 800 on both, but in both cases, I studied for the test and getting my verbal score up was much harder work. However, I know there are plenty people who are much better at math than I am. In high school, I was one of the very few students from my city to qualify for the American Regions Math League competition (ARML), but did not do especially well.&nbsp;</p>\n<p>Going to ARML persuaded me that I was probably not quite smart enough to be a world-class anything. I entered college as a biochemistry major, with the idea that I would go to medical school and then join an organization like Doctors Without Borders to just do as much good as I could, even if I wasn't a world-class anything. I did know at the time that I was better at math than biology, but hadn't yet read Peter Unger's <em>Living High and Letting Die, </em>so \"figure out the best way to covert math aptitude into dollars and donate what you don't need to charity\" wasn't a strategy I even considered.</p>\n<p>After getting mostly B's in biology and organic chemistry my sophomore year, I decided maybe I wasn't well-suited for medical school and began looking for something else to do with my life. To this day, I'm genuinely unsure why I don't do so well in biology. Did the better students in my classes have some aptitude I lacked? Or was it that being really good at math made me lazy about things that are inherently time-consuming to study, like (possibly) anatomy?</p>\n<p>I took a couple of neuroscience classes junior year, and considered a career in the subject, but eventually ended up settling on philosophy, silencing some inner doubts I had about philosophy as a field. I applied to grad school in philosophy at over a dozen programs and was accepted in to exactly one: the University of Notre Dame. I accepted, which was in retrospect the first or second stupidest decision I've made in my life.</p>\n<p>Why was it a stupid decision? To give only three of the reasons: (1) Notre Dame is a department where evangelical Christian anti-evolutionists like Alvin Plantinga are given high status (2) it was weak in philosophy of mind, which is what I really wanted to study (3) I was squishing what were, in retrospect, legitimate doubts about academic philosophy, because once I made the decision to go, I had to make it sound as good as possible to myself.</p>\n<p>Why did I do it? I'm not entirely sure, and I'd like to better understand this mistake so as to not make a similar one again. Possible contributing factors: (1) I didn't want to admit to myself that I didn't know what I was doing with my life (2) I had an irrational belief that if I said \"no\" I'd never get another opportunity like that again (3) my mom and dad went straight from undergrad to graduate school in biochemistry and dental school, respectively, and I was using that as a model for what my life should look like without really questioning it (4) Notre Dame initially waitlisted me and then, when they finally accepted me, gave me very little time to decide whether or not to accept, which probably unintentionally invoked one or two effects straight out of Cialdini.</p>\n<p>So a couple years later, I dropped out of the program and now I'm working a not-especially-challenging, not-especially-exciting, not-especially-well-paying job while I figure out what I should do next.</p>\n<p>My main reason for now being interested in AI is that through several years of reading LW/OB, and the formal publications of the people who are popular around here, I've become persuaded that even if specific theses endorsed by the Singularity Institute are wrong potentially world-changing AI is close enough to be worth putting a lot of thought into seriously thinking about.</p>\n<p>It helps that it fits with interests I've had for a long time in cognitive science and philosophy of mind. I think I actually was interested in the idea of being an AI researcher some time around middle school, but by the time I was entering college I had gotten the impression that human-like AI was about as likely in the near future as FTL travel.</p>\n<p>The other broad life-plan I'm considering is the thing I should have considered going into college, \"figure out the best way to covert math aptitude into dollars and donate what you don't need to charity.\" One sub-option is to look into computer programming as suggested in HPMOR author's note a month or two ago. My dad thinks I should take some more stats and go for work as an analyst for some big eastern firm. And there are very likely options in this area that I'm missing.</p>\n<p>~~~~~</p>\n<p>I think that covers most of the relevant information I have. Now, what am I missing?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XqhAyKfkzhnJpmuw8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 17, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "16810", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-11T15:06:07.622Z", "modifiedAt": null, "url": null, "title": "Opaque fragile systems/institutions dominate.", "slug": "opaque-fragile-systems-institutions-dominate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:05.001Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "saliency", "createdAt": "2009-10-25T03:59:58.587Z", "isAdmin": false, "displayName": "saliency"}, "userId": "RNx6ydjKM2J3Heae3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aC5Ws3NeXAvjmf84L/opaque-fragile-systems-institutions-dominate", "pageUrlRelative": "/posts/aC5Ws3NeXAvjmf84L/opaque-fragile-systems-institutions-dominate", "linkUrl": "https://www.lesswrong.com/posts/aC5Ws3NeXAvjmf84L/opaque-fragile-systems-institutions-dominate", "postedAtFormatted": "Monday, June 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Opaque%20fragile%20systems%2Finstitutions%20dominate.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpaque%20fragile%20systems%2Finstitutions%20dominate.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaC5Ws3NeXAvjmf84L%2Fopaque-fragile-systems-institutions-dominate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Opaque%20fragile%20systems%2Finstitutions%20dominate.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaC5Ws3NeXAvjmf84L%2Fopaque-fragile-systems-institutions-dominate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaC5Ws3NeXAvjmf84L%2Fopaque-fragile-systems-institutions-dominate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 206, "htmlBody": "<p>&nbsp;</p>\n<h2 style=\"margin: 10px 0px 0px; padding: 0px 5px 3px; font-family: Verdana, 'BitStream vera Sans'; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #cccccc; clear: both; color: #555555;\"><a class=\"title\" style=\"margin: 0px; padding: 0px; color: #2970a6; text-decoration: none;\" rel=\"bookmark\" href=\"http://saliency.wordpress.com/2012/06/11/opaque-fragile-systems-dominate/\">Opaque fragile systems/institutions dominate.</a></h2>\n<p><em style=\"font-family: Verdana, 'BitStream vera Sans', Helvetica, sans-serif; font-size: 12px; line-height: 17px; margin: 0px; padding: 0px;\">&ldquo;Out of the crooked timber of humanity no straight thing was ever made.&rdquo;</em></p>\n<div class=\"content\" style=\"margin: 0px; padding: 5px 5px 0px; line-height: 17px; overflow: hidden; color: #555555; font-family: Verdana, 'BitStream vera Sans', Helvetica, sans-serif; font-size: 12px;\">\n<p style=\"margin: 0px 0px 10px; padding: 0px;\">All systems compete against each other for users. &nbsp;I believe opaque fragile systems dominate transparent robust systems. &nbsp;First I believe individuals choose shrouded systems more tightly bounding their rationality. &nbsp;Second I believe even when individuals know a system is fragile they believe they will not be victim to its fragility; the greater fools will be.</p>\n<p style=\"margin: 0px 0px 10px; padding: 0px;\">First sophisticated consumers like price discrimination while myopic consumers are ignorant they are being discriminated against or unwilling to commit the time needed to exploit the system. &nbsp;Information asymmetry is a feature of the system not a bug.</p>\n<p style=\"margin: 0px 0px 10px; padding: 0px;\">See &ldquo;Shrouded Attributes, Consumer Myopia, and Information Suppression in Competitive Markets&rdquo;<a style=\"margin: 0px; padding: 0px; color: #2970a6; text-decoration: none;\" href=\"http://aida.wss.yale.edu/~shiller/behmacro/2003-11/gabaix-laibson.pdf\">&nbsp;http://aida.wss.yale.edu/~shiller/behmacro/2003-11/gabaix-laibson.pdf</a>&nbsp;for more detail.</p>\n<p style=\"margin: 0px 0px 10px; padding: 0px;\">Second sophisticated individuals, even when they perceive the system to be fragile, often subscribe to the the greater fool theory. &nbsp;They feel they will win out over the greater fools, but they do not understand how tightly bound their rationality has become due to the layers upon layers of shrouding. &nbsp;The myopics of course are largely ignorant of the risks.</p>\n<p style=\"margin: 0px 0px 10px; padding: 0px;\">This is a very pessimistic view that offers no solution to the problem of system fragility. &nbsp;I think though most solutions to fragility only create larger equally fragile systems.</p>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aC5Ws3NeXAvjmf84L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -13, "extendedScore": null, "score": -2.5e-05, "legacy": true, "legacyId": "16853", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-11T15:10:36.294Z", "modifiedAt": null, "url": null, "title": "Intellectual insularity and productivity ", "slug": "intellectual-insularity-and-productivity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:32.535Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M65WWD49bKZYLmAtN/intellectual-insularity-and-productivity", "pageUrlRelative": "/posts/M65WWD49bKZYLmAtN/intellectual-insularity-and-productivity", "linkUrl": "https://www.lesswrong.com/posts/M65WWD49bKZYLmAtN/intellectual-insularity-and-productivity", "postedAtFormatted": "Monday, June 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intellectual%20insularity%20and%20productivity%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntellectual%20insularity%20and%20productivity%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM65WWD49bKZYLmAtN%2Fintellectual-insularity-and-productivity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intellectual%20insularity%20and%20productivity%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM65WWD49bKZYLmAtN%2Fintellectual-insularity-and-productivity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM65WWD49bKZYLmAtN%2Fintellectual-insularity-and-productivity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 825, "htmlBody": "<p><em>Guys I'd like your opinion on something.</em><strong></strong></p>\n<p><strong>Do you think LessWrong is too intellectually insular? </strong>What I mean by this is that we very seldom seem to adopt useful vocabulary or arguments or information from outside of LessWrong. For example all I can think of is some of Robin Hanson's and Paul Graham's stuff. But I don't think Robin Hanson really counts as <a href=\"http://www.overcomingbias.com/about\">Overcoming Bias used to be LessWrong</a>. <br /><br />The community seems to not update on ideas and concepts that <a href=\"http://en.wikipedia.org/wiki/Not_invented_here\">didn't originate here</a>. The only major examples fellow LWers brought up in conversation where works that Eliezer cited as great or influential. :/</p>\n<address><strong>Edit:</strong> Apparently this has been a source of much confusion and mistargeted replies. While I wouldn't mind even more references to quality outside writing, this wasn't my concern. I'm surprised this was problematic to understand for two reasons. First I gave examples of two thinker that <em>aren't</em> often linked to by recent articles on LW yet have clearly <em>greatly influenced</em> <em>us</em>. Secondly this is a trivially false interpretation, <em>as my own submission history shows</em> (it is littered with well received outside links). I think this arises because when I wrote \"we seem to not <strong><em>update</em></strong> on ideas and concepts that didn't originate here\" people read it as \"we don't <em><strong>link</strong> </em>to ideas and concepts\" or maybe \"we don't <strong>talk</strong> about ideas and concepts\" from outside. I clarified this several times in the comments, most extensively <a href=\"/r/discussion/lw/d06/intellectual_insularity_and_productivity/6t3q\">here</a>. Yet it <span>doesn't</span> seem to have made much of an impact. Maybe it will be easier to understand if I put it this way, interesting material from the outside never seems to get added to something like the sequences or the wiki. The sole exception to this is hunting even more academic references for the conclusions and concepts we already know and embrace. Thus while individuals will update on them and perhaps even reference them in the future <strong>the community as a whole</strong> <strong>will not</strong>. They don't become part of the expected background knowledge when discussing certain topics. Over time <a href=\"/r/discussion/lw/d06/intellectual_insularity_and_productivity/6t3m\">their impact thus fades</a> in a way the old core material <span>doesn't</span>. <br /></address><address><br /></address>\n<p>Another thing, I could be wrong about this naturally, but it seems to clear that LessWrong has <strong>not</strong> grown. I'm not talking numerically. I can't put my finger to major progress done in the past 2 years. I have heard several other users express similar sentiments. To quote one <a href=\"/lw/bql/our_phyg_is_not_exclusive_enough/6ch1\">user</a>:</p>\n<blockquote>\n<p>I notice that, in topics that Eliezer did not <em>explicitly</em> cover in the sequences (and some that he <a href=\"/lw/kn/torture_vs_dust_specks/\">did</a>), LW has made zero progress in general.</p>\n</blockquote>\n<p>I've recently come to think this is probably true to the first approximation. I was checking out a blogroll and saw LessWrong listed as Eliezer's blog about rationality. I realized that essentially <em>it is</em>. And worse this makes it a very crappy blog since the author <span>doesn't</span> make new updates any more. Originally the man had high hopes for the site. He wanted to build something that could keep going on its own, growing without him. It turned out to be a community mostly dedicated to studying the scrolls he left behind. We don't even seem to do a good job of getting others to <em>read</em> the scrolls.&nbsp; <br /><br />Overall there seems to be little enthusiasm for actually systematically reading the old material. I'm going to share my take on what is I think a symptom of this. I was debating which title to pick for my first ever <a href=\"/lw/cz7/conspiracy_theories_as_agency_fictions\">original content Main article</a> (it was originally titled \"On Conspiracy Theories\") and made what at first felt like a joke but then took on a horrible ring of:</p>\n<blockquote>\n<p><strong>Over time the meaning of an article will tend to converge with the literal meaning of its title.</strong></p>\n</blockquote>\n<p>We like linking articles, and while people may read a link the first time, they don't tend to read it the second or third time they run across it. The phrase is eventually picked up and used out the appropriate of context. Something that was supposed to be shorthand for a nuanced argument starts to mean <em>exactly</em> what \"it says\". Well not <em>exactly</em>, people still recall it is a vague <a href=\"/lw/jb/applause_lights/\">applause light</a>. Which is actually <em>worse</em>.<br /><br />I cited precisely \"Politics is the Mindkiller\" as an example of this. In the original article Eliezer basically argues that gratuitous politics, political thinking that isn't outweighed by its value to the art of rationality, is to be avoided. This soon came to meant it is forbidden to discuss politics in Main and Discussion articles, though it does live in the comment sections.</p>\n<p><strong>Now the question if LessWrong remains productive intellectually, is separate from the question of it being insular. But I feel both need to be discussed.</strong> If our community wasn't growing and it wasn't insular either, it could at least remain relevant.</p>\n<p>This site has a wonderful ethos for discussion and thought. Why do we seem to be wasting it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M65WWD49bKZYLmAtN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 73, "baseScore": 79, "extendedScore": null, "score": 0.000184, "legacy": true, "legacyId": "16854", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 79, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 169, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN", "Peo8jAyjGL9kWoYAH", "dLbkrPu5STNCBLRjr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-11T21:40:14.138Z", "modifiedAt": null, "url": null, "title": "Atkins Diet - How Should I Update?", "slug": "atkins-diet-how-should-i-update", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.093Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Liron", "createdAt": "2009-02-27T04:43:11.294Z", "isAdmin": false, "displayName": "Liron"}, "userId": "AyzRrs8hNm54QptLi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q5seWQkcsJr9pLZNT/atkins-diet-how-should-i-update", "pageUrlRelative": "/posts/q5seWQkcsJr9pLZNT/atkins-diet-how-should-i-update", "linkUrl": "https://www.lesswrong.com/posts/q5seWQkcsJr9pLZNT/atkins-diet-how-should-i-update", "postedAtFormatted": "Monday, June 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Atkins%20Diet%20-%20How%20Should%20I%20Update%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAtkins%20Diet%20-%20How%20Should%20I%20Update%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq5seWQkcsJr9pLZNT%2Fatkins-diet-how-should-i-update%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Atkins%20Diet%20-%20How%20Should%20I%20Update%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq5seWQkcsJr9pLZNT%2Fatkins-diet-how-should-i-update", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq5seWQkcsJr9pLZNT%2Fatkins-diet-how-should-i-update", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<p>This seems like an authoritative 25-year research project that the Atkins diet is pretty bad:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://www.nutritionj.com/content/11/1/40/abstract\">http://www.nutritionj.com/content/11/1/40/abstract</a></p>\n<p>&nbsp;</p>\n<p>Right now my belief is that the Atkins diet is good. It's backed by anecdotal evidence of trying a low-carb diet for 18 months following a 12-month low-fat diet and seemingly getting better results with the low-carb diet.</p>\n<p>I'm counting on LWers to tell me how to update my belief in light of this study. Thanks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q5seWQkcsJr9pLZNT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 9.207682170619497e-07, "legacy": true, "legacyId": "16857", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T03:01:32.951Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Optimization and the Singularity", "slug": "seq-rerun-optimization-and-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.617Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8jRbZYrC2mh2oKXAY/seq-rerun-optimization-and-the-singularity", "pageUrlRelative": "/posts/8jRbZYrC2mh2oKXAY/seq-rerun-optimization-and-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/8jRbZYrC2mh2oKXAY/seq-rerun-optimization-and-the-singularity", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Optimization%20and%20the%20Singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Optimization%20and%20the%20Singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8jRbZYrC2mh2oKXAY%2Fseq-rerun-optimization-and-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Optimization%20and%20the%20Singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8jRbZYrC2mh2oKXAY%2Fseq-rerun-optimization-and-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8jRbZYrC2mh2oKXAY%2Fseq-rerun-optimization-and-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>Today's post, <a href=\"/lw/rk/optimization_and_the_singularity/\">Optimization and the Singularity</a> was originally published on 23 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Optimization_and_the_Singularity\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An introduction to optimization processes and why Yudkowsky thinks that a singularity would be far more powerful than calculations based on human progress would suggest.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/czm/seq_rerun_surface_analogies_and_deep_causes/\">Surface Analogies and Deep Causes</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8jRbZYrC2mh2oKXAY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.209140885752648e-07, "legacy": true, "legacyId": "16867", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFTn3bAT6uXSNwv4m", "Qar8jkDbpa9xfMgHL", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T06:39:20.052Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 6/11/12", "slug": "group-rationality-diary-6-11-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.676Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4B6Zn4up5xPHTq5iC/group-rationality-diary-6-11-12", "pageUrlRelative": "/posts/4B6Zn4up5xPHTq5iC/group-rationality-diary-6-11-12", "linkUrl": "https://www.lesswrong.com/posts/4B6Zn4up5xPHTq5iC/group-rationality-diary-6-11-12", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%206%2F11%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%206%2F11%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4B6Zn4up5xPHTq5iC%2Fgroup-rationality-diary-6-11-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%206%2F11%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4B6Zn4up5xPHTq5iC%2Fgroup-rationality-diary-6-11-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4B6Zn4up5xPHTq5iC%2Fgroup-rationality-diary-6-11-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 190, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the public group instrumental rationality diary for the week of June 11th. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">(Previously:&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/cew/group_rationality_diary_51412/\">5/14/12</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/ckm/group_rationality_diary_52112/\">5/21/12</a>,&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"/r/discussion/lw/cpg/group_rationality_diary_52812\">5/28/12</a>, <a href=\"/lw/cvk/group_rationality_diary_6412/\">6/4/12</a></span>)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4B6Zn4up5xPHTq5iC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 9.210129828088775e-07, "legacy": true, "legacyId": "16879", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JA2MnwunfZnFXb3by", "aFRC3J3TNmKMf2p4T", "RpeHoK89Ra3hBR26r", "GBxYEuDpyRPvZaN22"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T12:44:05.494Z", "modifiedAt": null, "url": null, "title": "[Link] Some notes on Rationality in Harry Potter and the Methods of Rationality", "slug": "link-some-notes-on-rationality-in-harry-potter-and-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:08.146Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mstevens", "createdAt": "2009-05-07T14:46:38.489Z", "isAdmin": false, "displayName": "mstevens"}, "userId": "wgKFztEMLyjFRm4ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eEjfow7naRmqapSky/link-some-notes-on-rationality-in-harry-potter-and-the", "pageUrlRelative": "/posts/eEjfow7naRmqapSky/link-some-notes-on-rationality-in-harry-potter-and-the", "linkUrl": "https://www.lesswrong.com/posts/eEjfow7naRmqapSky/link-some-notes-on-rationality-in-harry-potter-and-the", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Some%20notes%20on%20Rationality%20in%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Some%20notes%20on%20Rationality%20in%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeEjfow7naRmqapSky%2Flink-some-notes-on-rationality-in-harry-potter-and-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Some%20notes%20on%20Rationality%20in%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeEjfow7naRmqapSky%2Flink-some-notes-on-rationality-in-harry-potter-and-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeEjfow7naRmqapSky%2Flink-some-notes-on-rationality-in-harry-potter-and-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<p><a href=\"http://www.etla.org/hpmor_notes.html\">http://www.etla.org/hpmor_notes.html</a></p>\n<p>This is an ongoing project of mine, although I haven't worked on it in a while. I've been trying to extract the references to Rationality - the Methods of Rationality from <a href=\"http://www.hpmor.com/\">HPMoR</a>. It also ended up having a few quotes that seemed interesting about how the story's going. I've linked references where I could find them.</p>\n<p>I've only got as far as Chapter 40. Any extra submissions welcome.</p>\n<p>At least one person - <a href=\"http://wiki.lesswrong.com/wiki/User:David_Gerard\">User:DavidGerard</a> suggested it deserved being posted as a discussion link.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eEjfow7naRmqapSky", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 19, "extendedScore": null, "score": 9.2117865820359e-07, "legacy": true, "legacyId": "16891", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T13:12:13.283Z", "modifiedAt": null, "url": null, "title": "[Video] Presentation on metacognition contains good intro to basic LW ideas ", "slug": "video-presentation-on-metacognition-contains-good-intro-to", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4hYpoAEZemyJFoxry/video-presentation-on-metacognition-contains-good-intro-to", "pageUrlRelative": "/posts/4hYpoAEZemyJFoxry/video-presentation-on-metacognition-contains-good-intro-to", "linkUrl": "https://www.lesswrong.com/posts/4hYpoAEZemyJFoxry/video-presentation-on-metacognition-contains-good-intro-to", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BVideo%5D%20Presentation%20on%20metacognition%20contains%20good%20intro%20to%20basic%20LW%20ideas%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BVideo%5D%20Presentation%20on%20metacognition%20contains%20good%20intro%20to%20basic%20LW%20ideas%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hYpoAEZemyJFoxry%2Fvideo-presentation-on-metacognition-contains-good-intro-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BVideo%5D%20Presentation%20on%20metacognition%20contains%20good%20intro%20to%20basic%20LW%20ideas%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hYpoAEZemyJFoxry%2Fvideo-presentation-on-metacognition-contains-good-intro-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hYpoAEZemyJFoxry%2Fvideo-presentation-on-metacognition-contains-good-intro-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<p>I attended a talk yesterday given under the auspices of the Ottawa Skeptics on the subject of \"metacognition\" or thinking about thinking -- basically, it was about core rationality concepts. It was designed to appeal to a broad group of lay people interested in science and consisted of a number of examples drawn from pop-sci books such as <em>Thinking, Fast and Slow</em>&nbsp;and <em>Predictably Irrational</em>. (Also mentioned: <a href=\"http://wiki.lesswrong.com/wiki/Straw_Vulcan\">straw vulcans</a>&nbsp;as described by CFAR's own <a href=\"http://measureofdoubt.com/\">Julia Galef</a>.) If people who aren't familiar with LW ask you what LW is about, I'd strongly recommend pointing them to this video.</p>\n<p>Here's <a href=\"http://vimeo.com/43872725\">the link</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4hYpoAEZemyJFoxry", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 9.211914371567053e-07, "legacy": true, "legacyId": "16892", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T15:00:33.320Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Sydney - June", "slug": "meetup-less-wrong-sydney-june", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oklord", "createdAt": "2011-03-22T11:37:19.291Z", "isAdmin": false, "displayName": "Oklord"}, "userId": "EusNQMHkhmSq2k9Y9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TYHX8ETx6XzFvRF5x/meetup-less-wrong-sydney-june", "pageUrlRelative": "/posts/TYHX8ETx6XzFvRF5x/meetup-less-wrong-sydney-june", "linkUrl": "https://www.lesswrong.com/posts/TYHX8ETx6XzFvRF5x/meetup-less-wrong-sydney-june", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Sydney%20-%20June&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Sydney%20-%20June%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTYHX8ETx6XzFvRF5x%2Fmeetup-less-wrong-sydney-june%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Sydney%20-%20June%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTYHX8ETx6XzFvRF5x%2Fmeetup-less-wrong-sydney-june", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTYHX8ETx6XzFvRF5x%2Fmeetup-less-wrong-sydney-june", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/b1'>Less Wrong Sydney - June</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 June 2012 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">77 Liverpool Street, Sydney, Australia, Level 2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[Note - because of a large proportion of people attending who do not regularly read less wrong, a more general topic was chosen]</p>\n\n<p>On this night, the topic is something recently highlighted on Less Wrong's sister blog, Overcoming bias.  <a href=\"http://www.overcomingbias.com/2012/06/the-smart-are-more-biased-to-think-they-are-less-biased.html\" rel=\"nofollow\">http://www.overcomingbias.com/2012/06/the-smart-are-more-biased-to-think-they-are-less-biased.html</a></p>\n\n<p>Essentially, do you think being smarter makes you more fair in your judgement? Does the ability to see more patterns make you better, or merely more likley to be a conspiracy theorist?</p>\n\n<p>Don't know the topic? Worried this is just another cult out for your organs? Doesn't matter. Come for the mindset, board games and to meet new people. Also, the increasingly absurd desserts available at Norita's Cafe and Board games.</p>\n\n<p>Feel free to bring whomever you wish, as always. If you want to invite friends to the group, ask for admin status in the FB group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/b1'>Less Wrong Sydney - June</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TYHX8ETx6XzFvRF5x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.212406546158668e-07, "legacy": true, "legacyId": "16893", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney___June\">Discussion article for the meetup : <a href=\"/meetups/b1\">Less Wrong Sydney - June</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 June 2012 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">77 Liverpool Street, Sydney, Australia, Level 2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[Note - because of a large proportion of people attending who do not regularly read less wrong, a more general topic was chosen]</p>\n\n<p>On this night, the topic is something recently highlighted on Less Wrong's sister blog, Overcoming bias.  <a href=\"http://www.overcomingbias.com/2012/06/the-smart-are-more-biased-to-think-they-are-less-biased.html\" rel=\"nofollow\">http://www.overcomingbias.com/2012/06/the-smart-are-more-biased-to-think-they-are-less-biased.html</a></p>\n\n<p>Essentially, do you think being smarter makes you more fair in your judgement? Does the ability to see more patterns make you better, or merely more likley to be a conspiracy theorist?</p>\n\n<p>Don't know the topic? Worried this is just another cult out for your organs? Doesn't matter. Come for the mindset, board games and to meet new people. Also, the increasingly absurd desserts available at Norita's Cafe and Board games.</p>\n\n<p>Feel free to bring whomever you wish, as always. If you want to invite friends to the group, ask for admin status in the FB group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney___June1\">Discussion article for the meetup : <a href=\"/meetups/b1\">Less Wrong Sydney - June</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Sydney - June", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney___June", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Sydney - June", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney___June1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T15:31:14.543Z", "modifiedAt": null, "url": null, "title": "Let's all learn stats!", "slug": "let-s-all-learn-stats", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:29.471Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mstevens", "createdAt": "2009-05-07T14:46:38.489Z", "isAdmin": false, "displayName": "mstevens"}, "userId": "wgKFztEMLyjFRm4ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MXJN9kwNRTNJkWgYM/let-s-all-learn-stats", "pageUrlRelative": "/posts/MXJN9kwNRTNJkWgYM/let-s-all-learn-stats", "linkUrl": "https://www.lesswrong.com/posts/MXJN9kwNRTNJkWgYM/let-s-all-learn-stats", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let's%20all%20learn%20stats!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet's%20all%20learn%20stats!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXJN9kwNRTNJkWgYM%2Flet-s-all-learn-stats%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let's%20all%20learn%20stats!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXJN9kwNRTNJkWgYM%2Flet-s-all-learn-stats", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXJN9kwNRTNJkWgYM%2Flet-s-all-learn-stats", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p><a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">I want to become Stronger!</a></p>\n<p><a href=\"http://www.udacity.com/\">Udacity</a> are running an<a href=\"http://www.udacity.com/overview/Course/st101/CourseRev/1\"> Introduction to Statistics</a> course starting on the 25th June 2012.</p>\n<p>Many of us could stand to learn some more stats, I certainly could. This seems like a great opportunity!</p>\n<p>It is mandatory for&nbsp;all LWers to enroll in this course.</p>\n<p>Update: the last line was a joke. Obviously people are not finding it funny. Sorry.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MXJN9kwNRTNJkWgYM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 19, "extendedScore": null, "score": 9.212545969661573e-07, "legacy": true, "legacyId": "16894", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T16:45:50.380Z", "modifiedAt": null, "url": null, "title": "Glenn Beck discusses the Singularity, cites SI researchers", "slug": "glenn-beck-discusses-the-singularity-cites-si-researchers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Brihaspati", "createdAt": "2011-09-16T05:30:20.643Z", "isAdmin": false, "displayName": "Brihaspati"}, "userId": "LpvgDCTuGPWEaBXgs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fg5LZQDd3YZncrWbw/glenn-beck-discusses-the-singularity-cites-si-researchers", "pageUrlRelative": "/posts/fg5LZQDd3YZncrWbw/glenn-beck-discusses-the-singularity-cites-si-researchers", "linkUrl": "https://www.lesswrong.com/posts/fg5LZQDd3YZncrWbw/glenn-beck-discusses-the-singularity-cites-si-researchers", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Glenn%20Beck%20discusses%20the%20Singularity%2C%20cites%20SI%20researchers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGlenn%20Beck%20discusses%20the%20Singularity%2C%20cites%20SI%20researchers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffg5LZQDd3YZncrWbw%2Fglenn-beck-discusses-the-singularity-cites-si-researchers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Glenn%20Beck%20discusses%20the%20Singularity%2C%20cites%20SI%20researchers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffg5LZQDd3YZncrWbw%2Fglenn-beck-discusses-the-singularity-cites-si-researchers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffg5LZQDd3YZncrWbw%2Fglenn-beck-discusses-the-singularity-cites-si-researchers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3135, "htmlBody": "<p>From the final chapter of his new book <em>Cowards</em>, titled \"Adapt or Die: The Coming Intelligence Explosion.\"</p>\n<blockquote>\n<p>The year is 1678 and you&rsquo;ve just arrived in England via a time machine. You take out your new iPhone in front of a group of scientists who have gathered to marvel at your arrival.</p>\n<p>&ldquo;Siri,&rdquo; you say, addressing the phone&rsquo;s voice-activated artificial intelligence system, &ldquo;play me some Beethoven.&rdquo;</p>\n<p><em>Dunh-Dunh-Dunh-Duuunnnhhh!</em> The famous opening notes of Beethoven&rsquo;s Fifth Symphony, stored in your music library, play loudly.</p>\n<p>&ldquo;Siri, call my mother.&rdquo;</p>\n<p>Your mother&rsquo;s face appears on the screen, a Hawaiian beach behind her. &ldquo;Hi, Mom!&rdquo; you say. &ldquo;How many fingers am I holding up?&rdquo;</p>\n<p>&ldquo;Three,&rdquo; she correctly answers. &ldquo;Why haven&rsquo;t you called more&mdash;&rdquo;</p>\n<p>&ldquo;Thanks, Mom! Gotta run!&rdquo; you interrupt, hanging up.</p>\n<p>&ldquo;Now,&rdquo; you say. &ldquo;Watch this.&rdquo;</p>\n<p>Your new friends look at the iPhone expectantly.</p>\n<p>&ldquo;Siri, I need to hide a body.&rdquo;</p>\n<p>Without hesitation, Siri asks: &ldquo;What kind of place are you looking for? Mines, reservoirs, metal foundries, dumps, or swamps?&rdquo; (I&rsquo;m not kidding. If you have an iPhone 4S, try it.)</p>\n<p>You respond &ldquo;Swamps,&rdquo; and Siri pulls up a satellite map showing you nearby swamps.</p>\n<p>The scientists are shocked into silence. <em>What is this thing that plays music, instantly teleports video of someone across the globe, helps you get away with&nbsp;murder, and is small enough to fit into a pocket?</em></p>\n<p>At best, your seventeenth-century friends would worship you as a messenger of God. At worst, you&rsquo;d be burned at the stake for witchcraft. After all, as science fiction author Arthur C. Clarke once said, &ldquo;Any sufficiently advanced technology is indistinguishable from magic.&rdquo;</p>\n<p>Now, imagine telling this group that capitalism and representative democracy will take the world by storm, lifting hundreds of millions of people out of poverty. Imagine telling them their descendants will eradicate smallpox and regularly live seventy-five or more years. Imagine telling them that men will walk on the moon, that planes, flying hundreds of miles an hour, will transport people around the world, or that cities will be filled with buildings reaching thousands of feet into the air.</p>\n<p>They&rsquo;d probably escort you to the madhouse.</p>\n<p>Unless, that is, one of the people in that group had been a man named Ray Kurzweil.</p>\n<p>Kurzweil is an inventor and futurist who has done a better job than most at predicting the future. Dozens of the predictions from his 1990 book <em>The Age of Intelligent Machines</em> came true during the 1990s and 2000s. His follow-up book, <em>The Age of Spiritual Machines</em>, published in 1999, fared even better. Of the 147 predictions that Kurzweil made for 2009, 78 percent turned out to be entirely correct, and another 8 percent were roughly correct. For example, even though every portable computer had a keyboard in 1999, Kurzweil predicted that most portable computers would lack a keyboard by 2009. It turns out he was right: by 2009, most portable computers were MP3 players, smartphones, tablets, portable game machines, and other devices that lacked keyboards.</p>\n<p>Kurzweil is most famous for his &ldquo;law of accelerating returns,&rdquo; the idea that&nbsp;technological progress is generally &ldquo;exponential&rdquo; (like a hockey stick, curving up sharply) rather than &ldquo;linear&rdquo; (like a straight line, rising slowly). In nongeek-speak that means that our knowledge is like the compound interest you get on your bank account: it increases exponentially as time goes on because it keeps building on itself. We won&rsquo;t experience one hundred years of progress in the twenty-first century, but rather twenty thousand years of progress (measured at today&rsquo;s rate).</p>\n<p>Many experts have criticized Kurzweil&rsquo;s forecasting methods, but a careful and extensive review of technological trends by researchers at the Santa Fe Institute came to the same basic conclusion: technological progress generally tends to be exponential (or even faster than exponential), not linear.</p>\n<p>So, what does this mean? In his 2005 book <em>The Singularity Is Near</em>, Kurzweil shares his predictions for the next few decades:</p>\n<ul>\n<li>In our current decade, Kurzweil expects real-time translation tools and automatic house-cleaning robots to become common.</li>\n<li>In the 2020s he expects to see the invention of tiny robots that can be injected into our bodies to intelligently find and repair damage and cure infections.</li>\n<li>By the 2030s he expects &ldquo;mind uploading&rdquo; to be possible, meaning that your memories and personality and consciousness could be copied to a machine. You could then make backup copies of yourself, and achieve a kind of technological immortality.</li>\n</ul>\n<p>&nbsp;</p>\n<p>[sidebar]</p>\n<p><em>Age of the Machines?</em></p>\n<p>&ldquo;We became the dominant species on this planet by being the most intelligent species around. This century we are going to cede that crown to machines. After we do that, it will be them steering history rather than us.&rdquo;</p>\n<p>&mdash;Jaan Tallinn, co-creator of Skype and Kazaa</p>\n<p>[/sidebar]</p>\n<p>&nbsp;</p>\n<p>If any of that sounds absurd, remember again how absurd the eradication of smallpox or the iPhone 4S would have seemed to those seventeenth-century scientists. That&rsquo;s because the human brain is conditioned to believe that the past is a great predictor of the future. While that might work fine in some areas, technology is not one of them. Just because it took decades to put two hundred transistors onto a computer chip doesn&rsquo;t mean that it will take decades to get to four hundred. In fact, Moore&rsquo;s Law, which states (roughly) that computing power doubles every two years, shows how technological progress must be thought of in terms of &ldquo;hockey stick&rdquo; progress, not &ldquo;straight line&rdquo; progress. Moore&rsquo;s Law has held for more than half a century already (we can currently fit 2.6 billion transistors onto a single chip) and there&rsquo;s little reason to expect that it won&rsquo;t continue to.</p>\n<p>But the aspect of his book that has the most far-ranging ramifications for us&nbsp;is Kurzweil&rsquo;s prediction that we will achieve a &ldquo;technological singularity&rdquo; in 2045. He defines this term rather vaguely as &ldquo;a future period during which the pace of technological change will be so rapid, its impact so deep, that human life will be irreversibly transformed.&rdquo;</p>\n<p>Part of what Kurzweil is talking about is based on an older, more precise notion of &ldquo;technological singularity&rdquo; called an <em>intelligence explosion</em>. An intelligence explosion is what happens when we create artificial intelligence (AI) that is better than we are <em>at the task of designing artificial intelligences</em>. If the AI we create can improve its own intelligence without waiting for humans to make the next innovation, this will make it even more capable of improving its intelligence, which will . . . well, you get the point. The AI can, with enough improvements, make itself smarter than all of us mere humans put together.</p>\n<p>The really exciting part (or the scary part, if your vision of the future is more like the movie The Terminator) is that, once the intelligence explosion happens, we&rsquo;ll get an AI that is as superior to us at science, politics, invention, and social skills as your computer&rsquo;s calculator is to you at arithmetic. The problems that have occupied mankind for decades&mdash; curing diseases, finding better energy sources, etc.&mdash; could, in many cases, be solved in a matter of weeks or months.</p>\n<p>Again, this might sound far-fetched, but Ray Kurzweil isn&rsquo;t the only one who thinks an intelligence explosion could occur sometime this century. Justin Rattner, the chief technology officer at Intel, predicts some kind of Singularity by 2048. Michael Nielsen, co-author of the leading textbook on quantum computation, thinks there&rsquo;s a decent chance of an intelligence explosion by 2100. Richard Sutton, one of the biggest names in AI, predicts an intelligence explosion near the middle of the century. Leading philosopher David Chalmers is 50 percent confident an intelligence explosion will occur by 2100. Participants at a 2009 conference on AI tended to be 50 percent confident that an intelligence explosion would occur by 2045.</p>\n<p>If we can properly prepare for the intelligence explosion and ensure that it goes well for humanity, it could be the best thing that has ever happened on this fragile planet. Consider the difference between humans and chimpanzees, which share 95 percent of their genetic code. A relatively small difference in intelligence gave humans the ability to invent farming, writing, science, democracy, capitalism, birth control, vaccines, space travel, and iPhones&mdash; all while chimpanzees kept flinging poo at each other.</p>\n<p>&nbsp;</p>\n<p>[sidebar]</p>\n<p><em>Intelligent Design?</em></p>\n<p>The thought that machines could one day have superhuman abilities should&nbsp;make us nervous. Once the machines are smarter and more capable than we are, we won&rsquo;t be able to negotiate with them any more than chimpanzees can negotiate with us. What if the machines don&rsquo;t want the same things we do?</p>\n<p>The truth, unfortunately, is that every kind of AI we know how to build today definitely would not want the same things we do. To build an AI that does, we would need a more flexible &ldquo;decision theory&rdquo; for AI design and new techniques for making sense of human preferences. I know that sounds kind of nerdy, but AIs are made of math and so math is really important for choosing which results you get from building an AI.</p>\n<p>These are the kinds of research problems being tackled by the Singularity Institute in America and the Future of Humanity Institute in Great Britain. Unfortunately, our silly species still spends more money each year on lipstick research than we do on figuring out how to make sure that the most important event of this century (maybe of all human history)&mdash; the intelligence explosion&mdash; actually goes well for us.</p>\n<p>[/sidebar]</p>\n<p>&nbsp;</p>\n<p>Likewise, self-improving machines could perform scientific experiments and build new technologies much faster and more intelligently than humans can. Curing cancer, finding clean energy, and extending life expectancies would be child&rsquo;s play for them. Imagine living out your own personal fantasy in a different virtual world every day. Imagine exploring the galaxy at near light speed, with a few backup copies of your mind safe at home on earth in case you run into an exploding supernova. Imagine a world where resources are harvested so efficiently that everyone&rsquo;s basic needs are taken care of, and political and economic incentives are so intelligently fine-tuned that &ldquo;world peace&rdquo; becomes, for the first time ever, more than a Super Bowl halftime show&nbsp;slogan.</p>\n<p>With self-improving AI we may be able to eradicate suffering and death just as we once eradicated smallpox. It is not the limits of nature that prevent us from doing this, but only the limits of our current understanding. It may sound like a paradox, but it&rsquo;s our brains that prevent us from fully understanding our brains.</p>\n<p>&nbsp;</p>\n<p><strong>Turf Wars</strong></p>\n<p>At this point you might be asking yourself: &ldquo;Why is this topic in this book? What does any of this have to do with the economy or national security or politics?&rdquo;</p>\n<p>In fact, it has everything to do with all of those issues, plus a whole lot more. The intelligence explosion will bring about change on a scale and scope not seen in the history of the world. If we don&rsquo;t prepare for it, things could get very bad, very fast. But if we do prepare for it, the intelligence explosion could be the best thing that has happened since . . . literally ever.</p>\n<p>But before we get to the kind of life-altering progress that would come after the Singularity, we will first have to deal with a lot of smaller changes, many of which will throw entire industries and ways of life into turmoil. Take the music business, for example. It was not long ago that stores like Tower Records and Sam Goody were doing billions of dollars a year in compact disc sales; now people buy music from home via the Internet. Publishing is currently facing a similar upheaval. Newspapers and magazines have struggled to keep subscribers, booksellers like Borders have been forced into bankruptcy, and customers are forcing publishers to switch to ebooks faster than the publishers might like.</p>\n<p>All of this is to say that some people are already witnessing the early stages of upheaval firsthand. But for everyone else, there is still a feeling that something is different this time; that all of those years of education and experience might be turned upside down in an instant. They might not be able to identify it exactly but they realize that the world they&rsquo;ve known for forty, fifty, or sixty years is no longer the same.</p>\n<p>There&rsquo;s a good reason for that. We feel it and sense it because it&rsquo;s true. It&rsquo;s happening. There&rsquo;s absolutely no question that the world in 2030 will be a very different place than the one we live in today. But there is a question, a large one, about whether that place will be better or worse.</p>\n<p>It&rsquo;s human nature to resist change. We worry about our families, our careers, and our bank accounts. The executives in industries that are already experiencing cataclysmic shifts would much prefer to go back to the way things&nbsp;were ten years ago, when people still bought music, magazines, and books in stores. The future was predictable. Humans like that; it&rsquo;s part of our nature.</p>\n<p>But predictability is no longer an option. The intelligence explosion, when it comes in earnest, is going to change everything&mdash; we can either be prepared for it and take advantage of it, or we can resist it and get run over.</p>\n<p>Unfortunately, there are a good number of people who are going to resist it. Not only those in affected industries, but those who hold power at all levels. They see how technology is cutting out the middlemen, how people are becoming empowered, how bloggers can break national news and YouTube videos can create superstars.</p>\n<p>And they don&rsquo;t like it.</p>\n<p>&nbsp;</p>\n<p><strong>A Battle for the Future</strong></p>\n<p>Power bases in business and politics that have been forged over decades, if not centuries, are being threatened with extinction, and they know it. So the owners of that power are trying to hold on. They think they can do that by dragging us backward. They think that, by growing the public&rsquo;s dependency on government, by taking away the entrepreneurial spirit and rewards and by limiting personal freedoms, they can slow down progress.</p>\n<p>But they&rsquo;re wrong. The intelligence explosion is coming so long as science itself continues. Trying to put the genie back in the bottle by dragging us toward serfdom won&rsquo;t stop it and will, in fact, only leave the world with an economy and society that are completely unprepared for the amazing things that it could bring.</p>\n<p>Robin Hanson, author of &ldquo;The Economics of the Singularity&rdquo; and an associate professor of economics at George Mason University, wrote that after the Singularity, &ldquo;The world economy, which now doubles in 15 years or so, would soon double in somewhere from a week to a month.&rdquo;</p>\n<p>That is unfathomable. But even if the rate were much slower, say a doubling of the world economy in two years, the shock-waves from that kind of growth would still change everything we&rsquo;ve come to know and rely on. A machine could offer the ideal farming methods to double or triple crop production, but it can&rsquo;t force a farmer or an industry to implement them. A machine could find the cure for cancer, but it would be meaningless if the pharmaceutical industry or Food and Drug Administration refused to allow it. The machines won&rsquo;t be the problem; humans will be.</p>\n<p>And that&rsquo;s why I wanted to write about this topic. We are at the forefront of something great, something that will make the Industrial Revolution look in&nbsp;comparison like a child discovering his hands. But we have to be prepared. We must be open to the changes that will come, because they will come. Only when we accept that will we be in a position to thrive. We can&rsquo;t allow politicians to blame progress for our problems. We can&rsquo;t allow entrenched bureaucrats and power-hungry executives to influence a future that they may have no place in.</p>\n<p>Many people are afraid of these changes&mdash; of course they are: it&rsquo;s part of being human to fear the unknown&mdash; but we can&rsquo;t be so entrenched in the way the world works now that we are unable to handle change out of fear for what those changes might bring.</p>\n<p>Change is going to be as much a part of our future as it has been of our past. Yes, it will happen faster and the changes themselves will be far more dramatic, but if we prepare for it, the change will mostly be positive. But that preparation is the key: we need to become more well-rounded as individuals so that we&rsquo;re able to constantly adapt to new ways of doing things. In the future, the way you do your job may change four to five or fifty times over the course of your life. Those who cannot, or will not, adapt will be left behind.</p>\n<p>At the same time, the Singularity will give many more people the opportunity to be successful. Because things will change so rapidly there is a much greater likelihood that people will find something they excel at. But it could also mean that people&rsquo;s successes are much shorter-lived. The days of someone becoming a legend in any one business (think Clive Davis in music, Steven Spielberg in movies, or the Hearst family in publishing) are likely over. But those who embrace and adapt to the coming changes, and surround themselves with others who have done the same, will flourish.</p>\n<p>When major companies, set in their ways, try to convince us that change is bad and that we must stick to the status quo, no matter how much human&nbsp;inquisitiveness and ingenuity try to propel us forward, we must look past them. We must know in our hearts that these changes will come, and that if we welcome them into our world, we&rsquo;ll become more successful, more free, and more full of light than we could have ever possibly imagined.</p>\n<p>Ray Kurzweil once wrote, &ldquo;The Singularity is near.&rdquo; The only question will be whether we are ready for it.</p>\n</blockquote>\n<p>The citations for the chapter include:</p>\n<ul>\n<li>Luke Muehlhauser and Anna Salamon, \"Intelligence Explosion: Evidence and Import\"</li>\n<li>Daniel Dewey, \"Learning What to Value\"</li>\n<li>Eliezer Yudkowsky, \"Artificial Intelligence as a Positive and a Negative Factor in Global Risk\"</li>\n<li>Luke Muehlhauser and Louie Helm, \"The Singularity and Machine Ethics\"</li>\n<li>Luke Muehlhauser, \"So You Want to Save the World\"</li>\n<li>Michael Anissimov, \"The Benefits of a Successful Singularity\"</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fg5LZQDd3YZncrWbw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 73, "extendedScore": null, "score": 0.0001665826585136417, "legacy": true, "legacyId": "16895", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>From the final chapter of his new book <em>Cowards</em>, titled \"Adapt or Die: The Coming Intelligence Explosion.\"</p>\n<blockquote>\n<p>The year is 1678 and you\u2019ve just arrived in England via a time machine. You take out your new iPhone in front of a group of scientists who have gathered to marvel at your arrival.</p>\n<p>\u201cSiri,\u201d you say, addressing the phone\u2019s voice-activated artificial intelligence system, \u201cplay me some Beethoven.\u201d</p>\n<p><em>Dunh-Dunh-Dunh-Duuunnnhhh!</em> The famous opening notes of Beethoven\u2019s Fifth Symphony, stored in your music library, play loudly.</p>\n<p>\u201cSiri, call my mother.\u201d</p>\n<p>Your mother\u2019s face appears on the screen, a Hawaiian beach behind her. \u201cHi, Mom!\u201d you say. \u201cHow many fingers am I holding up?\u201d</p>\n<p>\u201cThree,\u201d she correctly answers. \u201cWhy haven\u2019t you called more\u2014\u201d</p>\n<p>\u201cThanks, Mom! Gotta run!\u201d you interrupt, hanging up.</p>\n<p>\u201cNow,\u201d you say. \u201cWatch this.\u201d</p>\n<p>Your new friends look at the iPhone expectantly.</p>\n<p>\u201cSiri, I need to hide a body.\u201d</p>\n<p>Without hesitation, Siri asks: \u201cWhat kind of place are you looking for? Mines, reservoirs, metal foundries, dumps, or swamps?\u201d (I\u2019m not kidding. If you have an iPhone 4S, try it.)</p>\n<p>You respond \u201cSwamps,\u201d and Siri pulls up a satellite map showing you nearby swamps.</p>\n<p>The scientists are shocked into silence. <em>What is this thing that plays music, instantly teleports video of someone across the globe, helps you get away with&nbsp;murder, and is small enough to fit into a pocket?</em></p>\n<p>At best, your seventeenth-century friends would worship you as a messenger of God. At worst, you\u2019d be burned at the stake for witchcraft. After all, as science fiction author Arthur C. Clarke once said, \u201cAny sufficiently advanced technology is indistinguishable from magic.\u201d</p>\n<p>Now, imagine telling this group that capitalism and representative democracy will take the world by storm, lifting hundreds of millions of people out of poverty. Imagine telling them their descendants will eradicate smallpox and regularly live seventy-five or more years. Imagine telling them that men will walk on the moon, that planes, flying hundreds of miles an hour, will transport people around the world, or that cities will be filled with buildings reaching thousands of feet into the air.</p>\n<p>They\u2019d probably escort you to the madhouse.</p>\n<p>Unless, that is, one of the people in that group had been a man named Ray Kurzweil.</p>\n<p>Kurzweil is an inventor and futurist who has done a better job than most at predicting the future. Dozens of the predictions from his 1990 book <em>The Age of Intelligent Machines</em> came true during the 1990s and 2000s. His follow-up book, <em>The Age of Spiritual Machines</em>, published in 1999, fared even better. Of the 147 predictions that Kurzweil made for 2009, 78 percent turned out to be entirely correct, and another 8 percent were roughly correct. For example, even though every portable computer had a keyboard in 1999, Kurzweil predicted that most portable computers would lack a keyboard by 2009. It turns out he was right: by 2009, most portable computers were MP3 players, smartphones, tablets, portable game machines, and other devices that lacked keyboards.</p>\n<p>Kurzweil is most famous for his \u201claw of accelerating returns,\u201d the idea that&nbsp;technological progress is generally \u201cexponential\u201d (like a hockey stick, curving up sharply) rather than \u201clinear\u201d (like a straight line, rising slowly). In nongeek-speak that means that our knowledge is like the compound interest you get on your bank account: it increases exponentially as time goes on because it keeps building on itself. We won\u2019t experience one hundred years of progress in the twenty-first century, but rather twenty thousand years of progress (measured at today\u2019s rate).</p>\n<p>Many experts have criticized Kurzweil\u2019s forecasting methods, but a careful and extensive review of technological trends by researchers at the Santa Fe Institute came to the same basic conclusion: technological progress generally tends to be exponential (or even faster than exponential), not linear.</p>\n<p>So, what does this mean? In his 2005 book <em>The Singularity Is Near</em>, Kurzweil shares his predictions for the next few decades:</p>\n<ul>\n<li>In our current decade, Kurzweil expects real-time translation tools and automatic house-cleaning robots to become common.</li>\n<li>In the 2020s he expects to see the invention of tiny robots that can be injected into our bodies to intelligently find and repair damage and cure infections.</li>\n<li>By the 2030s he expects \u201cmind uploading\u201d to be possible, meaning that your memories and personality and consciousness could be copied to a machine. You could then make backup copies of yourself, and achieve a kind of technological immortality.</li>\n</ul>\n<p>&nbsp;</p>\n<p>[sidebar]</p>\n<p><em>Age of the Machines?</em></p>\n<p>\u201cWe became the dominant species on this planet by being the most intelligent species around. This century we are going to cede that crown to machines. After we do that, it will be them steering history rather than us.\u201d</p>\n<p>\u2014Jaan Tallinn, co-creator of Skype and Kazaa</p>\n<p>[/sidebar]</p>\n<p>&nbsp;</p>\n<p>If any of that sounds absurd, remember again how absurd the eradication of smallpox or the iPhone 4S would have seemed to those seventeenth-century scientists. That\u2019s because the human brain is conditioned to believe that the past is a great predictor of the future. While that might work fine in some areas, technology is not one of them. Just because it took decades to put two hundred transistors onto a computer chip doesn\u2019t mean that it will take decades to get to four hundred. In fact, Moore\u2019s Law, which states (roughly) that computing power doubles every two years, shows how technological progress must be thought of in terms of \u201chockey stick\u201d progress, not \u201cstraight line\u201d progress. Moore\u2019s Law has held for more than half a century already (we can currently fit 2.6 billion transistors onto a single chip) and there\u2019s little reason to expect that it won\u2019t continue to.</p>\n<p>But the aspect of his book that has the most far-ranging ramifications for us&nbsp;is Kurzweil\u2019s prediction that we will achieve a \u201ctechnological singularity\u201d in 2045. He defines this term rather vaguely as \u201ca future period during which the pace of technological change will be so rapid, its impact so deep, that human life will be irreversibly transformed.\u201d</p>\n<p>Part of what Kurzweil is talking about is based on an older, more precise notion of \u201ctechnological singularity\u201d called an <em>intelligence explosion</em>. An intelligence explosion is what happens when we create artificial intelligence (AI) that is better than we are <em>at the task of designing artificial intelligences</em>. If the AI we create can improve its own intelligence without waiting for humans to make the next innovation, this will make it even more capable of improving its intelligence, which will . . . well, you get the point. The AI can, with enough improvements, make itself smarter than all of us mere humans put together.</p>\n<p>The really exciting part (or the scary part, if your vision of the future is more like the movie The Terminator) is that, once the intelligence explosion happens, we\u2019ll get an AI that is as superior to us at science, politics, invention, and social skills as your computer\u2019s calculator is to you at arithmetic. The problems that have occupied mankind for decades\u2014 curing diseases, finding better energy sources, etc.\u2014 could, in many cases, be solved in a matter of weeks or months.</p>\n<p>Again, this might sound far-fetched, but Ray Kurzweil isn\u2019t the only one who thinks an intelligence explosion could occur sometime this century. Justin Rattner, the chief technology officer at Intel, predicts some kind of Singularity by 2048. Michael Nielsen, co-author of the leading textbook on quantum computation, thinks there\u2019s a decent chance of an intelligence explosion by 2100. Richard Sutton, one of the biggest names in AI, predicts an intelligence explosion near the middle of the century. Leading philosopher David Chalmers is 50 percent confident an intelligence explosion will occur by 2100. Participants at a 2009 conference on AI tended to be 50 percent confident that an intelligence explosion would occur by 2045.</p>\n<p>If we can properly prepare for the intelligence explosion and ensure that it goes well for humanity, it could be the best thing that has ever happened on this fragile planet. Consider the difference between humans and chimpanzees, which share 95 percent of their genetic code. A relatively small difference in intelligence gave humans the ability to invent farming, writing, science, democracy, capitalism, birth control, vaccines, space travel, and iPhones\u2014 all while chimpanzees kept flinging poo at each other.</p>\n<p>&nbsp;</p>\n<p>[sidebar]</p>\n<p><em>Intelligent Design?</em></p>\n<p>The thought that machines could one day have superhuman abilities should&nbsp;make us nervous. Once the machines are smarter and more capable than we are, we won\u2019t be able to negotiate with them any more than chimpanzees can negotiate with us. What if the machines don\u2019t want the same things we do?</p>\n<p>The truth, unfortunately, is that every kind of AI we know how to build today definitely would not want the same things we do. To build an AI that does, we would need a more flexible \u201cdecision theory\u201d for AI design and new techniques for making sense of human preferences. I know that sounds kind of nerdy, but AIs are made of math and so math is really important for choosing which results you get from building an AI.</p>\n<p>These are the kinds of research problems being tackled by the Singularity Institute in America and the Future of Humanity Institute in Great Britain. Unfortunately, our silly species still spends more money each year on lipstick research than we do on figuring out how to make sure that the most important event of this century (maybe of all human history)\u2014 the intelligence explosion\u2014 actually goes well for us.</p>\n<p>[/sidebar]</p>\n<p>&nbsp;</p>\n<p>Likewise, self-improving machines could perform scientific experiments and build new technologies much faster and more intelligently than humans can. Curing cancer, finding clean energy, and extending life expectancies would be child\u2019s play for them. Imagine living out your own personal fantasy in a different virtual world every day. Imagine exploring the galaxy at near light speed, with a few backup copies of your mind safe at home on earth in case you run into an exploding supernova. Imagine a world where resources are harvested so efficiently that everyone\u2019s basic needs are taken care of, and political and economic incentives are so intelligently fine-tuned that \u201cworld peace\u201d becomes, for the first time ever, more than a Super Bowl halftime show&nbsp;slogan.</p>\n<p>With self-improving AI we may be able to eradicate suffering and death just as we once eradicated smallpox. It is not the limits of nature that prevent us from doing this, but only the limits of our current understanding. It may sound like a paradox, but it\u2019s our brains that prevent us from fully understanding our brains.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Turf_Wars\">Turf Wars</strong></p>\n<p>At this point you might be asking yourself: \u201cWhy is this topic in this book? What does any of this have to do with the economy or national security or politics?\u201d</p>\n<p>In fact, it has everything to do with all of those issues, plus a whole lot more. The intelligence explosion will bring about change on a scale and scope not seen in the history of the world. If we don\u2019t prepare for it, things could get very bad, very fast. But if we do prepare for it, the intelligence explosion could be the best thing that has happened since . . . literally ever.</p>\n<p>But before we get to the kind of life-altering progress that would come after the Singularity, we will first have to deal with a lot of smaller changes, many of which will throw entire industries and ways of life into turmoil. Take the music business, for example. It was not long ago that stores like Tower Records and Sam Goody were doing billions of dollars a year in compact disc sales; now people buy music from home via the Internet. Publishing is currently facing a similar upheaval. Newspapers and magazines have struggled to keep subscribers, booksellers like Borders have been forced into bankruptcy, and customers are forcing publishers to switch to ebooks faster than the publishers might like.</p>\n<p>All of this is to say that some people are already witnessing the early stages of upheaval firsthand. But for everyone else, there is still a feeling that something is different this time; that all of those years of education and experience might be turned upside down in an instant. They might not be able to identify it exactly but they realize that the world they\u2019ve known for forty, fifty, or sixty years is no longer the same.</p>\n<p>There\u2019s a good reason for that. We feel it and sense it because it\u2019s true. It\u2019s happening. There\u2019s absolutely no question that the world in 2030 will be a very different place than the one we live in today. But there is a question, a large one, about whether that place will be better or worse.</p>\n<p>It\u2019s human nature to resist change. We worry about our families, our careers, and our bank accounts. The executives in industries that are already experiencing cataclysmic shifts would much prefer to go back to the way things&nbsp;were ten years ago, when people still bought music, magazines, and books in stores. The future was predictable. Humans like that; it\u2019s part of our nature.</p>\n<p>But predictability is no longer an option. The intelligence explosion, when it comes in earnest, is going to change everything\u2014 we can either be prepared for it and take advantage of it, or we can resist it and get run over.</p>\n<p>Unfortunately, there are a good number of people who are going to resist it. Not only those in affected industries, but those who hold power at all levels. They see how technology is cutting out the middlemen, how people are becoming empowered, how bloggers can break national news and YouTube videos can create superstars.</p>\n<p>And they don\u2019t like it.</p>\n<p>&nbsp;</p>\n<p><strong id=\"A_Battle_for_the_Future\">A Battle for the Future</strong></p>\n<p>Power bases in business and politics that have been forged over decades, if not centuries, are being threatened with extinction, and they know it. So the owners of that power are trying to hold on. They think they can do that by dragging us backward. They think that, by growing the public\u2019s dependency on government, by taking away the entrepreneurial spirit and rewards and by limiting personal freedoms, they can slow down progress.</p>\n<p>But they\u2019re wrong. The intelligence explosion is coming so long as science itself continues. Trying to put the genie back in the bottle by dragging us toward serfdom won\u2019t stop it and will, in fact, only leave the world with an economy and society that are completely unprepared for the amazing things that it could bring.</p>\n<p>Robin Hanson, author of \u201cThe Economics of the Singularity\u201d and an associate professor of economics at George Mason University, wrote that after the Singularity, \u201cThe world economy, which now doubles in 15 years or so, would soon double in somewhere from a week to a month.\u201d</p>\n<p>That is unfathomable. But even if the rate were much slower, say a doubling of the world economy in two years, the shock-waves from that kind of growth would still change everything we\u2019ve come to know and rely on. A machine could offer the ideal farming methods to double or triple crop production, but it can\u2019t force a farmer or an industry to implement them. A machine could find the cure for cancer, but it would be meaningless if the pharmaceutical industry or Food and Drug Administration refused to allow it. The machines won\u2019t be the problem; humans will be.</p>\n<p>And that\u2019s why I wanted to write about this topic. We are at the forefront of something great, something that will make the Industrial Revolution look in&nbsp;comparison like a child discovering his hands. But we have to be prepared. We must be open to the changes that will come, because they will come. Only when we accept that will we be in a position to thrive. We can\u2019t allow politicians to blame progress for our problems. We can\u2019t allow entrenched bureaucrats and power-hungry executives to influence a future that they may have no place in.</p>\n<p>Many people are afraid of these changes\u2014 of course they are: it\u2019s part of being human to fear the unknown\u2014 but we can\u2019t be so entrenched in the way the world works now that we are unable to handle change out of fear for what those changes might bring.</p>\n<p>Change is going to be as much a part of our future as it has been of our past. Yes, it will happen faster and the changes themselves will be far more dramatic, but if we prepare for it, the change will mostly be positive. But that preparation is the key: we need to become more well-rounded as individuals so that we\u2019re able to constantly adapt to new ways of doing things. In the future, the way you do your job may change four to five or fifty times over the course of your life. Those who cannot, or will not, adapt will be left behind.</p>\n<p>At the same time, the Singularity will give many more people the opportunity to be successful. Because things will change so rapidly there is a much greater likelihood that people will find something they excel at. But it could also mean that people\u2019s successes are much shorter-lived. The days of someone becoming a legend in any one business (think Clive Davis in music, Steven Spielberg in movies, or the Hearst family in publishing) are likely over. But those who embrace and adapt to the coming changes, and surround themselves with others who have done the same, will flourish.</p>\n<p>When major companies, set in their ways, try to convince us that change is bad and that we must stick to the status quo, no matter how much human&nbsp;inquisitiveness and ingenuity try to propel us forward, we must look past them. We must know in our hearts that these changes will come, and that if we welcome them into our world, we\u2019ll become more successful, more free, and more full of light than we could have ever possibly imagined.</p>\n<p>Ray Kurzweil once wrote, \u201cThe Singularity is near.\u201d The only question will be whether we are ready for it.</p>\n</blockquote>\n<p>The citations for the chapter include:</p>\n<ul>\n<li>Luke Muehlhauser and Anna Salamon, \"Intelligence Explosion: Evidence and Import\"</li>\n<li>Daniel Dewey, \"Learning What to Value\"</li>\n<li>Eliezer Yudkowsky, \"Artificial Intelligence as a Positive and a Negative Factor in Global Risk\"</li>\n<li>Luke Muehlhauser and Louie Helm, \"The Singularity and Machine Ethics\"</li>\n<li>Luke Muehlhauser, \"So You Want to Save the World\"</li>\n<li>Michael Anissimov, \"The Benefits of a Successful Singularity\"</li>\n</ul>", "sections": [{"title": "Turf Wars", "anchor": "Turf_Wars", "level": 1}, {"title": "A Battle for the Future", "anchor": "A_Battle_for_the_Future", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "181 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 183, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T17:18:21.170Z", "modifiedAt": null, "url": null, "title": "[Link] The Greek Heliocentric Theory", "slug": "link-the-greek-heliocentric-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:38.706Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/noBKApHwFm8aMuvnw/link-the-greek-heliocentric-theory", "pageUrlRelative": "/posts/noBKApHwFm8aMuvnw/link-the-greek-heliocentric-theory", "linkUrl": "https://www.lesswrong.com/posts/noBKApHwFm8aMuvnw/link-the-greek-heliocentric-theory", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20Greek%20Heliocentric%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20Greek%20Heliocentric%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoBKApHwFm8aMuvnw%2Flink-the-greek-heliocentric-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20Greek%20Heliocentric%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoBKApHwFm8aMuvnw%2Flink-the-greek-heliocentric-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoBKApHwFm8aMuvnw%2Flink-the-greek-heliocentric-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1033, "htmlBody": "<p><em>Summary: The Greeks likely rejected a heliocentric theory because it would conflict with the lack of any visible stellar parallax, not for egotistical, common-sense, or aesthetic reasons.</em></p>\n<p>I had always heard that the Greeks embraced a geocentric universe for common-sense, aesthetic reasons - not scientific ones. But it seems as if the real story is more complicated than that:</p>\n<p>From <a title=\"Isomorphismes\" href=\"http://isomorphismes.tumblr.com/post/24522850119/geocentric\">Isomorphismes</a>:</p>\n<blockquote>\n<p>Now this is the kicker in your Popperian dirtsack. The Greeks had the right theory (heliocentric solar system) but discarded it on the basis of experimental evidence! Never preach to me about progress-in-science when all you&rsquo;ve heard is a one-liner about Popper and the communal acceptance of general relativity. Especially don&rsquo;t follow it up by saying that science marches toward the Truth whilst religion thwarts its progress. According to Astronomer Lisa, it&rsquo;s not true that the Greeks simply thought they and their Gods were at the centre of the Universe because they were egotistical. They&nbsp;reasoned&nbsp;to the geocentric conclusion based on quantitative evidence. How? They measured parallax.(Difference in stellar appearance from spring to fall, when we&rsquo;re on opposite sides of the Sun.) Given the insensitivity of their measurement tools at the time, the stars didn&rsquo;t change positions at all when the Earth moved to the other side of the Sun. Based on that, they rejected the heliocentric hypothesis. If the Earth actually did move around the Sun, then the stars would logically have to appear different from one time to another. But they remain ever fixed in the same place in the Heavens, therefore the Earth must be still (geocentric).</p>\n</blockquote>\n<p>I dug a little bit deeper, and this seems to be more or less accurate. From <a title=\"Greek Heliocentric\" href=\"http://www.jstor.org/discover/10.2307/283344?uid=3739600&amp;uid=2129&amp;uid=2&amp;uid=70&amp;uid=4&amp;uid=3739256&amp;sid=21100835885521\">The Greek Heliocentric Theory and its Abandonment</a>:</p>\n<blockquote>\n<p>This paper then examines possible reasons for the Greek abandonment of the heliocentric theory and concludes that there is no reason to deplore its abandonment. In developing the heliocentric theory the Greeks had run the gamut of theorizing. We are indebted to the Alexandrians and Hipparchus for turning away from speculation to take up the recording of precise astronomical data. Here was laid the foundation upon which modern astronomy was built.</p>\n<p>Let &nbsp;us now suppose&nbsp;that Aristarchus&rsquo;&nbsp;theory was widely circulated and that it was given careful consideration by leading&nbsp;astronomers. There is one objection&nbsp;that&nbsp;immediately &nbsp;arises when&nbsp;the earth is put&nbsp;in motion, the very difficulty which must &nbsp;have&nbsp;disquieted Copernicus and which caused Tycho Brahe shortly &nbsp;afterwards to renounce Copernicus&rsquo; heliocentric system and to put&nbsp;the&nbsp;earth again at rest.&nbsp;(Tycho reverted to a system&nbsp;first suggested&nbsp;by some ancient Greek, who made the planets revolve about the&nbsp;sun and the sun about the earth.) The difficulty&nbsp;is this. As soon&nbsp;as the &nbsp;earth is set in motion &nbsp;in an annual revolution &nbsp;about the sun,&nbsp;the distance between any&nbsp;two of the earth&rsquo;s positions&nbsp;that are six months apart will be twice as &nbsp;great as the earth&rsquo;s distance from&nbsp;the sun. Over such vast distances some displacement&nbsp;in the positions of the stars ought&nbsp;to be observed. The more accurate the&nbsp;astronomical instruments and the greater&nbsp;the estimated distance&nbsp;of the sun, the more reason should there be to expect stellar displacement. Now it so happened&nbsp;that Aristarchus reached his&nbsp;conclusions at the very&nbsp;time when interest was keen at Alexandria&nbsp;and elsewhere in the Greek world in accurate observations and when marked improvements were being made in precision instruments. To appreciate these developments we need only recall the careful stellar catalogues of Aristyllus and Timocharis early in the third century B.C., the work of the latter enabling Hipparchus to discover the precession of the equinoxes, and the armillary sphere of Eratosthenes by which he was able to &nbsp;determine &nbsp;the obliquity of the ecliptic and the circumference of the &nbsp;earth. Hipparchus continued&nbsp;to make improvements in the next century. He, as we shall &nbsp;see,&nbsp;had a much better appreciation of the sun&rsquo;s great distance than&nbsp;Copernicus. Of course it was&nbsp;impossible&nbsp;to observe stellar displacement without the aid of a&nbsp;telescope. Inability&nbsp;to observe it left astronomers with only two alternatives: either the stars were so remote that it was impossible to detect displacement, or the earth&nbsp;would have to remain at rest.</p>\n<p>..Heath was of &nbsp;the opinion&nbsp;that Hipparchus was responsible&nbsp;for&nbsp;the &nbsp;death of Aristarchus&rsquo; theory, that the adherence of so preeminent an astronomer to a geocentric orientation sealed the doom&nbsp;of the heliocentric&nbsp;theory. This is a reasonable conjecture.&nbsp;Hipparchus was &nbsp;noted for his careful observations, his stellar&nbsp;catalogues, and the remarkable precision of his recordings of solar&nbsp;and &nbsp;lunar &nbsp;motions.&nbsp;According&nbsp;to Ptolemy he was devoted to truth above all else and &nbsp;because he did not possess sufficient data,&nbsp;he refused to attempt&nbsp;to account for planetary motions as he had&nbsp;for those of the sun and moon. His discovery of the precession of the equinoxes attests to the keenness of his observations. He&nbsp;came much closer to appreciating&nbsp;the vast distance of the &nbsp;sun&nbsp;than Copernicus did.</p>\n<p>..We do not know whether or not Hipparchus ever seriously entertained &nbsp;Aristarchus&rsquo; views about the earth&rsquo;s motions, but from what we have seen of his cautious and accurate methods, it&nbsp;is&nbsp;likely&nbsp;that he would have quickly rejected&nbsp;the heliocentric theory&nbsp;in the absence of visible stellar displacement.</p>\n</blockquote>\n<p>And from <a title=\"Ancient Greek Astronomers\" href=\"http://www.jstor.org/discover/10.2307/3292433?uid=3739600&amp;uid=2&amp;uid=4&amp;uid=3739256&amp;sid=47699082551257\">The Ancient Greek Astronomers: A Remarkable Record of Ingenuity</a>:</p>\n<blockquote>\n<p>Aristarchus was successful in explaining&nbsp;variations in brilliance and reverse courses of&nbsp;the planets, but planetary motions are far&nbsp;more complicated&nbsp;than that. Kepler was the&nbsp;first to realize that the planets do not describe&nbsp;circular orbits, but rather ellipses, and that&nbsp;the sun is not in the middle of these orbits but&nbsp;in the foci of the ellipses. That something was&nbsp;wrong might have been suspected as early as&nbsp;330 &nbsp;B.C., for Callippus noticed that the&nbsp;seasons were not of the same&nbsp;length. He estimated their&nbsp;lengths between solstices and&nbsp;equinoxes&nbsp;to &nbsp;be 94, 92, 89, and 90 days-&nbsp;figures&nbsp;that are very nearly correct. Or to&nbsp;show the&nbsp;irregularities&nbsp;that might result from&nbsp;combining&nbsp;the eccentricities of the orbits of&nbsp;two &nbsp;planets,&nbsp;in &nbsp;some years Mars and the&nbsp;earth at closest approximation are 36 million&nbsp;miles apart and in other years (as in 1948) may&nbsp;be 63 million miles apart at their nearest approach.&nbsp;Now the Alexandrians were pointing&nbsp;their&nbsp;precision sights at the planets and must have&nbsp;been disturbed by&nbsp;these peculiarities. Furthermore&nbsp;they would have been less kindly&nbsp;disposed&nbsp;towards Aristarchus&rsquo; explanation of&nbsp;the absence of visible stellar parallax by placing&nbsp;the stars at &nbsp;an almost infinite distance&nbsp;away because they had a better appreciation&nbsp;of the sun&rsquo;s vast distance and consequently&nbsp;would have&nbsp;stronger reason to expect&nbsp;to find&nbsp;parallax.&nbsp;It would seem that the more precise&nbsp;the &nbsp;instruments, the &nbsp;less &nbsp;likelihood there&nbsp;would be of the earth&rsquo;s being&nbsp;in motion.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bY5MaF2EATwDkomvu": 1, "csMv9MvvjYJyeHqoo": 1, "ZpG9rheyAkgCoEQea": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "noBKApHwFm8aMuvnw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 52, "extendedScore": null, "score": 0.000127, "legacy": true, "legacyId": "16896", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T18:00:51.950Z", "modifiedAt": null, "url": null, "title": "Reply to Holden on 'Tool AI'", "slug": "reply-to-holden-on-tool-ai", "viewCount": null, "lastCommentedAt": "2020-02-19T01:27:05.522Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sizjfDgCgAsuLJQmm/reply-to-holden-on-tool-ai", "pageUrlRelative": "/posts/sizjfDgCgAsuLJQmm/reply-to-holden-on-tool-ai", "linkUrl": "https://www.lesswrong.com/posts/sizjfDgCgAsuLJQmm/reply-to-holden-on-tool-ai", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reply%20to%20Holden%20on%20'Tool%20AI'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReply%20to%20Holden%20on%20'Tool%20AI'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsizjfDgCgAsuLJQmm%2Freply-to-holden-on-tool-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reply%20to%20Holden%20on%20'Tool%20AI'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsizjfDgCgAsuLJQmm%2Freply-to-holden-on-tool-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsizjfDgCgAsuLJQmm%2Freply-to-holden-on-tool-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5189, "htmlBody": "<p>I begin by thanking&nbsp;Holden Karnofsky of <a href=\"http://www.givewell.org/\">Givewell</a> for his rare gift of his detailed, engaged, and helpfully-meant critical article <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">Thoughts on the Singularity Institute (SI)</a>. In this reply I will engage with only one of the <em>many</em> subjects raised therein, the topic of, as I would term them, non-self-modifying planning Oracles, a.k.a. 'Google Maps AGI' a.k.a. 'tool AI', this being the topic that requires me personally to answer. &nbsp;I hope that my reply will be accepted as addressing the most important central points, though I did not have time to explore every avenue. &nbsp;I certainly do not wish to be logically rude, and if I have failed, please remember with compassion that it's not always obvious to one person what another person will think was the central point.</p>\n<p>Luke Mueulhauser and Carl Shulman contributed to this article, but the final edit was my own, likewise any flaws.</p>\n<h3>Summary:</h3>\n<p>Holden's concern is that \"SI appears to neglect the potentially important distinction between 'tool' and 'agent' AI.\" His archetypal example is <a href=\"https://maps.google.com/\">Google Maps</a>:</p>\n<blockquote>\n<p>Google Maps is not an <em>agent</em>, taking actions in order to maximize a utility parameter. It is a <em>tool</em>, generating information and then displaying it in a user-friendly manner for me to consider, use and export or discard as I wish.</p>\n</blockquote>\n<p>The reply breaks down into four heavily interrelated points:</p>\n<p>First, Holden seems to think (and Jaan Tallinn doesn't apparently object to, in their exchange) that if a non-self-modifying planning Oracle is indeed the best strategy, then all of SIAI's past and intended future work is wasted. &nbsp;To me it looks like there's a huge amount of overlap in underlying processes in the AI that would have to be built and the insights required to build it, and I would be trying to assemble mostly - though not quite exactly - the same kind of&nbsp;<em>team </em>if I was trying to build a non-self-modifying planning Oracle, with the same initial mix of talents and skills.</p>\n<p>Second, a non-self-modifying planning Oracle doesn't sound nearly as safe once you stop saying human-English phrases like \"describe the consequences of an action to the user\" and start trying to come up with math that says scary dangerous things like (he translated into English) \"increase the correspondence between the user's belief about relevant consequences and reality\". &nbsp;Hence&nbsp;why&nbsp;the people on the team would have to solve the same sorts of problems.</p>\n<p>Appreciating the force of the third point is a lot easier if one appreciates the difficulties discussed in points 1 and 2, but is actually empirically verifiable independently: &nbsp;Whether or not a non-self-modifying planning Oracle is the <em>best</em> solution in the end, it's not such an <em>obvious</em>&nbsp;privileged-point-in-solution-space that someone should be alarmed at SIAI not discussing it. &nbsp;This is empirically verifiable in the sense that 'tool AI' wasn't the obvious solution to e.g. John McCarthy, Marvin Minsky, I. J. Good, Peter Norvig, Vernor Vinge, or for that matter Isaac Asimov. &nbsp;At one point, Holden says:</p>\n<blockquote>\n<p>One of the things that bothers me most about SI is that there is practically no public content, as far as I can tell, explicitly addressing the idea of a \"tool\" and giving arguments for why AGI is likely to work only as an \"agent.\"</p>\n</blockquote>\n<p>If I take literally that this is one of the things that bothers Holden <em>most...</em>&nbsp;I think I'd start stacking&nbsp;up some of the literature on the number of different things that <em>just respectable academics</em>&nbsp;have suggested as the <em>obvious solution</em>&nbsp;to what-to-do-about-AI - none of which would be about non-self-modifying smarter-than-human planning Oracles - and beg him to have some compassion on us for what we <em>haven't addressed yet</em>. &nbsp;It might be the right suggestion, but it's not so obviously right that our failure to prioritize discussing it reflects negligence.</p>\n<p>The final point at the end is looking over all the preceding discussion and realizing that, yes, you want to have people specializing in Friendly AI who know this stuff, but as all that preceding discussion is actually the following discussion at this point, I shall reserve it for later.<a id=\"more\"></a></p>\n<h3>1. &nbsp;The math of optimization, and the similar parts of a planning Oracle.</h3>\n<p>What does it take to build a smarter-than-human intelligence, of whatever sort, and have it go well?</p>\n<p>A \"Friendly AI programmer\" is somebody who specializes in seeing the correspondence of mathematical structures to What Happens in the Real World. It's somebody who looks at Hutter's specification of <a href=\"http://www.amazon.com/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3642060528/\">AIXI</a> and reads the actual equations - actually stares at the Greek symbols and not just the accompanying English text -&nbsp;and sees, \"Oh, this AI will try to gain control of its reward channel,\" as well as numerous subtler issues like, \"This AI presumes a Cartesian boundary separating itself from the environment; it may drop an anvil on its own head.\" Similarly, working on <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">TDT</a> means e.g. looking at a mathematical specification of decision theory, and seeing \"Oh, this is vulnerable to blackmail\" and coming up with a mathematical counter-specification of an AI that isn't so vulnerable to blackmail.</p>\n<p>Holden's post seems to imply that if you're building a non-self-modifying planning Oracle (aka 'tool AI') rather than an acting-in-the-world agent, you don't need a Friendly AI programmer because FAI programmers only work on agents. But this isn't how the engineering skills are split up. Inside the AI, whether an agent AI or a planning Oracle, there would be similar AGI-challenges like \"build a predictive model of the world\", and similar FAI-conjugates of those challenges like finding the 'user' inside an AI-created model of the universe. &nbsp;The insides would look a lot more similar than the outsides. &nbsp;An analogy would be supposing that a machine learning professional who does sales optimization for an orange company couldn't possibly do sales optimization for a banana company, because their skills must be about oranges rather than bananas.</p>\n<p>Admittedly, if it turns out to be possible to use a human understanding of cognitive algorithms to build and run a smarter-than-human Oracle without it being self-improving - this seems unlikely, but not impossible - then you wouldn't have to solve problems that arise with self-modification. &nbsp;But this eliminates only one dimension of the work. &nbsp;And on an even more meta level, it seems like you would call upon almost identical <em>talents and skills</em>&nbsp;to come up with whatever insights were required&nbsp;- though if it were predictable in advance that we'd abjure self-modification, then, yes, we'd place less emphasis on e.g. finding a team member with past experience in reflective math, and wouldn't waste (additional) time specializing in reflection. &nbsp;But if you wanted math inside the planning Oracle that <em>operated the way you thought it did</em>, and you wanted somebody who <em>understood what could possibly go wrong</em>&nbsp;and how to avoid it, you would need to make a function call to the same sort of talents and skills&nbsp;to build an agent AI, or an Oracle that <em>was</em>&nbsp;self-modifying, etc.</p>\n<h3>2. &nbsp;Yes, planning Oracles have hidden gotchas too.</h3>\n<p>\"Tool AI\" may sound simple in English, a short sentence in the language of empathically-modeled agents &mdash; it's just \"a thingy that shows you plans instead of a thingy that goes and does things.\" If you want to know whether this hypothetical entity does X, you just check whether the outcome of X sounds like \"showing someone a plan\" or \"going and doing things\", and you've got your answer. &nbsp;It starts sounding much scarier once you try to say something more formal and internally-causal like \"Model the user and the universe, predict the degree of correspondence between the user's model and the universe, and select from among possible explanation-actions on this basis.\"</p>\n<p>Holden, in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Tallinn-Karnofsky-2011.pdf\">his dialogue with Jaan Tallinn</a>, writes out this attempt at formalizing:</p>\n<blockquote>\n<p>Here's how I picture the Google Maps AGI ...</p>\n<p style=\"padding-left: 30px; \">utility_function = construct_utility_function(process_user_input());</p>\n<p style=\"padding-left: 30px; \">foreach $action in $all_possible_actions {</p>\n<p style=\"padding-left: 60px; \">$action_outcome = prediction_function($action,$data);</p>\n<p style=\"padding-left: 60px; \">$utility = utility_function($action_outcome);</p>\n<p style=\"padding-left: 60px; \">if ($utility &gt; $leading_utility) { $leading_utility = $utility;</p>\n<p style=\"padding-left: 60px; \">$leading_action = $action; }</p>\n<p style=\"padding-left: 30px; \">}</p>\n<p style=\"padding-left: 30px; \">report($leading_action);</p>\n<p>construct_utility_function(process_user_input()) is just a human-quality function for understanding what the speaker wants. prediction_function is an implementation of a human-quality data-&gt;prediction function in superior hardware. $data is fixed (it's a dataset larger than any human can process); same with $all_possible_actions. report($leading_action) calls a Google Maps-like interface for understanding the consequences of $leading_action; it basically breaks the action into component parts and displays predictions for different times and conditional on different parameters.</p>\n</blockquote>\n<p>Google Maps doesn't check all possible routes. If I wanted to design Google Maps, I would start out by throwing out a standard planning technique on a connected graph where each edge has a cost function and there's a good heuristic measure of the distance, e.g. <a href=\"http://en.wikipedia.org/wiki/A*_search_algorithm\">A* search</a>. If that was too slow, I'd next try some more efficient version like weighted A* (or bidirectional weighted memory-bounded A*, which I expect I could also get off-the-shelf somewhere).&nbsp;Once you introduce weighted A*, you no longer have a guarantee that you're selecting the optimal path. &nbsp;You have a guarantee to within a known factor of the cost of the optimal path &mdash; but the actual path selected wouldn't be quite optimal. The suggestion produced would be an approximation whose exact steps depended on the exact algorithm you used. That's true even if you can predict the exact cost &mdash; exact utility &mdash; of any particular path you actually look at; and even if you have a heuristic that never overestimates the cost.</p>\n<p>The reason we don't have <a href=\"http://en.wikipedia.org/wiki/Optimal_solutions_for_Rubik's_Cube\">God's Algorithm for solving the Rubik's Cube </a> is that there's no perfect way of measuring the distance between any two Rubik's Cube positions &mdash; you can't look at two Rubik's cube positions, and figure out the minimum number of moves required to get from one to another. It took 15 years to prove that there was a position requiring at least 20 moves to solve, and then another 15 years to come up with a computer algorithm that could solve any position in at most 20 moves, but we still can't compute the actual, minimum solution to all Cubes (\"God's Algorithm\"). This, even though we can exactly calculate the cost and consequence of any actual Rubik's-solution-path we consider.</p>\n<p>When it comes to AGI &mdash; solving general cross-domain \"Figure out how to do X\" problems &mdash; you're not going to get anywhere near the one, true, optimal answer. You're going to &mdash; at best, if everything works right &mdash; get <em>a</em> good answer that's a cross-product of the \"utility function\" and all the other algorithmic properties that determine what sort of answer the AI finds easy to invent (i.e. can be invented using bounded computing time).</p>\n<p>As for the notion that this AGI runs on a \"human predictive algorithm\" that we got off of neuroscience and then implemented using more computing power, without knowing how it works or being able to enhance it further: It took 30 years of multiple computer scientists doing basic math research, and inventing code, and running that code on a computer cluster, for them to come up with a 20-move solution to the Rubik's Cube. If a planning Oracle is going to produce better solutions than humanity has yet managed to the Rubik's Cube, it needs to be capable of doing original computer science research and writing its own code. You can't get a 20-move solution out of a human brain, using the native human planning algorithm. Humanity can do it, but only by exploiting the ability of humans to explicitly comprehend the deep structure of the domain (not just rely on intuition) and then inventing an artifact, a new design, running code which uses a different and superior cognitive algorithm, to solve that Rubik's Cube in 20 moves. We do all that without being <em>self</em>-modifying, but it's still a capability to respect.</p>\n<p>And I'm not even going into what it would take for a planning Oracle to out-strategize any human, come up with a plan for persuading someone, solve original scientific problems by looking over experimental data (like Einstein did), design a nanomachine, and so on.</p>\n<p>Talking like there's this one simple \"predictive algorithm\" that we can read out of the brain using neuroscience and overpower to produce better plans... doesn't seem quite congruous with what humanity actually does to produce its predictions and plans.</p>\n<p>If we take the concept of the Google Maps AGI at face value, then it actually has four key magical components. &nbsp;(In this case, \"magical\" isn't to be taken as prejudicial, it's a term of art that means we haven't said how the component works yet.) &nbsp;There's a magical comprehension of the user's utility function, a magical world-model that GMAGI uses to comprehend the consequences of actions, a magical planning element that selects a <em>non-optimal</em> path using some method <em>other</em> than exploring all possible actions, and a magical explain-to-the-user function.</p>\n<p>report($leading_action) isn't exactly a trivial step either. Deep Blue tells you to move your pawn or you'll lose the game. You ask \"Why?\" and the answer is a gigantic search tree of billions of possible move-sequences, leafing at positions which are heuristically rated using a static-position evaluation algorithm trained on millions of games. Or the planning Oracle tells you that a certain DNA sequence will produce a protein that cures cancer, you ask \"Why?\", and then humans aren't even capable of verifying, for themselves, the assertion that the peptide sequence will fold into the protein the planning Oracle says it does.</p>\n<p>\"So,\" you say, after the first dozen times you ask the Oracle a question and it returns an answer that you'd have to take on faith, \"we'll just specify in the utility function that the plan should be understandable.\"</p>\n<p>Whereupon other things start going wrong. Viliam_Bur, in the comments thread, gave this example, which I've slightly simplified:</p>\n<blockquote>\n<p>Example question: \"How should I get rid of my disease most cheaply?\" Example answer: \"You won't. You will die soon, unavoidably. This report is 99.999% reliable\". Predicted human reaction: Decides to kill self and get it over with. Success rate: 100%, the disease is gone. Costs of cure: zero. Mission completed.</p>\n</blockquote>\n<p>Bur is trying to give an example of how things might go wrong if the preference function is over the accuracy of the predictions explained to the human&mdash; rather than <em>just</em> the human's 'goodness' of the outcome. And if the preference function <em>was</em> just over the human's 'goodness' of the end result, rather than the accuracy of the human's understanding of the predictions, the AI might tell you something that was predictively false but whose implementation would lead you to what the AI defines as a 'good' outcome. And if we ask how happy the human is, the resulting decision procedure would exert optimization pressure to convince the human to take drugs, and so on.</p>\n<p>I'm not saying any particular failure is 100% certain to occur; rather I'm trying to explain - as handicapped by the need to describe the AI in the native human agent-description language, using empathy to simulate a spirit-in-a-box instead of trying to think in mathematical structures like A* search or Bayesian updating - how, even so, one can still see that the issue is a tad more fraught than it sounds on an immediate examination.</p>\n<p>If you see the world just in terms of math, it's even worse; you've got some program with inputs from a USB cable connecting to a webcam, output to a computer monitor, and optimization criteria expressed over some combination of the monitor, the humans looking at the monitor, and the rest of the world. It's a whole lot easier to call what's inside a 'planning Oracle' or some other English phrase than to write a program that does the optimization safely without serious unintended consequences. Show me any attempted specification, and I'll point to the vague parts and ask for clarification in more formal and mathematical terms, and as soon as the design is clarified enough to be a hundred light years from implementation instead of a thousand light years, I'll show a neutral judge how that math would go wrong. (Experience shows that if you try to explain to would-be AGI designers how their design goes wrong, in most cases they just say \"Oh, but of course that's not what I meant.\" Marcus Hutter is a rare exception who specified his AGI in such unambiguous mathematical terms that he actually succeeded at realizing, after some discussion with SIAI personnel, that AIXI would kill off its users and seize control of its reward button. But based on past sad experience with many other would-be designers, I say \"Explain to a neutral judge how the math kills\" and not \"Explain to the person who invented that math and likes it.\")</p>\n<p>Just as the gigantic gap between smart-sounding English instructions and actually smart algorithms is the main source of difficulty in AI, there's a gap between benevolent-sounding English and actually benevolent algorithms which is the source of difficulty in FAI. &nbsp;\"Just make suggestions - don't&nbsp;<em>do</em>&nbsp;anything!\" is, in the end, just more English.</p>\n<h3>3. &nbsp;Why we haven't already discussed Holden's suggestion</h3>\n<blockquote>\n<p>One of the things that bothers me most about SI is that there is practically no public content, as far as I can tell, explicitly addressing the idea of a \"tool\" and giving arguments for why AGI is likely to work only as an \"agent.\"</p>\n</blockquote>\n<p>The above&nbsp;statement seems to lack perspective on how&nbsp;<em>many</em>&nbsp;different things various people see as&nbsp;<em>the one obvious solution</em>&nbsp;to Friendly AI. Tool AI wasn't the obvious solution to John McCarthy,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Good-Some-future-social-repurcussions-of-computers.pdf\">I.J. Good</a>, or&nbsp;<a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\">Marvin Minsky</a>. Today's leading AI textbook,&nbsp;<em>Artificial Intelligence: A Modern Approach</em>&nbsp;- where you can learn all about A* search, by the way - discusses Friendly AI and AI risk for 3.5 pages but doesn't mention tool AI as an obvious solution. For&nbsp;<a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\">Ray Kurzweil</a>, the obvious solution is merging humans and AIs. For&nbsp;<a href=\"http://www.idsia.ch/~juergen/\">Jurgen Schmidhuber</a>, the obvious solution is AIs that value a certain complicated definition of complexity in their sensory inputs. Ben Goertzel, J. Storrs Hall, and Bill Hibbard, among others, have all written about how silly Singinst is to pursue Friendly AI when the solution is obviously X, for various different X. Among current leading people working on serious AGI programs labeled as such, neither&nbsp;<a href=\"http://en.wikipedia.org/wiki/Demis_Hassabis\">Demis Hassabis</a>&nbsp;(VC-funded to the tune of several million dollars) nor&nbsp;<a href=\"http://metacog.org/doc.html\">Moshe Looks</a>&nbsp;(head of AGI research at Google) nor&nbsp;<a href=\"http://metacog.org/doc.html\">Henry Markram</a>&nbsp;(Blue Brain at IBM) think that the obvious answer is Tool AI. Vernor Vinge, Isaac Asimov, and any number of other SF writers with technical backgrounds who spent serious time thinking about these issues didn't converge on that solution.</p>\n<p>Obviously I'm not saying that nobody should be allowed to propose solutions because someone else would propose a different solution. I have been known to advocate for particular developmental pathways for Friendly AI myself. But I haven't, for example, told Peter Norvig that deterministic self-modification is such an obvious solution to Friendly AI that I would mistrust his whole AI textbook if he didn't spend time discussing it.</p>\n<p>At one point in his conversation with Tallinn, Holden argues that AI will inevitably be developed along planning-Oracle lines, because making suggestions to humans is the natural course that most software takes. Searching for counterexamples instead of positive examples makes it clear that most lines of code don't do this. &nbsp;Your computer, when it reallocates RAM, doesn't pop up a button asking you if it's okay to reallocate RAM in such-and-such a fashion. Your car doesn't pop up a suggestion when it wants to change the fuel mix or apply dynamic stability control. Factory robots don't operate as human-worn bracelets whose blinking lights suggest motion. High-frequency trading programs execute stock orders on a microsecond timescale. Software that does happen to interface with humans is selectively visible and salient to humans, especially the tiny part of the software that does the interfacing; but this is a special case of a general cost/benefit tradeoff which, more often than not, turns out to swing the other way, because human advice is either too costly or doesn't provide enough benefit. Modern AI programmers are generally more interested in e.g. pushing the technological envelope to allow self-driving cars than to \"just\" do Google Maps. Branches of AI that invoke human aid, like hybrid chess-playing algorithms designed to incorporate human advice, are a field of study; but they're the exception rather than the rule, and occur primarily where AIs can't yet do something humans do, e.g. humans acting as oracles for theorem-provers, where the humans suggest a route to a proof and the AI actually follows that route. This is another reason why planning Oracles were not a uniquely obvious solution to the various academic AI researchers, would-be AI-creators, SF writers, etcetera, listed above.&nbsp;Again, regardless of whether a planning Oracle is actually the best solution, Holden seems to be empirically-demonstrably overestimating the degree to which other people will automatically have his preferred solution come up first in their search ordering.</p>\n<h3>4. &nbsp;Why we should have full-time Friendly AI specialists just like we have trained professionals doing anything else mathy that somebody actually cares about getting right, like pricing interest-rate options or something</h3>\n<p>I hope that the preceding discussion has made, by example instead of mere argument, what's probably the most important point: If you want to have a sensible discussion about which AI designs are safer, there are specialized skills you can apply to that discussion, as built up over years of study and practice by someone who specializes in answering that sort of question.</p>\n<p>This isn't meant as an argument from authority. It's not meant as an attempt to say that only experts should be allowed to contribute to the conversation. But it is meant to say that there is (and ought to be) room in the world for Friendly AI specialists, just like there's room in the world for specialists on optimal philanthropy (e.g. Holden).</p>\n<p>The decision to build a non-self-modifying planning Oracle would be properly made by someone who: understood the risk gradient for self-modifying vs. non-self-modifying programs; understood the risk gradient for having the AI thinking about the thought processes of the human watcher and trying to come up with plans implementable by the human watcher in the service of locally absorbed utility functions, vs. trying to implement its own plans in the service of more globally descriptive utility functions; and who, above all, understood on a technical level what exactly gets <em>accomplished </em>by having the plans routed through a human. I've given substantial previous thought to describing more precisely what happens &mdash; what is being gained, and how much is being gained &mdash; when a human \"approves a suggestion\" made by an AI. But that would be another a different topic, plus I haven't made too much progress on saying it precisely anyway.</p>\n<p>In the transcript of Holden's conversation with Jaan Tallinn, it looked like Tallinn didn't deny the assertion that Friendly AI skills would be inapplicable if we're building a Google Maps AGI. I would deny that assertion and emphasize that denial, because to me it seems that it is exactly Friendly AI programmers who would be able to tell you if the risk gradient for non-self-modification vs. self-modification, the risk gradient for routing plans through humans vs. acting as an agent, the risk gradient for requiring human approval vs. unapproved action, and the actual feasibility of directly constructing transhuman modeling-prediction-and-planning algorithms through directly design of sheerly better computations than are presently run by the human brain, had the right combination of properties to imply that you ought to go construct a non-self-modifying planning Oracle. Similarly if you wanted an AI that took a limited set of actions in the world with human approval, or if you wanted an AI that \"just answered questions instead of making plans\".</p>\n<p>It is similarly implied that a \"philosophical AI\" might obsolete Friendly AI programmers. If we're talking about PAI that can start with a human's terrible decision theory and come up with a good decision theory, or PAI that can start from a human talking about bad metaethics and then construct a good metaethics... I don't want to say \"impossible\", because, after all, that's just what human philosophers do. But we are not talking about a trivial invention here. Constructing a \"philosophical AI\" is a Holy Grail precisely because it's FAI-complete (just ask it \"What AI should we build?\"), and has been discussed (e.g. with and by Wei Dai) over the years on the old SL4 mailing list and the modern Less Wrong. But it's really not at all clear how you could write an algorithm which would knowably produce the correct answer to the entire puzzle of anthropic reasoning, without being in possession of that correct answer yourself (in the same way that we can have Deep Blue win chess games without knowing the exact moves, but understanding exactly what abstract work Deep Blue is doing to solve the problem).</p>\n<p>Holden's post presents a restrictive view of what \"Friendly AI\" people are supposed to learn and know &mdash; that it's about machine learning for optimizing orange sales but not apple sales, or about producing an \"agent\" that implements CEV &mdash; which is something of a straw view, much weaker than the view that a Friendly AI programmer takes of Friendly AI programming. What the human species needs from an x-risk perspective is experts on This Whole Damn Problem, who will acquire whatever skills are needed to that end. The Singularity Institute exists to host such people and enable their research&mdash;once we have enough funding to find and recruit them. &nbsp;See also,&nbsp;<a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a>.</p>\n<p>I'm pretty sure Holden has met people who think that having a whole institute to rate the efficiency of charities is pointless overhead, especially people who think that their own charity-solution is too obviously good to have to contend with busybodies pretending to specialize in thinking about 'marginal utility'. &nbsp;Which Holden knows about, I would guess, from being paid quite well to think about that economic details when he was a hedge fundie, and learning from books written by professional researchers before then; and the really key point is that people who haven't studied all that stuff don't even realize what they're missing by trying to wing it. &nbsp;If you don't know, you don't know <em>what </em>you don't know, or the cost of not knowing. &nbsp;Is there a problem of figuring out who might know something you don't, if Holden insists that there's this strange new stuff called 'marginal utility' you ought to learn about? &nbsp;Yes, there is. &nbsp;But is someone who trusts their philanthropic dollars to be steered just by the warm fuzzies of their heart, doing something wrong? &nbsp;Yes, they are. &nbsp;It's one thing to say that SIAI isn't known-to-you to be doing it right - another thing still to say that SIAI is known-to-you to be doing it wrong - and then quite another thing entirely to say that there's no need for Friendly AI programmers <em>and you know it,</em>&nbsp;that anyone can see it without resorting to math or cracking a copy of AI: A Modern Approach. &nbsp;I do wish that Holden would at least credit that the task SIAI is taking on contains at least as many gotchas, relative to the instinctive approach, as optimal philanthropy compared to instinctive philanthropy, and might likewise benefit from some full-time professionally specialized attention, just as our society creates trained professionals to handle any other problem that someone actually cares about getting right.</p>\n<p>On the other side of things, Holden says that <em>even if</em> Friendly AI is proven and checked:</p>\n<blockquote>\"I believe that the probability of an unfavorable outcome - by which I mean an outcome essentially equivalent to what a UFAI would bring about - exceeds 90% in such a scenario.\"</blockquote>\n<p>It's nice that this appreciates that the problem is hard. &nbsp;Associating all of the difficulty with agenty proposals and thinking that it goes away as soon as you invoke tooliness is, well, of this I've already spoken.&nbsp;I'm not sure whether this irreducible-90%-doom assessment is based on a common straw version of FAI where all the work of the FAI programmer goes into \"proving\" something and doing this carefully checked proof which then - alas, poor Spock! - turns out to be no more relevant than proving that the underlying CPU does floating-point arithmetic correctly if the transistors work as stated. I've repeatedly said that the idea behind proving determinism of self-modification isn't that this guarantees safety, but that if you prove the self-modification stable the AI <em>might </em>work, whereas if you try to get by with no proofs at all, doom is <em>guaranteed</em>. My mind keeps turning up Ben Goertzel as the one who invented this caricature - \"Don't you understand, poor fool Eliezer, life is full of uncertainty, your attempt to flee from it by refuge in 'mathematical proof' is doomed\" - but I'm not sure he was actually the inventor. In any case, the burden of safety isn't carried just by the proof, it's carried mostly by proving the right thing. If Holden is assuming that we're just running away from the inherent uncertainty of life by taking refuge in mathematical proof, then, yes, 90% probability of doom is an understatement, the vast majority of plausible-on-first-glance goal criteria you can prove stable will also kill you.</p>\n<p>If Holden's assessment does take into account a great effort to select the right theorem to prove - and attempts to incorporate the difficult but finitely difficult feature of meta-level error-detection, as it appears in e.g. the CEV proposal - and he is still assessing 90% doom probability, then I must ask, \"What do you think you know and how do you think you know it?\" The complexity of the human mind is finite; there's only so many things we want or would-want. Why would someone claim to know that proving the right thing is beyond human ability, even if \"100 of the world's most intelligent and relevantly experienced people\" (Holden's terms) check it over? There's hidden complexity of wishes, but not infinite complexity of wishes or unlearnable complexity of wishes. There are deep and subtle gotchas but not an unending number of them. And if that <em>were</em>&nbsp;the setting of the hidden variables&nbsp;- how would you end up knowing that with 90% probability in advance? I don't mean to wield my own ignorance as a sword or engage in motivated uncertainty - I hate it when people argue that if they don't know something, nobody else is allowed to know either - so please note that I'm also counterarguing from positive facts pointing the other way:&nbsp;the human brain is complicated but not infinitely complicated, there are hundreds or thousands of cytoarchitecturally distinct brain areas but not trillions or googols. &nbsp;If humanity had two hundred years to solve FAI using human-level intelligence<em>&nbsp;and there was no penalty for guessing wrong</em>&nbsp;I would be pretty relaxed about the outcome. &nbsp;If&nbsp;Holden says there's 90% doom probability left over no matter what sane intelligent people do (all of which goes away if you just build Google Maps AGI, but leave that aside for now) I would ask him what he knows now, in advance, that all those sane intelligent people will miss. &nbsp;I don't see how you could (well-justifiedly) access that epistemic state.</p>\n<p>I acknowledge that there are points in Holden's post which are not addressed in this reply, acknowledge that these points are also deserving of reply, and hope that other SIAI personnel will be able to reply to them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZFrgTgzwEfStg26JL": 2, "sYm3HiWcfZvrGu3ui": 1, "LXk7bxNkYSjgatdAt": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sizjfDgCgAsuLJQmm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 122, "baseScore": 150, "extendedScore": null, "score": 0.000313, "legacy": true, "legacyId": "16826", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 150, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I begin by thanking&nbsp;Holden Karnofsky of <a href=\"http://www.givewell.org/\">Givewell</a> for his rare gift of his detailed, engaged, and helpfully-meant critical article <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">Thoughts on the Singularity Institute (SI)</a>. In this reply I will engage with only one of the <em>many</em> subjects raised therein, the topic of, as I would term them, non-self-modifying planning Oracles, a.k.a. 'Google Maps AGI' a.k.a. 'tool AI', this being the topic that requires me personally to answer. &nbsp;I hope that my reply will be accepted as addressing the most important central points, though I did not have time to explore every avenue. &nbsp;I certainly do not wish to be logically rude, and if I have failed, please remember with compassion that it's not always obvious to one person what another person will think was the central point.</p>\n<p>Luke Mueulhauser and Carl Shulman contributed to this article, but the final edit was my own, likewise any flaws.</p>\n<h3 id=\"Summary_\">Summary:</h3>\n<p>Holden's concern is that \"SI appears to neglect the potentially important distinction between 'tool' and 'agent' AI.\" His archetypal example is <a href=\"https://maps.google.com/\">Google Maps</a>:</p>\n<blockquote>\n<p>Google Maps is not an <em>agent</em>, taking actions in order to maximize a utility parameter. It is a <em>tool</em>, generating information and then displaying it in a user-friendly manner for me to consider, use and export or discard as I wish.</p>\n</blockquote>\n<p>The reply breaks down into four heavily interrelated points:</p>\n<p>First, Holden seems to think (and Jaan Tallinn doesn't apparently object to, in their exchange) that if a non-self-modifying planning Oracle is indeed the best strategy, then all of SIAI's past and intended future work is wasted. &nbsp;To me it looks like there's a huge amount of overlap in underlying processes in the AI that would have to be built and the insights required to build it, and I would be trying to assemble mostly - though not quite exactly - the same kind of&nbsp;<em>team </em>if I was trying to build a non-self-modifying planning Oracle, with the same initial mix of talents and skills.</p>\n<p>Second, a non-self-modifying planning Oracle doesn't sound nearly as safe once you stop saying human-English phrases like \"describe the consequences of an action to the user\" and start trying to come up with math that says scary dangerous things like (he translated into English) \"increase the correspondence between the user's belief about relevant consequences and reality\". &nbsp;Hence&nbsp;why&nbsp;the people on the team would have to solve the same sorts of problems.</p>\n<p>Appreciating the force of the third point is a lot easier if one appreciates the difficulties discussed in points 1 and 2, but is actually empirically verifiable independently: &nbsp;Whether or not a non-self-modifying planning Oracle is the <em>best</em> solution in the end, it's not such an <em>obvious</em>&nbsp;privileged-point-in-solution-space that someone should be alarmed at SIAI not discussing it. &nbsp;This is empirically verifiable in the sense that 'tool AI' wasn't the obvious solution to e.g. John McCarthy, Marvin Minsky, I. J. Good, Peter Norvig, Vernor Vinge, or for that matter Isaac Asimov. &nbsp;At one point, Holden says:</p>\n<blockquote>\n<p>One of the things that bothers me most about SI is that there is practically no public content, as far as I can tell, explicitly addressing the idea of a \"tool\" and giving arguments for why AGI is likely to work only as an \"agent.\"</p>\n</blockquote>\n<p>If I take literally that this is one of the things that bothers Holden <em>most...</em>&nbsp;I think I'd start stacking&nbsp;up some of the literature on the number of different things that <em>just respectable academics</em>&nbsp;have suggested as the <em>obvious solution</em>&nbsp;to what-to-do-about-AI - none of which would be about non-self-modifying smarter-than-human planning Oracles - and beg him to have some compassion on us for what we <em>haven't addressed yet</em>. &nbsp;It might be the right suggestion, but it's not so obviously right that our failure to prioritize discussing it reflects negligence.</p>\n<p>The final point at the end is looking over all the preceding discussion and realizing that, yes, you want to have people specializing in Friendly AI who know this stuff, but as all that preceding discussion is actually the following discussion at this point, I shall reserve it for later.<a id=\"more\"></a></p>\n<h3 id=\"1___The_math_of_optimization__and_the_similar_parts_of_a_planning_Oracle_\">1. &nbsp;The math of optimization, and the similar parts of a planning Oracle.</h3>\n<p>What does it take to build a smarter-than-human intelligence, of whatever sort, and have it go well?</p>\n<p>A \"Friendly AI programmer\" is somebody who specializes in seeing the correspondence of mathematical structures to What Happens in the Real World. It's somebody who looks at Hutter's specification of <a href=\"http://www.amazon.com/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3642060528/\">AIXI</a> and reads the actual equations - actually stares at the Greek symbols and not just the accompanying English text -&nbsp;and sees, \"Oh, this AI will try to gain control of its reward channel,\" as well as numerous subtler issues like, \"This AI presumes a Cartesian boundary separating itself from the environment; it may drop an anvil on its own head.\" Similarly, working on <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">TDT</a> means e.g. looking at a mathematical specification of decision theory, and seeing \"Oh, this is vulnerable to blackmail\" and coming up with a mathematical counter-specification of an AI that isn't so vulnerable to blackmail.</p>\n<p>Holden's post seems to imply that if you're building a non-self-modifying planning Oracle (aka 'tool AI') rather than an acting-in-the-world agent, you don't need a Friendly AI programmer because FAI programmers only work on agents. But this isn't how the engineering skills are split up. Inside the AI, whether an agent AI or a planning Oracle, there would be similar AGI-challenges like \"build a predictive model of the world\", and similar FAI-conjugates of those challenges like finding the 'user' inside an AI-created model of the universe. &nbsp;The insides would look a lot more similar than the outsides. &nbsp;An analogy would be supposing that a machine learning professional who does sales optimization for an orange company couldn't possibly do sales optimization for a banana company, because their skills must be about oranges rather than bananas.</p>\n<p>Admittedly, if it turns out to be possible to use a human understanding of cognitive algorithms to build and run a smarter-than-human Oracle without it being self-improving - this seems unlikely, but not impossible - then you wouldn't have to solve problems that arise with self-modification. &nbsp;But this eliminates only one dimension of the work. &nbsp;And on an even more meta level, it seems like you would call upon almost identical <em>talents and skills</em>&nbsp;to come up with whatever insights were required&nbsp;- though if it were predictable in advance that we'd abjure self-modification, then, yes, we'd place less emphasis on e.g. finding a team member with past experience in reflective math, and wouldn't waste (additional) time specializing in reflection. &nbsp;But if you wanted math inside the planning Oracle that <em>operated the way you thought it did</em>, and you wanted somebody who <em>understood what could possibly go wrong</em>&nbsp;and how to avoid it, you would need to make a function call to the same sort of talents and skills&nbsp;to build an agent AI, or an Oracle that <em>was</em>&nbsp;self-modifying, etc.</p>\n<h3 id=\"2___Yes__planning_Oracles_have_hidden_gotchas_too_\">2. &nbsp;Yes, planning Oracles have hidden gotchas too.</h3>\n<p>\"Tool AI\" may sound simple in English, a short sentence in the language of empathically-modeled agents \u2014 it's just \"a thingy that shows you plans instead of a thingy that goes and does things.\" If you want to know whether this hypothetical entity does X, you just check whether the outcome of X sounds like \"showing someone a plan\" or \"going and doing things\", and you've got your answer. &nbsp;It starts sounding much scarier once you try to say something more formal and internally-causal like \"Model the user and the universe, predict the degree of correspondence between the user's model and the universe, and select from among possible explanation-actions on this basis.\"</p>\n<p>Holden, in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Tallinn-Karnofsky-2011.pdf\">his dialogue with Jaan Tallinn</a>, writes out this attempt at formalizing:</p>\n<blockquote>\n<p>Here's how I picture the Google Maps AGI ...</p>\n<p style=\"padding-left: 30px; \">utility_function = construct_utility_function(process_user_input());</p>\n<p style=\"padding-left: 30px; \">foreach $action in $all_possible_actions {</p>\n<p style=\"padding-left: 60px; \">$action_outcome = prediction_function($action,$data);</p>\n<p style=\"padding-left: 60px; \">$utility = utility_function($action_outcome);</p>\n<p style=\"padding-left: 60px; \">if ($utility &gt; $leading_utility) { $leading_utility = $utility;</p>\n<p style=\"padding-left: 60px; \">$leading_action = $action; }</p>\n<p style=\"padding-left: 30px; \">}</p>\n<p style=\"padding-left: 30px; \">report($leading_action);</p>\n<p>construct_utility_function(process_user_input()) is just a human-quality function for understanding what the speaker wants. prediction_function is an implementation of a human-quality data-&gt;prediction function in superior hardware. $data is fixed (it's a dataset larger than any human can process); same with $all_possible_actions. report($leading_action) calls a Google Maps-like interface for understanding the consequences of $leading_action; it basically breaks the action into component parts and displays predictions for different times and conditional on different parameters.</p>\n</blockquote>\n<p>Google Maps doesn't check all possible routes. If I wanted to design Google Maps, I would start out by throwing out a standard planning technique on a connected graph where each edge has a cost function and there's a good heuristic measure of the distance, e.g. <a href=\"http://en.wikipedia.org/wiki/A*_search_algorithm\">A* search</a>. If that was too slow, I'd next try some more efficient version like weighted A* (or bidirectional weighted memory-bounded A*, which I expect I could also get off-the-shelf somewhere).&nbsp;Once you introduce weighted A*, you no longer have a guarantee that you're selecting the optimal path. &nbsp;You have a guarantee to within a known factor of the cost of the optimal path \u2014 but the actual path selected wouldn't be quite optimal. The suggestion produced would be an approximation whose exact steps depended on the exact algorithm you used. That's true even if you can predict the exact cost \u2014 exact utility \u2014 of any particular path you actually look at; and even if you have a heuristic that never overestimates the cost.</p>\n<p>The reason we don't have <a href=\"http://en.wikipedia.org/wiki/Optimal_solutions_for_Rubik's_Cube\">God's Algorithm for solving the Rubik's Cube </a> is that there's no perfect way of measuring the distance between any two Rubik's Cube positions \u2014 you can't look at two Rubik's cube positions, and figure out the minimum number of moves required to get from one to another. It took 15 years to prove that there was a position requiring at least 20 moves to solve, and then another 15 years to come up with a computer algorithm that could solve any position in at most 20 moves, but we still can't compute the actual, minimum solution to all Cubes (\"God's Algorithm\"). This, even though we can exactly calculate the cost and consequence of any actual Rubik's-solution-path we consider.</p>\n<p>When it comes to AGI \u2014 solving general cross-domain \"Figure out how to do X\" problems \u2014 you're not going to get anywhere near the one, true, optimal answer. You're going to \u2014 at best, if everything works right \u2014 get <em>a</em> good answer that's a cross-product of the \"utility function\" and all the other algorithmic properties that determine what sort of answer the AI finds easy to invent (i.e. can be invented using bounded computing time).</p>\n<p>As for the notion that this AGI runs on a \"human predictive algorithm\" that we got off of neuroscience and then implemented using more computing power, without knowing how it works or being able to enhance it further: It took 30 years of multiple computer scientists doing basic math research, and inventing code, and running that code on a computer cluster, for them to come up with a 20-move solution to the Rubik's Cube. If a planning Oracle is going to produce better solutions than humanity has yet managed to the Rubik's Cube, it needs to be capable of doing original computer science research and writing its own code. You can't get a 20-move solution out of a human brain, using the native human planning algorithm. Humanity can do it, but only by exploiting the ability of humans to explicitly comprehend the deep structure of the domain (not just rely on intuition) and then inventing an artifact, a new design, running code which uses a different and superior cognitive algorithm, to solve that Rubik's Cube in 20 moves. We do all that without being <em>self</em>-modifying, but it's still a capability to respect.</p>\n<p>And I'm not even going into what it would take for a planning Oracle to out-strategize any human, come up with a plan for persuading someone, solve original scientific problems by looking over experimental data (like Einstein did), design a nanomachine, and so on.</p>\n<p>Talking like there's this one simple \"predictive algorithm\" that we can read out of the brain using neuroscience and overpower to produce better plans... doesn't seem quite congruous with what humanity actually does to produce its predictions and plans.</p>\n<p>If we take the concept of the Google Maps AGI at face value, then it actually has four key magical components. &nbsp;(In this case, \"magical\" isn't to be taken as prejudicial, it's a term of art that means we haven't said how the component works yet.) &nbsp;There's a magical comprehension of the user's utility function, a magical world-model that GMAGI uses to comprehend the consequences of actions, a magical planning element that selects a <em>non-optimal</em> path using some method <em>other</em> than exploring all possible actions, and a magical explain-to-the-user function.</p>\n<p>report($leading_action) isn't exactly a trivial step either. Deep Blue tells you to move your pawn or you'll lose the game. You ask \"Why?\" and the answer is a gigantic search tree of billions of possible move-sequences, leafing at positions which are heuristically rated using a static-position evaluation algorithm trained on millions of games. Or the planning Oracle tells you that a certain DNA sequence will produce a protein that cures cancer, you ask \"Why?\", and then humans aren't even capable of verifying, for themselves, the assertion that the peptide sequence will fold into the protein the planning Oracle says it does.</p>\n<p>\"So,\" you say, after the first dozen times you ask the Oracle a question and it returns an answer that you'd have to take on faith, \"we'll just specify in the utility function that the plan should be understandable.\"</p>\n<p>Whereupon other things start going wrong. Viliam_Bur, in the comments thread, gave this example, which I've slightly simplified:</p>\n<blockquote>\n<p>Example question: \"How should I get rid of my disease most cheaply?\" Example answer: \"You won't. You will die soon, unavoidably. This report is 99.999% reliable\". Predicted human reaction: Decides to kill self and get it over with. Success rate: 100%, the disease is gone. Costs of cure: zero. Mission completed.</p>\n</blockquote>\n<p>Bur is trying to give an example of how things might go wrong if the preference function is over the accuracy of the predictions explained to the human\u2014 rather than <em>just</em> the human's 'goodness' of the outcome. And if the preference function <em>was</em> just over the human's 'goodness' of the end result, rather than the accuracy of the human's understanding of the predictions, the AI might tell you something that was predictively false but whose implementation would lead you to what the AI defines as a 'good' outcome. And if we ask how happy the human is, the resulting decision procedure would exert optimization pressure to convince the human to take drugs, and so on.</p>\n<p>I'm not saying any particular failure is 100% certain to occur; rather I'm trying to explain - as handicapped by the need to describe the AI in the native human agent-description language, using empathy to simulate a spirit-in-a-box instead of trying to think in mathematical structures like A* search or Bayesian updating - how, even so, one can still see that the issue is a tad more fraught than it sounds on an immediate examination.</p>\n<p>If you see the world just in terms of math, it's even worse; you've got some program with inputs from a USB cable connecting to a webcam, output to a computer monitor, and optimization criteria expressed over some combination of the monitor, the humans looking at the monitor, and the rest of the world. It's a whole lot easier to call what's inside a 'planning Oracle' or some other English phrase than to write a program that does the optimization safely without serious unintended consequences. Show me any attempted specification, and I'll point to the vague parts and ask for clarification in more formal and mathematical terms, and as soon as the design is clarified enough to be a hundred light years from implementation instead of a thousand light years, I'll show a neutral judge how that math would go wrong. (Experience shows that if you try to explain to would-be AGI designers how their design goes wrong, in most cases they just say \"Oh, but of course that's not what I meant.\" Marcus Hutter is a rare exception who specified his AGI in such unambiguous mathematical terms that he actually succeeded at realizing, after some discussion with SIAI personnel, that AIXI would kill off its users and seize control of its reward button. But based on past sad experience with many other would-be designers, I say \"Explain to a neutral judge how the math kills\" and not \"Explain to the person who invented that math and likes it.\")</p>\n<p>Just as the gigantic gap between smart-sounding English instructions and actually smart algorithms is the main source of difficulty in AI, there's a gap between benevolent-sounding English and actually benevolent algorithms which is the source of difficulty in FAI. &nbsp;\"Just make suggestions - don't&nbsp;<em>do</em>&nbsp;anything!\" is, in the end, just more English.</p>\n<h3 id=\"3___Why_we_haven_t_already_discussed_Holden_s_suggestion\">3. &nbsp;Why we haven't already discussed Holden's suggestion</h3>\n<blockquote>\n<p>One of the things that bothers me most about SI is that there is practically no public content, as far as I can tell, explicitly addressing the idea of a \"tool\" and giving arguments for why AGI is likely to work only as an \"agent.\"</p>\n</blockquote>\n<p>The above&nbsp;statement seems to lack perspective on how&nbsp;<em>many</em>&nbsp;different things various people see as&nbsp;<em>the one obvious solution</em>&nbsp;to Friendly AI. Tool AI wasn't the obvious solution to John McCarthy,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Good-Some-future-social-repurcussions-of-computers.pdf\">I.J. Good</a>, or&nbsp;<a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\">Marvin Minsky</a>. Today's leading AI textbook,&nbsp;<em>Artificial Intelligence: A Modern Approach</em>&nbsp;- where you can learn all about A* search, by the way - discusses Friendly AI and AI risk for 3.5 pages but doesn't mention tool AI as an obvious solution. For&nbsp;<a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\">Ray Kurzweil</a>, the obvious solution is merging humans and AIs. For&nbsp;<a href=\"http://www.idsia.ch/~juergen/\">Jurgen Schmidhuber</a>, the obvious solution is AIs that value a certain complicated definition of complexity in their sensory inputs. Ben Goertzel, J. Storrs Hall, and Bill Hibbard, among others, have all written about how silly Singinst is to pursue Friendly AI when the solution is obviously X, for various different X. Among current leading people working on serious AGI programs labeled as such, neither&nbsp;<a href=\"http://en.wikipedia.org/wiki/Demis_Hassabis\">Demis Hassabis</a>&nbsp;(VC-funded to the tune of several million dollars) nor&nbsp;<a href=\"http://metacog.org/doc.html\">Moshe Looks</a>&nbsp;(head of AGI research at Google) nor&nbsp;<a href=\"http://metacog.org/doc.html\">Henry Markram</a>&nbsp;(Blue Brain at IBM) think that the obvious answer is Tool AI. Vernor Vinge, Isaac Asimov, and any number of other SF writers with technical backgrounds who spent serious time thinking about these issues didn't converge on that solution.</p>\n<p>Obviously I'm not saying that nobody should be allowed to propose solutions because someone else would propose a different solution. I have been known to advocate for particular developmental pathways for Friendly AI myself. But I haven't, for example, told Peter Norvig that deterministic self-modification is such an obvious solution to Friendly AI that I would mistrust his whole AI textbook if he didn't spend time discussing it.</p>\n<p>At one point in his conversation with Tallinn, Holden argues that AI will inevitably be developed along planning-Oracle lines, because making suggestions to humans is the natural course that most software takes. Searching for counterexamples instead of positive examples makes it clear that most lines of code don't do this. &nbsp;Your computer, when it reallocates RAM, doesn't pop up a button asking you if it's okay to reallocate RAM in such-and-such a fashion. Your car doesn't pop up a suggestion when it wants to change the fuel mix or apply dynamic stability control. Factory robots don't operate as human-worn bracelets whose blinking lights suggest motion. High-frequency trading programs execute stock orders on a microsecond timescale. Software that does happen to interface with humans is selectively visible and salient to humans, especially the tiny part of the software that does the interfacing; but this is a special case of a general cost/benefit tradeoff which, more often than not, turns out to swing the other way, because human advice is either too costly or doesn't provide enough benefit. Modern AI programmers are generally more interested in e.g. pushing the technological envelope to allow self-driving cars than to \"just\" do Google Maps. Branches of AI that invoke human aid, like hybrid chess-playing algorithms designed to incorporate human advice, are a field of study; but they're the exception rather than the rule, and occur primarily where AIs can't yet do something humans do, e.g. humans acting as oracles for theorem-provers, where the humans suggest a route to a proof and the AI actually follows that route. This is another reason why planning Oracles were not a uniquely obvious solution to the various academic AI researchers, would-be AI-creators, SF writers, etcetera, listed above.&nbsp;Again, regardless of whether a planning Oracle is actually the best solution, Holden seems to be empirically-demonstrably overestimating the degree to which other people will automatically have his preferred solution come up first in their search ordering.</p>\n<h3 id=\"4___Why_we_should_have_full_time_Friendly_AI_specialists_just_like_we_have_trained_professionals_doing_anything_else_mathy_that_somebody_actually_cares_about_getting_right__like_pricing_interest_rate_options_or_something\">4. &nbsp;Why we should have full-time Friendly AI specialists just like we have trained professionals doing anything else mathy that somebody actually cares about getting right, like pricing interest-rate options or something</h3>\n<p>I hope that the preceding discussion has made, by example instead of mere argument, what's probably the most important point: If you want to have a sensible discussion about which AI designs are safer, there are specialized skills you can apply to that discussion, as built up over years of study and practice by someone who specializes in answering that sort of question.</p>\n<p>This isn't meant as an argument from authority. It's not meant as an attempt to say that only experts should be allowed to contribute to the conversation. But it is meant to say that there is (and ought to be) room in the world for Friendly AI specialists, just like there's room in the world for specialists on optimal philanthropy (e.g. Holden).</p>\n<p>The decision to build a non-self-modifying planning Oracle would be properly made by someone who: understood the risk gradient for self-modifying vs. non-self-modifying programs; understood the risk gradient for having the AI thinking about the thought processes of the human watcher and trying to come up with plans implementable by the human watcher in the service of locally absorbed utility functions, vs. trying to implement its own plans in the service of more globally descriptive utility functions; and who, above all, understood on a technical level what exactly gets <em>accomplished </em>by having the plans routed through a human. I've given substantial previous thought to describing more precisely what happens \u2014 what is being gained, and how much is being gained \u2014 when a human \"approves a suggestion\" made by an AI. But that would be another a different topic, plus I haven't made too much progress on saying it precisely anyway.</p>\n<p>In the transcript of Holden's conversation with Jaan Tallinn, it looked like Tallinn didn't deny the assertion that Friendly AI skills would be inapplicable if we're building a Google Maps AGI. I would deny that assertion and emphasize that denial, because to me it seems that it is exactly Friendly AI programmers who would be able to tell you if the risk gradient for non-self-modification vs. self-modification, the risk gradient for routing plans through humans vs. acting as an agent, the risk gradient for requiring human approval vs. unapproved action, and the actual feasibility of directly constructing transhuman modeling-prediction-and-planning algorithms through directly design of sheerly better computations than are presently run by the human brain, had the right combination of properties to imply that you ought to go construct a non-self-modifying planning Oracle. Similarly if you wanted an AI that took a limited set of actions in the world with human approval, or if you wanted an AI that \"just answered questions instead of making plans\".</p>\n<p>It is similarly implied that a \"philosophical AI\" might obsolete Friendly AI programmers. If we're talking about PAI that can start with a human's terrible decision theory and come up with a good decision theory, or PAI that can start from a human talking about bad metaethics and then construct a good metaethics... I don't want to say \"impossible\", because, after all, that's just what human philosophers do. But we are not talking about a trivial invention here. Constructing a \"philosophical AI\" is a Holy Grail precisely because it's FAI-complete (just ask it \"What AI should we build?\"), and has been discussed (e.g. with and by Wei Dai) over the years on the old SL4 mailing list and the modern Less Wrong. But it's really not at all clear how you could write an algorithm which would knowably produce the correct answer to the entire puzzle of anthropic reasoning, without being in possession of that correct answer yourself (in the same way that we can have Deep Blue win chess games without knowing the exact moves, but understanding exactly what abstract work Deep Blue is doing to solve the problem).</p>\n<p>Holden's post presents a restrictive view of what \"Friendly AI\" people are supposed to learn and know \u2014 that it's about machine learning for optimizing orange sales but not apple sales, or about producing an \"agent\" that implements CEV \u2014 which is something of a straw view, much weaker than the view that a Friendly AI programmer takes of Friendly AI programming. What the human species needs from an x-risk perspective is experts on This Whole Damn Problem, who will acquire whatever skills are needed to that end. The Singularity Institute exists to host such people and enable their research\u2014once we have enough funding to find and recruit them. &nbsp;See also,&nbsp;<a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a>.</p>\n<p>I'm pretty sure Holden has met people who think that having a whole institute to rate the efficiency of charities is pointless overhead, especially people who think that their own charity-solution is too obviously good to have to contend with busybodies pretending to specialize in thinking about 'marginal utility'. &nbsp;Which Holden knows about, I would guess, from being paid quite well to think about that economic details when he was a hedge fundie, and learning from books written by professional researchers before then; and the really key point is that people who haven't studied all that stuff don't even realize what they're missing by trying to wing it. &nbsp;If you don't know, you don't know <em>what </em>you don't know, or the cost of not knowing. &nbsp;Is there a problem of figuring out who might know something you don't, if Holden insists that there's this strange new stuff called 'marginal utility' you ought to learn about? &nbsp;Yes, there is. &nbsp;But is someone who trusts their philanthropic dollars to be steered just by the warm fuzzies of their heart, doing something wrong? &nbsp;Yes, they are. &nbsp;It's one thing to say that SIAI isn't known-to-you to be doing it right - another thing still to say that SIAI is known-to-you to be doing it wrong - and then quite another thing entirely to say that there's no need for Friendly AI programmers <em>and you know it,</em>&nbsp;that anyone can see it without resorting to math or cracking a copy of AI: A Modern Approach. &nbsp;I do wish that Holden would at least credit that the task SIAI is taking on contains at least as many gotchas, relative to the instinctive approach, as optimal philanthropy compared to instinctive philanthropy, and might likewise benefit from some full-time professionally specialized attention, just as our society creates trained professionals to handle any other problem that someone actually cares about getting right.</p>\n<p>On the other side of things, Holden says that <em>even if</em> Friendly AI is proven and checked:</p>\n<blockquote>\"I believe that the probability of an unfavorable outcome - by which I mean an outcome essentially equivalent to what a UFAI would bring about - exceeds 90% in such a scenario.\"</blockquote>\n<p>It's nice that this appreciates that the problem is hard. &nbsp;Associating all of the difficulty with agenty proposals and thinking that it goes away as soon as you invoke tooliness is, well, of this I've already spoken.&nbsp;I'm not sure whether this irreducible-90%-doom assessment is based on a common straw version of FAI where all the work of the FAI programmer goes into \"proving\" something and doing this carefully checked proof which then - alas, poor Spock! - turns out to be no more relevant than proving that the underlying CPU does floating-point arithmetic correctly if the transistors work as stated. I've repeatedly said that the idea behind proving determinism of self-modification isn't that this guarantees safety, but that if you prove the self-modification stable the AI <em>might </em>work, whereas if you try to get by with no proofs at all, doom is <em>guaranteed</em>. My mind keeps turning up Ben Goertzel as the one who invented this caricature - \"Don't you understand, poor fool Eliezer, life is full of uncertainty, your attempt to flee from it by refuge in 'mathematical proof' is doomed\" - but I'm not sure he was actually the inventor. In any case, the burden of safety isn't carried just by the proof, it's carried mostly by proving the right thing. If Holden is assuming that we're just running away from the inherent uncertainty of life by taking refuge in mathematical proof, then, yes, 90% probability of doom is an understatement, the vast majority of plausible-on-first-glance goal criteria you can prove stable will also kill you.</p>\n<p>If Holden's assessment does take into account a great effort to select the right theorem to prove - and attempts to incorporate the difficult but finitely difficult feature of meta-level error-detection, as it appears in e.g. the CEV proposal - and he is still assessing 90% doom probability, then I must ask, \"What do you think you know and how do you think you know it?\" The complexity of the human mind is finite; there's only so many things we want or would-want. Why would someone claim to know that proving the right thing is beyond human ability, even if \"100 of the world's most intelligent and relevantly experienced people\" (Holden's terms) check it over? There's hidden complexity of wishes, but not infinite complexity of wishes or unlearnable complexity of wishes. There are deep and subtle gotchas but not an unending number of them. And if that <em>were</em>&nbsp;the setting of the hidden variables&nbsp;- how would you end up knowing that with 90% probability in advance? I don't mean to wield my own ignorance as a sword or engage in motivated uncertainty - I hate it when people argue that if they don't know something, nobody else is allowed to know either - so please note that I'm also counterarguing from positive facts pointing the other way:&nbsp;the human brain is complicated but not infinitely complicated, there are hundreds or thousands of cytoarchitecturally distinct brain areas but not trillions or googols. &nbsp;If humanity had two hundred years to solve FAI using human-level intelligence<em>&nbsp;and there was no penalty for guessing wrong</em>&nbsp;I would be pretty relaxed about the outcome. &nbsp;If&nbsp;Holden says there's 90% doom probability left over no matter what sane intelligent people do (all of which goes away if you just build Google Maps AGI, but leave that aside for now) I would ask him what he knows now, in advance, that all those sane intelligent people will miss. &nbsp;I don't see how you could (well-justifiedly) access that epistemic state.</p>\n<p>I acknowledge that there are points in Holden's post which are not addressed in this reply, acknowledge that these points are also deserving of reply, and hope that other SIAI personnel will be able to reply to them.</p>", "sections": [{"title": "Summary:", "anchor": "Summary_", "level": 1}, {"title": "1. \u00a0The math of optimization, and the similar parts of a planning Oracle.", "anchor": "1___The_math_of_optimization__and_the_similar_parts_of_a_planning_Oracle_", "level": 1}, {"title": "2. \u00a0Yes, planning Oracles have hidden gotchas too.", "anchor": "2___Yes__planning_Oracles_have_hidden_gotchas_too_", "level": 1}, {"title": "3. \u00a0Why we haven't already discussed Holden's suggestion", "anchor": "3___Why_we_haven_t_already_discussed_Holden_s_suggestion", "level": 1}, {"title": "4. \u00a0Why we should have full-time Friendly AI specialists just like we have trained professionals doing anything else mathy that somebody actually cares about getting right, like pricing interest-rate options or something", "anchor": "4___Why_we_should_have_full_time_Friendly_AI_specialists_just_like_we_have_trained_professionals_doing_anything_else_mathy_that_somebody_actually_cares_about_getting_right__like_pricing_interest_rate_options_or_something", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "356 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 357, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm", "8LrrHdt8HSvZFKmRb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 16, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T19:58:47.734Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin Meetup", "slug": "meetup-berlin-meetup-7", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j6s8NNqAosD9DZ2Nv/meetup-berlin-meetup-7", "pageUrlRelative": "/posts/j6s8NNqAosD9DZ2Nv/meetup-berlin-meetup-7", "linkUrl": "https://www.lesswrong.com/posts/j6s8NNqAosD9DZ2Nv/meetup-berlin-meetup-7", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj6s8NNqAosD9DZ2Nv%2Fmeetup-berlin-meetup-7%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj6s8NNqAosD9DZ2Nv%2Fmeetup-berlin-meetup-7", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj6s8NNqAosD9DZ2Nv%2Fmeetup-berlin-meetup-7", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/b2'>Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 June 2012 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Susuru, Rosa-Luxemburg-Str. 17, 10178 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time we meet in the <a href=\"http://www.susuru.de\" rel=\"nofollow\">Japanese noodle bar Susuru</a>. I'll bring a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/b2'>Berlin Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j6s8NNqAosD9DZ2Nv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.213761728837113e-07, "legacy": true, "legacyId": "16897", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup\">Discussion article for the meetup : <a href=\"/meetups/b2\">Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 June 2012 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Susuru, Rosa-Luxemburg-Str. 17, 10178 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time we meet in the <a href=\"http://www.susuru.de\" rel=\"nofollow\">Japanese noodle bar Susuru</a>. I'll bring a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/b2\">Berlin Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T21:32:42.605Z", "modifiedAt": null, "url": null, "title": "How to Run a Successful Less Wrong Meetup", "slug": "how-to-run-a-successful-less-wrong-meetup", "viewCount": null, "lastCommentedAt": "2018-06-21T19:57:09.373Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qMuAazqwJvkvo8teR/how-to-run-a-successful-less-wrong-meetup", "pageUrlRelative": "/posts/qMuAazqwJvkvo8teR/how-to-run-a-successful-less-wrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/qMuAazqwJvkvo8teR/how-to-run-a-successful-less-wrong-meetup", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Run%20a%20Successful%20Less%20Wrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Run%20a%20Successful%20Less%20Wrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqMuAazqwJvkvo8teR%2Fhow-to-run-a-successful-less-wrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Run%20a%20Successful%20Less%20Wrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqMuAazqwJvkvo8teR%2Fhow-to-run-a-successful-less-wrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqMuAazqwJvkvo8teR%2Fhow-to-run-a-successful-less-wrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 464, "htmlBody": "<p><a href=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf\"><img style=\"float: right; padding: 10px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/cover-of-meetup-booklet.png\" alt=\"\" /></a>Always wanted to run a Less Wrong meetup, but been unsure of how? The <em><a href=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf\">How to Run a Successful Less Wrong Meetup</a></em> booklet is here to help you!</p>\n<p>The 33-page document draws from consultations with more than a dozen Less Wrong meetup group organizers. Stanislaw Boboryk created the document design. Luke provided direction, feedback, and initial research, and I did almost all the writing.</p>\n<p>The booklet starts by providing some motivational suggestions on <em>why</em> you'd want to create a meetup in the first place, and then moves on to the subject of organizing your first one. Basics such as choosing a venue, making an announcement, and finding something to talk about once at the meetup, are all covered. This section also discusses pioneering meetups in foreign cities and restarting inactive meetup groups.</p>\n<p>For those who have already established a meetup group, the booklet offers suggestions on things such as attracting new members, maintaining a pleasant atmosphere, and dealing with conflicts within the group. The \"How to Build Your Team of Heroes\" section explains the roles that are useful for a meetup group to fill, ranging from visionaries to organizers.</p>\n<p>If you're unsure of what exactly to <em>do </em>at meetups, the guide describes many options, from different types of discussions to nearly 20 different games and exercises. All the talk and philosophizing in the world won't do much good if you don't actually <em>do </em>things, so the booklet also discusses long-term projects that you can undertake. Some people attend meetups to just have fun and to be social, and others to improve themselves and the world. The booklet has been written to be useful for both kinds of people.</p>\n<p>In order to inspire you and let you see what others have done, the booklet also has brief case studies and examples from real meetup groups around the world. You can find these sprinkled throughout the guide.</p>\n<p><strong>This is just the first version of the guide. </strong>We <em>will</em> continue working on it. If you find mistakes, or think that something is unclear, or would like to see some part expanded, or if you've got good advice you think should be included... please let me know! You can contact me at kaj.sotala@intelligence.org.</p>\n<p>A large number of people have helped in various ways, and I hope that I've remembered to mention most of them in the acknowledgements. If you've contributed to the document but don't see your name mentioned, please send me a message and I'll have that fixed!</p>\n<p>The booklet has been illustrated with pictures from various meetup groups. Meetup organizers sent me the pictures for this use, and I explicitly asked them to make sure that everyone in the photos was fine with it. Regardless, if there's a picture that you find objectionable, please contact me and I'll have it replaced with something else.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"T57Qd9J3AfxmwhQtY": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qMuAazqwJvkvo8teR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 67, "baseScore": 81, "extendedScore": null, "score": 0.000181, "legacy": true, "legacyId": "16552", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 81, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-06-12T21:32:42.605Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-12T21:36:43.668Z", "modifiedAt": null, "url": null, "title": "The Creating Bob the Jerk problem. Is it a real problem in decision theory?", "slug": "the-creating-bob-the-jerk-problem-is-it-a-real-problem-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:07.530Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ghatanathoah", "createdAt": "2012-01-27T20:44:00.945Z", "isAdmin": false, "displayName": "Ghatanathoah"}, "userId": "CzhiZYDsQs87MM5yH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/noWqRhugq4bXW5QwE/the-creating-bob-the-jerk-problem-is-it-a-real-problem-in", "pageUrlRelative": "/posts/noWqRhugq4bXW5QwE/the-creating-bob-the-jerk-problem-is-it-a-real-problem-in", "linkUrl": "https://www.lesswrong.com/posts/noWqRhugq4bXW5QwE/the-creating-bob-the-jerk-problem-is-it-a-real-problem-in", "postedAtFormatted": "Tuesday, June 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Creating%20Bob%20the%20Jerk%20problem.%20Is%20it%20a%20real%20problem%20in%20decision%20theory%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Creating%20Bob%20the%20Jerk%20problem.%20Is%20it%20a%20real%20problem%20in%20decision%20theory%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoWqRhugq4bXW5QwE%2Fthe-creating-bob-the-jerk-problem-is-it-a-real-problem-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Creating%20Bob%20the%20Jerk%20problem.%20Is%20it%20a%20real%20problem%20in%20decision%20theory%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoWqRhugq4bXW5QwE%2Fthe-creating-bob-the-jerk-problem-is-it-a-real-problem-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoWqRhugq4bXW5QwE%2Fthe-creating-bob-the-jerk-problem-is-it-a-real-problem-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 960, "htmlBody": "<p>I was recently reading about the <a href=\"/lw/43t/youre_in_newcombs_box/\">Transparent Newcomb with your Existence at Stake problem</a>, which, to make a long story short, states that you were created by Prometheus, who foresaw that you would one-box on <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's problem</a> and wouldn't have created you if he had foreseen otherwise.&nbsp; The implication is that you might need to one-box just to exist.&nbsp; It's a disturbing problem, and as I read it another even more disturbing problem started to form in my head.&nbsp; However, I'm not sure it's logically coherent (I'm really hoping it's not) and wanted to know what the rest of you thought.&nbsp; The problem goes:</p>\n<p>&nbsp;</p>\n<p>One day you start thinking about a hypothetical nonexistant person named Bob who is a real jerk.&nbsp; If he existed he would make your life utterly miserable.&nbsp; However, if he existed he would want to make a deal with you. If he ever found himself existing in a universe where you have never existed he would create you, on the condition that if you found yourself existing in a universe where he had never existed you would create him.&nbsp; Hypothetical Bob is very good at predicting the behavior of other people, not quite Omega quality, but pretty darn good.&nbsp; Assume for the sake of the argument that you like your life and enjoy existing.</p>\n<p>&nbsp;</p>\n<p>At first you dismiss the problem because of technical difficulties.&nbsp; Science hasn't advanced to the point where we can make people with such precision.&nbsp; Plus, there is a near-infinite number of far nicer hypothetical people who would make the same deal, when science reaches that point you should give creating them priority.</p>\n<p>&nbsp;</p>\n<p>But then you see <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Omega</a> drive by in its pickup truck.&nbsp; A large complicated machine falls off the back of the truck as it passes you by.&nbsp; Written on it, in Omega's handwriting, is a note that says \"This is the machine that will create Bob the Jerk, a hypothetical person that&nbsp; [insert your name here] has been thinking about recently, if one presses the big red button on the side.\"&nbsp; You know Omega never lies, not even in notes to itself.</p>\n<p>&nbsp;</p>\n<p>Do <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">Timeless Decision Theory</a> and <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless Decision Theory</a> say you have a counterfactual obligation to create Bob the Jerk, the same way you have an obligation to pay Omega in the <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">Counterfactual Mugging</a>, and the same way you might (I'm still not sure about this) have an obligation to one-box <a href=\"/lw/43t/youre_in_newcombs_box/\">when dealing with Prometheus</a>? Does this in turn mean that when we develop the ability to create people from scratch we should tile the universe with people who would make the counterfactual deal?&nbsp; Obviously it's that last implication that disturbs me.</p>\n<p>&nbsp;</p>\n<p>I can think of multiple reasons why it might not be rational to create Bob the Jerk:</p>\n<ul>\n<li>It might not be logically coherent to not update to acknowledge the fact of your own existence, even in UDT (this also implies one should two-box when dealing with Prometheus).</li>\n<li>An essential part of who you are is the fact that you were created by your parents, not by Bob the Jerk, so the counterfactual deal isn't logically coherent.&nbsp; Someone he creates wouldn't be you, it would be someone else.&nbsp; At his very best he could create someone with a very similar personality who has falsified memories, which would be rather horrifying.</li>\n<li>An essential part of who Bob the Jerk is is that he was created by you, with some help from Omega.&nbsp; He can't exist in a universe where you don't, so the hypothetical bargain he offered you isn't logically coherent.</li>\n<li>Prometheus will exist no matter what you do in his problem, Bob the Jerk won't.&nbsp; This makes these two problems qualitatively different in some way I don't quite understand.</li>\n<li>You have a moral duty to not inflict Bob the Jerk on others, even if it means you don't exist in some other possibility.</li>\n<li>You have a moral duty to not overpopulate the world, even if it means you might not exist in some other possibility, and the end result of the logic of this problem implies overpopulating the world.</li>\n<li>Bob the Jerk already exists because <a href=\"/lw/ws/for_the_people_who_are_still_alive/\">we live in a Big World</a>, so you have no need to fulfill your part of the bargain because he's already out there somewhere.</li>\n<li>Making these sorts of counterfactual deals is individually rational, but collectively harmful in the same way that paying a ransom is.&nbsp; If you create Bob the Jerk some civic-minded vigilante decision theorist might see the implications and find some way to punish you.</li>\n<li>While it is possible to want to keep on existing if you already exist, it isn't logically possible to \"want to exist\" if you don't already, this defeats the problem in some way.</li>\n<li>After some thought you spend some time thinking about a hypothetical individual called Bizarro-Bob.&nbsp; Bizarro-Bob doesn't want Bob the Jerk to be created and is just as good at modeling your behavior as Bob the Jerk is.&nbsp; He has vowed that if he ends up existing in a universe where you'll end up creating Bob the Jerk he'll kill you.&nbsp; As you stand by Omega's machine you start looking around anxiously for the glint of light off a gun barrel.</li>\n<li>I don't understand UDT or TDT properly, they don't imply I should create Bob the Jerk for some other reason I haven't thought of because of my lack of understanding.</li>\n</ul>\n<p>Are any of these objections valid, or am I just grasping at straws?&nbsp; I find the problem extremely disturbing because of its wider implications, so I'd appreciate it if someone with a better grasp of UDT and TDT analyzed it.&nbsp; I'd very much like to be refuted.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "noWqRhugq4bXW5QwE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "16898", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4MYYr8YmN2fonASCi", "6ddcsdA2c2XpNpE5x", "cfZ8zveqrTZbQrjeD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-13T01:36:01.863Z", "modifiedAt": null, "url": null, "title": "Meetup : Big Berkeley meetup", "slug": "meetup-big-berkeley-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GQLZavHddZ2J26f86/meetup-big-berkeley-meetup-2", "pageUrlRelative": "/posts/GQLZavHddZ2J26f86/meetup-big-berkeley-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/GQLZavHddZ2J26f86/meetup-big-berkeley-meetup-2", "postedAtFormatted": "Wednesday, June 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Big%20Berkeley%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Big%20Berkeley%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGQLZavHddZ2J26f86%2Fmeetup-big-berkeley-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Big%20Berkeley%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGQLZavHddZ2J26f86%2Fmeetup-big-berkeley-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGQLZavHddZ2J26f86%2Fmeetup-big-berkeley-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/b3'>Big Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2128 Oxford Street, Berkeley ,CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Tomorrow there will be a Berkeley meetup that counts as a big meetup. We'll meet at 7pm in the Starbucks at 2128 Oxford St., and then decide where to go from there. I will not be able to come myself, but I know for sure that at least one person will be there. (If you also go, there will be at least two people.)</p>\n\n<p>We do not have a discussion topic planned, but please feel free to suggest one!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/b3'>Big Berkeley meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GQLZavHddZ2J26f86", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.215294528511115e-07, "legacy": true, "legacyId": "16899", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_meetup\">Discussion article for the meetup : <a href=\"/meetups/b3\">Big Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2128 Oxford Street, Berkeley ,CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Tomorrow there will be a Berkeley meetup that counts as a big meetup. We'll meet at 7pm in the Starbucks at 2128 Oxford St., and then decide where to go from there. I will not be able to come myself, but I know for sure that at least one person will be there. (If you also go, there will be at least two people.)</p>\n\n<p>We do not have a discussion topic planned, but please feel free to suggest one!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_meetup1\">Discussion article for the meetup : <a href=\"/meetups/b3\">Big Berkeley meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Big Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_meetup", "level": 1}, {"title": "Discussion article for the meetup : Big Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-13T04:20:20.047Z", "modifiedAt": null, "url": null, "title": "Neuroscience basics for LessWrongians", "slug": "neuroscience-basics-for-lesswrongians-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:05.720Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4vYobGEz8E7qAZgi9/neuroscience-basics-for-lesswrongians-0", "pageUrlRelative": "/posts/4vYobGEz8E7qAZgi9/neuroscience-basics-for-lesswrongians-0", "linkUrl": "https://www.lesswrong.com/posts/4vYobGEz8E7qAZgi9/neuroscience-basics-for-lesswrongians-0", "postedAtFormatted": "Wednesday, June 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Neuroscience%20basics%20for%20LessWrongians&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeuroscience%20basics%20for%20LessWrongians%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vYobGEz8E7qAZgi9%2Fneuroscience-basics-for-lesswrongians-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Neuroscience%20basics%20for%20LessWrongians%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vYobGEz8E7qAZgi9%2Fneuroscience-basics-for-lesswrongians-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vYobGEz8E7qAZgi9%2Fneuroscience-basics-for-lesswrongians-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 342, "htmlBody": "<p>The origins of this article are in my <a href=\"/lw/bug/partial_transcript_of_the_hansonyudkowsky_june/\">partial transcript</a> of the live June 2011 debate between Robin Hanson and Eliezer Yudkowsky. While I still feel like I don't entirely understand his arguments, a few of his comments about neuroscience made me strongly go, \"no, that's not right.\"</p>\n<p>Furthermore, I've noticed that while LessWrong in general seems to be very strong on the psychological or \"black box\"&nbsp;side of cognitive science, there isn't as much discussion of neuroscience here. This is somewhat understandable. Our current understanding of neuroscience is frustratingly incomplete, and too much journalism on neuroscience is sensationalistic nonsense. However, I think what we do know is worth knowing. (And part of what makes much neuroscience journalism annoying is that it makes a big deal out of things that are <em>totally unsurprising, </em>given what we <em>already know.)</em></p>\n<p>My qualifications to do this: while my degrees are in philosophy, for awhile in undergrad I was a neuroscience major, and ended up taking quite a bit of neuroscience as a result. This means I can assure you that most of what I say here is standard neuroscience which could be found in an introductory textbook like Nichols, Martin, Wallace, &amp; Fuchs' <em>From Neuron to Brain </em>(one of the text books I used as an undergraduate). The only things that might not be totally standard are the conjecture I make about how complex currently-poorly-understood areas of the brain are likely to be, and also some of the points I make in criticism of Eliezer at the end (though I believe these are not a very big jump from current textbook neuroscience.)</p>\n<p>One of the main themes of this article will be specialization within the brain. In particular, we know that the brain is divided into specialized areas at the macro level, and we understand some (though not very much) of the micro-level wiring that supports this specialization. It seems likely that each region of the brain has its own micro-level wiring to support its specialized function, and in some regions the wiring is likely to be quite complex.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4vYobGEz8E7qAZgi9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "16901", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XvmrwJsRMogXrYG3h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-13T04:25:05.856Z", "modifiedAt": null, "url": null, "title": "Meetup : Chicago games at Harold Washington Library (Sun 6/17)", "slug": "meetup-chicago-games-at-harold-washington-library-sun-6-17", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n43owTsdYDx7ot6Py/meetup-chicago-games-at-harold-washington-library-sun-6-17", "pageUrlRelative": "/posts/n43owTsdYDx7ot6Py/meetup-chicago-games-at-harold-washington-library-sun-6-17", "linkUrl": "https://www.lesswrong.com/posts/n43owTsdYDx7ot6Py/meetup-chicago-games-at-harold-washington-library-sun-6-17", "postedAtFormatted": "Wednesday, June 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Chicago%20games%20at%20Harold%20Washington%20Library%20(Sun%206%2F17)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Chicago%20games%20at%20Harold%20Washington%20Library%20(Sun%206%2F17)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn43owTsdYDx7ot6Py%2Fmeetup-chicago-games-at-harold-washington-library-sun-6-17%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Chicago%20games%20at%20Harold%20Washington%20Library%20(Sun%206%2F17)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn43owTsdYDx7ot6Py%2Fmeetup-chicago-games-at-harold-washington-library-sun-6-17", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn43owTsdYDx7ot6Py%2Fmeetup-chicago-games-at-harold-washington-library-sun-6-17", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/b4'>Chicago games at Harold Washington Library (Sun 6/17)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 June 2012 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">400 S State St., Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Instead of our typical Saturday Corner Bakery meetup, we'll be meeting this Sunday (6/17) from 2-4pm at the Harold Washington Library.</p>\n\n<p>We've reserved a group study room in the fifth floor. Meet at the fifth floor desk at 1:55 and we will go to the room from there. (getting up there is slightly convoluted. You'll need to take one set of escalators/elevators to the third floor, then go into the third floor and take another set of elevators/escalators to the fifth floor)</p>\n\n<p>What: various rationality games. See below for descriptions.</p>\n\n<p>How (to get there): The library entrance is on State St. between Van Buren and Congress.  The red, blue, green, brown, orange, and pink L lines all stop at the library. For those coming from outside the city, it's a 15 minute walk from union station.</p>\n\n<p>Games: Zendo and the Calibration Game</p>\n\n<p>rules from here: <a href=\"http://lesswrong.com/lw/crs/how_to_run_a_successful_less_wrong_meetup/\" rel=\"nofollow\">http://lesswrong.com/lw/crs/how_to_run_a_successful_less_wrong_meetup/</a></p>\n\n<p>Zendo</p>\n\n<p>Zendo, also known as \u201cScience, the game,\u201d involves one player picking rules and creating structures that follow that rule. The other players try to discover the rule by building their own structures and asking whether those structures follow the rule. See Wikipedia for the exact rules.</p>\n\n<p>Calibration Game</p>\n\n<p>The Calibration Game requires a large number of numerical trivia questions and their answers. A couple of examples might be \u201chow many lakes are there in Canada\u201d or \u201cwhich percentage of the world\u2019s countries are landlocked\u201d. The game Wits &amp; Wagers comes with a large number of such trivia questions and answers.</p>\n\n<p>There are several possible variants of the Calibration game:</p>\n\n<p>Personal Calibration. One person reads the question aloud, and everyone writes down their 50% and 90% confidence intervals. For example, if you\u2019re 50% sure that 20% - 40% of the world\u2019s countries are landlocked, write that down as your 50% confidence interval. After ten questions, the correct answers are revealed. People can now check whether half of their guesses in the 50% confidence interval really were right, and whether they really only got one question out of ten in their 90% confi- dence interval wrong. Alternatively, the correct answer may be revealed as soon as everyone has made their guess, rather than waiting until all ten questions are asked.</p>\n\n<p>Single-Round Aumann. Same as Personal Calibration, but after everyone has writ- ten down their initial confidence intervals, they state them aloud. People then have one chance to alter their guesses based on what the others guessed.</p>\n\n<p>Multiple-Round Aumann. Same as Single- Round Aumann, but repeat the \u201cstate your guess aloud\u201d part until nobody changes their opinions.</p>\n\n<p>Aumann with Discussion. Same as Multiple- Round Aumann, but people are also allowed to discuss the reasons for their estimates in- stead of just stating them.</p>\n\n<p>Paranoid Debating. Same as Aumann with discussion, but one person is secretly desig- nated as the traitor. The traitor tries to make the group\u2019s guess to be as off-mark as possible. For variants and accounts of this game, see the LW Wiki on Paranoid Debating.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/b4'>Chicago games at Harold Washington Library (Sun 6/17)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n43owTsdYDx7ot6Py", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.216063138020948e-07, "legacy": true, "legacyId": "16902", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Chicago_games_at_Harold_Washington_Library__Sun_6_17_\">Discussion article for the meetup : <a href=\"/meetups/b4\">Chicago games at Harold Washington Library (Sun 6/17)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 June 2012 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">400 S State St., Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Instead of our typical Saturday Corner Bakery meetup, we'll be meeting this Sunday (6/17) from 2-4pm at the Harold Washington Library.</p>\n\n<p>We've reserved a group study room in the fifth floor. Meet at the fifth floor desk at 1:55 and we will go to the room from there. (getting up there is slightly convoluted. You'll need to take one set of escalators/elevators to the third floor, then go into the third floor and take another set of elevators/escalators to the fifth floor)</p>\n\n<p>What: various rationality games. See below for descriptions.</p>\n\n<p>How (to get there): The library entrance is on State St. between Van Buren and Congress.  The red, blue, green, brown, orange, and pink L lines all stop at the library. For those coming from outside the city, it's a 15 minute walk from union station.</p>\n\n<p>Games: Zendo and the Calibration Game</p>\n\n<p>rules from here: <a href=\"http://lesswrong.com/lw/crs/how_to_run_a_successful_less_wrong_meetup/\" rel=\"nofollow\">http://lesswrong.com/lw/crs/how_to_run_a_successful_less_wrong_meetup/</a></p>\n\n<p>Zendo</p>\n\n<p>Zendo, also known as \u201cScience, the game,\u201d involves one player picking rules and creating structures that follow that rule. The other players try to discover the rule by building their own structures and asking whether those structures follow the rule. See Wikipedia for the exact rules.</p>\n\n<p>Calibration Game</p>\n\n<p>The Calibration Game requires a large number of numerical trivia questions and their answers. A couple of examples might be \u201chow many lakes are there in Canada\u201d or \u201cwhich percentage of the world\u2019s countries are landlocked\u201d. The game Wits &amp; Wagers comes with a large number of such trivia questions and answers.</p>\n\n<p>There are several possible variants of the Calibration game:</p>\n\n<p>Personal Calibration. One person reads the question aloud, and everyone writes down their 50% and 90% confidence intervals. For example, if you\u2019re 50% sure that 20% - 40% of the world\u2019s countries are landlocked, write that down as your 50% confidence interval. After ten questions, the correct answers are revealed. People can now check whether half of their guesses in the 50% confidence interval really were right, and whether they really only got one question out of ten in their 90% confi- dence interval wrong. Alternatively, the correct answer may be revealed as soon as everyone has made their guess, rather than waiting until all ten questions are asked.</p>\n\n<p>Single-Round Aumann. Same as Personal Calibration, but after everyone has writ- ten down their initial confidence intervals, they state them aloud. People then have one chance to alter their guesses based on what the others guessed.</p>\n\n<p>Multiple-Round Aumann. Same as Single- Round Aumann, but repeat the \u201cstate your guess aloud\u201d part until nobody changes their opinions.</p>\n\n<p>Aumann with Discussion. Same as Multiple- Round Aumann, but people are also allowed to discuss the reasons for their estimates in- stead of just stating them.</p>\n\n<p>Paranoid Debating. Same as Aumann with discussion, but one person is secretly desig- nated as the traitor. The traitor tries to make the group\u2019s guess to be as off-mark as possible. For variants and accounts of this game, see the LW Wiki on Paranoid Debating.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Chicago_games_at_Harold_Washington_Library__Sun_6_17_1\">Discussion article for the meetup : <a href=\"/meetups/b4\">Chicago games at Harold Washington Library (Sun 6/17)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Chicago games at Harold Washington Library (Sun 6/17)", "anchor": "Discussion_article_for_the_meetup___Chicago_games_at_Harold_Washington_Library__Sun_6_17_", "level": 1}, {"title": "Discussion article for the meetup : Chicago games at Harold Washington Library (Sun 6/17)", "anchor": "Discussion_article_for_the_meetup___Chicago_games_at_Harold_Washington_Library__Sun_6_17_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qMuAazqwJvkvo8teR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-13T04:47:00.563Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Psychological Unity of Humankind", "slug": "seq-rerun-the-psychological-unity-of-humankind", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g8Sn3w4Mz2BJhJwcT/seq-rerun-the-psychological-unity-of-humankind", "pageUrlRelative": "/posts/g8Sn3w4Mz2BJhJwcT/seq-rerun-the-psychological-unity-of-humankind", "linkUrl": "https://www.lesswrong.com/posts/g8Sn3w4Mz2BJhJwcT/seq-rerun-the-psychological-unity-of-humankind", "postedAtFormatted": "Wednesday, June 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Psychological%20Unity%20of%20Humankind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Psychological%20Unity%20of%20Humankind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg8Sn3w4Mz2BJhJwcT%2Fseq-rerun-the-psychological-unity-of-humankind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Psychological%20Unity%20of%20Humankind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg8Sn3w4Mz2BJhJwcT%2Fseq-rerun-the-psychological-unity-of-humankind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg8Sn3w4Mz2BJhJwcT%2Fseq-rerun-the-psychological-unity-of-humankind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 192, "htmlBody": "<p>Today's post, <a href=\"/lw/rl/the_psychological_unity_of_humankind/\">The Psychological Unity of Humankind</a> was originally published on 24 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Psychological_Unity_of_Humankind\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Because humans are a sexually reproducing species, human brains are nearly identical. All human beings share similar emotions, tell stories, and employ identical facial expressions. We naively expect all other minds to work like ours, which cause problems when trying to predict the actions of non-human intelligences.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d0j/seq_rerun_optimization_and_the_singularity/\">Optimization and the Singularity</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g8Sn3w4Mz2BJhJwcT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.216162761432e-07, "legacy": true, "legacyId": "16903", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Cyj6wQLW6SeF6aGLy", "8jRbZYrC2mh2oKXAY", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-13T06:14:49.170Z", "modifiedAt": null, "url": null, "title": "The New Yorker article on cognitive biases", "slug": "the-new-yorker-article-on-cognitive-biases", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:09.066Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BlazeOrangeDeer", "createdAt": "2011-11-03T17:37:26.646Z", "isAdmin": false, "displayName": "BlazeOrangeDeer"}, "userId": "aEDpte65EAsTZSja6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/opf728MMWiS26vus2/the-new-yorker-article-on-cognitive-biases", "pageUrlRelative": "/posts/opf728MMWiS26vus2/the-new-yorker-article-on-cognitive-biases", "linkUrl": "https://www.lesswrong.com/posts/opf728MMWiS26vus2/the-new-yorker-article-on-cognitive-biases", "postedAtFormatted": "Wednesday, June 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20New%20Yorker%20article%20on%20cognitive%20biases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20New%20Yorker%20article%20on%20cognitive%20biases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fopf728MMWiS26vus2%2Fthe-new-yorker-article-on-cognitive-biases%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20New%20Yorker%20article%20on%20cognitive%20biases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fopf728MMWiS26vus2%2Fthe-new-yorker-article-on-cognitive-biases", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fopf728MMWiS26vus2%2Fthe-new-yorker-article-on-cognitive-biases", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>A New Yorker article on cognitive biases was on the reddit front page in the last day. It seems like a good opportunity to discuss what the article got right, what it didn't, how reddit reacted, and how one might better publicize the topic in the future.</p>\n<p><a href=\"http://www.newyorker.com/online/blogs/frontal-cortex/2012/06/daniel-kahneman-bias-studies.html\">reddit comments</a>(this link doesn't work any more, see VincentYu's comment for links)</p>\n<p><a href=\"http://www.newyorker.com/online/blogs/frontal-cortex/2012/06/daniel-kahneman-bias-studies.html\">article</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "opf728MMWiS26vus2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 9.216562012329834e-07, "legacy": true, "legacyId": "16914", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-13T10:41:50.494Z", "modifiedAt": null, "url": null, "title": "Advices needed for a presentation on rationality", "slug": "advices-needed-for-a-presentation-on-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:07.567Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Worthstream", "createdAt": "2010-03-30T13:28:43.252Z", "isAdmin": false, "displayName": "Worthstream"}, "userId": "G3R4wxH456T7CyQ3i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/976oqqHS7GCToXnrX/advices-needed-for-a-presentation-on-rationality", "pageUrlRelative": "/posts/976oqqHS7GCToXnrX/advices-needed-for-a-presentation-on-rationality", "linkUrl": "https://www.lesswrong.com/posts/976oqqHS7GCToXnrX/advices-needed-for-a-presentation-on-rationality", "postedAtFormatted": "Wednesday, June 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advices%20needed%20for%20a%20presentation%20on%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvices%20needed%20for%20a%20presentation%20on%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F976oqqHS7GCToXnrX%2Fadvices-needed-for-a-presentation-on-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advices%20needed%20for%20a%20presentation%20on%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F976oqqHS7GCToXnrX%2Fadvices-needed-for-a-presentation-on-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F976oqqHS7GCToXnrX%2Fadvices-needed-for-a-presentation-on-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<div id=\"entry_t3_46v\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>Hi, next month I'm going to be doing an hour long presentation on rationality to Mensa members. It needs to be rather introductory, since High QI != Rationality and most of them are not familiar with the concepts discussed here.</p>\n<p>I'm planning to talk about what rationality is (any good quotes?), what is the difference between the brain and the conscience, why being rational does not mean having a perfect willpower, some common and easily avoided fallacies (sunk cost, scope insensitivity).</p>\n<p>I did a search on the site for this kind of introductory posts and have quite a large pool of interesting arguments to touch. Does anyone have any suggestion on which topics should be included, any pointers to interesting posts that should be summarized or used as source material, etc.?</p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "976oqqHS7GCToXnrX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 9.217776292256496e-07, "legacy": true, "legacyId": "16518", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-13T15:42:58.015Z", "modifiedAt": null, "url": null, "title": "MIT is working on industrial robots that (attempt to) learn what humans want from them [link]", "slug": "mit-is-working-on-industrial-robots-that-attempt-to-learn", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.061Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4ojpZ9A3rWGkYv3zs/mit-is-working-on-industrial-robots-that-attempt-to-learn", "pageUrlRelative": "/posts/4ojpZ9A3rWGkYv3zs/mit-is-working-on-industrial-robots-that-attempt-to-learn", "linkUrl": "https://www.lesswrong.com/posts/4ojpZ9A3rWGkYv3zs/mit-is-working-on-industrial-robots-that-attempt-to-learn", "postedAtFormatted": "Wednesday, June 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MIT%20is%20working%20on%20industrial%20robots%20that%20(attempt%20to)%20learn%20what%20humans%20want%20from%20them%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMIT%20is%20working%20on%20industrial%20robots%20that%20(attempt%20to)%20learn%20what%20humans%20want%20from%20them%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4ojpZ9A3rWGkYv3zs%2Fmit-is-working-on-industrial-robots-that-attempt-to-learn%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MIT%20is%20working%20on%20industrial%20robots%20that%20(attempt%20to)%20learn%20what%20humans%20want%20from%20them%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4ojpZ9A3rWGkYv3zs%2Fmit-is-working-on-industrial-robots-that-attempt-to-learn", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4ojpZ9A3rWGkYv3zs%2Fmit-is-working-on-industrial-robots-that-attempt-to-learn", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p><a href=\"http://web.mit.edu/newsoffice/2012/robot-manufacturing-0612.html\">http://web.mit.edu/newsoffice/2012/robot-manufacturing-0612.html</a></p>\n<p>\"Massachusetts Institute of Technology (MIT) researchers have developed an&nbsp;algorithm that enables a robot to quickly learn an individual's preference for&nbsp;a certain task and adapt accordingly to help complete the task.\"</p>\n<div>It would be interesting to see what kind of issues they will run into. (Granted this is a very restricted environment)</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fuZZ64fNz24BLrXnY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4ojpZ9A3rWGkYv3zs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 9.219145997808466e-07, "legacy": true, "legacyId": "16931", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-13T19:40:12.728Z", "modifiedAt": null, "url": null, "title": "Meetup : S\u00e3o Paulo Meet Up 3", "slug": "meetup-sao-paulo-meet-up-3-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dyokomizo", "createdAt": "2009-11-29T23:39:39.746Z", "isAdmin": false, "displayName": "dyokomizo"}, "userId": "goHFmdghXiKKmR8a4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LdCdhQE2Aw52xpJuv/meetup-sao-paulo-meet-up-3-0", "pageUrlRelative": "/posts/LdCdhQE2Aw52xpJuv/meetup-sao-paulo-meet-up-3-0", "linkUrl": "https://www.lesswrong.com/posts/LdCdhQE2Aw52xpJuv/meetup-sao-paulo-meet-up-3-0", "postedAtFormatted": "Wednesday, June 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20S%C3%A3o%20Paulo%20Meet%20Up%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20S%C3%A3o%20Paulo%20Meet%20Up%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLdCdhQE2Aw52xpJuv%2Fmeetup-sao-paulo-meet-up-3-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20S%C3%A3o%20Paulo%20Meet%20Up%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLdCdhQE2Aw52xpJuv%2Fmeetup-sao-paulo-meet-up-3-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLdCdhQE2Aw52xpJuv%2Fmeetup-sao-paulo-meet-up-3-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> 15<span class=\"date\"> June 2012 02:00:00PM (-0300)</span></p>\n<p><strong>WHERE:</strong> Sala 08 do <span style=\"font-size:12px;\"><span style=\"font-family: arial,helvetica,sans-serif;\">Pr&eacute;dio de Filosofia e Ci&ecirc;ncias Sociais, USP - Av. Prof. Luciano Gualberto, 315 (Cidade Universit&aacute;ria) S&atilde;o Paulo, SP</span></span></p>\n</div>\n<div class=\"content\">\n<div class=\"md\">\n<p>There's going to be an event at USP titled <a title=\"1&ordf; Jornada Transhumanista\" href=\"http://comunicacao.fflch.usp.br/node/1772\">1&ordf; Jornada Transhumanista</a>. After the talks we're planning to have at least one hour of discussions related to rationality and transhumanism.</p>\n<p>Most of previous meetups' attendees are going to be there, two of them presenting at the event.</p>\nSee you there.<br /></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LdCdhQE2Aw52xpJuv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "16933", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-13T19:44:54.023Z", "modifiedAt": null, "url": null, "title": "Meetup : S\u00e3o Paulo Meet Up 3", "slug": "meetup-sao-paulo-meet-up-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.071Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dyokomizo", "createdAt": "2009-11-29T23:39:39.746Z", "isAdmin": false, "displayName": "dyokomizo"}, "userId": "goHFmdghXiKKmR8a4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SGLkHA4zhahQGoNCc/meetup-sao-paulo-meet-up-3", "pageUrlRelative": "/posts/SGLkHA4zhahQGoNCc/meetup-sao-paulo-meet-up-3", "linkUrl": "https://www.lesswrong.com/posts/SGLkHA4zhahQGoNCc/meetup-sao-paulo-meet-up-3", "postedAtFormatted": "Wednesday, June 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20S%C3%A3o%20Paulo%20Meet%20Up%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20S%C3%A3o%20Paulo%20Meet%20Up%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSGLkHA4zhahQGoNCc%2Fmeetup-sao-paulo-meet-up-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20S%C3%A3o%20Paulo%20Meet%20Up%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSGLkHA4zhahQGoNCc%2Fmeetup-sao-paulo-meet-up-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSGLkHA4zhahQGoNCc%2Fmeetup-sao-paulo-meet-up-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/b5'>S\u00e3o Paulo Meet Up 3</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 June 2012 02:00:00PM (-0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Sala 08 do Pr\u00e9dio de Filosofia e Ci\u00eancias Sociais, USP - Av. Prof. Luciano Gualberto, 315 (Cidade Universit\u00e1ria) S\u00e3o Paulo, SP</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There's going to be an event at USP titled 1\u00aa Jornada Transhumanista (<a href=\"http://comunicacao.fflch.usp.br/node/1772\" rel=\"nofollow\">http://comunicacao.fflch.usp.br/node/1772</a>). After the talks we're planning to have at least one hour of discussions related to rationality and transhumanism.</p>\n\n<p>Most of previous meetups' attendees are going to be there, two of them presenting at the event.\nSee you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/b5'>S\u00e3o Paulo Meet Up 3</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SGLkHA4zhahQGoNCc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.220246723059412e-07, "legacy": true, "legacyId": "16934", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___S_o_Paulo_Meet_Up_3\">Discussion article for the meetup : <a href=\"/meetups/b5\">S\u00e3o Paulo Meet Up 3</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 June 2012 02:00:00PM (-0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Sala 08 do Pr\u00e9dio de Filosofia e Ci\u00eancias Sociais, USP - Av. Prof. Luciano Gualberto, 315 (Cidade Universit\u00e1ria) S\u00e3o Paulo, SP</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There's going to be an event at USP titled 1\u00aa Jornada Transhumanista (<a href=\"http://comunicacao.fflch.usp.br/node/1772\" rel=\"nofollow\">http://comunicacao.fflch.usp.br/node/1772</a>). After the talks we're planning to have at least one hour of discussions related to rationality and transhumanism.</p>\n\n<p>Most of previous meetups' attendees are going to be there, two of them presenting at the event.\nSee you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___S_o_Paulo_Meet_Up_31\">Discussion article for the meetup : <a href=\"/meetups/b5\">S\u00e3o Paulo Meet Up 3</a></h2>", "sections": [{"title": "Discussion article for the meetup : S\u00e3o Paulo Meet Up 3", "anchor": "Discussion_article_for_the_meetup___S_o_Paulo_Meet_Up_3", "level": 1}, {"title": "Discussion article for the meetup : S\u00e3o Paulo Meet Up 3", "anchor": "Discussion_article_for_the_meetup___S_o_Paulo_Meet_Up_31", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-13T21:49:19.986Z", "modifiedAt": null, "url": null, "title": "Computation Hazards", "slug": "computation-hazards", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:20.783Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alex_Altair", "createdAt": "2010-07-21T18:30:40.806Z", "isAdmin": false, "displayName": "Alex_Altair"}, "userId": "5wu9jG4pm9q6xjZ9R", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JLHipmPf55x6cuvsN/computation-hazards", "pageUrlRelative": "/posts/JLHipmPf55x6cuvsN/computation-hazards", "linkUrl": "https://www.lesswrong.com/posts/JLHipmPf55x6cuvsN/computation-hazards", "postedAtFormatted": "Wednesday, June 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Computation%20Hazards&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComputation%20Hazards%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJLHipmPf55x6cuvsN%2Fcomputation-hazards%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Computation%20Hazards%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJLHipmPf55x6cuvsN%2Fcomputation-hazards", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJLHipmPf55x6cuvsN%2Fcomputation-hazards", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2146, "htmlBody": "<h5><span style=\"font-weight: normal;\">This is a summary of material from various posts and discussions. My thanks to Eliezer Yudkowsky, Daniel Dewey, Paul Christiano, Nick Beckstead, and several others.</span></h5>\n<p>Several ideas have been floating around LessWrong that can be organized under one concept, relating to a subset of AI safety problems. I&rsquo;d like to gather these ideas in one place so they can be discussed as a unified concept. To give a definition:</p>\n<p>A <strong>computation hazard</strong> is a large negative consequence that may arise <em>merely</em> from vast amounts of computation, such as in a future supercomputer.</p>\n<p>For example, suppose a computer program needs to model people very accurately to make some predictions, and it models those people so accurately that the \"simulated\" people can experience conscious suffering. In a very large computation of this type, millions of people could be created, suffer for some time, and then be destroyed when they are no longer needed for making the predictions desired by the program. This idea was first mentioned by Eliezer Yudkowsky in <a href=\"/lw/x4/nonperson_predicates/\">Nonperson Predicates</a>.</p>\n<p>There are other hazards that may arise in the course of running large-scale computations. In general, we might say that:</p>\n<p>Large amounts of computation will likely consist in running many diverse algorithms. Many algorithms are computation hazards. Therefore, all else equal, the larger the computation, the more likely it is to produce a computation hazard.</p>\n<p>Of course, most algorithms may be morally neutral. Furthermore, algorithms must be somewhat complex before they could possibly be a hazard. For instance, it is intuitively clear that no eight-bit program could possibly be a computation hazard on a normal computer. Worrying computations therefore fall into two categories: computations that run <em>most</em> algorithms, and computations that are particularly likely to run algorithms that are computation hazards.</p>\n<p>An example of a computation that runs most algorithms is a mathematical formalism called Solomonoff induction. First published in 1964, it is an attempt to formalize the scientific process of induction using the theory of Turing machines. It is a brute-force method that finds hypotheses to explain data by testing all possible hypotheses. Many of these hypotheses may be algorithms that describe the functioning of people. At a sufficient precision, these algorithms themselves may experience consciousness and suffering. Taken literally, Solomonoff induction runs all algorithms; therefore it produces all possible computation hazards. If we are to avoid computation hazards, any implemented approximations of Solomonoff induction will need to determine <em>ahead of time</em> which algorithms are computation hazards.</p>\n<p>Computations that run most algorithms could also hide in other places. Imagine a supercomputer&rsquo;s power is being tested on a simple game, like chess or Go. The testing program simply tries all possible strategies, according to some enumeration. The best strategy that the supercomputer finds would be a measure of how many computations it could perform, compared to other computers that ran the same program. If the rules of the game are complex enough to be Turing complete (<a href=\"http://en.wikipedia.org/wiki/Billiard-ball_computer\">a</a> <a href=\"http://igoro.com/archive/human-heart-is-a-turing-machine-research-on-xbox-360-shows-wait-what/\">surprisingly</a> <a href=\"http://www.newscientist.com/blogs/onepercent/2012/04/researchers-build-crab-powered.html\">easy</a> <a href=\"http://en.wikipedia.org/wiki/Domino_computer\">achievement</a>) then this game-playing program would eventually simulate all algorithms, including ones with moral status.</p>\n<p>Of course, running <em>most</em> algorithms is quite infeasible simply because of the vast number of possible algorithms. Depending on the fraction of algorithms that are computation hazards, it may be enough that a computation run an enormous number which act as a random sample of all algorithms. Computations of this type might include evolutionary programs, which are blind to the types of algorithms they run until the results are evaluated for fitness. Or they may be Monte Carlo approximations of massive computations.</p>\n<p>But if computation hazards are relatively rare, then it will still be unlikely for large-scale computations to stumble across them unguided. Several computations may fall into the second category of computations that are particularly likely to run algorithms that <em>are</em> computation hazards. Here we focus on three types of computations in particular: agents, predictors and oracles. The last two types are especially important because they are often considered safer types of AI than agent-based AI architectures. First I will stipulate definitions for these three types of computations, and then I will discuss the types of computation hazards they may produce.</p>\n<h4>Agents</h4>\n<p>An agent is a computation which decides between possible actions based on the consequences of those actions. They can be thought of as &ldquo;steering&rdquo; the future towards some target, or as selecting a future from the set of possible futures. Therefore they can also be thought of as having a goal, or as maximizing a utility function.</p>\n<p>Sufficiently powerful agents are extremely powerful because they constitute a feedback loop. Well-known from physics, feedback loops often change their surroundings incredibly quickly and dramatically. Examples include the growth of biological populations, and nuclear reactions. Feedback loops are dangerous if their target is undesirable. Agents will be feedback loops as soon as they are able to improve their ability to improve their ability to move towards their goal. For example, humans can improve their ability to move towards their goal by using their intelligence to make decisions. A student aiming to create cures can use her intelligence to learn chemistry, therefore improving her ability to decide what to study next. But presently, humans cannot improve their intelligence, which would improve their ability to improve their ability to make decisions. The student cannot yet learn how to modify her brain in order for her to more quickly learn subjects.</p>\n<h4>Predictors</h4>\n<p>A predictor is a computation which takes data as input, and predicts what data will come next. An example would be certain types of trained neural networks, or any <em>approximation</em> of Solomonoff induction. Intuitively, this feels safer than an agent AI because predictors do not seem to have goals or take actions; they just report predictions as requested by human.</p>\n<h4>Oracles</h4>\n<p>An oracle is a computation which takes questions as input, and returns answers. They are broader than predictors in that one could ask an oracle about predictions. Similar to a predictor, oracles do not seem to have goals or take actions. (Some material summarized <a href=\"/lw/any/a_taxonomy_of_oracle_ais/\">here</a>.)</p>\n<h3>Examples of hazards</h3>\n<p>Agent-like computations are the most clearly dangerous computation hazards. If any large computation starts running the beginning of a <strong>self-improving agent</strong> computation, it is difficult to say how far the agent may safely be run before it is a computation hazard. As soon as the agent is sufficiently intelligent, it will attempt to acquire more resources like computing substrate and energy. It may also attempt to free itself from control of the parent computation.</p>\n<p>Another major concern is that, because people are an important part of the surroundings, even non-agent predictors or oracles will <strong>simulate people</strong> in order to make predictions or give answers respectively. Someone could ask a predictor, &ldquo;What will this engineer do if we give him a contract?&rdquo; It may be that the easiest way for the predictor to determine the answer is to simulate the internal workings of the given engineer's mind. If these simulations are sufficiently precise, then they will be people in and of themselves. The simulations could cause those people to suffer, and will likely kill them by ending the simulation when the prediction or answer is given.</p>\n<p>Similarly, one can imagine that a predictor or oracle might <strong>simulate powerful agents</strong>; that is, algorithms which efficiently maximize some utility function. Agents may be simulated because many agent-like entities exist in the real world, and their behavior would need to be modeled. Or, perhaps oracles would investigate agents for the purpose of answering questions better. These agents, while being simulated, may have goals that require acting independently of the oracle. These agents may also be more powerful than the oracles, especially since the oracles were not designed with self-improvement behavior in mind. Therefore these agents may attempt to &ldquo;unbox&rdquo; themselves from the simulation and begin controlling the rest of the universe. For instance, the agents may use previous questions given to the oracle to deduce the nature of the universe and the psychology of the oracle-creators. (For a fictional example, see <a href=\"/lw/qk/that_alien_message\">That Alien Message</a>.) Or, the agent might somehow distort the output of the predictor, in a way that what the oracle predicts will cause us to unbox the agent.</p>\n<p>Predictors also have the problem of <strong>self-fulfilling prophecies</strong> (first suggested <a href=\"/lw/5bu/is_it_possible_to_build_a_safe_oracle_ai/3zbo\">here</a>). An arbitrarily accurate predictor will know that its prediction will affect the future. Therefore, to be a correct prediction, it must make sure that delivering its prediction doesn&rsquo;t cause the receiver to act in a way that negates the prediction. Therefore, the predictor may have to choose between predictions which cause the receiver to act in a way that fulfills the prediction. This is a type of control over the user. Since the predictor is super-intelligent, any control may rapidly optimize the universe towards some unknown goal.</p>\n<p>Overall, there is a large worry that sufficiently intelligent oracles or predictors <strong>may become agents</strong>. Beside the above possibilities, some are worried that intelligence is inherently an optimization process, and therefore oracles and predictors are inherently satisfying some utility function. This, combined with the fact that nothing can be causally isolated from the rest of the universe, seems to invite an eventual AI-takeoff.</p>\n<h3>Methods for avoiding computational hazards</h3>\n<p>It is often thought that, while no proposal has yet been shown safe from computational hazards, oracles and predictors are safer than deliberately agent-based AGI. Other methods have been proposed to make these even safer. Armstrong et al. describe many AI safety measures in general. Below we review some possible techniques for avoiding computational hazards specifically.</p>\n<p>One obvious safety practice is to limit the complexity, or the size of computations. In general, this will also limit the algorithm below general intelligence, but it is a good step while progressing towards FAI. Indeed, it is clear that all current prediction or AI systems are too simple to either be general intelligences, or pose as a computational hazard.</p>\n<p>A proposal for regulating complex oracles or predictors is to develop safety indicators. That is, develop some function that will evaluate the proposed algorithm or model, and return whether it is potentially dangerous. For instance, one could write a simple program that rejects running an algorithm if any part of it is isomorphic to the human genome (since DNA clearly creates general intelligence and people under the right circumstances). Or, to measure the impact of an action suggested by an oracle, one could ask how many humans would be alive one year after the action was taken.</p>\n<p>But one could only run an algorithm if they were <em>sure</em> it was not a person. A function that could evaluate an algorithm and return 0 <em>only</em> if it is <em>not</em> a person is called a <a href=\"/lw/x4/nonperson_predicates/\">nonperson predicate</a>. Some algorithms are obviously not people. For example, squaring the numbers from 1 to 100 will not simulate people. Any algorithm whose behavior is periodic with a short period is unlikely to be a person, or nearly any presently constructed software. But in general this seems extremely difficult to verify. It could be that writing nonperson predicates or other safety indicators is FAI-complete in that sense that if we solve them, we will have discovered friendliness theory. Furthermore, it may be that some attempts to evaluate whether an algorithm is a person actually causes a simulation of a person, by running parts of the algorithm, by modeling a person for comparison, or by other means. Similarly, it may be that attempts to investigate the friendliness of a particular agent cause that agent to unbox itself.</p>\n<p>Predictors seem to be one of the most goal-agnostic forms of AGI. This makes them a very attractive model in which to perfect safety. Some ideas for avoiding self-fulfilling predictions suggest that we ask the predictor to tell us what it would have predicted if we hadn&rsquo;t asked (first suggested <a href=\"/lw/any/a_taxonomy_of_oracle_ais/5zkg\">here</a>). This frees the predictor from requiring itself to make predictions consistent with our behavior. Whether this will work depends on the exact process of the predictor; it may be so accurate that it <a href=\"http://ordinaryideas.wordpress.com/2011/12/15/hazards/\">cannot deal with counterfactuals</a>, and will simply report that it would have predicted that we would have asked anyway. It is also problematic that the prediction is now inaccurate; because it has told us, we will act, possibly voiding any part of the prediction.</p>\n<p>A very plausible but non-formal solution is to aim for a soft takeoff. For example, we could build a predictor that is not generally intelligent, and use it to investigate safe ways advance the situation. Perhaps we could use a sub-general intelligence to safely improve our own intelligence.</p>\n<p>Have I missed any major examples in this post? Does &ldquo;computation hazards&rdquo; seem like a valid concept as distinct from other types of AI-risks?</p>\n<h3>References</h3>\n<p>Armstrong S., Sandberg A., Bostrom N. (2012). &ldquo;<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Thinking inside the box: using and controlling an Oracle AI</a>&rdquo;. Minds and Machines, forthcoming.</p>\n<p>Solomonoff, R., \"<a href=\"http://world.std.com/~rjs/1964pt1.pdf\">A Formal Theory of Inductive Inference, Part I</a>\" Information and Control, Vol 7, No. 1 pp 1-22, March 1964.</p>\n<p>Solomonoff, R., \"<a href=\"http://world.std.com/~rjs/1964pt2.pdf\">A Formal Theory of Inductive Inference, Part II</a>\" Information and Control, Vol 7, No. 2 pp 224-254, June 1964.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JLHipmPf55x6cuvsN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 26, "extendedScore": null, "score": 9.220812945279782e-07, "legacy": true, "legacyId": "16935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h5><span style=\"font-weight: normal;\">This is a summary of material from various posts and discussions. My thanks to Eliezer Yudkowsky, Daniel Dewey, Paul Christiano, Nick Beckstead, and several others.</span></h5>\n<p>Several ideas have been floating around LessWrong that can be organized under one concept, relating to a subset of AI safety problems. I\u2019d like to gather these ideas in one place so they can be discussed as a unified concept. To give a definition:</p>\n<p>A <strong>computation hazard</strong> is a large negative consequence that may arise <em>merely</em> from vast amounts of computation, such as in a future supercomputer.</p>\n<p>For example, suppose a computer program needs to model people very accurately to make some predictions, and it models those people so accurately that the \"simulated\" people can experience conscious suffering. In a very large computation of this type, millions of people could be created, suffer for some time, and then be destroyed when they are no longer needed for making the predictions desired by the program. This idea was first mentioned by Eliezer Yudkowsky in <a href=\"/lw/x4/nonperson_predicates/\">Nonperson Predicates</a>.</p>\n<p>There are other hazards that may arise in the course of running large-scale computations. In general, we might say that:</p>\n<p>Large amounts of computation will likely consist in running many diverse algorithms. Many algorithms are computation hazards. Therefore, all else equal, the larger the computation, the more likely it is to produce a computation hazard.</p>\n<p>Of course, most algorithms may be morally neutral. Furthermore, algorithms must be somewhat complex before they could possibly be a hazard. For instance, it is intuitively clear that no eight-bit program could possibly be a computation hazard on a normal computer. Worrying computations therefore fall into two categories: computations that run <em>most</em> algorithms, and computations that are particularly likely to run algorithms that are computation hazards.</p>\n<p>An example of a computation that runs most algorithms is a mathematical formalism called Solomonoff induction. First published in 1964, it is an attempt to formalize the scientific process of induction using the theory of Turing machines. It is a brute-force method that finds hypotheses to explain data by testing all possible hypotheses. Many of these hypotheses may be algorithms that describe the functioning of people. At a sufficient precision, these algorithms themselves may experience consciousness and suffering. Taken literally, Solomonoff induction runs all algorithms; therefore it produces all possible computation hazards. If we are to avoid computation hazards, any implemented approximations of Solomonoff induction will need to determine <em>ahead of time</em> which algorithms are computation hazards.</p>\n<p>Computations that run most algorithms could also hide in other places. Imagine a supercomputer\u2019s power is being tested on a simple game, like chess or Go. The testing program simply tries all possible strategies, according to some enumeration. The best strategy that the supercomputer finds would be a measure of how many computations it could perform, compared to other computers that ran the same program. If the rules of the game are complex enough to be Turing complete (<a href=\"http://en.wikipedia.org/wiki/Billiard-ball_computer\">a</a> <a href=\"http://igoro.com/archive/human-heart-is-a-turing-machine-research-on-xbox-360-shows-wait-what/\">surprisingly</a> <a href=\"http://www.newscientist.com/blogs/onepercent/2012/04/researchers-build-crab-powered.html\">easy</a> <a href=\"http://en.wikipedia.org/wiki/Domino_computer\">achievement</a>) then this game-playing program would eventually simulate all algorithms, including ones with moral status.</p>\n<p>Of course, running <em>most</em> algorithms is quite infeasible simply because of the vast number of possible algorithms. Depending on the fraction of algorithms that are computation hazards, it may be enough that a computation run an enormous number which act as a random sample of all algorithms. Computations of this type might include evolutionary programs, which are blind to the types of algorithms they run until the results are evaluated for fitness. Or they may be Monte Carlo approximations of massive computations.</p>\n<p>But if computation hazards are relatively rare, then it will still be unlikely for large-scale computations to stumble across them unguided. Several computations may fall into the second category of computations that are particularly likely to run algorithms that <em>are</em> computation hazards. Here we focus on three types of computations in particular: agents, predictors and oracles. The last two types are especially important because they are often considered safer types of AI than agent-based AI architectures. First I will stipulate definitions for these three types of computations, and then I will discuss the types of computation hazards they may produce.</p>\n<h4 id=\"Agents\">Agents</h4>\n<p>An agent is a computation which decides between possible actions based on the consequences of those actions. They can be thought of as \u201csteering\u201d the future towards some target, or as selecting a future from the set of possible futures. Therefore they can also be thought of as having a goal, or as maximizing a utility function.</p>\n<p>Sufficiently powerful agents are extremely powerful because they constitute a feedback loop. Well-known from physics, feedback loops often change their surroundings incredibly quickly and dramatically. Examples include the growth of biological populations, and nuclear reactions. Feedback loops are dangerous if their target is undesirable. Agents will be feedback loops as soon as they are able to improve their ability to improve their ability to move towards their goal. For example, humans can improve their ability to move towards their goal by using their intelligence to make decisions. A student aiming to create cures can use her intelligence to learn chemistry, therefore improving her ability to decide what to study next. But presently, humans cannot improve their intelligence, which would improve their ability to improve their ability to make decisions. The student cannot yet learn how to modify her brain in order for her to more quickly learn subjects.</p>\n<h4 id=\"Predictors\">Predictors</h4>\n<p>A predictor is a computation which takes data as input, and predicts what data will come next. An example would be certain types of trained neural networks, or any <em>approximation</em> of Solomonoff induction. Intuitively, this feels safer than an agent AI because predictors do not seem to have goals or take actions; they just report predictions as requested by human.</p>\n<h4 id=\"Oracles\">Oracles</h4>\n<p>An oracle is a computation which takes questions as input, and returns answers. They are broader than predictors in that one could ask an oracle about predictions. Similar to a predictor, oracles do not seem to have goals or take actions. (Some material summarized <a href=\"/lw/any/a_taxonomy_of_oracle_ais/\">here</a>.)</p>\n<h3 id=\"Examples_of_hazards\">Examples of hazards</h3>\n<p>Agent-like computations are the most clearly dangerous computation hazards. If any large computation starts running the beginning of a <strong>self-improving agent</strong> computation, it is difficult to say how far the agent may safely be run before it is a computation hazard. As soon as the agent is sufficiently intelligent, it will attempt to acquire more resources like computing substrate and energy. It may also attempt to free itself from control of the parent computation.</p>\n<p>Another major concern is that, because people are an important part of the surroundings, even non-agent predictors or oracles will <strong>simulate people</strong> in order to make predictions or give answers respectively. Someone could ask a predictor, \u201cWhat will this engineer do if we give him a contract?\u201d It may be that the easiest way for the predictor to determine the answer is to simulate the internal workings of the given engineer's mind. If these simulations are sufficiently precise, then they will be people in and of themselves. The simulations could cause those people to suffer, and will likely kill them by ending the simulation when the prediction or answer is given.</p>\n<p>Similarly, one can imagine that a predictor or oracle might <strong>simulate powerful agents</strong>; that is, algorithms which efficiently maximize some utility function. Agents may be simulated because many agent-like entities exist in the real world, and their behavior would need to be modeled. Or, perhaps oracles would investigate agents for the purpose of answering questions better. These agents, while being simulated, may have goals that require acting independently of the oracle. These agents may also be more powerful than the oracles, especially since the oracles were not designed with self-improvement behavior in mind. Therefore these agents may attempt to \u201cunbox\u201d themselves from the simulation and begin controlling the rest of the universe. For instance, the agents may use previous questions given to the oracle to deduce the nature of the universe and the psychology of the oracle-creators. (For a fictional example, see <a href=\"/lw/qk/that_alien_message\">That Alien Message</a>.) Or, the agent might somehow distort the output of the predictor, in a way that what the oracle predicts will cause us to unbox the agent.</p>\n<p>Predictors also have the problem of <strong>self-fulfilling prophecies</strong> (first suggested <a href=\"/lw/5bu/is_it_possible_to_build_a_safe_oracle_ai/3zbo\">here</a>). An arbitrarily accurate predictor will know that its prediction will affect the future. Therefore, to be a correct prediction, it must make sure that delivering its prediction doesn\u2019t cause the receiver to act in a way that negates the prediction. Therefore, the predictor may have to choose between predictions which cause the receiver to act in a way that fulfills the prediction. This is a type of control over the user. Since the predictor is super-intelligent, any control may rapidly optimize the universe towards some unknown goal.</p>\n<p>Overall, there is a large worry that sufficiently intelligent oracles or predictors <strong>may become agents</strong>. Beside the above possibilities, some are worried that intelligence is inherently an optimization process, and therefore oracles and predictors are inherently satisfying some utility function. This, combined with the fact that nothing can be causally isolated from the rest of the universe, seems to invite an eventual AI-takeoff.</p>\n<h3 id=\"Methods_for_avoiding_computational_hazards\">Methods for avoiding computational hazards</h3>\n<p>It is often thought that, while no proposal has yet been shown safe from computational hazards, oracles and predictors are safer than deliberately agent-based AGI. Other methods have been proposed to make these even safer. Armstrong et al. describe many AI safety measures in general. Below we review some possible techniques for avoiding computational hazards specifically.</p>\n<p>One obvious safety practice is to limit the complexity, or the size of computations. In general, this will also limit the algorithm below general intelligence, but it is a good step while progressing towards FAI. Indeed, it is clear that all current prediction or AI systems are too simple to either be general intelligences, or pose as a computational hazard.</p>\n<p>A proposal for regulating complex oracles or predictors is to develop safety indicators. That is, develop some function that will evaluate the proposed algorithm or model, and return whether it is potentially dangerous. For instance, one could write a simple program that rejects running an algorithm if any part of it is isomorphic to the human genome (since DNA clearly creates general intelligence and people under the right circumstances). Or, to measure the impact of an action suggested by an oracle, one could ask how many humans would be alive one year after the action was taken.</p>\n<p>But one could only run an algorithm if they were <em>sure</em> it was not a person. A function that could evaluate an algorithm and return 0 <em>only</em> if it is <em>not</em> a person is called a <a href=\"/lw/x4/nonperson_predicates/\">nonperson predicate</a>. Some algorithms are obviously not people. For example, squaring the numbers from 1 to 100 will not simulate people. Any algorithm whose behavior is periodic with a short period is unlikely to be a person, or nearly any presently constructed software. But in general this seems extremely difficult to verify. It could be that writing nonperson predicates or other safety indicators is FAI-complete in that sense that if we solve them, we will have discovered friendliness theory. Furthermore, it may be that some attempts to evaluate whether an algorithm is a person actually causes a simulation of a person, by running parts of the algorithm, by modeling a person for comparison, or by other means. Similarly, it may be that attempts to investigate the friendliness of a particular agent cause that agent to unbox itself.</p>\n<p>Predictors seem to be one of the most goal-agnostic forms of AGI. This makes them a very attractive model in which to perfect safety. Some ideas for avoiding self-fulfilling predictions suggest that we ask the predictor to tell us what it would have predicted if we hadn\u2019t asked (first suggested <a href=\"/lw/any/a_taxonomy_of_oracle_ais/5zkg\">here</a>). This frees the predictor from requiring itself to make predictions consistent with our behavior. Whether this will work depends on the exact process of the predictor; it may be so accurate that it <a href=\"http://ordinaryideas.wordpress.com/2011/12/15/hazards/\">cannot deal with counterfactuals</a>, and will simply report that it would have predicted that we would have asked anyway. It is also problematic that the prediction is now inaccurate; because it has told us, we will act, possibly voiding any part of the prediction.</p>\n<p>A very plausible but non-formal solution is to aim for a soft takeoff. For example, we could build a predictor that is not generally intelligent, and use it to investigate safe ways advance the situation. Perhaps we could use a sub-general intelligence to safely improve our own intelligence.</p>\n<p>Have I missed any major examples in this post? Does \u201ccomputation hazards\u201d seem like a valid concept as distinct from other types of AI-risks?</p>\n<h3 id=\"References\">References</h3>\n<p>Armstrong S., Sandberg A., Bostrom N. (2012). \u201c<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Thinking inside the box: using and controlling an Oracle AI</a>\u201d. Minds and Machines, forthcoming.</p>\n<p>Solomonoff, R., \"<a href=\"http://world.std.com/~rjs/1964pt1.pdf\">A Formal Theory of Inductive Inference, Part I</a>\" Information and Control, Vol 7, No. 1 pp 1-22, March 1964.</p>\n<p>Solomonoff, R., \"<a href=\"http://world.std.com/~rjs/1964pt2.pdf\">A Formal Theory of Inductive Inference, Part II</a>\" Information and Control, Vol 7, No. 2 pp 224-254, June 1964.</p>", "sections": [{"title": "Agents", "anchor": "Agents", "level": 2}, {"title": "Predictors", "anchor": "Predictors", "level": 2}, {"title": "Oracles", "anchor": "Oracles", "level": 2}, {"title": "Examples of hazards", "anchor": "Examples_of_hazards", "level": 1}, {"title": "Methods for avoiding computational hazards", "anchor": "Methods_for_avoiding_computational_hazards", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "58 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wqDRRx9RqwKLzWt7R", "XddMs9kSGtm6L8522", "5wMcKNAwB6X4mp9og"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T02:53:15.258Z", "modifiedAt": null, "url": null, "title": "Sebastian Thrun AMA on reddit [link]", "slug": "sebastian-thrun-ama-on-reddit-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:08.438Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z3EAMitr8LJxvcnFq/sebastian-thrun-ama-on-reddit-link", "pageUrlRelative": "/posts/z3EAMitr8LJxvcnFq/sebastian-thrun-ama-on-reddit-link", "linkUrl": "https://www.lesswrong.com/posts/z3EAMitr8LJxvcnFq/sebastian-thrun-ama-on-reddit-link", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sebastian%20Thrun%20AMA%20on%20reddit%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASebastian%20Thrun%20AMA%20on%20reddit%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3EAMitr8LJxvcnFq%2Fsebastian-thrun-ama-on-reddit-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sebastian%20Thrun%20AMA%20on%20reddit%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3EAMitr8LJxvcnFq%2Fsebastian-thrun-ama-on-reddit-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3EAMitr8LJxvcnFq%2Fsebastian-thrun-ama-on-reddit-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p>http://udacity.blogspot.com/2012/06/sebastian-thrun-udacity-ceo-will.html</p>\n<p>While Sebastian is not an AGI researcher his reputation in the broad AI community is high due to him largely making self-driving cars happen (and those are autonomous AI systems, even if narrow). I think his opinion of AGI might worth asking (he has a high&nbsp;position&nbsp;in Google's X-labs)</p>\n<p>He is also interesting from due to a number of innovations he partook in that are likely to be pretty historical</p>\n<ul>\n<li>Self-driving cars</li>\n<li>Formation of Udacity is likely to be remembered as the \"event that changed college-level online education\"</li>\n<li>I don't know how much he is responsible for this directly, but Google X is making headway with pretty futuristic HUD devices that are likely to significantly increase human-computer integration</li>\n</ul>\n<div>Link to thread:&nbsp;</div>\n<div>http://www.reddit.com/r/IAmA/comments/v59z3/iam_sebastian_thrun_stanford_professor_google_x/</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YgizoZqa7LEb3LEJn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z3EAMitr8LJxvcnFq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 9.222196175907029e-07, "legacy": true, "legacyId": "16938", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T03:14:01.884Z", "modifiedAt": null, "url": null, "title": "Interest in Meetups starting this fall at the University of Illinois at Urbana-Champaign?", "slug": "interest-in-meetups-starting-this-fall-at-the-university-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:52.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "eJgCe9t2yKiCNAZxF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wg8G5NKbwmiZdM646/interest-in-meetups-starting-this-fall-at-the-university-of", "pageUrlRelative": "/posts/wg8G5NKbwmiZdM646/interest-in-meetups-starting-this-fall-at-the-university-of", "linkUrl": "https://www.lesswrong.com/posts/wg8G5NKbwmiZdM646/interest-in-meetups-starting-this-fall-at-the-university-of", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interest%20in%20Meetups%20starting%20this%20fall%20at%20the%20University%20of%20Illinois%20at%20Urbana-Champaign%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInterest%20in%20Meetups%20starting%20this%20fall%20at%20the%20University%20of%20Illinois%20at%20Urbana-Champaign%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwg8G5NKbwmiZdM646%2Finterest-in-meetups-starting-this-fall-at-the-university-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interest%20in%20Meetups%20starting%20this%20fall%20at%20the%20University%20of%20Illinois%20at%20Urbana-Champaign%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwg8G5NKbwmiZdM646%2Finterest-in-meetups-starting-this-fall-at-the-university-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwg8G5NKbwmiZdM646%2Finterest-in-meetups-starting-this-fall-at-the-university-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<p>I will be starting as a freshman at UIUC this fall, as a Math/CS major. The only interest in a Less Wrong meetup I've found is <a href=\"http://www.reddit.com/r/UIUC/comments/l4vaz/does_anybody_here_readcontribute_to_lesswrongcom/\">this /r/uiuc thread from 8 months ago</a>. I would like to possibly start a google group of interested people so that we can hit the ground running with a meetup group in the fall. If you will be at UIUC in the fall and are interested, PM me or post in this thread. Thank you! I'm really excited about this.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wg8G5NKbwmiZdM646", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 9.222290751218237e-07, "legacy": true, "legacyId": "16941", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T03:41:08.384Z", "modifiedAt": null, "url": null, "title": "[Link] Can We Reverse The Stanford Prison Experiment?", "slug": "link-can-we-reverse-the-stanford-prison-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.753Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QeZbm7vx3iXfn5nAZ/link-can-we-reverse-the-stanford-prison-experiment", "pageUrlRelative": "/posts/QeZbm7vx3iXfn5nAZ/link-can-we-reverse-the-stanford-prison-experiment", "linkUrl": "https://www.lesswrong.com/posts/QeZbm7vx3iXfn5nAZ/link-can-we-reverse-the-stanford-prison-experiment", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Can%20We%20Reverse%20The%20Stanford%20Prison%20Experiment%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Can%20We%20Reverse%20The%20Stanford%20Prison%20Experiment%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQeZbm7vx3iXfn5nAZ%2Flink-can-we-reverse-the-stanford-prison-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Can%20We%20Reverse%20The%20Stanford%20Prison%20Experiment%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQeZbm7vx3iXfn5nAZ%2Flink-can-we-reverse-the-stanford-prison-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQeZbm7vx3iXfn5nAZ%2Flink-can-we-reverse-the-stanford-prison-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 344, "htmlBody": "<p>From the Harvard Business Review, an article entitled: \"Can We Reverse The Stanford Prison Experiment?\"</p>\n<p>By: Greg McKeown<br />Posted: June 12, 2012</p>\n<p><a href=\"http://blogs.hbr.org/cs/2012/06/can_we_reverse_the_stanford_pr.html\">Clicky Link of Awesome! Wheee! Push me!</a></p>\n<p><strong>Summary</strong>:</p>\n<p>Royal Canadian Mounted Police attempt a program where they hand out \"Positive Tickets\"&nbsp;</p>\n<blockquote>\n<p style=\"margin: 0px 0px 15px; padding: 0px; line-height: 22px; font-family: Helvetica, Arial, sans-serif; font-size: 13px;\">Their approach was to try to catch youth doing the right things and give them a&nbsp;<a style=\"color: #b20022; outline: none; text-decoration: none;\" href=\"http://www.positivetickets.com/\">Positive Ticket</a>. The ticket granted the recipient free entry to the movies or to a local youth center. They gave out an average of 40,000 tickets per year. That is three times the number of negative tickets over the same period. As it turns out, and unbeknownst to Clapham, that ratio (2.9 positive affects to 1 negative affect, to be precise) is called the&nbsp;<a style=\"color: #b20022; outline: none; text-decoration: none;\" href=\"http://en.wikipedia.org/wiki/Losada_line\">Losada Line</a>. It is the minimum ratio of positive to negatives that has to exist for a team to flourish. On higher-performing teams (and marriages for that matter) the ratio jumps to&nbsp;<a style=\"color: #b20022; outline: none; text-decoration: none;\" href=\"http://en.wikipedia.org/wiki/Positivity/negativity_ratio\">5:1</a>. But does it hold true in policing?</p>\n<p style=\"margin: 0px 0px 15px; padding: 0px; line-height: 22px; font-family: Helvetica, Arial, sans-serif; font-size: 13px;\">According to Clapham, youth recidivism was reduced from 60% to 8%. Overall crime was reduced by 40%. Youth crime was cut in half. And it cost one-tenth of the traditional judicial system.</p>\n</blockquote>\n<p><br />This idea can be applied to Real Life</p>\n<blockquote>\n<p style=\"margin: 0px 0px 15px; padding: 0px; line-height: 22px; font-family: Helvetica, Arial, sans-serif; font-size: 13px;\">The lesson here is to create a culture that&nbsp;<em style=\"font-style: oblique;\">immediately</em>&nbsp;and sincerely celebrates victories. Here are three simple ways to begin:</p>\n<p style=\"margin: 0px 0px 15px; padding: 0px; line-height: 22px; font-family: Helvetica, Arial, sans-serif; font-size: 13px;\"><strong>1. Start your next staff meeting with five minutes on the question: \"What has gone right since our last meeting?\"</strong>&nbsp;Have each person acknowledge someone else's achievement in a concrete, sincere way. Done right, this very small question can begin to shift the conversation.</p>\n<p style=\"margin: 0px 0px 15px; padding: 0px; line-height: 22px; font-family: Helvetica, Arial, sans-serif; font-size: 13px;\"><strong>2. Take two minutes every day to try to catch someone doing the right thing.</strong>&nbsp;It is the fastest and most positive way for the people around you to learn when they are getting it right.</p>\n<p style=\"margin: 0px 0px 15px; padding: 0px; line-height: 22px; font-family: Helvetica, Arial, sans-serif; font-size: 13px;\"><strong>3. Create a virtual community board where employees, partners and even customers can share what they are grateful for daily.&nbsp;</strong>Sounds idealistic? Vishen Lakhiani, CEO of&nbsp;<a style=\"color: #b20022; outline: none; text-decoration: none;\" href=\"http://www.mindvalley.com/get-published/\">Mind Valley</a>, a new generation media and publishing company, has done just that at&nbsp;<a style=\"color: #b20022; outline: none; text-decoration: none;\" href=\"http://www.gratitudelog.com/vishen\">Gratitude Log</a>. (Watch him explain how it works&nbsp;<a style=\"color: #b20022; outline: none; text-decoration: none;\" href=\"http://www.youtube.com/watch?v=RKCb82ZzZyg\">here</a>).</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 1, "gHCNhqxuJq2bZ2akb": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QeZbm7vx3iXfn5nAZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 66, "extendedScore": null, "score": 0.000153, "legacy": true, "legacyId": "16945", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T04:58:05.647Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Design Space of Minds in General", "slug": "seq-rerun-the-design-space-of-minds-in-general", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:08.985Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SbYk376L5upoKYinp/seq-rerun-the-design-space-of-minds-in-general", "pageUrlRelative": "/posts/SbYk376L5upoKYinp/seq-rerun-the-design-space-of-minds-in-general", "linkUrl": "https://www.lesswrong.com/posts/SbYk376L5upoKYinp/seq-rerun-the-design-space-of-minds-in-general", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Design%20Space%20of%20Minds%20in%20General&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Design%20Space%20of%20Minds%20in%20General%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSbYk376L5upoKYinp%2Fseq-rerun-the-design-space-of-minds-in-general%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Design%20Space%20of%20Minds%20in%20General%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSbYk376L5upoKYinp%2Fseq-rerun-the-design-space-of-minds-in-general", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSbYk376L5upoKYinp%2Fseq-rerun-the-design-space-of-minds-in-general", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<p>Today's post, <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">The Design Space of Minds-In-General</a> was originally published on 25 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Design_Space_of_Minds-In-General\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When people talk about \"AI\", they're talking about an incredibly wide range of possibilities. Having a word like \"AI\" is like having a word for everything which isn't a duck.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d1j/seq_rerun_the_psychological_unity_of_humankind/\">The Psychological Unity of Humankind</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SbYk376L5upoKYinp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.222764462017652e-07, "legacy": true, "legacyId": "16946", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tnWRXkcDi5Tw9rzXw", "g8Sn3w4Mz2BJhJwcT", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T07:55:41.377Z", "modifiedAt": null, "url": null, "title": "Brainstorming additional AI risk reduction ideas", "slug": "brainstorming-additional-ai-risk-reduction-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:08.906Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8HPsRYKE2pYHtqRhw/brainstorming-additional-ai-risk-reduction-ideas", "pageUrlRelative": "/posts/8HPsRYKE2pYHtqRhw/brainstorming-additional-ai-risk-reduction-ideas", "linkUrl": "https://www.lesswrong.com/posts/8HPsRYKE2pYHtqRhw/brainstorming-additional-ai-risk-reduction-ideas", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brainstorming%20additional%20AI%20risk%20reduction%20ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrainstorming%20additional%20AI%20risk%20reduction%20ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8HPsRYKE2pYHtqRhw%2Fbrainstorming-additional-ai-risk-reduction-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brainstorming%20additional%20AI%20risk%20reduction%20ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8HPsRYKE2pYHtqRhw%2Fbrainstorming-additional-ai-risk-reduction-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8HPsRYKE2pYHtqRhw%2Fbrainstorming-additional-ai-risk-reduction-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 295, "htmlBody": "<p>It looks as though lukeprog has finished his series&nbsp;on how to purchase AI risk reduction.&nbsp;But the ideas lukeprog shares are not the only available strategies.&nbsp;Can Less Wrong come up with more?</p>\n<p>A summary of recommendations from <a href=\"/lw/bjm/exploring_the_idea_space_efficiently/\">Exploring the Idea Space Efficiently</a>:</p>\n<ul>\n<li>Deliberately avoid exposing yourself to existing lines of thought on how to solve a problem. (The idea here is to defeat anchoring and the availability heuristic.) So <strong>don't review lukeprog's series or read the comments on this thread before generating ideas</strong>.</li>\n<li>Start by identifying broad categories where ideas might be found.&nbsp;If you're trying to think of calculus word problems, your broad categories might be \"jobs, personal life, the natural world, engineering, other\".</li>\n<li>With these initial broad categories, try to include all the categories that might contain a solution and none that will not.</li>\n<li>Then generate&nbsp;subcategories. Subcategories of \"jobs\" might include \"agriculture, teaching, customer service, manufacturing, research, IT, other\". You're also encouraged to&nbsp;generate subsubcategories and so on.</li>\n<li>Spend more time on those categories that seem promising.</li>\n<li>You may wish to map your categories and subcategories on a piece of paper.</li>\n</ul>\n<div>If you don't like that approach, <a href=\"/lw/cja/how_to_brainstorm_effectively/\">here's another</a> that's more difficult to summarize.&nbsp;Of course, unstructured idea generation is fine too.</div>\n<p>If you're strictly a lurker, you can send your best ideas to lukeprog anonymously using&nbsp;<a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dG1tUklRelVZdkJscThTV3dKZ1NJb0E6MQ\">his feedback box</a>. Or send them to me anonymously using&nbsp;<a href=\"http://www.admonymous.com/johnmaxwell\">my feedback box</a>&nbsp;so I can post them here and get all your karma.</p>\n<h3>Thread Usage</h3>\n<p>Please&nbsp;<a href=\"/r/discussion/lw/d34/brainstorming_additional_ideas_on_how_to_purchase/6tr1\">reply here</a>&nbsp;if you wish to comment on the idea of this thread.</p>\n<p>You're encouraged to discuss the ideas of others in addition to coming up with your own ideas.</p>\n<p>If you split your ideas into individual comments, they can be voted on individually and you will probably increase your karma haul.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZFrgTgzwEfStg26JL": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8HPsRYKE2pYHtqRhw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 9.223573005447893e-07, "legacy": true, "legacyId": "16960", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7PuMfJAAaQwaPWAZY", "PrPk5ogJNK7QcPead"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T11:01:21.335Z", "modifiedAt": null, "url": null, "title": "Wanted: \"The AIs will need humans\" arguments", "slug": "wanted-the-ais-will-need-humans-arguments", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.724Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2yfAPHbwJNG8bNM4k/wanted-the-ais-will-need-humans-arguments", "pageUrlRelative": "/posts/2yfAPHbwJNG8bNM4k/wanted-the-ais-will-need-humans-arguments", "linkUrl": "https://www.lesswrong.com/posts/2yfAPHbwJNG8bNM4k/wanted-the-ais-will-need-humans-arguments", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wanted%3A%20%22The%20AIs%20will%20need%20humans%22%20arguments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWanted%3A%20%22The%20AIs%20will%20need%20humans%22%20arguments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yfAPHbwJNG8bNM4k%2Fwanted-the-ais-will-need-humans-arguments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wanted%3A%20%22The%20AIs%20will%20need%20humans%22%20arguments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yfAPHbwJNG8bNM4k%2Fwanted-the-ais-will-need-humans-arguments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yfAPHbwJNG8bNM4k%2Fwanted-the-ais-will-need-humans-arguments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>As <a href=\"/lw/cr6/building_the_ai_risk_research_community/\">Luke mentioned</a>, I am in the process of writing \"<strong>Responses to Catastrophic AGI Risk</strong>\": A journal-bound  summary of the AI risk problem, and a taxonomy of the societal proposals  (e.g. denial of the risk, no action, legal and economic controls,  differential technological development) and AI design proposals (e.g. AI  confinement, chaining, Oracle AI, FAI) that have been made.</p>\n<p>One of the categories is \"They Will Need Us\" - claims that AI is no big risk, because AI will always have a need of something that humans have, and that they will therefore preserve us. Currently this section is pretty empty:</p>\n<blockquote>\n<p><span id=\"internal-source-marker_0.8530873957221754\" style=\"font-size: 16px; font-family: Garamond; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Supporting  a mutually beneficial legal or economic arrangement is the view that  AGIs will need humans. For example, Butler (1863) argues that machines  will need us to help them reproduce, and Lucas (1961) suggests that  machines could never show G&ouml;delian sentences true, though humans can see  them as true.</span></p>\n</blockquote>\n<p>But I'm certain that I've heard this claim made more often than in just those two sources. Does anyone remember having seen such arguments somewhere else? While \"academically reputable\" sources (papers, books) are preferred, blog posts and websites are fine as well.</p>\n<p>Note that this claim is distinct from the claim that (due to general economic theory) it's more beneficial for the AIs to trade with us than to destroy us. We already have enough citations for that argument, what we're looking for are arguments saying that destroying humans would mean losing something essentially irreplaceable.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2yfAPHbwJNG8bNM4k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 9.224418426809635e-07, "legacy": true, "legacyId": "16966", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 83, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kkkCuCLiT8G9gDEJK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T11:37:21.652Z", "modifiedAt": null, "url": null, "title": "AGI Impacts conference in Oxford in December, with Call for Papers", "slug": "agi-impacts-conference-in-oxford-in-december-with-call-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XTfPjyJzot4FkQmgX/agi-impacts-conference-in-oxford-in-december-with-call-for", "pageUrlRelative": "/posts/XTfPjyJzot4FkQmgX/agi-impacts-conference-in-oxford-in-december-with-call-for", "linkUrl": "https://www.lesswrong.com/posts/XTfPjyJzot4FkQmgX/agi-impacts-conference-in-oxford-in-december-with-call-for", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AGI%20Impacts%20conference%20in%20Oxford%20in%20December%2C%20with%20Call%20for%20Papers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAGI%20Impacts%20conference%20in%20Oxford%20in%20December%2C%20with%20Call%20for%20Papers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXTfPjyJzot4FkQmgX%2Fagi-impacts-conference-in-oxford-in-december-with-call-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AGI%20Impacts%20conference%20in%20Oxford%20in%20December%2C%20with%20Call%20for%20Papers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXTfPjyJzot4FkQmgX%2Fagi-impacts-conference-in-oxford-in-december-with-call-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXTfPjyJzot4FkQmgX%2Fagi-impacts-conference-in-oxford-in-december-with-call-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 490, "htmlBody": "<p>From the 8th to the 12th of December, the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> will be hosting the <a href=\"http://www.winterintelligence.org/oxford2012/\">Winter Intelligence Multi-Conference</a>, a dual conference including <a href=\"http://www.winterintelligence.org/oxford2012/agi-12/\">AGI-12</a> (the <a href=\"http://agi-conf.org/2012/\">Fifth Conference on Artificial General Intelligence</a>), followed by the <a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/\">AGI impacts conference</a>. Of great relevance to the people on Less Wrong, the impacts conference will about the safety, risks and impacts of AGI, and how best to prepare now for these challenges.</p>\n<p>The conference now has a&nbsp;<a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/cfp/ \">Call for Papers</a>, with an associated prize offered by the <a href=\"http://intelligence.org/\">Singularity&nbsp;Institute</a>. Please&nbsp;publicise&nbsp;on any relevant places:</p>\n<p>&nbsp;</p>\n<h1>'IMPACTS AND RISKS OF ARTIFICIAL GENERAL INTELLIGENCE'</h1>\n<h2>AGI Impacts, 10-11.12.2012, OXFORD</h2>\n<p>The first conference on the <a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/\">Impacts and Risks of Artificial General Intelligence</a> will take place at the University of Oxford, St. Anne&rsquo;s College, on December 10th and 11th, 2012 &ndash; immediately following the fifth annual conference on Artificial General Intelligence <a href=\"http://agi-conf.org/2012\">AGI-12</a>. AGI-Impacts is organized by the &ldquo;<a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a>&rdquo; (FHI) at Oxford University through its &ldquo;Programme on the Impacts of Future Technology&rdquo;. The two events form the&nbsp;<a href=\"http://www.winterintelligence.org/oxford2012/\">Winter Intelligence Multi-Conference</a>&nbsp;2012, hosted by FHI.</p>\n<p><a id=\"more\"></a>The conference will explore questions such as: How can we best predict the impact of future intelligent and superintelligent machines? How can we combine ideas from computer science, mathematics and philosophy to best estimate this impact? What will be the impacts of AGI on the world? Which directions of research should be most explored, and which should be de-emphasized or avoided? &nbsp; What can we do to best ensure scientific rigour in this non-experimental academic field? What are the best ideas and methods for ensuring both safety and predictability of advanced AI systems? Can we lay the foundations to a field of rigorous study of realistic AGI control methods that lead to implementable security protocols?</p>\n<p>The scope is wide, but all <a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/cfp/\">papers</a> are expected to be of high quality and with the maximal amount of rigour that is possible for the subject area.</p>\n<h2>SUBMISSION &ndash; PUBLICATION - PRIZE</h2>\n<p>Please submit your paper to our <a href=\"http://www.easychair.org/conferences/?conf=agiimpacts2012\">easychair conference website</a>.&nbsp;Submit an abstract of ca. 400-800 words, with author contact information by August 31st, 2012.&nbsp;A selection of papers from AGI-Impacts 2012 will be published in a special volume of the '<a href=\"http://www.tandf.co.uk/journals/journal.asp?issn=1362-3079&amp;linktype=145\">Journal of Experimental &amp; Theoretical Artificial Intelligence</a>' in 2013.</p>\n<p>Attendees at this conference and at the AGI-12 conference are eligible for the '<a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/cfp/#prize\">2012 Turing Prize for Best AGI Safety Paper</a>': The <a href=\"http://intelligence.org/\">Singularity Institute</a> sponsors a $1000 prize for a paper or contribution to 'AGI-12' and 'AGI-Impacts 2012' in recognition of exceptional research on the question of how to develop safe architectures or goals for artificial general intelligence.</p>\n<h2>DATES:</h2>\n<ul>\n<li>July 30th, 2012: Deadline for the proposal of workshops</li>\n<li>August 31st, 2012: Deadline for paper submissions</li>\n<li>September 30th, 2012: Notification of paper acceptance or rejection</li>\n<li>Dec 10th and 11th, 2012: AGI-Impacts Conference</li>\n</ul>\n<h2>ORGANISERS:</h2>\n<p><a href=\"http://www.futuretech.ox.ac.uk/\">Programme on the Impacts of Future Technology</a>, <a href=\"http://www.oxfordmartin.ox.ac.uk/\">Oxford Martin School</a>, <a href=\"http://www.ox.ac.uk/\">University of Oxford</a>:</p>\n<ul>\n<li><a href=\"http://www.fhi.ox.ac.uk/our_staff/research/stuart_armstrong\">Stuart Armstrong</a></li>\n<li><a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a></li>\n<li><a href=\"http://www.futuretech.ox.ac.uk/professor-vincent-c-m%C3%BCller\">Vincent C. M&uuml;ller</a></li>\n<li><a href=\"http://www.futuretech.ox.ac.uk/dr-se%C3%A1n-%C3%B3-h%C3%A9igeartaigh\">Se&aacute;n &Oacute; h&Eacute;igeartaigh</a></li>\n<li><a href=\"http://www.aleph.se/andart/\">Anders Sandberg</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XTfPjyJzot4FkQmgX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 9.224582391048254e-07, "legacy": true, "legacyId": "16965", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>From the 8th to the 12th of December, the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> will be hosting the <a href=\"http://www.winterintelligence.org/oxford2012/\">Winter Intelligence Multi-Conference</a>, a dual conference including <a href=\"http://www.winterintelligence.org/oxford2012/agi-12/\">AGI-12</a> (the <a href=\"http://agi-conf.org/2012/\">Fifth Conference on Artificial General Intelligence</a>), followed by the <a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/\">AGI impacts conference</a>. Of great relevance to the people on Less Wrong, the impacts conference will about the safety, risks and impacts of AGI, and how best to prepare now for these challenges.</p>\n<p>The conference now has a&nbsp;<a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/cfp/ \">Call for Papers</a>, with an associated prize offered by the <a href=\"http://intelligence.org/\">Singularity&nbsp;Institute</a>. Please&nbsp;publicise&nbsp;on any relevant places:</p>\n<p>&nbsp;</p>\n<h1 id=\"_IMPACTS_AND_RISKS_OF_ARTIFICIAL_GENERAL_INTELLIGENCE_\">'IMPACTS AND RISKS OF ARTIFICIAL GENERAL INTELLIGENCE'</h1>\n<h2 id=\"AGI_Impacts__10_11_12_2012__OXFORD\">AGI Impacts, 10-11.12.2012, OXFORD</h2>\n<p>The first conference on the <a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/\">Impacts and Risks of Artificial General Intelligence</a> will take place at the University of Oxford, St. Anne\u2019s College, on December 10th and 11th, 2012 \u2013 immediately following the fifth annual conference on Artificial General Intelligence <a href=\"http://agi-conf.org/2012\">AGI-12</a>. AGI-Impacts is organized by the \u201c<a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a>\u201d (FHI) at Oxford University through its \u201cProgramme on the Impacts of Future Technology\u201d. The two events form the&nbsp;<a href=\"http://www.winterintelligence.org/oxford2012/\">Winter Intelligence Multi-Conference</a>&nbsp;2012, hosted by FHI.</p>\n<p><a id=\"more\"></a>The conference will explore questions such as: How can we best predict the impact of future intelligent and superintelligent machines? How can we combine ideas from computer science, mathematics and philosophy to best estimate this impact? What will be the impacts of AGI on the world? Which directions of research should be most explored, and which should be de-emphasized or avoided? &nbsp; What can we do to best ensure scientific rigour in this non-experimental academic field? What are the best ideas and methods for ensuring both safety and predictability of advanced AI systems? Can we lay the foundations to a field of rigorous study of realistic AGI control methods that lead to implementable security protocols?</p>\n<p>The scope is wide, but all <a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/cfp/\">papers</a> are expected to be of high quality and with the maximal amount of rigour that is possible for the subject area.</p>\n<h2 id=\"SUBMISSION___PUBLICATION___PRIZE\">SUBMISSION \u2013 PUBLICATION - PRIZE</h2>\n<p>Please submit your paper to our <a href=\"http://www.easychair.org/conferences/?conf=agiimpacts2012\">easychair conference website</a>.&nbsp;Submit an abstract of ca. 400-800 words, with author contact information by August 31st, 2012.&nbsp;A selection of papers from AGI-Impacts 2012 will be published in a special volume of the '<a href=\"http://www.tandf.co.uk/journals/journal.asp?issn=1362-3079&amp;linktype=145\">Journal of Experimental &amp; Theoretical Artificial Intelligence</a>' in 2013.</p>\n<p>Attendees at this conference and at the AGI-12 conference are eligible for the '<a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/cfp/#prize\">2012 Turing Prize for Best AGI Safety Paper</a>': The <a href=\"http://intelligence.org/\">Singularity Institute</a> sponsors a $1000 prize for a paper or contribution to 'AGI-12' and 'AGI-Impacts 2012' in recognition of exceptional research on the question of how to develop safe architectures or goals for artificial general intelligence.</p>\n<h2 id=\"DATES_\">DATES:</h2>\n<ul>\n<li>July 30th, 2012: Deadline for the proposal of workshops</li>\n<li>August 31st, 2012: Deadline for paper submissions</li>\n<li>September 30th, 2012: Notification of paper acceptance or rejection</li>\n<li>Dec 10th and 11th, 2012: AGI-Impacts Conference</li>\n</ul>\n<h2 id=\"ORGANISERS_\">ORGANISERS:</h2>\n<p><a href=\"http://www.futuretech.ox.ac.uk/\">Programme on the Impacts of Future Technology</a>, <a href=\"http://www.oxfordmartin.ox.ac.uk/\">Oxford Martin School</a>, <a href=\"http://www.ox.ac.uk/\">University of Oxford</a>:</p>\n<ul>\n<li><a href=\"http://www.fhi.ox.ac.uk/our_staff/research/stuart_armstrong\">Stuart Armstrong</a></li>\n<li><a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a></li>\n<li><a href=\"http://www.futuretech.ox.ac.uk/professor-vincent-c-m%C3%BCller\">Vincent C. M\u00fcller</a></li>\n<li><a href=\"http://www.futuretech.ox.ac.uk/dr-se%C3%A1n-%C3%B3-h%C3%A9igeartaigh\">Se\u00e1n \u00d3 h\u00c9igeartaigh</a></li>\n<li><a href=\"http://www.aleph.se/andart/\">Anders Sandberg</a></li>\n</ul>", "sections": [{"title": "'IMPACTS AND RISKS OF ARTIFICIAL GENERAL INTELLIGENCE'", "anchor": "_IMPACTS_AND_RISKS_OF_ARTIFICIAL_GENERAL_INTELLIGENCE_", "level": 1}, {"title": "AGI Impacts, 10-11.12.2012, OXFORD", "anchor": "AGI_Impacts__10_11_12_2012__OXFORD", "level": 2}, {"title": "SUBMISSION \u2013 PUBLICATION - PRIZE", "anchor": "SUBMISSION___PUBLICATION___PRIZE", "level": 2}, {"title": "DATES:", "anchor": "DATES_", "level": 2}, {"title": "ORGANISERS:", "anchor": "ORGANISERS_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T13:00:32.607Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Meetup", "slug": "meetup-helsinki-meetup-7", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.000Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mgYo8fr2k2HRBqdGa/meetup-helsinki-meetup-7", "pageUrlRelative": "/posts/mgYo8fr2k2HRBqdGa/meetup-helsinki-meetup-7", "linkUrl": "https://www.lesswrong.com/posts/mgYo8fr2k2HRBqdGa/meetup-helsinki-meetup-7", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmgYo8fr2k2HRBqdGa%2Fmeetup-helsinki-meetup-7%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmgYo8fr2k2HRBqdGa%2Fmeetup-helsinki-meetup-7", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmgYo8fr2k2HRBqdGa%2Fmeetup-helsinki-meetup-7", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/b6'>Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 June 2012 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Caf\u00e9 Vanha, Mannerheimintie 3, Helsinki, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>User:TCB is visiting Helsinki in June. Since it's nice to have various LWers get to know each other, there will be a meetup at Vanhan Kuppila / Caf\u00e9 Vanha with her as the special guest star. If we get tired of just hanging out and talking about random stuff, we can try something fun from the Meetup Booklet. Kaj will bring Zendo equipment.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/b6'>Helsinki Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mgYo8fr2k2HRBqdGa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.22496121569627e-07, "legacy": true, "legacyId": "16967", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup\">Discussion article for the meetup : <a href=\"/meetups/b6\">Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 June 2012 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Caf\u00e9 Vanha, Mannerheimintie 3, Helsinki, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>User:TCB is visiting Helsinki in June. Since it's nice to have various LWers get to know each other, there will be a meetup at Vanhan Kuppila / Caf\u00e9 Vanha with her as the special guest star. If we get tired of just hanging out and talking about random stuff, we can try something fun from the Meetup Booklet. Kaj will bring Zendo equipment.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/b6\">Helsinki Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T15:25:13.736Z", "modifiedAt": null, "url": null, "title": "Organic food, conventional food", "slug": "organic-food-conventional-food", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:33.835Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PkNtkLLyuSbmdBEKc/organic-food-conventional-food", "pageUrlRelative": "/posts/PkNtkLLyuSbmdBEKc/organic-food-conventional-food", "linkUrl": "https://www.lesswrong.com/posts/PkNtkLLyuSbmdBEKc/organic-food-conventional-food", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Organic%20food%2C%20conventional%20food&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOrganic%20food%2C%20conventional%20food%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkNtkLLyuSbmdBEKc%2Forganic-food-conventional-food%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Organic%20food%2C%20conventional%20food%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkNtkLLyuSbmdBEKc%2Forganic-food-conventional-food", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkNtkLLyuSbmdBEKc%2Forganic-food-conventional-food", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 215, "htmlBody": "<p>I've been wondering whether there's any solid evidence that organic food is healthier than conventionally produced food-- the arguments I've seen on the subject have been theoretical/aesthetic.</p>\n<p>I'm interested in anything in the range from personal stories to scientific studies, but would prefer to avoid extremely general arguments or claims that people who prefer one or the other are demonstrating character defects.</p>\n<p><strong>Edited to add:</strong>&nbsp;Thanks for the replies. I'm hoping for experiments which test the effects of food produced in various ways on organisms, especially multi-cellular organisms. It was interesting to find out that plants which have to fight off insects for themselves have more mutagens.</p>\n<p>The mutation experiment is very cute, but it leaves out the possibility of damage that isn't related to mutation-- for example, hormonal effects. Also, if it's done on a standard bacteria acquiring the ability to make a particular nutrient, this might not be a test for mutation in general.</p>\n<p>I realize experiments on organic vs. conventional would be difficult, especially if you're tracking human health. It would be very hard to avoid confounding factors like other lifestyle factors and income.</p>\n<p>\"Conventional food\" is actually a large blob of a concept-- different pesticides, fertilizers, etc. are used on different foods at different times, so finding out what people are actually exposed to would be difficult.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PkNtkLLyuSbmdBEKc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 9.225620198362278e-07, "legacy": true, "legacyId": "16968", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T19:05:58.951Z", "modifiedAt": null, "url": null, "title": "Brainstorming help request: teaching rationality basics in an RPG setting", "slug": "brainstorming-help-request-teaching-rationality-basics-in-an", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.593Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MarkusRamikin", "createdAt": "2011-06-08T11:45:41.506Z", "isAdmin": false, "displayName": "MarkusRamikin"}, "userId": "eiLDv2Z6N8zDn87mK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K9Ja44YrGD7NRZ5XF/brainstorming-help-request-teaching-rationality-basics-in-an", "pageUrlRelative": "/posts/K9Ja44YrGD7NRZ5XF/brainstorming-help-request-teaching-rationality-basics-in-an", "linkUrl": "https://www.lesswrong.com/posts/K9Ja44YrGD7NRZ5XF/brainstorming-help-request-teaching-rationality-basics-in-an", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brainstorming%20help%20request%3A%20teaching%20rationality%20basics%20in%20an%20RPG%20setting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrainstorming%20help%20request%3A%20teaching%20rationality%20basics%20in%20an%20RPG%20setting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9Ja44YrGD7NRZ5XF%2Fbrainstorming-help-request-teaching-rationality-basics-in-an%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brainstorming%20help%20request%3A%20teaching%20rationality%20basics%20in%20an%20RPG%20setting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9Ja44YrGD7NRZ5XF%2Fbrainstorming-help-request-teaching-rationality-basics-in-an", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9Ja44YrGD7NRZ5XF%2Fbrainstorming-help-request-teaching-rationality-basics-in-an", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 528, "htmlBody": "<p>EDIT: Minor updates happened.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>I'd like to ask you all for thoughts on a certain idea I'm toying with. Especially any of you who are familiar with the Wheel of Time fantasy series by Robert Jordan.<br /><br />I play a <a href=\"http://wotmud.org/\">MUD</a> (multi-user dungeon, basically a text-based MMORPG), based on that series. One of my characters is a member of the White Tower, which is basically a mage organisation/school, and as part of our roleplay activities we sometimes hold classes (<a href=\"http://wtclasses.wikispaces.com/Art+of+Escape\">example</a>, long, probably not worth your time) for lower rank members. These typically last an hour or two and sometimes get used to convey interesting real life knowledge. For instance there has been a class on mnemonic techniques.<br /><br />I see an opportunity to spread rationality a little. One of the Ajah (subdivisions) of the Tower is specifically concerned with pursuing truth, logic etc. which means if I joined it, I would have no trouble teaching a class or two with some material from the Sequences. I wonder if any of us here have done things like that in the past?<br /><br />What sort of essentials would you pack into a class or at most a few classes 1-2 hours each (not just me reading stuff out but including a discussion), for people without technical backgrounds? Conducted at typing speed, so basically imagine you're going to spend two hours talking to 3-6 people about rationality on IRC chat or some such setting.<br /><br />Also, should I involve or steer away from the metaphysics of the Wheel of Time setting (the Creator/Dark One, the Pattern etc)?<br /><br />My ideas so far:<br /><br />Part 1: \"Cognitive biases, or why you, yes you, are an idiot\".<br />- which ones would be most interesting/simple/useful to teach about?<br />- Obviously i need to start with how knowing about biases can hurt you...<br />- Confirmation bias: I might try the 2-4-6 game, though it'll be a bit of a mess in a group setting.<br />- what other biases and examples would you use?</p>\n<p>Part 2: Truth and evidence<br />- truth, map/territory<br />- what is evidence<br />- rational evidence vs other kinds of evidence<br />- what is not evidence (instead of <a href=\"/lw/1ww/undiscriminating_skepticism/\">UFO cults</a> I'd speak of False Dragon followers)</p>\n<p>A question I anticipate coming up: Is there rational evidence for the Creator/Dark One/the Pattern? Ideas for handling this needed.<br />Note: I am NOT aiming at atheism at all costs, like a <a href=\"http://www.rogermwilcox.com/force_skeptics.html\">Force Skeptic</a> approach. It's neither very rational if we're in WoT, nor practical for my character. In fact I intend to not talk about religion if possible. Wrong setting, wrong audience for that.</p>\n<p>Part 3: Bayes' theorem<br />- the <a href=\"http://stattrek.com/probability/bayes-theorem.aspx\">wedding in the desert example</a> looks easily adaptable (Aiel!)<br />- more examples of practical Bayes Theorem application needed!<br /><br />Or is the very idea of teaching Bayes in such a setting an outrageous underestimation of the inferential distance?<br /><br />So yeah. Any ideas or advice that might help me give this shape and make it interesting and successful would be appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K9Ja44YrGD7NRZ5XF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 9.226625800401064e-07, "legacy": true, "legacyId": "16969", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Jko7pt7MwwTBrfG3A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-14T20:18:33.257Z", "modifiedAt": null, "url": null, "title": "How confident is your atheism?", "slug": "how-confident-is-your-atheism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:04.578Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "r_claypool", "createdAt": "2011-02-13T13:43:23.915Z", "isAdmin": false, "displayName": "r_claypool"}, "userId": "afmiWkqiSCpzHD4Xd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ihuhk8P5YLEScpK8y/how-confident-is-your-atheism", "pageUrlRelative": "/posts/ihuhk8P5YLEScpK8y/how-confident-is-your-atheism", "linkUrl": "https://www.lesswrong.com/posts/ihuhk8P5YLEScpK8y/how-confident-is-your-atheism", "postedAtFormatted": "Thursday, June 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20confident%20is%20your%20atheism%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20confident%20is%20your%20atheism%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fihuhk8P5YLEScpK8y%2Fhow-confident-is-your-atheism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20confident%20is%20your%20atheism%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fihuhk8P5YLEScpK8y%2Fhow-confident-is-your-atheism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fihuhk8P5YLEScpK8y%2Fhow-confident-is-your-atheism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 305, "htmlBody": "<p>A friend recently asked how strongly I believe that my deconversion from Christianity was not a mistake. Here's my response, and for those of you who are not Christians, I'm just wondering what numbers you would give:</p>\n<hr />\n<p>\"There is a part of me that wants to say the chance is far less than 1 percent. But when I consider what 1% must mean about my ability to follow complex arguments and base my judgement on the right premises, it seems absurd to say that.</p>\n<p>Trying to honestly estimate the chance that I'm wrong about the Bible being generally reliable is a fascinating exercise... I know the number is low, but I'm not sure how low. &nbsp;</p>\n<p>Today I would give myself a 1 in 20 chance of being wrong. If I were to consider the arguments of 20 other groups similar to Christian theologians, I would probably misunderstand them at least 1 time in 20. After talking with 20 groups that have a very different worldview, I might think they are all are mistaken, but once in a while, maybe 5% of the time, it would actually be me.</p>\n<p>Wow, 5%!?! If I convert that into <em>\"There is a 5% probability that the God of the Bible exists and will send me to hell\"</em>, I feel scared. But I know how to cheer myself up: I just say, <em>\"No way, the chance I'll end up in hell MUST be less than 5%. After all, the God of the Bible is CLEARLY just a big,&nbsp;<a title=\"Thank you Jesus!\" href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Thank-you-Jesus.png\">mean</a> alpha-monkey and... [<a title=\"One Argument Against an Army\" href=\"/lw/ik/one_argument_against_an_army/\">rehearse all the atheistic arguments here</a>]\"</em>.</p>\n<p>This back-and-forth from&nbsp;certainty&nbsp;to&nbsp;uncertainty&nbsp;makes me feel like I'm doing something seriously&nbsp;wrong.</p>\n<p>So what about you? What chance do you place on some variant of&nbsp;Christianity&nbsp;turning up to be true,&nbsp;and what chance do you think a god of some sort exists?\"</p>\n<p>Numbers please.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ihuhk8P5YLEScpK8y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 16, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "16970", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 150, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WN73eiLQkuDtSC8Ag"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-15T00:45:13.765Z", "modifiedAt": null, "url": null, "title": "What's the best way to rest?", "slug": "what-s-the-best-way-to-rest", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.481Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cSgtgqwt7DGMWdBBN/what-s-the-best-way-to-rest", "pageUrlRelative": "/posts/cSgtgqwt7DGMWdBBN/what-s-the-best-way-to-rest", "linkUrl": "https://www.lesswrong.com/posts/cSgtgqwt7DGMWdBBN/what-s-the-best-way-to-rest", "postedAtFormatted": "Friday, June 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What's%20the%20best%20way%20to%20rest%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat's%20the%20best%20way%20to%20rest%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcSgtgqwt7DGMWdBBN%2Fwhat-s-the-best-way-to-rest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What's%20the%20best%20way%20to%20rest%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcSgtgqwt7DGMWdBBN%2Fwhat-s-the-best-way-to-rest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcSgtgqwt7DGMWdBBN%2Fwhat-s-the-best-way-to-rest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>So, it's well-known (or, at least, oft-thought) that you can't just work 16 hours a day; if you want to get stuff done, you need to <em>rest</em>&nbsp;from time to time. You have to take breaks.</p>\n<p>Today, just now, I realized that I don't really know what the best way to rest is. If I want to rest, should I do something that's fun and interesting, like reading a fantasy novel? Should I do something that's&nbsp;<em>boring</em>, like building roads in Minecraft, so that work will seem comparatively interesting when I get back? (And besides, reading a fantasy novel is a bit of a challenge, at least for me, whereas building roads in Minecraft is trivial.) Physical activity probably helps, but how much? Do relaxing activities, like taking a warm shower, help more than just sitting there? How can I tell when it's time to take a break? How can I tell when it's time to get back to work?</p>\n<p>(Suggestions for tags would be appreciated.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "udPbn9RthmgTtHMiG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cSgtgqwt7DGMWdBBN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 35, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "16971", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-15T04:45:10.875Z", "modifiedAt": null, "url": null, "title": "Open Thread, June 16-30, 2012", "slug": "open-thread-june-16-30-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.785Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pqDCfahoRfshQkW98/open-thread-june-16-30-2012", "pageUrlRelative": "/posts/pqDCfahoRfshQkW98/open-thread-june-16-30-2012", "linkUrl": "https://www.lesswrong.com/posts/pqDCfahoRfshQkW98/open-thread-june-16-30-2012", "postedAtFormatted": "Friday, June 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20June%2016-30%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20June%2016-30%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpqDCfahoRfshQkW98%2Fopen-thread-june-16-30-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20June%2016-30%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpqDCfahoRfshQkW98%2Fopen-thread-june-16-30-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpqDCfahoRfshQkW98%2Fopen-thread-june-16-30-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pqDCfahoRfshQkW98", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 9.229265154883429e-07, "legacy": true, "legacyId": "16973", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 350, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-15T05:08:09.954Z", "modifiedAt": null, "url": null, "title": "[Cryonics News] Australian cryonics startup: Stasis Systems Australia update", "slug": "cryonics-news-australian-cryonics-startup-stasis-systems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:09.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pEqcTWkdmQZyYDbTg/cryonics-news-australian-cryonics-startup-stasis-systems", "pageUrlRelative": "/posts/pEqcTWkdmQZyYDbTg/cryonics-news-australian-cryonics-startup-stasis-systems", "linkUrl": "https://www.lesswrong.com/posts/pEqcTWkdmQZyYDbTg/cryonics-news-australian-cryonics-startup-stasis-systems", "postedAtFormatted": "Friday, June 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BCryonics%20News%5D%20Australian%20cryonics%20startup%3A%20Stasis%20Systems%20Australia%20update&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BCryonics%20News%5D%20Australian%20cryonics%20startup%3A%20Stasis%20Systems%20Australia%20update%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpEqcTWkdmQZyYDbTg%2Fcryonics-news-australian-cryonics-startup-stasis-systems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BCryonics%20News%5D%20Australian%20cryonics%20startup%3A%20Stasis%20Systems%20Australia%20update%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpEqcTWkdmQZyYDbTg%2Fcryonics-news-australian-cryonics-startup-stasis-systems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpEqcTWkdmQZyYDbTg%2Fcryonics-news-australian-cryonics-startup-stasis-systems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 338, "htmlBody": "<p>Potentially of interest to my fellow antipodean LessWrongers. <a title=\"Stasis Systems Australia\" href=\"http://www.stasissystemsaustralia.com\" target=\"_blank\">Stasis Systems Australia</a>&nbsp;is a company seeking to start a cryonics facility in Australia. Their website is pretty sparse, but they just sent out a mailing list update on how they are going, and it doesn't appear that the information therein is located in any news section of the website, so I thought I'd post it here.</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>Hello,</p>\n<p>We at&nbsp; <a href=\"http://www.stasissystemsaustralia.com\">Stasis Systems Australia Ltd</a>&nbsp;are happy to report our plans to build a cryonics facility in Australia are progressing well.</p>\n<p>You may remember contacting us or attending our online meeting last year, but here's a quick reminder of what we're doing.</p>\n<p>We are a group of Australasian cryonicists putting together a non-for-profit organisation to build and run the first cryonic storage facility in the southern hemisphere.</p>\n<p>We're proud to have WA-based Marta Sandberg on the board of directors as an advisor, as she has a wealth of knowledge and experience from her ongoing role as a director of the &nbsp;well-established Cryonics Institute.</p>\n<p>We are now officially incorporated as a not-for-profit company, and one investor away from the magic number of ten that will trigger the next stage of the project - selecting a piece of land and starting construction!</p>\n<p>We have had productive discussions with the NSW Department of Health, and are developing positive relationships with the Cryonics Institute, Alcor, and KrioRus.</p>\n<p>We think what we&rsquo;re doing is worthwhile and in the long term will be of great benefit to the Australasian community. &nbsp;If you&rsquo;d like to get involved either as an investor or a volunteer, that would be fantastic. &nbsp;We&rsquo;d love help with articles for the website, search engine optimisation, web graphics, or any other skill you have that we might need.</p>\n<p>We would especially appreciate you passing this update on to anyone you know who might be interested.</p>\n<p>&nbsp;</p>\n<p>Best regards,</p>\n<p>The Stasis Systems Australia team</p>\n<p><a href=\"http://www.stasissystemsaustralia.com\">www.StasisSystemsAustralia.com</a></p>\n<p>&nbsp;</p>\n<p>Follow <a href=\"https://twitter.com/#%21/StaSysAus\">@StaSysAus</a> on Twitter</p>\n<p>Like <a href=\"http://www.facebook.com/StaSysAus\">StaSysAus</a> on Facebook</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pEqcTWkdmQZyYDbTg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 9.229369921209766e-07, "legacy": true, "legacyId": "16974", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-15T05:15:14.210Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] No Universally Compelling Arguments", "slug": "seq-rerun-no-universally-compelling-arguments", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JhhGpriY6cFxzJSon/seq-rerun-no-universally-compelling-arguments", "pageUrlRelative": "/posts/JhhGpriY6cFxzJSon/seq-rerun-no-universally-compelling-arguments", "linkUrl": "https://www.lesswrong.com/posts/JhhGpriY6cFxzJSon/seq-rerun-no-universally-compelling-arguments", "postedAtFormatted": "Friday, June 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20No%20Universally%20Compelling%20Arguments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20No%20Universally%20Compelling%20Arguments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJhhGpriY6cFxzJSon%2Fseq-rerun-no-universally-compelling-arguments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20No%20Universally%20Compelling%20Arguments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJhhGpriY6cFxzJSon%2Fseq-rerun-no-universally-compelling-arguments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJhhGpriY6cFxzJSon%2Fseq-rerun-no-universally-compelling-arguments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>Today's post, <a href=\"/lw/rn/no_universally_compelling_arguments/\">No Universally Compelling Arguments</a> was originally published on 26 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#No_Universally_Compelling_Arguments\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Because minds are physical processes, it is theoretically possible to specify a mind which draws any conclusion in response to any argument. There is no argument that will convince every possible mind.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/d2q/seq_rerun_the_design_space_of_minds_in_general/\">The Design Space of Minds in General</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JhhGpriY6cFxzJSon", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.229402151470939e-07, "legacy": true, "legacyId": "16975", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PtoQdG7E8MxYJrigu", "SbYk376L5upoKYinp", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-15T06:42:23.289Z", "modifiedAt": null, "url": null, "title": "Beer with Charlie Stross in Munich", "slug": "beer-with-charlie-stross-in-munich", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:07.715Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mapnoterritory", "createdAt": "2012-03-14T20:44:02.961Z", "isAdmin": false, "displayName": "mapnoterritory"}, "userId": "EvXiCAYEyRqnXMWAD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8FHS7o9ZfgSsgRr2z/beer-with-charlie-stross-in-munich", "pageUrlRelative": "/posts/8FHS7o9ZfgSsgRr2z/beer-with-charlie-stross-in-munich", "linkUrl": "https://www.lesswrong.com/posts/8FHS7o9ZfgSsgRr2z/beer-with-charlie-stross-in-munich", "postedAtFormatted": "Friday, June 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beer%20with%20Charlie%20Stross%20in%20Munich&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeer%20with%20Charlie%20Stross%20in%20Munich%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8FHS7o9ZfgSsgRr2z%2Fbeer-with-charlie-stross-in-munich%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beer%20with%20Charlie%20Stross%20in%20Munich%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8FHS7o9ZfgSsgRr2z%2Fbeer-with-charlie-stross-in-munich", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8FHS7o9ZfgSsgRr2z%2Fbeer-with-charlie-stross-in-munich", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p style=\"text-align: left;\">From <a href=\"http://www.antipope.org/charlie/blog-static/2012/06/beer-munich.html#comments\" target=\"_blank\">Charlie Stross' blog</a>:</p>\n<blockquote>\n<p style=\"text-align: left;\">I'm in Munich this week, and I plan to be drinking in the Paulaner Brauhaus(Kapuzinerplatz 5, 80337 M&uuml;nchen; click here for map) from 7pm on Monday 18th. All welcome! (Yes, I will sign books if you bring them.) If in doubt, look for the plush Cthulhu!</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8FHS7o9ZfgSsgRr2z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 4, "extendedScore": null, "score": 9.229799416259732e-07, "legacy": true, "legacyId": "16987", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-15T07:53:37.615Z", "modifiedAt": null, "url": null, "title": "Coursera Behavioural Neurology Course", "slug": "coursera-behavioural-neurology-course", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:41.630Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Patrick", "createdAt": "2009-02-27T08:09:44.663Z", "isAdmin": false, "displayName": "Patrick"}, "userId": "KC7mjSorWj2XsdL3v", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CFkqF55zusbmSuKng/coursera-behavioural-neurology-course", "pageUrlRelative": "/posts/CFkqF55zusbmSuKng/coursera-behavioural-neurology-course", "linkUrl": "https://www.lesswrong.com/posts/CFkqF55zusbmSuKng/coursera-behavioural-neurology-course", "postedAtFormatted": "Friday, June 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Coursera%20Behavioural%20Neurology%20Course&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACoursera%20Behavioural%20Neurology%20Course%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCFkqF55zusbmSuKng%2Fcoursera-behavioural-neurology-course%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Coursera%20Behavioural%20Neurology%20Course%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCFkqF55zusbmSuKng%2Fcoursera-behavioural-neurology-course", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCFkqF55zusbmSuKng%2Fcoursera-behavioural-neurology-course", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p>Coursera is running a course on behavioural neurology here: https://www.coursera.org/course/neurobehavior</p>\n<p>Any LWers that to want the associated study group should visit ##patrickclass on irc.freenode.net or email patrick.robotham2@gmail.com .</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CFkqF55zusbmSuKng", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 9.230124169399754e-07, "legacy": true, "legacyId": "16988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-15T14:21:46.036Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Melbourne, Phoenix, Sydney", "slug": "weekly-lw-meetups-melbourne-phoenix-sydney", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:04.180Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vLRHGbLAyKJ8mDqjM/weekly-lw-meetups-melbourne-phoenix-sydney", "pageUrlRelative": "/posts/vLRHGbLAyKJ8mDqjM/weekly-lw-meetups-melbourne-phoenix-sydney", "linkUrl": "https://www.lesswrong.com/posts/vLRHGbLAyKJ8mDqjM/weekly-lw-meetups-melbourne-phoenix-sydney", "postedAtFormatted": "Friday, June 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Melbourne%2C%20Phoenix%2C%20Sydney&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Melbourne%2C%20Phoenix%2C%20Sydney%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLRHGbLAyKJ8mDqjM%2Fweekly-lw-meetups-melbourne-phoenix-sydney%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Melbourne%2C%20Phoenix%2C%20Sydney%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLRHGbLAyKJ8mDqjM%2Fweekly-lw-meetups-melbourne-phoenix-sydney", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLRHGbLAyKJ8mDqjM%2Fweekly-lw-meetups-melbourne-phoenix-sydney", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 407, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/ao\">Less Wrong Sydney:&nbsp;<span class=\"date\">11 June 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/9p\">Phoenix, Arizona:&nbsp;<span class=\"date\">15 June 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/av\">Brussels meetup:&nbsp;<span class=\"date\">16 June 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/at\">Tucson, Arizona:&nbsp;<span class=\"date\">20 June 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/aq\">First Cali, Colombia meetup:&nbsp;<span class=\"date\">02 July 2012 07:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/as\">Melbourne social/games meetup:&nbsp;<span class=\"date\">15 June 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/ad\">Less Wrong Cambridge (MA) third-Sundays meetup:&nbsp;<span class=\"date\">17 June 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/ap\">Summer Festival Megameetup at NYC:&nbsp;<span class=\"date\">23 June 2012 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vLRHGbLAyKJ8mDqjM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.231893926937644e-07, "legacy": true, "legacyId": "16788", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-16T01:28:09.752Z", "modifiedAt": null, "url": null, "title": "Brief response to kalla724 on preserving personal identity with vitrification", "slug": "brief-response-to-kalla724-on-preserving-personal-identity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:25.479Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Synaptic", "createdAt": "2011-09-26T14:13:36.154Z", "isAdmin": false, "displayName": "Synaptic"}, "userId": "cXSPuYAf5pC9XTzcm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pbgjkw98k2SYxndWM/brief-response-to-kalla724-on-preserving-personal-identity", "pageUrlRelative": "/posts/Pbgjkw98k2SYxndWM/brief-response-to-kalla724-on-preserving-personal-identity", "linkUrl": "https://www.lesswrong.com/posts/Pbgjkw98k2SYxndWM/brief-response-to-kalla724-on-preserving-personal-identity", "postedAtFormatted": "Saturday, June 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brief%20response%20to%20kalla724%20on%20preserving%20personal%20identity%20with%20vitrification&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrief%20response%20to%20kalla724%20on%20preserving%20personal%20identity%20with%20vitrification%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbgjkw98k2SYxndWM%2Fbrief-response-to-kalla724-on-preserving-personal-identity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brief%20response%20to%20kalla724%20on%20preserving%20personal%20identity%20with%20vitrification%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbgjkw98k2SYxndWM%2Fbrief-response-to-kalla724-on-preserving-personal-identity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbgjkw98k2SYxndWM%2Fbrief-response-to-kalla724-on-preserving-personal-identity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 908, "htmlBody": "<p>About a month ago kalla724 posted a number of <a href=\"/r/discussion/lw/8f4/neil_degrasse_tyson_on_cryonics/\">comments on this post</a>, many of which were highly upvoted.</p>\n<p><strong>Synopsis</strong>: I) I think that&nbsp;kalla724 is too pessimistic about the practice of cryopreservation to preserve personal identity, because we don't know what level of synapse/active zone/protein structure is preserved in human brains, and we also don't know what level is required for personal identity. II) I think&nbsp;kalla724 is wrong about the required detail necessary to simulate a C. elegans. This is testable in the relatively near-term, and the results of that test might yield insight into whose argument in point I is stronger.&nbsp;</p>\n<p>I</p>\n<p>kalla724's main argument: it is not possible (p = 10^-22) that cryonics will preserve personal identity, because replacing water with cryoprotectant will cause too much damage to proteins and lipids in the brain.</p>\n<p>My view is that&nbsp;kalla724 is too pessimistic.&nbsp;To find a specific example to expand upon this intuition, I searched for \"c elegans memory\". I chose one of the first reviews in the results:&nbsp;http://learnmem.cshlp.org/content/17/4/191.full, published in 2010 by Ardiel and Rankin. Here's their first example:&nbsp;</p>\n<blockquote>\n<p>Rankin et al. (1990) were the first to characterize learning and memory in C. elegans. They studied plasticity of the &ldquo;tap-withdrawal response&rdquo; (TWR), a behavior whereby worms swim backward in response to a nonlocalized mechanical stimulus generated by tapping the Petri plate containing the worm. The magnitude of this reversal response is around 1 mm (roughly the length of the worm), but this can change with experience. Repeated administration of the tap results in a decrement of both the amplitude and the frequency of the response</p>\n</blockquote>\n<p>The specific neurons mediating this are known:&nbsp;</p>\n<blockquote>\n<p>Using the circuits described by Chalfie et al. (1985) in conjunction with the neural wiring diagram (White et al. 1986), Wicks and Rankin (1995) identified the mechanosensory cells (ALM, AVM, PLM, and PVD) and interneurons (AVD, AVA, AVB, PVC, and DVA) mediating the TWR.</p>\n</blockquote>\n<p>Through more science, they found that:&nbsp;</p>\n<blockquote>\n<p>the locus of mechanosensory habituation is in a part of the circuit unique to the TWR, i.e., the touch cells and/or the synapses between the touch cells and the interneurons. Now the hunt for the underlying molecular mechanism could begin.</p>\n</blockquote>\n<p>There is some evidence for how the short-term component of the tap-withdrawal response plasticity. This is it:&nbsp;</p>\n<blockquote>\n<p>repeated activation of the touch cells results in autophosphorylation of the SHW-3&ndash;MPS-1 complex, thus diminishing K+ flux and prolonging the duration of mechanoreceptor potentials. This would slow the recovery from inactivation of EGL-19 (the L-type calcium channel mediating touch-evoked calcium currents) (Suzuki et al. 2003) and dampen cell excitability</p>\n</blockquote>\n<p>This means that the complexes of proteins, working together, <a href=\"http://en.wikipedia.org/wiki/Autophosphorylation\">add phosphate groups to themselves</a>&nbsp;as a post-translational modification. Each individual complex functions as a potassium ion channel, so changing its structure can alter the excitability of the cell.</p>\n<p>Whether vitrification will preserve this specific post-translational modification is, as far as I know, an open question. The current cryoprotectant solution, M22, is pretty physiologic, which means that it functions similarly to water. But, we don't have this data.&nbsp;</p>\n<p>It's likely that when the protein complex undergoes autophosphorylation, other changes occur in the cell as well. If this led to changes in the cell's epigenome, which is very common, and the structure of the epigenome is retained by the cryopreservation, then the cell's epigenome could allow reverse inference of the state of its ion channels. We also don't have this data.&nbsp;</p>\n<p>The authors also discuss evidence for the long-term component of the tap-withdrawal response plasticity:&nbsp;</p>\n<blockquote>\n<p>the AMPA-type glutamate receptor subunit, GLR-1, was required for long-term habituation&mdash;glr-1 loss-of-function mutants habituated but did not retain decremented responses ...&nbsp;long-term habituation was associated with a significant reduction in the size, but not the number, of the GLR-1::GFP clusters in the posterior ventral nerve cord</p>\n</blockquote>\n<p>This means that the number and distribution of a well characterized protein at the synapses of cells is highly correlated with the strength of the memory. This is consistent with current paradigms of <a href=\"http://en.wikipedia.org/wiki/Long-term_memory#Biological_underpinnings_at_the_cellular_level\">long-term memory</a>.&nbsp;</p>\n<p>Under ideal cryopreservation conditions, synaptic vesicles and receptor distributions are <a href=\"http://chronopause.com/index.php/2011/07/30/science-fiction-double-feature/\">likely retained</a>, even if some of the proteins may be a bit denatured. The data is far from perfect here, either.&nbsp;</p>\n<p>It's also important to stress that this only occurs under ideal conditions. Given <a href=\"http://chronopause.com/index.php/2012/05/20/cryonics-intelligence-test-responses/\">the current practice of cryonics</a>, cryoprotectant will not reach many or most areas of the brain. In these cases, there is a large amount of ice damage and the information is much more likely to be irretrievable.&nbsp;</p>\n<p>II</p>\n<p>kalla724 <a href=\"/r/discussion/lw/8f4/neil_degrasse_tyson_on_cryonics/6l0k\">says</a>:</p>\n<blockquote>\n<p>Uploading a particular C. elegans, so that the simulation reflects learning and experiences of that particular animal? Orders of magnitude more difficult. Might be possible, if we have really good technology and are looking at the living animal.</p>\n</blockquote>\n<p>kalla724's&nbsp;requirement is that we look at <strong>live</strong>&nbsp;C. elegans to simulate them. But, the evidence above indicates a good correlation between AMPA receptor distribution and tap-withdrawal reflex. And there is good reason to believe that these features are retained by vitrification under ideal conditions.&nbsp;</p>\n<p>So, it seems to me that if you were to emulate a particular C elegans, you could add more receptors (or just up the strength parameter) at those synapses, and thus mimic the plasticity of the tap-withdrawal reflex. Looking at live animals would not be required.&nbsp;</p>\n<p>One more note:</p>\n<p>Extrapolating results on personal identity from C. elegans to humans is not ideal. If the results are biased in one direction, we should expect more&nbsp;redundancy&nbsp;in mammalian neural systems than there are in nematode ones, because mammals have so many more brain cells.&nbsp;</p>\n<p>Edit 6/15: fixed format of quotes.&nbsp;</p>\n<p>Edit 6/16: added synopsis to clarify main points.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2fa": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pbgjkw98k2SYxndWM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 10, "extendedScore": null, "score": 9.234933811893779e-07, "legacy": true, "legacyId": "17002", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iGvkd5uaqS8Xp4dde"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-16T02:23:42.941Z", "modifiedAt": null, "url": null, "title": "Cards Against Rationality", "slug": "cards-against-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:37.372Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fubarobfusco", "createdAt": "2010-09-26T23:03:54.946Z", "isAdmin": false, "displayName": "fubarobfusco"}, "userId": "ikCc3sp9Zguwdqipb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fn5bJLvJgvxj72Yt9/cards-against-rationality", "pageUrlRelative": "/posts/fn5bJLvJgvxj72Yt9/cards-against-rationality", "linkUrl": "https://www.lesswrong.com/posts/fn5bJLvJgvxj72Yt9/cards-against-rationality", "postedAtFormatted": "Saturday, June 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cards%20Against%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACards%20Against%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffn5bJLvJgvxj72Yt9%2Fcards-against-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cards%20Against%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffn5bJLvJgvxj72Yt9%2Fcards-against-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffn5bJLvJgvxj72Yt9%2Fcards-against-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1035, "htmlBody": "<p>(This post won't make much sense if you don't know about the game&nbsp;<a href=\"http://www.cardsagainsthumanity.com/\"><em>Cards Against Humanity</em></a>. Fortunately it has a web site. If you know the game <em>Apples to Apples</em>, well, <em>CAH</em>'s gameplay is almost identical to&nbsp;<em>Apples to Apples</em>&nbsp;... but the cards range from snarky to perverted to shockingly un-PC.)</p>\n<p>After the LW meetup in Mountain View yesterday, the idea came up of a Less Wrong expansion set for <em>Cards Against Humanity ...</em>&nbsp;with a roughly <a href=\"/lw/9ki/shit_rationalists_say/\">Shit Rationalists Say</a> theme, with a little help from <a href=\"/lw/4g/eliezer_yudkowsky_facts/\">Eliezer Yudkowsky Facts</a>. Regardless of whether this ever happens, we felt the need to share the pain with the rest of the community.</p>\n<p>These are meant to be mixed with the standard deck. Hence, the completed phrase \"That which can be destroyed by <span style=\"text-decoration: underline;\">being a motherfucking sorceror</span>&nbsp;should be\" is a clearly winning combination, as is \"Why am I sticky? <span style=\"text-decoration: underline;\">Grass-fed butter.</span>\"</p>\n<p><strong>Black cards:</strong></p>\n<ul>\n<li>That which can be destroyed by _____ should be.</li>\n<li>_____ is the mind-killer.</li>\n<li>The thirteenth virtue of rationality is _____.</li>\n<li>_____ is truly part of you.</li>\n<li>\"Let me not become attached to _____ I may not want.\"</li>\n<li>_____ is vulnerable to counterfactual mugging.</li>\n<li>What is true is already so. _____ doesn't make it worse.</li>\n<li>_____ is not the territory.</li>\n<li>_____ will kill you because you are made of _____ that it could use for something else.</li>\n<li>\"I'm an <em>aspiring&nbsp;_____.\"</em></li>\n<li>In the new version of Newcomb's problem, you have to choose between a box containing _____ and a box containing _____.</li>\n<li>Instrumental rationality is the art of winning at _____.</li>\n<li>Less Wrong is not a cult so long as our meetups don't include _____.</li>\n<li>In an Iterated Prisoners' Dilemma, _____ beats _____.</li>\n<li>The latest hot fanfic: _____ and the Methods of _____.</li>\n<li>_____ is highly correlated with _____.</li>\n<li>Absence of _____ is evidence of _____.</li>\n<li>The coherent extrapolated volition of humanity includes a term for _____.</li>\n<li>We have encountered aliens who communicate through _____.</li>\n<li>In the future, Eliezer Yudkowsky will be remembered for _____.</li>\n<li>I'm signed up with Alcor, so _____ will be frozen when I die.</li>\n<li>\"I am running on corrupted _____.\"</li>\n<li>An improperly-programmed AI might tile the universe with _____.</li>\n<li>You know what they say: one person's _____ is another person's _____.</li>\n<li>\"I <em>want</em>&nbsp;to want _____.\"</li>\n<li>_____ is what _____ feels like from the inside.</li>\n<li>_____ is the unit of caring.</li>\n<li>If you're not getting _____, you're spending too many resources on _____.</li>\n<li>Every _____ wants to be _____.</li>\n<li>Inside Eliezer Yudkowsky's pineal gland is not an immortal soul, but _____.</li>\n<li>Before Bruce Schneier goes to sleep, he scans his computer for uploaded copies of _____.</li>\n<li>Eliezer Yudkowsky updates _____ to fit his priors.</li>\n<li>Eliezer Yudkowsky doesn't have a chin; under his beard is _____.</li>\n<li>Never go in against _____ when _____ is on the line.</li>\n<li>Reversed _____ is not _____.</li>\n<li>You have no idea how big _____ is.</li>\n<li>Why haven't I signed up for cryonics?</li>\n<li>What am I optimizing for?</li>\n<li>The Quantified Self people have finally figured out how to measure _____.</li>\n<li>You can't fit a sheep into a _____.</li>\n<li>Make beliefs pay rent in _____.</li>\n<li>Why did my comment get downvoted?</li>\n<li>\"You make a compelling argument for _____.\"</li>\n<li><em>\"My model of you likes&nbsp;_____.\"</em></li>\n<li><em>\"I can handle _____, because I am already enduring it.\"</em></li>\n</ul>\n<div><strong>White cards:</strong></div>\n<div>\n<ul>\n<li>Eliezer Yudkowsky</li>\n<li>Friendly AI</li>\n<li>Unfriendly AI</li>\n<li>Lukeprog's love life</li>\n<li>The New York meetup group</li>\n<li>Updating</li>\n<li>Ugh fields</li>\n<li>Ben Goertzel</li>\n<li>Guessing the teacher's password</li>\n<li>Confidence intervals</li>\n<li>Signaling</li>\n<li>Polyamory</li>\n<li>The paleo diet</li>\n<li>Asperger's syndrome</li>\n<li>Ephemerisle</li>\n<li>Burning Man</li>\n<li>Grass-fed butter</li>\n<li>Dropping acid</li>\n<li>Timeless Decision Theory</li>\n<li>Pascal's mugging</li>\n<li>The Sequences</li>\n<li>Deathism</li>\n<li>Alcor</li>\n<li>The Singularity Institute for Artificial Intelligence</li>\n<li>Quirrellmort</li>\n<li>Dark Arts</li>\n<li>Tenorman's family chili</li>\n<li>Affective death spirals</li>\n<li>Rejection therapy</li>\n<li>The cult attractor</li>\n<li>Akrasia</li>\n<li>The Bayesian Conspiracy</li>\n<li>Paperclips</li>\n<li>The Copenhagen interpretation</li>\n<li>Clippy</li>\n<li>Shit Rationalists Say</li>\n<li>Babyeaters</li>\n<li>Superhappies</li>\n<li>Aubrey de Grey's beard</li>\n<li>Robin Hanson</li>\n<li>The blind idiot god, Evolution</li>\n<li>Getting downvoted on Less Wrong</li>\n<li>Two-boxing</li>\n<li>The obvious Schelling point</li>\n<li>Negging</li>\n<li>Peacocking</li>\n<li>P-Zombies</li>\n<li>Tit-for-Tat</li>\n<li>Applause lights</li>\n<li>Rare diseases in cute puppies</li>\n<li>Rationalist fanfiction</li>\n<li>Sunk costs</li>\n<li>Vibram Fivefingers</li>\n<li>RationalWiki</li>\n<li>The Chaos Legion Marching Song</li>\n<li>Poor epistemic hygiene</li>\n<li>A sheep-counting machine</li>\n<li>A horcrux</li>\n<li>Getting timelessly physical</li>\n<li>The Stanford Prison Experiment</li>\n<li>A ridiculously complicated Zendo rule</li>\n<li>Utils</li>\n<li>Wireheading</li>\n<li>My karma score</li>\n<li>Wiggins</li>\n<li>Ontologically basic mental entities</li>\n<li>The invisible dragon in my garage</li>\n<li>Meta-contrarianism</li>\n<li>Mormon transhumanists</li>\n<li>Nootropics</li>\n<li>Quantum immortality</li>\n<li>Quantum immorality</li>\n<li>The least convenient possible world</li>\n<li>Cards Against Rationality</li>\n<li>Moldbuggery</li>\n<li>The #1 reviewed Harry Potter / The Fountainhead crossover fanfic, \"Howard Roark and the Prisoner of Altruism\"</li>\n<li>Low-hanging fruits</li>\n<li>The set of all possible fetishes</li>\n<li>Rationalist clopfic</li>\n<li>The Library of Babel's porn collection</li>\n<li>Counterfactual hugging</li>\n<li>Acausal sex</li>\n</ul>\n<ul>\n</ul>\n<div>Post your own!</div>\n<div><em><strong>EDIT, </strong>2012-08-29: Several additions from the thread and elsewhere.</em></div>\n<div><em><strong>EDIT,</strong>&nbsp;2012-12-25: This is licensed under Creative Commons <a href=\"http://creativecommons.org/licenses/by-nc-sa/2.0/\">CC BY-NC-SA 2.0 license</a>, because Cards Against Humanity is.</em></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "hNFdS3rRiYgqqD8aM": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fn5bJLvJgvxj72Yt9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 37, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "16952", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8xQ8hTxo6Rk2qqZfj", "Ndtb22KYBxpBsagpj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-16T03:26:45.321Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] 2-Place and 1-Place Words", "slug": "seq-rerun-2-place-and-1-place-words", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6TjwKXsoYtaY4QyFt/seq-rerun-2-place-and-1-place-words", "pageUrlRelative": "/posts/6TjwKXsoYtaY4QyFt/seq-rerun-2-place-and-1-place-words", "linkUrl": "https://www.lesswrong.com/posts/6TjwKXsoYtaY4QyFt/seq-rerun-2-place-and-1-place-words", "postedAtFormatted": "Saturday, June 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%202-Place%20and%201-Place%20Words&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%202-Place%20and%201-Place%20Words%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6TjwKXsoYtaY4QyFt%2Fseq-rerun-2-place-and-1-place-words%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%202-Place%20and%201-Place%20Words%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6TjwKXsoYtaY4QyFt%2Fseq-rerun-2-place-and-1-place-words", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6TjwKXsoYtaY4QyFt%2Fseq-rerun-2-place-and-1-place-words", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 201, "htmlBody": "<p>Today's post, <a href=\"/lw/ro/2place_and_1place_words/\">2-Place and 1-Place Words</a> was originally published on 27 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#2-Place_and_1-Place_Words\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It is possible to talk about \"sexiness\" as a property of an observer and a subject. It is also equally possible to talk about \"sexiness\" as a property of a subject, as long as each observer can have a different process to determine how sexy someone is. Failing to do either of these will cause you trouble.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d3j/seq_rerun_no_universally_compelling_arguments/\">No Universally Compelling Arguments</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6TjwKXsoYtaY4QyFt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.235474979964956e-07, "legacy": true, "legacyId": "17004", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5", "JhhGpriY6cFxzJSon", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-16T13:58:35.578Z", "modifiedAt": null, "url": null, "title": "Arrison, Vassar and de Grey speaking at Peter Thiel's startup class [link]", "slug": "arrison-vassar-and-de-grey-speaking-at-peter-thiel-s-startup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.322Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m4TDphcMrSNmwXMND/arrison-vassar-and-de-grey-speaking-at-peter-thiel-s-startup", "pageUrlRelative": "/posts/m4TDphcMrSNmwXMND/arrison-vassar-and-de-grey-speaking-at-peter-thiel-s-startup", "linkUrl": "https://www.lesswrong.com/posts/m4TDphcMrSNmwXMND/arrison-vassar-and-de-grey-speaking-at-peter-thiel-s-startup", "postedAtFormatted": "Saturday, June 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Arrison%2C%20Vassar%20and%20de%20Grey%20speaking%20at%20Peter%20Thiel's%20startup%20class%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArrison%2C%20Vassar%20and%20de%20Grey%20speaking%20at%20Peter%20Thiel's%20startup%20class%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4TDphcMrSNmwXMND%2Farrison-vassar-and-de-grey-speaking-at-peter-thiel-s-startup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Arrison%2C%20Vassar%20and%20de%20Grey%20speaking%20at%20Peter%20Thiel's%20startup%20class%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4TDphcMrSNmwXMND%2Farrison-vassar-and-de-grey-speaking-at-peter-thiel-s-startup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4TDphcMrSNmwXMND%2Farrison-vassar-and-de-grey-speaking-at-peter-thiel-s-startup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 13, "htmlBody": "<p>http://blakemasters.tumblr.com/post/25149261055/peter-thiels-cs183-startup-class-19-notes-essay</p>\n<p>as someone mentioned the rest of Thiel's notes are also an interesting read</p>\n<p>http://blakemasters.tumblr.com/peter-thiels-cs183-startup</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m4TDphcMrSNmwXMND", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 9.238359157331171e-07, "legacy": true, "legacyId": "17019", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-16T14:01:37.488Z", "modifiedAt": null, "url": null, "title": "SIAI May report", "slug": "siai-may-report", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.324Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PFv6iEk3bguGQdSHb/siai-may-report", "pageUrlRelative": "/posts/PFv6iEk3bguGQdSHb/siai-may-report", "linkUrl": "https://www.lesswrong.com/posts/PFv6iEk3bguGQdSHb/siai-may-report", "postedAtFormatted": "Saturday, June 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SIAI%20May%20report&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASIAI%20May%20report%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPFv6iEk3bguGQdSHb%2Fsiai-may-report%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SIAI%20May%20report%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPFv6iEk3bguGQdSHb%2Fsiai-may-report", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPFv6iEk3bguGQdSHb%2Fsiai-may-report", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 18, "htmlBody": "<p>http://singinst.org/blog/2012/06/16/singularity-institute-progress-report-may-2012/</p>\n<p>I'm posting it in an unofficial capacity as a reminder to Luke to post it here also :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PFv6iEk3bguGQdSHb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 9.238373000501404e-07, "legacy": true, "legacyId": "17020", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-17T03:50:05.672Z", "modifiedAt": null, "url": null, "title": "What is the best way to read the sequences?", "slug": "what-is-the-best-way-to-read-the-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:06.999Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AspiringRationalist", "createdAt": "2012-03-14T02:23:07.389Z", "isAdmin": false, "displayName": "AspiringRationalist"}, "userId": "SiQCqozmtW7TSjo6E", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xxCEAZRczgcfSaeEo/what-is-the-best-way-to-read-the-sequences", "pageUrlRelative": "/posts/xxCEAZRczgcfSaeEo/what-is-the-best-way-to-read-the-sequences", "linkUrl": "https://www.lesswrong.com/posts/xxCEAZRczgcfSaeEo/what-is-the-best-way-to-read-the-sequences", "postedAtFormatted": "Sunday, June 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20the%20best%20way%20to%20read%20the%20sequences%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20the%20best%20way%20to%20read%20the%20sequences%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxxCEAZRczgcfSaeEo%2Fwhat-is-the-best-way-to-read-the-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20the%20best%20way%20to%20read%20the%20sequences%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxxCEAZRczgcfSaeEo%2Fwhat-is-the-best-way-to-read-the-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxxCEAZRczgcfSaeEo%2Fwhat-is-the-best-way-to-read-the-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>I am a relative new-comer to LW, and I have read ~half the articles in the core sequences, but I think I have not been optimizing for comprehension/retention while reading them.&nbsp; Could some more experienced members of the community give some recommendations on how best to read them \\(both in terms of in which posts in which order and in terms of the actual reading process\\)?</p>\n<p>I will periodically edit this post to include some of the suggestions in order to provide the most benefit to future newcomers.</p>\n<p>EDIT: Here are the most popular suggestions:</p>\n<p>Order:</p>\n<ul>\n<li>Chronological order (5 or 6 comments)</li>\n<li>Whatever method you will actually do or already doing (1.5 comments)</li>\n</ul>\n<p>How to read them:</p>\n<ul>\n<li>e-reader or smartphone (3 comments)</li>\n<li>Text-to-speech (1 comment)</li>\n<li>Read How to <a href=\"http://pne.people.si.umich.edu/PDF/howtoread.pdf\">Read a Book</a> and implement its suggestions (1 comment) (note: the link is a 9-page PDF, so it shouldn't be too hard to read and see what's useful)</li>\n<li>Mind-mapping (1 comment)</li>\n<li>Try to guess what the links go to (1 comment)</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xxCEAZRczgcfSaeEo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 9.242157181843295e-07, "legacy": true, "legacyId": "16757", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-17T10:39:22.817Z", "modifiedAt": null, "url": null, "title": "[video] Kelly McGonigal on willpower", "slug": "video-kelly-mcgonigal-on-willpower", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:08.906Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bobertron", "createdAt": "2010-09-10T10:46:47.881Z", "isAdmin": false, "displayName": "Bobertron"}, "userId": "YASrnY7PdwMoLbETE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/noX9uizEdm69TQgyS/video-kelly-mcgonigal-on-willpower", "pageUrlRelative": "/posts/noX9uizEdm69TQgyS/video-kelly-mcgonigal-on-willpower", "linkUrl": "https://www.lesswrong.com/posts/noX9uizEdm69TQgyS/video-kelly-mcgonigal-on-willpower", "postedAtFormatted": "Sunday, June 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bvideo%5D%20Kelly%20McGonigal%20on%20willpower&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bvideo%5D%20Kelly%20McGonigal%20on%20willpower%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoX9uizEdm69TQgyS%2Fvideo-kelly-mcgonigal-on-willpower%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bvideo%5D%20Kelly%20McGonigal%20on%20willpower%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoX9uizEdm69TQgyS%2Fvideo-kelly-mcgonigal-on-willpower", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoX9uizEdm69TQgyS%2Fvideo-kelly-mcgonigal-on-willpower", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<p><a href=\"https://www.youtube.com/watch?v=V5BXuZL1HAg\">the video<br /></a></p>\n<blockquote>\n<p>Author and Stanford health psychologist Kelly McGonigal, PhD, talks about strategies from her new book \"The WillPower Instinct: How Self-Control Works, Why It Matters, and What You Can Do to Get More of It\" as part of the Authors@Google series. Topics include dieting/weight loss, health, addiction, quitting smoking, temptation, procrastination, mindfulness, stress, sleep, cravings, exercise, self-control, self-compassion, guilt, and shame.</p>\n</blockquote>\n<p class=\"title\">I'm posting this because akrasia, procrastination and willpower are often discussed on LW. I haven't read the book, but for those that are interested<span class=\"title\"> \"The Willpower Instinct\" and \"Maximum Willpower\" are, from what I can tell, exactly the same books.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "noX9uizEdm69TQgyS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 9.244027678764388e-07, "legacy": true, "legacyId": "17021", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-17T10:51:56.817Z", "modifiedAt": null, "url": null, "title": "[Link] Nick Bostrom on the Status Quo Bias", "slug": "link-nick-bostrom-on-the-status-quo-bias", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/27yKEvXqgLHkZk6cn/link-nick-bostrom-on-the-status-quo-bias", "pageUrlRelative": "/posts/27yKEvXqgLHkZk6cn/link-nick-bostrom-on-the-status-quo-bias", "linkUrl": "https://www.lesswrong.com/posts/27yKEvXqgLHkZk6cn/link-nick-bostrom-on-the-status-quo-bias", "postedAtFormatted": "Sunday, June 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Nick%20Bostrom%20on%20the%20Status%20Quo%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Nick%20Bostrom%20on%20the%20Status%20Quo%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27yKEvXqgLHkZk6cn%2Flink-nick-bostrom-on-the-status-quo-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Nick%20Bostrom%20on%20the%20Status%20Quo%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27yKEvXqgLHkZk6cn%2Flink-nick-bostrom-on-the-status-quo-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27yKEvXqgLHkZk6cn%2Flink-nick-bostrom-on-the-status-quo-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p><a href=\"http://traffic.libsyn.com/philosophybites/Nick_Bostrom_on_the_Status_Quo_Bias.mp3\">Here</a> (in MP3 format)&nbsp;is the <a href=\"http://philosophybites.com/\">Philosophy Bites</a> podcast from 05/14/2012. In it, <a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a> discusses the <a href=\"http://en.wikipedia.org/wiki/Status_quo_bias\">status quo bias</a> with <a href=\"http://www.open.ac.uk/Arts/philosophy/warburton.shtml\">Nigel Warburton</a>. <a href=\"http://www.nickbostrom.com/ethics/statusquo.pdf\">Here</a> (in PDF format) is Bostrom's paper outlining a technique&nbsp;(the reversal test)&nbsp;for&nbsp;debiasing within the context of applied ethics.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb17d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "27yKEvXqgLHkZk6cn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 9.244085120600792e-07, "legacy": true, "legacyId": "17022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-17T20:02:25.254Z", "modifiedAt": "2020-02-20T06:20:25.066Z", "url": null, "title": "Altruistic Kidney Donation", "slug": "altruistic-kidney-donation", "viewCount": null, "lastCommentedAt": "2016-03-17T02:19:53.326Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wzdjAmeoPRmBE8v8o/altruistic-kidney-donation", "pageUrlRelative": "/posts/wzdjAmeoPRmBE8v8o/altruistic-kidney-donation", "linkUrl": "https://www.lesswrong.com/posts/wzdjAmeoPRmBE8v8o/altruistic-kidney-donation", "postedAtFormatted": "Sunday, June 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Altruistic%20Kidney%20Donation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAltruistic%20Kidney%20Donation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwzdjAmeoPRmBE8v8o%2Faltruistic-kidney-donation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Altruistic%20Kidney%20Donation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwzdjAmeoPRmBE8v8o%2Faltruistic-kidney-donation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwzdjAmeoPRmBE8v8o%2Faltruistic-kidney-donation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 842, "htmlBody": "<p>People are dying because their kidneys have failed, and you probably have an extra one: should you donate it? A dialysis machine can do most of the work of a kidney, but it's not as good. A donated kidney gives someone an average of 10 additional years. [1] (Some of them reject it right away, others live for years before having problems.) This is pretty good: if you volunteer to be a kidney donor and are selected as a match, you give someone an average of another 10 years of life. [2]</p>\n<p>A kidney donation is a surgery, however, and does have some risk for you. It's hard to calculate the risk because people who are selected for kidney donation are a weird group: they're heavily screened (so they tend to be healthier) but they're likely to be related to the person they're donating to (so they tend to be at risk for kidney problems themself). An ideal study on the dangers of kidney donation would compare donors with people who were going to donate and then didn't for some nearly-random reason. I can't find such a study, but I've heard people <a href=\"http://www.guardian.co.uk/lifeandstyle/2011/oct/24/i-gave-stranger-my-kidney\">give odds of around 1:3000</a> for dying due to donating. You also spend several days in the hospital, miss some weeks of work, and take several months to fully recover. [3] This sounds like a lot, but giving someone 10 years of life is a lot too.</p>\n<p>The question isn't so much \"does this do good,\" however, but \"does this maximize the good I can do for the amount of sacrifice I'm willing to accept?\" GiveWell estimates that each $1600 given to the <a href=\"http://www.givewell.org/international/top-charities/AMF\">AMF</a> for antimalarial net distribution prevents the death of someone with an average life expectancy of maybe 40 years [4]. So another way to give a stranger 10 more years of life is to give $400 to the AMF. Kidney donation is in the same ballpark as this, in terms of the ratio of benefit (to others) to cost (to you) [5], but even ignoring the risk of death [6] I would pay that $400 just to avoid the hospital and recovery time.</p>\n<p><strong>Update 2012-06-22:</strong> BDan and Alexander Berger have convinced me I was wrong to write off chains. At least right now there are more possible chains than kidneys to start them, and they can be quite long. My updated estimate is that this gets you a 3x multiplier on the value of a donated kidney, bringing the $400 charity equivalent to $1200. Which is still less than the $10-$100K at which I think I would sell a kidney, but substantially higher.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><em>(I also posted this <a href=\"http://www.jefftk.com/news/2012-06-17.html\">on my blog</a>)</em></p>\n<p>&nbsp;</p>\n<p><br /> [1] <a href=\"http://scholar.google.com/scholar?cluster=13142015444256400318\">Comparison of mortality in all patients on dialysis, patients on dialysis awaiting transplantation, and recipients of a first cadaveric transplant. (1999)</a>, table 3. This is from a longitudinal study on 23,275 people who got kidney transplants from cadavers and had never had one before. In our case we're interested in live donation, but I would guess it's about the same.</p>\n<p>[2] It's possible that you could do better than this if your donation to a stranger set off a chain of donations. The idea is that there are lots of people who would give their kidney to a friend of family member if they could, but are for whatever reason incompatible. You can make donor-donee-donor-donee chains where it just takes one altruistic person to set the whole thing off. (Cycles are also possible, but they don't need someone to get them started.) The question is, though, how likely are these chains to happen anyway? And how long a chain can you reasonably expect? My understanding is that these are quite rare, with most altruistic kidney donors not starting one. I'm going to ignore chains.</p>\n<p>[3] Someone who gave their kidney wrote in a <a href=\"http://www.guardian.co.uk/lifeandstyle/2011/oct/24/i-gave-stranger-my-kidney\">Guardian article</a>: \"I was in hospital for four days--the worst part of the whole ordeal was removing the catheter. In just two weeks I was back at work (the full recovery is supposed to be three months).\"</p>\n<p>[4] I would love to find a GiveWell estimate of what life expectancy for the people who don't die of malaria because of a net, but I can't find one. So this \"40 years\" is a guess after looking at life expectancies for different ages in the countries where the AMF works.</p>\n<p>[5] There are huge costs to both dialysis and kidney transplant, so as a health intervention distributing antimalarial nets [makes much more sense]. From the perspective of a prospective donor, however, I don't think this matters.</p>\n<p>[6] You might think that if you're someone who gives a substantial fraction of their earnings to effective charity, a 1:3000 risk of death would be pretty bad because you'd be able to help other people much less. But 3000 is really big: if you're giving $20K/year and have an expected additional 30 years of working life, a 1:3000 chance of not being able to do that has an <a href=\"http://www.jefftk.com/news/2010-08-24.html\">undiscounted</a> expected value of only $200 less donated. [7]</p>\n<p>[7] $20K * 30 years * 1:3000 = $200</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JsJPrdgRGRqnci8cZ": 1, "xHjy88N2uJvGdgzfw": 1, "rfZ6DY88ApBDXFpyW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wzdjAmeoPRmBE8v8o", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 28, "extendedScore": null, "score": 7.5e-05, "legacy": true, "legacyId": "17023", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-06-17T20:02:25.254Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-18T03:07:47.979Z", "modifiedAt": null, "url": null, "title": "Local Ordinances of Fun", "slug": "local-ordinances-of-fun", "viewCount": null, "lastCommentedAt": "2018-05-25T00:09:53.196Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QSDRgQq5vzPKDSwF7/local-ordinances-of-fun", "pageUrlRelative": "/posts/QSDRgQq5vzPKDSwF7/local-ordinances-of-fun", "linkUrl": "https://www.lesswrong.com/posts/QSDRgQq5vzPKDSwF7/local-ordinances-of-fun", "postedAtFormatted": "Monday, June 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Local%20Ordinances%20of%20Fun&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALocal%20Ordinances%20of%20Fun%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQSDRgQq5vzPKDSwF7%2Flocal-ordinances-of-fun%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Local%20Ordinances%20of%20Fun%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQSDRgQq5vzPKDSwF7%2Flocal-ordinances-of-fun", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQSDRgQq5vzPKDSwF7%2Flocal-ordinances-of-fun", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1201, "htmlBody": "<p><strong>Prerequisite reading which you will probably want open in another tab for reference:</strong> <a href=\"/lw/y0/31_laws_of_fun/\">31 Laws of Fun</a></p>\n<p>Unprefaced, this post might sound a lot like I'm just picking on Eliezer, or Eliezer's particular set of \"laws\".&nbsp; I'm sort of doing that, but only as a template for ways to pick on Laws of Fun in general.&nbsp; The correct response to this post is not \"Here is my new, different list of N things that will satisfy everyone\".</p>\n<p>(Well, it would be if you could do that.&nbsp; I'm skeptical.)</p>\n<p>If <em>I</em> purported to come up with general laws of fun, I might or might not do a better job.&nbsp; Probably I'd do a better job coming up with a framework for <em>myself</em>; I might also be more cautious about assuming human homogeneity, but I doubt I'd do an <em>unassailable</em> job.&nbsp; And an unassailable job is probably necessary, if everyone will abide by Laws of Fun forever.&nbsp; An unassailable job of Legislating Fun is needed make sure that some people aren't caught between unwanted mental tampering and, probably not Hell, but a world that is subtly (or glaringly) wrong, wrong, wrong.</p>\n<p>Please do not assume that I outright endorse unmentioned laws; these are just the ones I can pick at most obviously.</p>\n<p>I fully expect to be told that I have misunderstood at least half of these items.</p>\n<p><strong>6</strong> sits uncomfortably.&nbsp; The savannah is where we were designed to survive, but evolution is miserly; it is not where we were designed to <em>thrive gloriously</em>.&nbsp; (Any species designed to thrive gloriously there which was actually put there would find its descendants getting away with more and more corner-cutting until they found a more efficient frontier.&nbsp; Creatures that can fly don't keep flight just because flying is awesome; they must also need it.)&nbsp; I want a home designed for me to thrive gloriously in, not one that takes its cues from the environment my ancestors eked out a living in.&nbsp; I suspect this is more like a temperate-clime park than a baking savannah, and it might be more like an architecturally excellent house than either.&nbsp; \"Windowless office\" is not the fair comparison.&nbsp; That is not how we design places to put people we like.<a id=\"more\"></a><br /><br /><strong>8</strong> sounds just wrong, or like a misstatement.&nbsp; Why should we get better and better?&nbsp; Objecting to flat awesomeness sounds like a matter of <em>denying that it is flat</em>.&nbsp; Perhaps it just <em>sounds</em> tedious for things to be the same level of awesomeness forever, perhaps it sounds inevitable that the hedonic treadmill will pull downward, but that tedium or treadmill means the awesomeness is not really <em>flat</em>.&nbsp; If it's actually awesome, by all means let's take a flying leap there and carry on forever.&nbsp; (Certainly things should not get worse over time.)<br /><br /><strong>9</strong> AAAAAAAAAAAAAAAAAAAAAAAH<br /><br /><strong>10</strong> sounds like it's missing an option.&nbsp; What can people do for <em>each other</em>?&nbsp; In our world, buttons do stuff for us, because people with whom we are interdependent made them do that.&nbsp; In the ancestral environment, people ate food that others hunted and gathered, they listened to music others played, they carried stuff in baskets others wove, etc.&nbsp; Gifts are good.&nbsp; Specialization is not evil.&nbsp; Certainly humans should do things, but what if I want to <em>play</em> the flute now and only have flute-<em>whittling</em> down as my activity for century seventeen?<br /><br /><strong>12</strong> sounds okay provided it is not interpreted to forbid really great video games, roleplaying scenarios, fiction in general, etc.<br /><br /><strong>13</strong> clashes with a lot of the other laws.&nbsp; (What if I don't <em>want</em> to live my life according to things like, oh, Law 9 [AAAAAH]?&nbsp; Is someone going to stop me from planning myself a predictable future?)&nbsp; People might work best under different rules, too.<br /><br /><strong>15</strong> isn't even a law, it's a problem statement.&nbsp; Same with <strong>16</strong>.<br /><br /><strong>17</strong> sounds like artificial difficulty.&nbsp; Just because there is a challenge there and I don't have the road to get around it doesn't stop me from acknowledging that someone else does.&nbsp; If there is an AI around, and it could get me out of this jam, a psychologically-untampered-with Alicorn will resent that it&nbsp; is making me do stuff if I don't happen to want to do it, the same way I resented busywork in school that didn't happen to interest me.&nbsp; Eventually I plan to try making my own butter.&nbsp; In the meantime, I'm glad that's not a step in making scrambled eggs.<br /><br /><strong>20</strong> sounds idiosyncratic or case-by-case.&nbsp; (I take it to be intended as a stronger statement than the mere \"there are situations where you can tell people true things and it doesn't help them\" - even if that's almost false there has to be some perverse scenario to make it so.)&nbsp; Sometimes - <em>often -</em> I just want people to tell me stuff.&nbsp; See also <strong>17</strong>; the fact that someone else knows, even if nothing I can do will get them to tell me, makes my not-knowing something of a fake problem.<br /><br /><strong>23</strong> sounds like an outright contradiction of <strong>22</strong>.&nbsp; What do you want to do - solve problems by messing with the environment, or \"nudge\" people to solve a \"statistical sex problem\"?&nbsp; Are there not any environmental changes that would accomplish this?&nbsp; Condoms made a dent, didn't they?&nbsp; What would literally perfect birth control and disease protection do?&nbsp; More energy, better health, more <em>time</em>?&nbsp; Literally perfect privacy?&nbsp; Better information for everyone about how to be good in bed?&nbsp; Non-twisted culture to raise new minds in?&nbsp; <em>Optional</em> perfect body/avatar modification for everyone?&nbsp; That took me three minutes to come up with.&nbsp; Leaping straight to \"nudges\" is... discomfiting!*<br /><br /><strong>27</strong> and <strong>28</strong> are useful tools for fiction and interesting thought exercises, but it is not how we build houses (mine is right-side-up and has its plumbing in its bathrooms rather than on the roof, thank you).&nbsp; It may not be how we should build eutopias in which we hope to actually live.&nbsp; I <em>like</em> comfort.&nbsp; I expect culture to change radically - the way cultures <em>do</em> when time passes and/or things change - once everyone has settled into transhumanity, but the form that transhumanity takes needn't itself be frightening, and trying to <em>design this in</em> sounds like a bad idea.<br /><br />The general undercurrent through the laws \"people should not have [access to] X\" thing sounds problematic.&nbsp; Unless we plan to deceive them about the nature of the postsingularity universe, their not being allowed X will be known to be <em>somebody's fault.&nbsp; </em>Someone believed in some Laws of Fun that they programmed into an AI that determined that the best way to optimize for that <em>kind</em> of Fun was to disallow X.&nbsp; Somebody is going to want X and somebody will be disappointed.<br /><br />It is one thing to have a Eutopia that is scary, but it sounds so terribly sad to have one that is <em>disappointing</em>.<br /><br />And I have never yet sincerely overestimated human heterogeneity.</p>\n<p>&nbsp;</p>\n<p>*I think Eliezer in general assigns more-than-average importance to gender as a factor in personality, anyway... often in a heteronormative way.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QSDRgQq5vzPKDSwF7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 72, "baseScore": 27, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "17025", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 156, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qZJBighPrnv9bSqTZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-18T04:12:53.652Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Opposite Sex", "slug": "seq-rerun-the-opposite-sex", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:02.708Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rxY4FB8mmGoPwo8bq/seq-rerun-the-opposite-sex", "pageUrlRelative": "/posts/rxY4FB8mmGoPwo8bq/seq-rerun-the-opposite-sex", "linkUrl": "https://www.lesswrong.com/posts/rxY4FB8mmGoPwo8bq/seq-rerun-the-opposite-sex", "postedAtFormatted": "Monday, June 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Opposite%20Sex&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Opposite%20Sex%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrxY4FB8mmGoPwo8bq%2Fseq-rerun-the-opposite-sex%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Opposite%20Sex%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrxY4FB8mmGoPwo8bq%2Fseq-rerun-the-opposite-sex", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrxY4FB8mmGoPwo8bq%2Fseq-rerun-the-opposite-sex", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>Today's post, <a href=\"/lw/rp/the_opposite_sex/\">The Opposite Sex</a> was originally published on 28 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A few thoughts from Eliezer Yudkowsky about a discussion of sexism on Overcoming Bias.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d4c/seq_rerun_2place_and_1place_words/\">2-Place and 1-Place Words</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rxY4FB8mmGoPwo8bq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.248845475679606e-07, "legacy": true, "legacyId": "17032", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XM9SwdBGn8ATf8kq3", "6TjwKXsoYtaY4QyFt", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-18T06:18:10.622Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal First Meetup ", "slug": "meetup-montreal-first-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LzmGk7jCFF83pP5ST/meetup-montreal-first-meetup", "pageUrlRelative": "/posts/LzmGk7jCFF83pP5ST/meetup-montreal-first-meetup", "linkUrl": "https://www.lesswrong.com/posts/LzmGk7jCFF83pP5ST/meetup-montreal-first-meetup", "postedAtFormatted": "Monday, June 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20First%20Meetup%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20First%20Meetup%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLzmGk7jCFF83pP5ST%2Fmeetup-montreal-first-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20First%20Meetup%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLzmGk7jCFF83pP5ST%2Fmeetup-montreal-first-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLzmGk7jCFF83pP5ST%2Fmeetup-montreal-first-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/b7'>Montreal First Meetup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 June 2012 01:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">100 Rue Saint Antoine Ouest Montreal, QC H2Z 1X8</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>First meetup for Montreal LessWrongers! I recently visited an Ottawa meetup, and it was a great time. I'd like to meet like minded rationalists and discuss what to do next.</p>\n\n<p>I'd prefer if this were the \"self improvement\" type of meetup, but I also plan on in the future discussion games and the like. Hope to see you there! I'll have a sign that says \"LessWrong Meetup Group\", and will be reading some sort of book.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/b7'>Montreal First Meetup </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LzmGk7jCFF83pP5ST", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 9.249418698182359e-07, "legacy": true, "legacyId": "17038", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_First_Meetup_\">Discussion article for the meetup : <a href=\"/meetups/b7\">Montreal First Meetup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 June 2012 01:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">100 Rue Saint Antoine Ouest Montreal, QC H2Z 1X8</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>First meetup for Montreal LessWrongers! I recently visited an Ottawa meetup, and it was a great time. I'd like to meet like minded rationalists and discuss what to do next.</p>\n\n<p>I'd prefer if this were the \"self improvement\" type of meetup, but I also plan on in the future discussion games and the like. Hope to see you there! I'll have a sign that says \"LessWrong Meetup Group\", and will be reading some sort of book.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_First_Meetup_1\">Discussion article for the meetup : <a href=\"/meetups/b7\">Montreal First Meetup </a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal First Meetup ", "anchor": "Discussion_article_for_the_meetup___Montreal_First_Meetup_", "level": 1}, {"title": "Discussion article for the meetup : Montreal First Meetup ", "anchor": "Discussion_article_for_the_meetup___Montreal_First_Meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-18T07:03:41.980Z", "modifiedAt": null, "url": null, "title": "Article about Kahneman", "slug": "article-about-kahneman", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:08.655Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "anemone42", "createdAt": "2011-01-12T04:08:54.720Z", "isAdmin": false, "displayName": "anemone42"}, "userId": "n2yqtrZQKrkK6Y7BD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WzzAigxLqGHxHFBzA/article-about-kahneman", "pageUrlRelative": "/posts/WzzAigxLqGHxHFBzA/article-about-kahneman", "linkUrl": "https://www.lesswrong.com/posts/WzzAigxLqGHxHFBzA/article-about-kahneman", "postedAtFormatted": "Monday, June 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Article%20about%20Kahneman&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArticle%20about%20Kahneman%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWzzAigxLqGHxHFBzA%2Farticle-about-kahneman%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Article%20about%20Kahneman%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWzzAigxLqGHxHFBzA%2Farticle-about-kahneman", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWzzAigxLqGHxHFBzA%2Farticle-about-kahneman", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>http://www.newyorker.com/online/blogs/frontal-cortex/2012/06/daniel-kahneman-bias-studies.html?mobify=0</p>\n<p>It doesn't say what happens, if you try to chance the nature of your bias ...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WzzAigxLqGHxHFBzA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -8, "extendedScore": null, "score": 9.24962699940566e-07, "legacy": true, "legacyId": "17039", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-18T07:57:17.887Z", "modifiedAt": null, "url": null, "title": "[Link] You Should Downvote Contrarian Anecdotes", "slug": "link-you-should-downvote-contrarian-anecdotes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:09.374Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Golovin", "createdAt": "2009-02-28T13:32:31.085Z", "isAdmin": false, "displayName": "Vladimir_Golovin"}, "userId": "Me2m84AhCn9H49riY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zbkESGYMwCmiWEZ49/link-you-should-downvote-contrarian-anecdotes", "pageUrlRelative": "/posts/zbkESGYMwCmiWEZ49/link-you-should-downvote-contrarian-anecdotes", "linkUrl": "https://www.lesswrong.com/posts/zbkESGYMwCmiWEZ49/link-you-should-downvote-contrarian-anecdotes", "postedAtFormatted": "Monday, June 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20You%20Should%20Downvote%20Contrarian%20Anecdotes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20You%20Should%20Downvote%20Contrarian%20Anecdotes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzbkESGYMwCmiWEZ49%2Flink-you-should-downvote-contrarian-anecdotes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20You%20Should%20Downvote%20Contrarian%20Anecdotes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzbkESGYMwCmiWEZ49%2Flink-you-should-downvote-contrarian-anecdotes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzbkESGYMwCmiWEZ49%2Flink-you-should-downvote-contrarian-anecdotes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p><a href=\"http://thobbs.github.com/blog/2012/06/17/you-should-downvote-anecdotes/\"></a></p>\n<p><a href=\"http://thobbs.github.com/blog/2012/06/17/you-should-downvote-anecdotes/\">http://thobbs.github.com/blog/2012/06/17/you-should-downvote-anecdotes/</a></p>\n<blockquote>\n<p><em>Anecdotal evidence has been shown to have a greater influence on opinion than it logically deserves, most visibly when the anecdote<a href=\"http://crx.sagepub.com/content/23/2/210.short\"> conflicts with the reader&rsquo;s opinion</a> and when <a href=\"http://crx.sagepub.com/content/37/6/825.abstract\">the reader is not highly analytical</a>, even if the anecdotes are accompanying statistical evidence. Though the anecdotes may not totally sway you, they can easily leave you with the sense that the research findings aren&rsquo;t as conclusive as they claim to be.</em></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6Qic6PwwBycopJFNN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zbkESGYMwCmiWEZ49", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 16, "extendedScore": null, "score": 9.249872263023598e-07, "legacy": true, "legacyId": "17049", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-18T14:29:31.239Z", "modifiedAt": null, "url": null, "title": "New Singularity.org", "slug": "new-singularity-org", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:33.374Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HGxXFjwRdWrFYmsF3/new-singularity-org", "pageUrlRelative": "/posts/HGxXFjwRdWrFYmsF3/new-singularity-org", "linkUrl": "https://www.lesswrong.com/posts/HGxXFjwRdWrFYmsF3/new-singularity-org", "postedAtFormatted": "Monday, June 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Singularity.org&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Singularity.org%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGxXFjwRdWrFYmsF3%2Fnew-singularity-org%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Singularity.org%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGxXFjwRdWrFYmsF3%2Fnew-singularity-org", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGxXFjwRdWrFYmsF3%2Fnew-singularity-org", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p><a href=\"http://intelligence.org/2013/02/28/welcome-to-intelligence-org/\">Here</a> is the welcome blog post for the new singularity.org.</p>\n<p>There's a bug on the media page, and another with blog comments, and these bugs will be fixed later today.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HGxXFjwRdWrFYmsF3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 17, "extendedScore": null, "score": 9.251667412446573e-07, "legacy": true, "legacyId": "17056", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-18T16:26:42.666Z", "modifiedAt": null, "url": null, "title": "Thwarting a Catholic conversion?", "slug": "thwarting-a-catholic-conversion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:34.656Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jay_Schweikert", "createdAt": "2011-04-29T03:53:40.789Z", "isAdmin": false, "displayName": "Jay_Schweikert"}, "userId": "6niJdEi2bPMT4Bd5t", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WiN2NGeyPMFWCm6fD/thwarting-a-catholic-conversion", "pageUrlRelative": "/posts/WiN2NGeyPMFWCm6fD/thwarting-a-catholic-conversion", "linkUrl": "https://www.lesswrong.com/posts/WiN2NGeyPMFWCm6fD/thwarting-a-catholic-conversion", "postedAtFormatted": "Monday, June 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thwarting%20a%20Catholic%20conversion%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThwarting%20a%20Catholic%20conversion%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWiN2NGeyPMFWCm6fD%2Fthwarting-a-catholic-conversion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thwarting%20a%20Catholic%20conversion%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWiN2NGeyPMFWCm6fD%2Fthwarting-a-catholic-conversion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWiN2NGeyPMFWCm6fD%2Fthwarting-a-catholic-conversion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 617, "htmlBody": "<p>I recently learned that a friend of mine, and a long-time atheist (and atheist blogger), is <a href=\"http://www.patheos.com/blogs/unequallyyoked/2012/06/this-is-my-last-post-for-the-patheos-atheist-portal.html#comments\">planning to convert to Catholicism</a>. It seems the impetus for her conversion was increasing frustration that she had no good naturalistic account for objective morality in the form of virtue ethics; that upon reflection, she decided she felt like morality \"loved\" her; that this feeling implied God; and that she had sufficient \"if God, then Catholicism\" priors to point toward Catholicism, even though she's bisexual (!) and purports to still feel uncertain about the Church's views on sexuality. (Side note: all of this information is material she's blogged about herself, so it's not as if I'm sharing personal details she would prefer to be kept private.)</p>\n<p>First, I want to state the rationality lesson I learned from this episode: atheists who spend a great deal of their time analyzing and even critiquing the views of a <em>particular</em> religion are at-risk atheists. Eliezer's <a href=\"/lw/19m/privileging_the_hypothesis/\">spoken about this sort of issue</a> before (\"Someone who spends all day thinking about whether the <em>Trinity </em>does or does not exist, rather than Allah or Thor or the Flying Spaghetti Monster, is more than halfway to Christianity.\"), but I guess it took a personal experience to really drive the point home. When I first read my friend's post, I had a major \"I notice that I am confused\" moment, because it just seemed so implausible that someone who understood actual atheist arguments (as opposed to dead little sister <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/HollywoodAtheist\">Hollywood Atheism</a>) could convert to religion, and <em>Catholicism</em> of all things. I seriously considered (and investigated) the possibility that her post was some kind of prank or experiment or otherwise not sincere, or that her account had been hijacked by a very good impersonator (both of these seem quite unlikely at this point).</p>\n<p>But then I remembered how I had been frustrated in the past by her tolerance for what seemed like rank religious bigotry and how often I thought she was taking seriously theological positions that seemed about as likely as the 9/11 attacks being genuinely inspired and ordained by Allah. I remembered how I thought she had a confused conception of meta-ethics and that she often seemed skeptical of reductionism, which in retrospect should have been a <em>major</em> red flag for purported atheists. So yeah, spending all your time arguing about Catholic doctrine <em>really</em> is a warning sign, no matter how strongly you seem to champion the \"atheist\" side of the debate. Seriously.</p>\n<p>But second, and more immediately, I wonder if anybody has advice on how to handle this, or if they've had similar experiences with their friends. I do care about this person, and I was devastated to hear this news, so if there's something I can do to help her, I want to. Of course, I would prefer most that she stop worrying about religion entirely and just grok the math that makes religious hypotheses so unlikely as to not be worth your time. But in the short term I'd settle for her not becoming a Catholic, and not immersing herself further in <a href=\"/lw/uy/dark_side_epistemology/\">Dark Side Epistemology</a> or surrounding herself with people trying to convince her that she needs to \"repent\" of her sexuality.</p>\n<p>I think I have a pretty good understanding of the theoretical concepts at stake here, but I'm not sure where to start or what style of argument is likely to have the best effect at this point. My tentative plan is to express my concern, try to get more information about what she's thinking, and get a dialogue going (I expect she'll be open to this), but I wanted to see if you all had more specific suggestions, especially if you've been through similar experiences yourself. Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WiN2NGeyPMFWCm6fD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 19, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "17057", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 204, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["X2AD2LgtKgkRNPj2a", "XTWkjCJScy2GFAgDt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-18T20:37:06.920Z", "modifiedAt": null, "url": null, "title": "Availability Heuristic and Getting Stranded: Stay With Your Car Or Seek Help?", "slug": "availability-heuristic-and-getting-stranded-stay-with-your", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:09.560Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JQuinton", "createdAt": "2011-04-29T16:29:16.319Z", "isAdmin": false, "displayName": "JQuinton"}, "userId": "edHNo35soNo2w6ZwN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FoaMGKsfcAvtXNbMg/availability-heuristic-and-getting-stranded-stay-with-your", "pageUrlRelative": "/posts/FoaMGKsfcAvtXNbMg/availability-heuristic-and-getting-stranded-stay-with-your", "linkUrl": "https://www.lesswrong.com/posts/FoaMGKsfcAvtXNbMg/availability-heuristic-and-getting-stranded-stay-with-your", "postedAtFormatted": "Monday, June 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Availability%20Heuristic%20and%20Getting%20Stranded%3A%20Stay%20With%20Your%20Car%20Or%20Seek%20Help%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAvailability%20Heuristic%20and%20Getting%20Stranded%3A%20Stay%20With%20Your%20Car%20Or%20Seek%20Help%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoaMGKsfcAvtXNbMg%2Favailability-heuristic-and-getting-stranded-stay-with-your%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Availability%20Heuristic%20and%20Getting%20Stranded%3A%20Stay%20With%20Your%20Car%20Or%20Seek%20Help%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoaMGKsfcAvtXNbMg%2Favailability-heuristic-and-getting-stranded-stay-with-your", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoaMGKsfcAvtXNbMg%2Favailability-heuristic-and-getting-stranded-stay-with-your", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<p>In every single news story that I can remember about someone getting stranded with a broken-down car, the person leaves their car looking for help and they end up dead. Or, if there are multiple people in the car, the <a href=\"http://www.kpho.com/story/18803233/mcso-woman-found-dead-in-desert-idd?hpt=u\">one person who goes off looking for help ends up dead</a> and the other people who stayed with the car survive.</p>\r\n<p>My question: If I get stranded in my car somewhere, should I go looking for help? Or should I follow my availability heuristic and stay put? Since I'm still in the process of debiasing myself, should I do the opposite of what this particular mental shortcut suggests, or would that be a sort of bias bias (analogous to the <a href=\"http://en.wikipedia.org/wiki/Argument_from_fallacy\">fallacy fallacy</a>)?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb19c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FoaMGKsfcAvtXNbMg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 9.253350404152087e-07, "legacy": true, "legacyId": "17059", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-18T20:54:11.305Z", "modifiedAt": null, "url": null, "title": "OB Repost: \"Plastination is Near\"", "slug": "ob-repost-plastination-is-near", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aCsQt8MXLHzCceHG6/ob-repost-plastination-is-near", "pageUrlRelative": "/posts/aCsQt8MXLHzCceHG6/ob-repost-plastination-is-near", "linkUrl": "https://www.lesswrong.com/posts/aCsQt8MXLHzCceHG6/ob-repost-plastination-is-near", "postedAtFormatted": "Monday, June 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20OB%20Repost%3A%20%22Plastination%20is%20Near%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOB%20Repost%3A%20%22Plastination%20is%20Near%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaCsQt8MXLHzCceHG6%2Fob-repost-plastination-is-near%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=OB%20Repost%3A%20%22Plastination%20is%20Near%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaCsQt8MXLHzCceHG6%2Fob-repost-plastination-is-near", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaCsQt8MXLHzCceHG6%2Fob-repost-plastination-is-near", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<blockquote>\n<p>\n<p style=\"border: 0px; margin: 0px 0px 12px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">These brain research techniques have now reached two key milestones:</p>\n<ol style=\"border: 0px; margin: 0px 0px 12px 40px; padding: 0px; vertical-align: baseline; color: #353537; font-family: Arial, Helvetica, sans-serif; font-size: 15px; line-height: 22px;\">\n<li style=\"background-color: transparent; border: 0px; margin: 0px; padding: 0px; vertical-align: baseline; background-position: initial initial; background-repeat: initial initial;\">They&rsquo;ve found new ways to &ldquo;fix&rdquo; brain samples by filling them with plastic, ways that seem impressively reliable, resilient, and long lasting, and which work on large brain volumes (e.g.,&nbsp;<a style=\"background-color: transparent; border: 0px; margin: 0px; padding: 0px; vertical-align: baseline; color: #314c85; background-position: initial initial; background-repeat: initial initial;\" href=\"http://homes.mpimf-heidelberg.mpg.de/~smikula/MIV.html?path=blockface/sm1329/&amp;width=97335&amp;height=152116&amp;notilegroups=1\">here</a>). Such&nbsp;<a style=\"background-color: transparent; border: 0px; margin: 0px; padding: 0px; vertical-align: baseline; color: #314c85; background-position: initial initial; background-repeat: initial initial;\" href=\"http://www.gwern.net/plastination\">plastination</a>techniques seem close to being able to save enough info in entire brains for centuries, without needing continual care. Just dumping a plastic brain in a box in a closet might work fine.</li>\n<li style=\"background-color: transparent; border: 0px; margin: 0px; padding: 0px; vertical-align: baseline; background-position: initial initial; background-repeat: initial initial;\">Today, for a few tens of thousands of dollars, less than the price charged for one cryonics customer, it is feasible to have independent lab(s) take random samples from whole mouse or human brains preserved via either cryonics or plastination, and do high (5nm) resolution 3D scans to map out thousands of neighboring cells, their connections, and connection strengths, to test if either of these approaches clearly preserve such key brain info.</li>\n</ol></p>\n</blockquote>\n<div><span style=\"font-family: Arial, Helvetica, sans-serif; color: #353537;\"><span style=\"font-size: 15px; line-height: 22px;\"><a href=\"http://www.overcomingbias.com/2012/06/plastination-is-near.html\">Plastination is Near</a></span></span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aCsQt8MXLHzCceHG6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "17060", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-19T04:04:59.360Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] What Would You Do Without Morality?", "slug": "seq-rerun-what-would-you-do-without-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:09.488Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5pMM7zYo6qtPFW2NE/seq-rerun-what-would-you-do-without-morality", "pageUrlRelative": "/posts/5pMM7zYo6qtPFW2NE/seq-rerun-what-would-you-do-without-morality", "linkUrl": "https://www.lesswrong.com/posts/5pMM7zYo6qtPFW2NE/seq-rerun-what-would-you-do-without-morality", "postedAtFormatted": "Tuesday, June 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20What%20Would%20You%20Do%20Without%20Morality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20What%20Would%20You%20Do%20Without%20Morality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pMM7zYo6qtPFW2NE%2Fseq-rerun-what-would-you-do-without-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20What%20Would%20You%20Do%20Without%20Morality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pMM7zYo6qtPFW2NE%2Fseq-rerun-what-would-you-do-without-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pMM7zYo6qtPFW2NE%2Fseq-rerun-what-would-you-do-without-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>Today's post, <a href=\"/lw/rq/what_would_you_do_without_morality/\">What Would You Do Without Morality?</a> was originally published on 29 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If your own theory of morality was disproved, and you were persuaded that there was no morality, that everything was permissible and nothing was forbidden, what would you do? Would you still tip cabdrivers?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d54/seq_rerun_the_opposite_sex/\">The Opposite Sex</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5pMM7zYo6qtPFW2NE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 9.255401677864076e-07, "legacy": true, "legacyId": "17064", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iGH7FSrdoCXa5AHGs", "rxY4FB8mmGoPwo8bq", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-19T04:06:40.905Z", "modifiedAt": null, "url": null, "title": "[Meetup] NYC Megameetup - Location and Potluck Info", "slug": "meetup-nyc-megameetup-location-and-potluck-info", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:25.555Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DuZ2xb3LDypGvyPkC/meetup-nyc-megameetup-location-and-potluck-info", "pageUrlRelative": "/posts/DuZ2xb3LDypGvyPkC/meetup-nyc-megameetup-location-and-potluck-info", "linkUrl": "https://www.lesswrong.com/posts/DuZ2xb3LDypGvyPkC/meetup-nyc-megameetup-location-and-potluck-info", "postedAtFormatted": "Tuesday, June 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMeetup%5D%20NYC%20Megameetup%20-%20Location%20and%20Potluck%20Info&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMeetup%5D%20NYC%20Megameetup%20-%20Location%20and%20Potluck%20Info%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDuZ2xb3LDypGvyPkC%2Fmeetup-nyc-megameetup-location-and-potluck-info%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMeetup%5D%20NYC%20Megameetup%20-%20Location%20and%20Potluck%20Info%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDuZ2xb3LDypGvyPkC%2Fmeetup-nyc-megameetup-location-and-potluck-info", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDuZ2xb3LDypGvyPkC%2Fmeetup-nyc-megameetup-location-and-potluck-info", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<p>This Saturday, June 23 is the Less Wrong Summer Festival Megameetup. Anyone who feels like coming to New York City is invited to attend. The official meetup description is <a href=\"/meetups/ap\">here</a>.</p>\n<p>So far the forecast for Saturday is sunny, so the event will be held outdoors in Central Park. In the event of last minute rain, we'll be heading to an apartment in Brooklyn. I'll post the address if necessary, otherwise assume the meetup is proceeding normally.</p>\n<p>We'll be meeting up at 2 PM on the East Side entrance at 97th street and 5th Avenue, and then heading to the nearby North Meadow. If you miss the initial gathering, just look for someone holding a large aluminum pole in the nearby field.</p>\n<p>The event is a potluck picnic. It's recommended that you bring some food to share and blankets to lay on.</p>\n<p>Hope to see you there.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DuZ2xb3LDypGvyPkC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [], "voteCount": 0, "baseScore": 0, "extendedScore": null, "score": 9.255409430470767e-07, "legacy": true, "legacyId": "17065", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": null, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-19T04:42:04.892Z", "modifiedAt": null, "url": null, "title": "Suggest alternate names for the \"Singularity Institute\"", "slug": "suggest-alternate-names-for-the-singularity-institute", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:39.023Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tnTxb3LJXJP6rRnM7/suggest-alternate-names-for-the-singularity-institute", "pageUrlRelative": "/posts/tnTxb3LJXJP6rRnM7/suggest-alternate-names-for-the-singularity-institute", "linkUrl": "https://www.lesswrong.com/posts/tnTxb3LJXJP6rRnM7/suggest-alternate-names-for-the-singularity-institute", "postedAtFormatted": "Tuesday, June 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suggest%20alternate%20names%20for%20the%20%22Singularity%20Institute%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuggest%20alternate%20names%20for%20the%20%22Singularity%20Institute%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtnTxb3LJXJP6rRnM7%2Fsuggest-alternate-names-for-the-singularity-institute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suggest%20alternate%20names%20for%20the%20%22Singularity%20Institute%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtnTxb3LJXJP6rRnM7%2Fsuggest-alternate-names-for-the-singularity-institute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtnTxb3LJXJP6rRnM7%2Fsuggest-alternate-names-for-the-singularity-institute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 393, "htmlBody": "<p>Once, a smart potential supporter stumbled upon the <a href=\"http://intelligence.org/\">Singularity Institute</a>'s (old) website and wanted to know if our mission was something to care about. So he sent our <a href=\"http://intelligence.org/summary/\">concise summary</a> to an AI researcher and asked if we were serious. The AI researcher saw the word 'Singularity' and, apparently without reading our concise summary, sent back a critique of Ray Kurzweil's \"accelerating change\" technology curves.&nbsp;(Even though SI researchers tend to be Moore's Law agnostics, and our concise summary says nothing about accelerating change.)</p>\n<p>Of course, the 'singularity' we're talking about at SI is <a href=\"http://facingthesingularity.com/\">intelligence explosion, not accelerating change</a>, and intelligence explosion doesn't depend on accelerating change. The term \"singularity\"&nbsp;<em>used</em>&nbsp;to mean intelligence explosion (or \"the arrival of machine superintelligence\" or \"an event horizon beyond which we can't predict the future because something smarter than humans is running the show\"). But with the success of <em><a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\">The Singularity is Near</a></em>&nbsp;in 2005, most people know \"the singularity\" as \"accelerating change.\"</p>\n<p>How often do we miss out on connecting to smart people because they think we're arguing for Kurzweil's curves? One friend in the U.K. told me he never uses the world \"singularity\" to talk about AI risk because the people he knows thinks the \"accelerating change\" singularity is \"a bit mental.\"&nbsp;</p>\n<p>LWers are likely to have attachments to the word 'singularity,' and the term <em>does</em>&nbsp;often mean intelligence explosion in <a href=\"http://singularityhypothesis.blogspot.com/\">the</a> <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">technical</a><a href=\"http://consc.net/papers/singularityjcs.pdf\"> literature</a>, but neither of these is a strong reason to keep the word 'singularity' in the name of our AI Risk Reduction organization. If the 'singularity' term is keeping us away from many of the people we care most about reaching, maybe we should change it.</p>\n<p>Here are some possible alternatives, without trying too hard:</p>\n<p>&nbsp;</p>\n<ul>\n<li>The Center for AI Safety</li>\n<li>The I.J. Good Institute</li>\n<li>Beneficial Architectures Research</li>\n<li>A.I. Impacts Research</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong>We almost certainly won't change our name within the next year</strong>, but it doesn't hurt to start gathering names now and do some market testing. You were all <a href=\"/lw/9lx/help_name_suggestions_needed_for_rationalityinst/\">very helpful</a> in naming \"Rationality Group\". (BTW, the winning name, \"Center for Applied Rationality,\" came from LWer <a href=\"/lw/9lx/help_name_suggestions_needed_for_rationalityinst/5s0k\">beoShaffer</a>.)</p>\n<p>And, before I am vilified by people who have as much positive affect toward the name \"Singularity Institute\" as I do, let me note that this was <em>not originally my idea</em>, but I do think it's an idea worth taking seriously enough to bother with some market testing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tnTxb3LJXJP6rRnM7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 33, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "12001", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 159, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YzzcrM92toD9dudau"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-19T16:46:56.934Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Book Discussion: \"Thinking, Fast and Slow\"", "slug": "meetup-west-la-meetup-book-discussion-thinking-fast-and-slow", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LtuC3PGkY8dv7mTuT/meetup-west-la-meetup-book-discussion-thinking-fast-and-slow", "pageUrlRelative": "/posts/LtuC3PGkY8dv7mTuT/meetup-west-la-meetup-book-discussion-thinking-fast-and-slow", "linkUrl": "https://www.lesswrong.com/posts/LtuC3PGkY8dv7mTuT/meetup-west-la-meetup-book-discussion-thinking-fast-and-slow", "postedAtFormatted": "Tuesday, June 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Book%20Discussion%3A%20%22Thinking%2C%20Fast%20and%20Slow%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Book%20Discussion%3A%20%22Thinking%2C%20Fast%20and%20Slow%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtuC3PGkY8dv7mTuT%2Fmeetup-west-la-meetup-book-discussion-thinking-fast-and-slow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Book%20Discussion%3A%20%22Thinking%2C%20Fast%20and%20Slow%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtuC3PGkY8dv7mTuT%2Fmeetup-west-la-meetup-book-discussion-thinking-fast-and-slow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtuC3PGkY8dv7mTuT%2Fmeetup-west-la-meetup-book-discussion-thinking-fast-and-slow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/b8'>West LA Meetup - Book Discussion: &quot;Thinking, Fast and Slow&quot;</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, June 20th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week we will discuss <a href=\"http://en.wikipedia.org/wiki/Daniel_Kahneman\" rel=\"nofollow\">Daniel Kahneman&#39;s</a> book titled <a href=\"http://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\" rel=\"nofollow\">Thinking, Fast and Slow</a>. You don't need to have read the book, but if you have a chance, read about some <a href=\"http://en.wikipedia.org/wiki/Heuristics\" rel=\"nofollow\">heuristics</a> and <a href=\"http://en.wikipedia.org/wiki/Cognitive_bias\" rel=\"nofollow\">biases</a>, or read up on <a href=\"http://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\" rel=\"nofollow\">prospect theory</a>.</p>\n\n<p>But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>We will probably also find time for a game, like The Resistance.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/b8'>West LA Meetup - Book Discussion: &quot;Thinking, Fast and Slow&quot;</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LtuC3PGkY8dv7mTuT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.258893317556026e-07, "legacy": true, "legacyId": "17084", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Book_Discussion___Thinking__Fast_and_Slow_\">Discussion article for the meetup : <a href=\"/meetups/b8\">West LA Meetup - Book Discussion: \"Thinking, Fast and Slow\"</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, June 20th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week we will discuss <a href=\"http://en.wikipedia.org/wiki/Daniel_Kahneman\" rel=\"nofollow\">Daniel Kahneman's</a> book titled <a href=\"http://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\" rel=\"nofollow\">Thinking, Fast and Slow</a>. You don't need to have read the book, but if you have a chance, read about some <a href=\"http://en.wikipedia.org/wiki/Heuristics\" rel=\"nofollow\">heuristics</a> and <a href=\"http://en.wikipedia.org/wiki/Cognitive_bias\" rel=\"nofollow\">biases</a>, or read up on <a href=\"http://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\" rel=\"nofollow\">prospect theory</a>.</p>\n\n<p>But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>We will probably also find time for a game, like The Resistance.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Book_Discussion___Thinking__Fast_and_Slow_1\">Discussion article for the meetup : <a href=\"/meetups/b8\">West LA Meetup - Book Discussion: \"Thinking, Fast and Slow\"</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Book Discussion: \"Thinking, Fast and Slow\"", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Book_Discussion___Thinking__Fast_and_Slow_", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Book Discussion: \"Thinking, Fast and Slow\"", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Book_Discussion___Thinking__Fast_and_Slow_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-19T20:14:15.842Z", "modifiedAt": null, "url": null, "title": "[LINK] Steven Pinker on \"The false allure of group selection\"", "slug": "link-steven-pinker-on-the-false-allure-of-group-selection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:26.947Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hairyfigment", "createdAt": "2010-10-14T22:12:59.035Z", "isAdmin": false, "displayName": "hairyfigment"}, "userId": "NesjW63eueLsbKrCY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rnwHgxweTRuWvQmhY/link-steven-pinker-on-the-false-allure-of-group-selection", "pageUrlRelative": "/posts/rnwHgxweTRuWvQmhY/link-steven-pinker-on-the-false-allure-of-group-selection", "linkUrl": "https://www.lesswrong.com/posts/rnwHgxweTRuWvQmhY/link-steven-pinker-on-the-false-allure-of-group-selection", "postedAtFormatted": "Tuesday, June 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Steven%20Pinker%20on%20%22The%20false%20allure%20of%20group%20selection%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Steven%20Pinker%20on%20%22The%20false%20allure%20of%20group%20selection%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnwHgxweTRuWvQmhY%2Flink-steven-pinker-on-the-false-allure-of-group-selection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Steven%20Pinker%20on%20%22The%20false%20allure%20of%20group%20selection%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnwHgxweTRuWvQmhY%2Flink-steven-pinker-on-the-false-allure-of-group-selection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnwHgxweTRuWvQmhY%2Flink-steven-pinker-on-the-false-allure-of-group-selection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<p><a href=\"http://edge.org/conversation/the-false-allure-of-group-selection\">This essay at Edge</a> touches on a few possible meanings for the term \"group selection.\" Pinker argues that as a form of memetic theory it has no explanatory power, and that group selection for genes does not fit the evidence. He focuses on humans with some mention of insects that live in hives. So the essay doesn't seem surprising, but it does seem rather Hansonian.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rnwHgxweTRuWvQmhY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "17087", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-19T21:25:43.773Z", "modifiedAt": null, "url": null, "title": "Ekman Training - Reviews and/or Testing", "slug": "ekman-training-reviews-and-or-testing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.023Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benquo", "createdAt": "2009-03-06T00:17:35.184Z", "isAdmin": false, "displayName": "Benquo"}, "userId": "nt2XsHkdksqZ3snNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q7Z4RoTS7v8T3Eo2f/ekman-training-reviews-and-or-testing", "pageUrlRelative": "/posts/Q7Z4RoTS7v8T3Eo2f/ekman-training-reviews-and-or-testing", "linkUrl": "https://www.lesswrong.com/posts/Q7Z4RoTS7v8T3Eo2f/ekman-training-reviews-and-or-testing", "postedAtFormatted": "Tuesday, June 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ekman%20Training%20-%20Reviews%20and%2For%20Testing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEkman%20Training%20-%20Reviews%20and%2For%20Testing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7Z4RoTS7v8T3Eo2f%2Fekman-training-reviews-and-or-testing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ekman%20Training%20-%20Reviews%20and%2For%20Testing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7Z4RoTS7v8T3Eo2f%2Fekman-training-reviews-and-or-testing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7Z4RoTS7v8T3Eo2f%2Fekman-training-reviews-and-or-testing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<p>I'm considering taking Ekman's <a href=\"https://face.paulekman.com/face/products.aspx\">microexpressions training</a> because it's cheap in both time and money. Has anyone here taken it? Did it work for you? How do you know?</p>\n<p>&nbsp;</p>\n<p>The course does seem to come with tests included (both before and after), but if anyone has any ideas for some <strong>cheap</strong> tests I can do before and after to see if it really works, I'd be happy to do those as well, and report the results. Cheap tests should cost me less than three hours total and less than $100 total.</p>\n<p>&nbsp;</p>\n<p>Alternately, if enough people here have done it we could pool our \"before\" and \"after\" scores to independently verify whether there's an effect.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q7Z4RoTS7v8T3Eo2f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 16, "extendedScore": null, "score": 9.260171398421981e-07, "legacy": true, "legacyId": "17088", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-19T23:44:48.513Z", "modifiedAt": null, "url": null, "title": "Advice- Places to live", "slug": "advice-places-to-live", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:15.834Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Michelle_Z", "createdAt": "2011-07-14T22:50:16.205Z", "isAdmin": false, "displayName": "Michelle_Z"}, "userId": "ExbXRgKKWx59L4m4W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RknTpaRWnzPgyCyBc/advice-places-to-live", "pageUrlRelative": "/posts/RknTpaRWnzPgyCyBc/advice-places-to-live", "linkUrl": "https://www.lesswrong.com/posts/RknTpaRWnzPgyCyBc/advice-places-to-live", "postedAtFormatted": "Tuesday, June 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice-%20Places%20to%20live&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice-%20Places%20to%20live%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRknTpaRWnzPgyCyBc%2Fadvice-places-to-live%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice-%20Places%20to%20live%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRknTpaRWnzPgyCyBc%2Fadvice-places-to-live", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRknTpaRWnzPgyCyBc%2Fadvice-places-to-live", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 410, "htmlBody": "<p>Hi!</p>\n<p>I have been wondering (the last few days) on where would be a good place to live and work. This is not a \"I-am-moving-out-in-two-months\" type of idea, but a long term, far goal. Basically, when I graduate college (or even a couple years after that- I need to save up money, first!) I may want to move away from my hometown to someplace that is a bit more... *forward* thinking. I've been doing searches on google, but so far have not found what I need.</p>\n<p>What am I looking for:</p>\n<p>At the moment, I am considering being a biologist, specifically a molecular biologist, though that might change. For this post, I am going to assume I keep this goal. Are there any place in the US that have a market or need for biologists? Is there a science-centric, or place where science-minded people live? I know that is vague, but I'm not sure how else to put it. I just recognize that if I were to have a discussion about science or cogsci or anything similar in my current community, I would get strange looks, and lose status.</p>\n<p>If such a place exists, a bonus would be an active LW community.</p>\n<p>I'm not sure if I will or won't like moving, so moving multiple times is something I am not really considering at the moment, and since I do eventually (far far down the road) plan on having children, and those children would require a really *really* good education, I would want someplace that has a good education system. It's a bit of a pet peeve of mine, since my educational experience was so awful, so I am dedicated to making sure that does *not* happen to my (far far in the future) family. (Yes, I do know that a lot of educational issues stem from a whole combination of things, and I know a good school system is not a fix-all, but it would help.)</p>\n<p>I've never moved before, so I wouldn't even know where to begin. My family doesn't even go on vacations. I've never been on a plane, nor do I know any protocol for moving between states, or moving in general. The most moving I've ever experienced was the move to college, and that is only 20 minutes from home. Thinking about it makes it seem stressful, so the more information I have, the better I can plan, and the less stressful this will eventually be for me.</p>\n<p>&nbsp;</p>\n<p>Thanks for the help!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RknTpaRWnzPgyCyBc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 0, "extendedScore": null, "score": 9.260809128357026e-07, "legacy": true, "legacyId": "17089", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-19T23:50:46.523Z", "modifiedAt": null, "url": null, "title": "A few questions about Discussion", "slug": "a-few-questions-about-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:15.150Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Blackened", "createdAt": "2012-05-09T10:50:12.050Z", "isAdmin": false, "displayName": "Blackened"}, "userId": "qNeT4N9rP7tSxwquv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KYXD428xy4gmYDiEH/a-few-questions-about-discussion", "pageUrlRelative": "/posts/KYXD428xy4gmYDiEH/a-few-questions-about-discussion", "linkUrl": "https://www.lesswrong.com/posts/KYXD428xy4gmYDiEH/a-few-questions-about-discussion", "postedAtFormatted": "Tuesday, June 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20few%20questions%20about%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20few%20questions%20about%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYXD428xy4gmYDiEH%2Fa-few-questions-about-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20few%20questions%20about%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYXD428xy4gmYDiEH%2Fa-few-questions-about-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYXD428xy4gmYDiEH%2Fa-few-questions-about-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 123, "htmlBody": "<p>Since there aren't any subforums and I couldn't find a thread with information of what is relevant for Discussion, and I saw the threads here are relatively free, I've decided to ask my questions in a thread.</p>\n<p>- Can I post questions about this section? (sorry if no)<strong></strong></p>\n<p>- Can I post about psychology in general?</p>\n<p>- Can I post about anything that might be of the interest of a rationalist? Like for example, a thread asking about how to reduce the risks for most cancers. Note that this is a huge range of possible questions.</p>\n<p>Edit: thanks, I think I figured it out. I'm still not sure if I want to respond to all the comments, whether I should comment myself or edit the original post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KYXD428xy4gmYDiEH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 9.260836490062012e-07, "legacy": true, "legacyId": "17090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T01:33:45.560Z", "modifiedAt": null, "url": null, "title": "Looking for an intuitive explanation of expected outcome", "slug": "looking-for-an-intuitive-explanation-of-expected-outcome", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:22.989Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Blackened", "createdAt": "2012-05-09T10:50:12.050Z", "isAdmin": false, "displayName": "Blackened"}, "userId": "qNeT4N9rP7tSxwquv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/on6QQo9btXoKBFPjE/looking-for-an-intuitive-explanation-of-expected-outcome", "pageUrlRelative": "/posts/on6QQo9btXoKBFPjE/looking-for-an-intuitive-explanation-of-expected-outcome", "linkUrl": "https://www.lesswrong.com/posts/on6QQo9btXoKBFPjE/looking-for-an-intuitive-explanation-of-expected-outcome", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Looking%20for%20an%20intuitive%20explanation%20of%20expected%20outcome&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALooking%20for%20an%20intuitive%20explanation%20of%20expected%20outcome%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fon6QQo9btXoKBFPjE%2Flooking-for-an-intuitive-explanation-of-expected-outcome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Looking%20for%20an%20intuitive%20explanation%20of%20expected%20outcome%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fon6QQo9btXoKBFPjE%2Flooking-for-an-intuitive-explanation-of-expected-outcome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fon6QQo9btXoKBFPjE%2Flooking-for-an-intuitive-explanation-of-expected-outcome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 312, "htmlBody": "<p>I'll first explain how I see expected outcome, because I'm not sure my definition is the same as the widely accepted definition.</p>\n<p>If I have 50% chance to win 10$, I take it as there are two alternative universes, the only difference being that in one of them, I win 10$ and in the other one, I win nothing. Then I treat the 50% chance as 100% chance to be in both of them, divided by two. If winning 10$ means I'll save myself from 1 hour of work, when divided by two it would be 30 minutes of work. In virtually all cases, when it's about winning small sums of money, you can simply multiply the percentage by the money (in this case, we'll get 5$). Exceptions would be the cases analogous to the one where I'm dying of an illness, I can't afford treatment, but I have all the money I need except for the last 10$ and there isn't any other way to obtain them. So if there's 30% chance to save 10 people's lives, that's the same as saving 3 lives.</p>\n<p>If you have no idea what you're talking about, then at least you can see a proof of my problem: I find it hard to explain this idea to people, and impossible for some.</p>\n<p>I'm not even sure if the idea is correct. I once posted it on a math forum, asking for evidence, but I didn't find any. So, can someone confirm whether is true, also giving any evidence?</p>\n<p>And my main question is, how can I explain this in a way that people can understand it as easily as possible.</p>\n<p>(it is possible that it's not clear what I meant - I'll check this thread later for that, and if it turns out to be the case, I'll edit it and add more examples and try to clarify and simplify)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "on6QQo9btXoKBFPjE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 0, "extendedScore": null, "score": 9.261308762955516e-07, "legacy": true, "legacyId": "17091", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T03:40:41.458Z", "modifiedAt": null, "url": null, "title": "Contest for Women Who Want to Meet Our Awesome Berkeley Crew!", "slug": "contest-for-women-who-want-to-meet-our-awesome-berkeley-crew", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ShannonFriedman", "createdAt": "2012-06-19T16:21:31.296Z", "isAdmin": false, "displayName": "ShannonFriedman"}, "userId": "yzRAjgwgXY3bbapsP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WACDovpZri7CDLKS9/contest-for-women-who-want-to-meet-our-awesome-berkeley-crew", "pageUrlRelative": "/posts/WACDovpZri7CDLKS9/contest-for-women-who-want-to-meet-our-awesome-berkeley-crew", "linkUrl": "https://www.lesswrong.com/posts/WACDovpZri7CDLKS9/contest-for-women-who-want-to-meet-our-awesome-berkeley-crew", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Contest%20for%20Women%20Who%20Want%20to%20Meet%20Our%20Awesome%20Berkeley%20Crew!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AContest%20for%20Women%20Who%20Want%20to%20Meet%20Our%20Awesome%20Berkeley%20Crew!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWACDovpZri7CDLKS9%2Fcontest-for-women-who-want-to-meet-our-awesome-berkeley-crew%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Contest%20for%20Women%20Who%20Want%20to%20Meet%20Our%20Awesome%20Berkeley%20Crew!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWACDovpZri7CDLKS9%2Fcontest-for-women-who-want-to-meet-our-awesome-berkeley-crew", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWACDovpZri7CDLKS9%2Fcontest-for-women-who-want-to-meet-our-awesome-berkeley-crew", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p>Are you a woman who wants a guy who's logical, reasonable, empirical, a careful thinker, reflective, doesn't believe in astrology like those crazy guys from your old high school, in a word, a rationalist?</p>\n<p><span>There is a thriving rationalist community in Berkeley, but unfortunately it is terribly gender imbalanced. &nbsp;Being a counselor in the extended community, I&rsquo;ve talked to a lot of people who are very isolated of both genders around the world, who are dying to meet other people who think like they do. &nbsp;My goal is to help everyone, but for now, I&rsquo;m starting with cases that are fun and easy. &nbsp;</span></p>\n<p><span>Thus, since there is such a high demand for rationalist women here, and there happens to be a bedroom free in my home for July and August, my housemates and I are offering a free stay in this room in downtown Berkeley for a week each to the two women who send me the most compelling questionnaires. &nbsp;&nbsp;</span></p>\n<p><span>The sorts of things that I am likely to find compelling are:</span></p>\n<ul>\n<li><span>Caring about progress for humanity in the areas of living longer, healthier, and happier</span></li>\n<li><span>Caring about FAI</span></li>\n<li><span>Being agenty</span></li>\n<li><span>Having interests in the sort of geeky things that a lot of the guys like (math/sciences)</span></li>\n</ul>\n<p>While you&rsquo;re here, you can meet our many eligible bachelors (mostly in their twenties), plus we&rsquo;ll introduce you around to the rest of the team - Anna, Eliezer, etc. &nbsp;<br /><br /><a href=\"http://shannonfriedman.wufoo.com/forms/entry-form/\"><span>Click here to go to the form!</span></a></p>\n<p>p.s. &nbsp;If this post ends up getting popular, I encourage agentiness in the comment section regarding the subject matter. &nbsp;Do you wish that you could meet someone as cool as you are in the ways that you are awesome, and do you have an offer or request that you might be able to make enticing? &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WACDovpZri7CDLKS9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "17085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T04:25:18.776Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Moral Void", "slug": "seq-rerun-the-moral-void", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:15.741Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kJFsuJPPcwHWzmitf/seq-rerun-the-moral-void", "pageUrlRelative": "/posts/kJFsuJPPcwHWzmitf/seq-rerun-the-moral-void", "linkUrl": "https://www.lesswrong.com/posts/kJFsuJPPcwHWzmitf/seq-rerun-the-moral-void", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Moral%20Void&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Moral%20Void%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkJFsuJPPcwHWzmitf%2Fseq-rerun-the-moral-void%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Moral%20Void%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkJFsuJPPcwHWzmitf%2Fseq-rerun-the-moral-void", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkJFsuJPPcwHWzmitf%2Fseq-rerun-the-moral-void", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<p>Today's post, <a href=\"/lw/rr/the_moral_void/\">The Moral Void</a> was originally published on 30 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If there were some great stone tablet upon which Morality was written, and you read it, and it was something horrible, that would be a rather unpleasant scenario. What would you want that tablet to say, if you could choose it? What would be the best case scenario?</blockquote>\n<blockquote>Why don't you just do that, and ignore the tablet completely?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d60/seq_rerun_what_would_you_do_without_morality/\">What Would You Do Without Morality?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kJFsuJPPcwHWzmitf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 9.262095583327769e-07, "legacy": true, "legacyId": "17093", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K9JSM7d7bLJguMxEp", "5pMM7zYo6qtPFW2NE", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T06:20:58.059Z", "modifiedAt": null, "url": null, "title": "Hyderabad Meetup : July 8, 2012", "slug": "hyderabad-meetup-july-8-2012", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nagendran", "createdAt": "2011-05-21T07:14:17.547Z", "isAdmin": false, "displayName": "Nagendran"}, "userId": "iDiDX7crw4zFn3EzP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5ab998Rh8RKy6yJM3/hyderabad-meetup-july-8-2012", "pageUrlRelative": "/posts/5ab998Rh8RKy6yJM3/hyderabad-meetup-july-8-2012", "linkUrl": "https://www.lesswrong.com/posts/5ab998Rh8RKy6yJM3/hyderabad-meetup-july-8-2012", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hyderabad%20Meetup%20%3A%20July%208%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHyderabad%20Meetup%20%3A%20July%208%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ab998Rh8RKy6yJM3%2Fhyderabad-meetup-july-8-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hyderabad%20Meetup%20%3A%20July%208%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ab998Rh8RKy6yJM3%2Fhyderabad-meetup-july-8-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ab998Rh8RKy6yJM3%2Fhyderabad-meetup-july-8-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<address><span style=\"font-style: normal;\">There has been no Lesswrong meetup in India in 2012. I suggest having a meetup in the afternoon on Sunday, July 8 at</span>&nbsp;<span style=\"font-family: mceinline;\"><span style=\"background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">Cafe Coffee Day,&nbsp;</span><span style=\"background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">NEXT TO Airtel Showroom,&nbsp;</span><span style=\"background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">Madhapur Main Road, Madhapur, Hyd (</span><span style=\"background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">Landmark: OPPOSITE Krishe Sapphire)</span></span></address>\n<pre><span style=\"font-size: x-small;\"><span style=\"white-space: normal;\"><em><br /></em></span></span></pre>\n<address><span style=\"font-style: normal;\">Me and couple of my friends commit ourselves to be there from 3pm to 6pm.&nbsp;<br />Comments showing interest will be greatly appreciated.</span></address><address><br /></address><address><span style=\"font-style: normal;\">Ph: +91 9566067018</span></address>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5ab998Rh8RKy6yJM3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 9.262626091735508e-07, "legacy": true, "legacyId": "17105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T06:26:20.288Z", "modifiedAt": null, "url": null, "title": "Meetup : Hyderabad Meetup", "slug": "meetup-hyderabad-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:33.538Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nagendran", "createdAt": "2011-05-21T07:14:17.547Z", "isAdmin": false, "displayName": "Nagendran"}, "userId": "iDiDX7crw4zFn3EzP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ScQ7Q25Aa9eXiciZW/meetup-hyderabad-meetup", "pageUrlRelative": "/posts/ScQ7Q25Aa9eXiciZW/meetup-hyderabad-meetup", "linkUrl": "https://www.lesswrong.com/posts/ScQ7Q25Aa9eXiciZW/meetup-hyderabad-meetup", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Hyderabad%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Hyderabad%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FScQ7Q25Aa9eXiciZW%2Fmeetup-hyderabad-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Hyderabad%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FScQ7Q25Aa9eXiciZW%2Fmeetup-hyderabad-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FScQ7Q25Aa9eXiciZW%2Fmeetup-hyderabad-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/b9'>Hyderabad Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 July 2012 03:00:00PM (+0530)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Hyderabad, India</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cafe Coffee Day, NEXT TO Airtel Showroom, Madhapur Main Road, Madhapur, Hyderabad (Landmark: OPPOSITE Krishe Sapphire)</p>\n\n<p>Ph: +91 9566057018</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/b9'>Hyderabad Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ScQ7Q25Aa9eXiciZW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.262650727394878e-07, "legacy": true, "legacyId": "17106", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Hyderabad_Meetup\">Discussion article for the meetup : <a href=\"/meetups/b9\">Hyderabad Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 July 2012 03:00:00PM (+0530)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Hyderabad, India</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cafe Coffee Day, NEXT TO Airtel Showroom, Madhapur Main Road, Madhapur, Hyderabad (Landmark: OPPOSITE Krishe Sapphire)</p>\n\n<p>Ph: +91 9566057018</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Hyderabad_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/b9\">Hyderabad Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Hyderabad Meetup", "anchor": "Discussion_article_for_the_meetup___Hyderabad_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Hyderabad Meetup", "anchor": "Discussion_article_for_the_meetup___Hyderabad_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T08:49:46.065Z", "modifiedAt": null, "url": null, "title": "Meetup : Small Berkeley meetup at Zendo", "slug": "meetup-small-berkeley-meetup-at-zendo", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hfdPpcnrg8JnFEAFD/meetup-small-berkeley-meetup-at-zendo", "pageUrlRelative": "/posts/hfdPpcnrg8JnFEAFD/meetup-small-berkeley-meetup-at-zendo", "linkUrl": "https://www.lesswrong.com/posts/hfdPpcnrg8JnFEAFD/meetup-small-berkeley-meetup-at-zendo", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Small%20Berkeley%20meetup%20at%20Zendo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Small%20Berkeley%20meetup%20at%20Zendo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhfdPpcnrg8JnFEAFD%2Fmeetup-small-berkeley-meetup-at-zendo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Small%20Berkeley%20meetup%20at%20Zendo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhfdPpcnrg8JnFEAFD%2Fmeetup-small-berkeley-meetup-at-zendo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhfdPpcnrg8JnFEAFD%2Fmeetup-small-berkeley-meetup-at-zendo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ba'>Small Berkeley meetup at Zendo</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">berkeley, ca</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will be another Berkeley meetup at my home, Zendo. The theme will be \"calibration games\". We've done calibration games before but this time I have software that the Center for Applied Rationality is using to train Bayesian commandos in its camps. Doors open at 7pm and games start at 7:30pm.</p>\n\n<p>For the location of Zendo, see the <a href=\"http://groups.google.com/group/bayarealesswrong\">mailing list</a>. This counts as a Small meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ba'>Small Berkeley meetup at Zendo</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hfdPpcnrg8JnFEAFD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.263308718628715e-07, "legacy": true, "legacyId": "17107", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_meetup_at_Zendo\">Discussion article for the meetup : <a href=\"/meetups/ba\">Small Berkeley meetup at Zendo</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">berkeley, ca</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will be another Berkeley meetup at my home, Zendo. The theme will be \"calibration games\". We've done calibration games before but this time I have software that the Center for Applied Rationality is using to train Bayesian commandos in its camps. Doors open at 7pm and games start at 7:30pm.</p>\n\n<p>For the location of Zendo, see the <a href=\"http://groups.google.com/group/bayarealesswrong\">mailing list</a>. This counts as a Small meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_meetup_at_Zendo1\">Discussion article for the meetup : <a href=\"/meetups/ba\">Small Berkeley meetup at Zendo</a></h2>", "sections": [{"title": "Discussion article for the meetup : Small Berkeley meetup at Zendo", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_meetup_at_Zendo", "level": 1}, {"title": "Discussion article for the meetup : Small Berkeley meetup at Zendo", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_meetup_at_Zendo1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T10:15:18.817Z", "modifiedAt": null, "url": null, "title": "[Link] Why don't people like markets?", "slug": "link-why-don-t-people-like-markets", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.317Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xa9mgxkBW6bwGkn72/link-why-don-t-people-like-markets", "pageUrlRelative": "/posts/xa9mgxkBW6bwGkn72/link-why-don-t-people-like-markets", "linkUrl": "https://www.lesswrong.com/posts/xa9mgxkBW6bwGkn72/link-why-don-t-people-like-markets", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Why%20don't%20people%20like%20markets%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Why%20don't%20people%20like%20markets%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxa9mgxkBW6bwGkn72%2Flink-why-don-t-people-like-markets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Why%20don't%20people%20like%20markets%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxa9mgxkBW6bwGkn72%2Flink-why-don-t-people-like-markets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxa9mgxkBW6bwGkn72%2Flink-why-don-t-people-like-markets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<div id=\"body_t1_6v5v\" class=\"comment-content \">\n<div class=\"md\">An <a rel=\"nofollow\" href=\"http://www.cognitionandculture.net/home/blog/35-pascals-blog/2423-why-dont-people-like-markets-the-largely-missing-cognition-and-culture-perspective\"><strong>interesting piece</strong></a> with speculation on possible reasons for why people seem to be biased against markets. To summarize: \n<ul>\n<li><strong>Market processes are not visible</strong>. For instance, when a government taxes its citizens and offers a subsidy to some producers, what is seen is the money taken and the money received. What is unseen is the amount of production that would occur in the absence of such transfers.</li>\n<li><strong>Markets are intrinsically probabilistic and therefore marked with uncertainty</strong>, like other living organisms, we are loss-averse and try to minimise uncertainty</li>\n<li>Humans may be motivated to place their trust in processes that are (or at least seem to be) <strong>driven by agents rather than impersonal factors</strong>.</li>\n</ul>\n<p>The last point reminded me of speculation from the recent LessWrong article <a href=\"/lw/cz7/conspiracy_theories_as_agency_fictions/\">Conspiracy Theories as Agency Fictions</a>:</p>\n<blockquote>\n<p>Do all theories of <a href=\"http://en.wikipedia.org/wiki/Legitimacy_%28political%29\">legitimacy</a> also perhaps rest on the <a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2589\">same cognitive failings</a> that conspiracy theories do? The difference between a shadowy cabal we need to get rid of and an institution worthy of respect may be just some bad luck.</p>\n</blockquote>\n<p>Before thinking about these points and debating them I strongly recommend you read the <strong></strong><a rel=\"nofollow\" href=\"http://www.cognitionandculture.net/home/blog/35-pascals-blog/2423-why-dont-people-like-markets-the-largely-missing-cognition-and-culture-perspective\"><strong>full article</strong></a>.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xa9mgxkBW6bwGkn72", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 10, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "17108", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Peo8jAyjGL9kWoYAH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T11:51:40.861Z", "modifiedAt": null, "url": null, "title": "Personality analysis in terms of parameters", "slug": "personality-analysis-in-terms-of-parameters", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:25.486Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Blackened", "createdAt": "2012-05-09T10:50:12.050Z", "isAdmin": false, "displayName": "Blackened"}, "userId": "qNeT4N9rP7tSxwquv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y8t5uWKxymr8q2vpk/personality-analysis-in-terms-of-parameters", "pageUrlRelative": "/posts/Y8t5uWKxymr8q2vpk/personality-analysis-in-terms-of-parameters", "linkUrl": "https://www.lesswrong.com/posts/Y8t5uWKxymr8q2vpk/personality-analysis-in-terms-of-parameters", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Personality%20analysis%20in%20terms%20of%20parameters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APersonality%20analysis%20in%20terms%20of%20parameters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY8t5uWKxymr8q2vpk%2Fpersonality-analysis-in-terms-of-parameters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Personality%20analysis%20in%20terms%20of%20parameters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY8t5uWKxymr8q2vpk%2Fpersonality-analysis-in-terms-of-parameters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY8t5uWKxymr8q2vpk%2Fpersonality-analysis-in-terms-of-parameters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>(copied and edited from my post in the Facebook LW group)</p>\n<p><span class=\"commentBody\">I suspect that it would be  obvious to most rationalists that the way people judge other people is  flawed. Typically for a heuristic approach, it's correct to a degree,  but with many faults. And it's wasting a big amount of information and a  potential for a more planned approach where you can ask questions that  assess certain qualities and exchange information about people's  personalities by giving their \"parameters\".<br /> <br /> I needn't think of  it in this way, it was natural for me to take this approach as soon as I  learnt my first measurable parameter and its implications (it was IQ). Then I  explored more of them and researched them some more.</span></p>\n<p><span class=\"commentBody\">So far, I know about IQ, rationality (Keith Stanovich's), Big Five personality traits, executive functions, intuition for social situations&nbsp; and a few more things. However, I can't seem to find any literature that helps describe them (how do I detect them in people and what are their implications?) and their implications (how *exactly* is someone with a higher IQ different from someone with a lower IQ?). Also, I can't find literature on other traits. </span></p>\n<p><span class=\"commentBody\">Any literature on any of that would be greatly appreciated. I wonder if there is a book that deals with the whole issue. Also, I need literature about IQ and Big Five, but anything else would still be useful.</span></p>\n<p><span class=\"commentBody\">Is that sort of thing popular on LessWrong?<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y8t5uWKxymr8q2vpk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 9.264143375290636e-07, "legacy": true, "legacyId": "17109", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T13:40:24.642Z", "modifiedAt": null, "url": null, "title": "Less Wrong used to like Bitcoin before it was cool. Time for a revisit?", "slug": "less-wrong-used-to-like-bitcoin-before-it-was-cool-time-for", "viewCount": null, "lastCommentedAt": "2020-11-26T16:21:47.843Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "betterthanwell", "createdAt": "2009-02-28T00:39:39.875Z", "isAdmin": false, "displayName": "betterthanwell"}, "userId": "W9LCYGeCRvhd5aREo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y3Y42GXckQuu6bC2m/less-wrong-used-to-like-bitcoin-before-it-was-cool-time-for", "pageUrlRelative": "/posts/y3Y42GXckQuu6bC2m/less-wrong-used-to-like-bitcoin-before-it-was-cool-time-for", "linkUrl": "https://www.lesswrong.com/posts/y3Y42GXckQuu6bC2m/less-wrong-used-to-like-bitcoin-before-it-was-cool-time-for", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20used%20to%20like%20Bitcoin%20before%20it%20was%20cool.%20Time%20for%20a%20revisit%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20used%20to%20like%20Bitcoin%20before%20it%20was%20cool.%20Time%20for%20a%20revisit%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3Y42GXckQuu6bC2m%2Fless-wrong-used-to-like-bitcoin-before-it-was-cool-time-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20used%20to%20like%20Bitcoin%20before%20it%20was%20cool.%20Time%20for%20a%20revisit%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3Y42GXckQuu6bC2m%2Fless-wrong-used-to-like-bitcoin-before-it-was-cool-time-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3Y42GXckQuu6bC2m%2Fless-wrong-used-to-like-bitcoin-before-it-was-cool-time-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Less Wrong used to like Bitcoin before it was cool. <a href=\"/lw/65e/general_bitcoin_discussion_thread_june_2011/\">Monthly threads</a> popped up around the same time a pricing bubble brought mainstream attention last year. When the bubble popped, and price continued to deflate, discussion on this site stopped entirely. Was there a change of sign in the social status of the topic, is the topic fully explored, or has there simply happened nothing of interest over the last year?</p>\n<p>If you are not familiar with Bitcoin, <a href=\"http://www.milkeninstitute.org/publications/review/2012_1/22-31MR53.pdf\">here</a> is one intro I happen to like.</p>\n<p><a href=\"/lw/5se/general_bitcoin_discussion_thread_may_2011/\">Kaj Sotala</a> lists a number of previous threads on the topic:</p>\n<blockquote>\n<p>There seems to be quite a bit of a Bitcoin interest around here, with several articles about it already: [<a href=\"/lw/4mc/singularity_institute_now_accepts_donations_via/\">1</a> <a href=\"/lw/4cs/making_money_with_bitcoin/\">2</a> <a href=\"/lw/4w9/google_lends_further_legitimacy_to_bitcoin/\">3</a> <a href=\"/\">4</a> <a href=\"/r/discussion/lw/5ru/links_bitcoin_hits_mainstream_intelligent/\">5</a> <a href=\"/r/discussion/lw/5rg/homomorphic_encryption_and_bitcoin/\">6</a> <a href=\"/r/discussion/lw/5pz/link_two_articles_on_bitcoin/\">7</a>]</p>\n</blockquote>\n<p>Less Wrong seems like a good place to discuss recent developments, if one does not want to suffer the inanity of the&nbsp;officially&nbsp;unofficial <a href=\"https://bitcointalk.org/\">forum</a>.&nbsp;If you are not longer interested in Bitcoin, perhaps&nbsp;<a href=\"http://intelligence.org/donate/?a=100\">send</a> your remaining balance to the Singularity Institute?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jgcAJnksReZRuvgzp": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y3Y42GXckQuu6bC2m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 26, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "17110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2598g336mAXc7gSCQ", "YgRNDRA5mo7dgGyn2", "jLAw6dPGZCRnpNgxM", "ijr8rsyvJci2edxot", "rPBgKcH3MRry78TTL", "4KszqwYq8PHui8bSS", "XCuwfWFuiGxCWXFtW", "7zSyriwymreKhESfP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T18:15:57.377Z", "modifiedAt": "2022-05-08T17:08:20.712Z", "url": null, "title": "Why Academic Papers Are A Terrible Discussion Forum", "slug": "why-academic-papers-are-a-terrible-discussion-forum", "viewCount": null, "lastCommentedAt": "2016-03-27T12:25:23.288Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gZgjqrotKyCJR9zGq/why-academic-papers-are-a-terrible-discussion-forum", "pageUrlRelative": "/posts/gZgjqrotKyCJR9zGq/why-academic-papers-are-a-terrible-discussion-forum", "linkUrl": "https://www.lesswrong.com/posts/gZgjqrotKyCJR9zGq/why-academic-papers-are-a-terrible-discussion-forum", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Academic%20Papers%20Are%20A%20Terrible%20Discussion%20Forum&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Academic%20Papers%20Are%20A%20Terrible%20Discussion%20Forum%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgZgjqrotKyCJR9zGq%2Fwhy-academic-papers-are-a-terrible-discussion-forum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Academic%20Papers%20Are%20A%20Terrible%20Discussion%20Forum%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgZgjqrotKyCJR9zGq%2Fwhy-academic-papers-are-a-terrible-discussion-forum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgZgjqrotKyCJR9zGq%2Fwhy-academic-papers-are-a-terrible-discussion-forum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1939, "htmlBody": "<p>Over the past few months, the Singularity Institute has published many <a href=\"/lw/axr/three_new_papers_on_ai_risk/\">papers</a> on topics related to Friendly AI. It's wonderful that these ideas are getting written up, and it's virtually always better to do something suboptimal than to do nothing. However, I will make the case below that academic papers are a <em>terrible</em> way to discuss Friendly AI, and other ideas in that region of thought space. We <em>need</em> something better.</p>\n<p>I won't try to argue that papers aren't worth publishing. There are many reasons to publish papers - prestige in certain communities and promises to grant agencies, for instance - and I haven't looked at them all in detail. However, I think there is a conclusive case that as a <em>discussion forum - </em>a way for ideas to be read by other people, evaluated, spread, criticized, and built on - academic papers fail. Why?</p>\n<p>&nbsp;</p>\n<p><em><strong>1. The time lag is huge; it's measured in months, or even years. </strong></em></p>\n<p>Ideas structured like the Less Wrong <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a>, with large <a href=\"/lw/kg/expecting_short_inferential_distances\">inferential distances</a> between beginning and ending, have huge webs of interdependencies: to read A you have to read B, which means you need to read C, which requires D and E, and on and on and on. Ideas build on each other. Einstein built on Maxwell, who built on Faraday, who built on Newton, who built on Kepler, who built on Galileo and Copernicus.</p>\n<p>For this to happen, ideas need to get out there - whether orally or in writing - so others can build on them. The publication cycle for ideas is like the release cycle for software. It determines how quickly you can get feedback, fix mistakes, and then use whatever you've already built to help make the next thing. Most academic papers take months to write up, and then once written up, take more months to publish. Compare that to Less Wrong articles or blog posts, where you can write an essay, get comments within a few hours, and then write up a reply or follow-up the next day.</p>\n<p>Of course, some of that extra time lag is that big formal documents are sometimes needed for discussion, and big formal documents take a while. But academic papers aren't just limited by writing and reviewing time - they still fundamentally operate on the schedule of the seventeenth-century <em>Transactions of the Royal Society</em>. When Holden published his <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">critique</a> of the Singularity Institute on Less Wrong, a big formal document, Eliezer could <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">reply</a> with another big formal document in about three weeks.</p>\n<p>&nbsp;</p>\n<p><em><strong>2. Most academic publications are inaccessible outside universities. </strong></em></p>\n<p>This problem is familiar to anyone who's done research outside a university. The ubiquitous journal paywall. People complain about how the New York Times and Wall Street Journal have paywalls, but at least you can pay for them if you really want to. It isn't practical for almost anyone doing research to pay for the articles they need out-of-pocket, since journals commonly charge $30 or more <em>per article</em>, and any serious research project involves dozens or even hundreds of articles. Sure, there are <a href=\"http://www.reddit.com/r/scholar\">ways</a> to get around the system, and you can try to publish (and get everyone else in your field to publish) in <a href=\"/arxiv.org\">open-access journals</a>, but why introduce a <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivial inconvenience</a>?</p>\n<p><em><strong><br /></strong></em></p>\n<p><em><strong>3. Virtually no one reads most academic publications.</strong></em></p>\n<p>This obviously goes together with point #2, but even within universities, it's rare for papers, dissertations or even books to be read outside a very narrow community. Most people don't regularly read journals outside their field, let alone outside their department. Academic papers are hard to get statistics on, but eg., I was a math major in undergrad, and I can't even understand the <a href=\"http://arxiv.org/list/math/new\">titles</a> of most new math papers. More broadly, the print run of most academic books is very small, only a few hundred or so. The average Less Wrong post gets more views than that.</p>\n<p>&nbsp;</p>\n<p><em><strong>4. It's very unusual to make successful philosophical arguments in paper form.</strong></em></p>\n<p>When doing research for <a href=\"mailto:thezvi@gmail.com\">Personalized Medicine</a>, I often read papers to discover the results of some experiment. Someone gave drug X to people with disease Y. What were the results? How many were cured? How many had side effects? What were the costs and benefits? All useful information.</p>\n<p>However, most recent Singularity Institute papers are neither empirical (\"we did experiment X, these are the results\") or mathematical (\"if you assume A, B, and C, then D and E follow\"). Rather, they are philosophical, like Paul Graham's <a href=\"http://www.paulgraham.com/articles.html\">essays</a>. I honestly can't think of a single instance where I was convinced of an informal, philosophical argument through an academic paper. Books, magazines, blog posts - sure, but papers just don't seem to be a thing.</p>\n<p>&nbsp;</p>\n<p><em><strong>5. Papers don't have prestige outside a narrow subset of society.</strong></em></p>\n<p>Several other arguments here - the time lag, for instance - also apply to books. However, society in general recognizes that writing a book is a noteworthy achievement, especially if it sells well. A successful author, even if not compensated well, is treated a little like a celebrity: media interviews, fan clubs, crazy people writing him letters in green ink, etc. (This is probably related to them not being paid well: in the labor market, payment in social status probably substitutes to a high degree for payment in money, as we see with actors and musicians.)</p>\n<p>There's nothing comparable for academic papers. No one ever writes a really successful paper, and then goes on <em>The Daily Show, </em>or gets written up in the New York Times, or gets harassed by crowds of screaming fangirls. (There are a few exceptions, like medicine, but philosophy and computer science are not among them.) Eg., a lot of people are familiar with Ioannidis's paper, <em>Why most published research findings are false.</em> However, he also wrote another paper, a few years earlier, titled <em>Replication validity of genetic association studies</em>. This paper actually has more citations - over 1300 at least count. But not only have we not heard of it, no one else outside the field has either. (Try Googling it, and you'll see what I mean.)</p>\n<p>&nbsp;</p>\n<p><em><strong>6. Getting people to read papers is difficult.</strong></em></p>\n<p>Most intellectual people regularly read books, blogs, newspapers, magazines, and other common forms of memetic transmission. However, it's much less common for people to read papers, and that reduces the <a href=\"http://en.wikipedia.org/wiki/Affordance\">affordances</a> that people have for doing so, if they are asking \"hey, this thing is a crazy idea, why should I believe it?\". Papers are, intentionally, written for an audience of specialists rather than a general interest group, which reduces both the tendency and ability of non-specialists to read them when asked (and also violates the \"Explainers shoot high - <a href=\"/lw/kh/explainers_shoot_high_aim_low/\">aim low</a>\" rule).</p>\n<p>&nbsp;</p>\n<p><em><strong>7. Academia selects for conformity.</strong></em></p>\n<p>The whole point of tenure is to <em>avoid</em> selecting for conformity - if you have tenure, the theory goes, you can work on whatever you want, without fear of being fired or otherwise punished. However, only a small (and shrinking) number of academics have tenure. In order to make sure fools didn't get tenure, it turns out academia resorted to lots and lots of <a href=\"http://rationalconspiracy.com/2012/06/19/negative-and-positive-selection/\">negative selection</a>. The famous <a href=\"http://www.chemistry-blog.com/tag/carreira-letter/\">letter</a> by chemistry professor Erick Carreira illustrates some of what the selection pressure is like, similar to medicine or investment banking: there's a single, narrow \"track\", and people who deviate at any point are pruned. Lee Smolin has written about this phenomenon in string theory, in his famous book <em><a href=\"http://www.amazon.com/The-Trouble-With-Physics-Science/dp/061891868X/ref=sr_1_1?ie=UTF8&amp;qid=1340128450&amp;sr=8-1&amp;keywords=the+trouble+with+physics\">The Trouble with Physics</a>.</em></p>\n<p>Things may change in the future, but as it stands now, many ideas like the Singularity are non-conformist, well outside the mainstream. They aren't likely to go very far in an environment where <a href=\"/lw/ajj/how_to_fix_science/\">deviations from the norm</a> are seen negatively.</p>\n<p>&nbsp;</p>\n<p><em><strong>8. The current community isn't academic in origin.</strong></em></p>\n<p>This isn't an airtight argument, because it's heuristic - \"things which worked well before will probably work again\". However, heuristic arguments still have a lot of validity. One of the key purposes of a discussion forum, like Less Wrong or the SL4 list that was, is to get new people with bright ideas interested in the topics under discussion. Academia's track record of getting new people interested isn't that great - of the current Singularity Institute directors and staff, only one (Anna Salamon) has an academic background, and she dropped out of her PhD program to work for SIAI. What has been successful, so far, at bringing new people into our community? I haven't analyzed it in depth, but whatever the answer is, the priors are that it will work well again.</p>\n<p>&nbsp;</p>\n<p><em><strong>9. Our ideas aren't academic in origin.</strong></em></p>\n<p>Similarly to #8, this is a \"heuristic argument\" rather than an airtight proof. But I still think it's important to note that our <em>current</em> ideas about Friendly AI - any given AI will probably destroy the world, mathematical proof is needed to prevent that, human value is complicated and hard to get right, and so on - were not developed through papers, but through in-person and mailing list discussions (primarily). I'm also not aware of any ideas which came <em>into</em> our community through papers. Even science fiction has a better track record - eg. some of our key concepts originated in Vinge's <em>True Names and Other Dangers</em>. What formats have <em>previously</em> worked well for discussing ideas?</p>\n<p>&nbsp;</p>\n<p><em><strong>10. Papers have a tradition of violating the <a href=\"/lw/js/the_bottom_line/\">bottom line</a> rule.</strong></em></p>\n<p>In a classic paper, one starts with the conclusion in the abstract, and then builds up an argument for it in the paper itself. Paul Graham has a fascinating <a href=\"http://www.paulgraham.com/essay.html\">essay</a> on this form of writing, and how it came to be - it ultimately derives from the legal tradition, where one takes a position (guilty or innocent), and then defends it. However, this style of writing violates the <a href=\"/lw/js/the_bottom_line/\">bottom line</a> rule. Once something is written on the paper, it is already either right or wrong, no matter what clever arguments you come up with in support of it. This doesn't make it wrong, of course, but it does tend to create a fitness environment where truth isn't selected for, just as <a href=\"http://www.paulgraham.com/hubs.html\">Alabama</a> creates a fitness environment where startups aren't selected for.</p>\n<p>&nbsp;</p>\n<p><strong><em>11. Academic moderation is both very strict and badly run.</em></strong></p>\n<p>All forums need some sort of <a href=\"/lw/c1/wellkept_gardens_die_by_pacifism/\">moderation</a> to avoid degenerating. However, academic moderation is <em>very</em> strict by normal standards - in a lot of journals, only a small fraction of submissions get approved. In addition, academic moderation has a large random element, and is just not very good overall; many quality papers get rejected, and many <a href=\"http://www.guardian.co.uk/commentisfree/2011/sep/09/bad-science-research-error\">obvious errors</a> slip through.</p>\n<p>As if that wasn't enough, most journals are single-blind rather than double-blind. You don't know who the moderators are, but <em>they</em> know who <em>you </em>are, raising the potential for all kinds of obvious unfairness. The most common kind of bias is one that hurts us unusually badly: people from prestigious universities are given a huge leg up, compared to people outside the system.</p>\n<p><em>(This article has been <a href=\"http://rationalconspiracy.com/2012/06/20/why-academic-papers-are-a-terrible-discussion-forum\">cross-posted</a> to my <a href=\"http://rationalconspiracy.com\">blog</a>, The Rationalist Conspiracy.) </em></p>\n<p>&nbsp;</p>\n<p>EDIT #1: As Lukeprog notes in the comments, academic papers are not our main discussion forum for FAI ideas. In practice, the main forum is still in-person conversations. However, in-person conversations have critical limitations too, albeit more obvious ones. Some crucial limits are the small number of people who can participate at any one time; the lack of any external record that can be looked up later; the lack of any way to \"broadcast\" key findings to a larger audience (you can shout, but that's not terribly effective); and the lack of lots of time to think, since each participant in the conversation can't really wait three hours before replying.</p>\n<p>EDIT #2: To give a specific example of an alternative forum for FAI discussion, I think the proposal for an <a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">AI Risk wiki</a> would solve most of the problems listed here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MXcpQvaPGtXpB6vkM": 1, "ZpG9rheyAkgCoEQea": 1, "aHjTRDkGypPqbXWpN": 3, "5f5c37ee1b5cdee568cfb124": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gZgjqrotKyCJR9zGq", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 45, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "17062", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KiJKQC2rccayKpE22", "HLqWn5LASfhhArZ7w", "6SGqkCgHuNr7d4yJm", "sizjfDgCgAsuLJQmm", "reitXJgJXFzKpdKyd", "2TPph4EGZ6trEbtku", "ETe2SZacmLvvr8H9n", "34XxbRFe54FycoCDw", "tscc3e5eujrsEeFN4", "mYuMdmMmGM7fFj382"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-06-20T18:15:57.377Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T20:00:25.676Z", "modifiedAt": "2020-05-14T20:50:39.904Z", "url": "http://www.overcomingbias.com/2012/06/plastination-is-near.html", "title": "Plastination is maturing and needs funding, says Hanson", "slug": "plastination-is-maturing-and-needs-funding-says-hanson", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.842Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": "overcomingbias.com", "pageUrl": "https://www.lesswrong.com/posts/whgNqLq2tJiri3GdY/plastination-is-maturing-and-needs-funding-says-hanson", "pageUrlRelative": "/posts/whgNqLq2tJiri3GdY/plastination-is-maturing-and-needs-funding-says-hanson", "linkUrl": "https://www.lesswrong.com/out?url=http%3A%2F%2Fwww.overcomingbias.com%2F2012%2F06%2Fplastination-is-near.html", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Plastination%20is%20maturing%20and%20needs%20funding%2C%20says%20Hanson&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlastination%20is%20maturing%20and%20needs%20funding%2C%20says%20Hanson%0Ahttp%3A%2F%2Fwww.overcomingbias.com%2F2012%2F06%2Fplastination-is-near.html%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Plastination%20is%20maturing%20and%20needs%20funding%2C%20says%20Hanson%20https%3A%2F%2Fwww.lesswrong.com%2Fout%3Furl%3Dhttp%253A%252F%252Fwww.overcomingbias.com%252F2012%252F06%252Fplastination-is-near.html", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fout%3Furl%3Dhttp%253A%252F%252Fwww.overcomingbias.com%252F2012%252F06%252Fplastination-is-near.html", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 335, "htmlBody": "<html><head></head><body><blockquote><p><span>Though cryonics has been practiced for forty years, its techniques have improved only slowly; its few customers can only induce a tiny research effort. The much larger brain research community, in contrast, has been rapidly improving their ways to do fast cheap detailed 3D brain scans, and to prepare samples for such scans. You see, brain researchers need ways to stop brain samples from changing, and to be strong against scanning disruptions, just so they can study brain samples at their leisure.</span></p></blockquote><blockquote><p>These brain research techniques have now reached two key milestones:</p><ol><li>They\u2019ve found new ways to \u201cfix\u201d brain samples by filling them with plastic, ways that seem impressively reliable, resilient, and long lasting, and which work on large brain volumes (e.g.,&nbsp;<a href=\"http://homes.mpimf-heidelberg.mpg.de/~smikula/MIV.html?path=blockface/sm1329/&amp;width=97335&amp;height=152116&amp;notilegroups=1\">here</a>). Such&nbsp;<a href=\"http://www.gwern.net/plastination\">plastination</a>&nbsp;techniques seem close to being able to save enough info in entire brains for centuries, without needing continual care. Just dumping a plastic brain in a box in a closet might work fine.</li><li>Today, for a few tens of thousands of dollars, less than the price charged for one cryonics customer, it is feasible to have independent lab(s) take random samples from whole mouse or human brains preserved via either cryonics or plastination, and do high (5nm) resolution 3D scans to map out thousands of neighboring cells, their connections, and connection strengths, to test if either of these approaches clearly preserve such key brain info.</li></ol><p>An anonymous donor has actually funded a $100K&nbsp;<a href=\"http://brainpreservation.org/content/technology-prize\">Brain Preservation Prize</a>, paid to the first team(s) to pass this test on a human brain, with a quarter of the prize going to those that first pass the test on a mouse brain. Cryonics and plastination teams have already submitted whole mouse brains to be tested. The only hitch is that the prize organization needs money (~25-50K$) to actually do the tests!</p></blockquote><p>Comments? &nbsp;If superior brain preservation can be demonstrated under a 5nm-resolution 3D scan, plastination wins over vitrification hands-down. &nbsp;Is Robin missing anything here, or is this indeed as important as he says?</p></body></html>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "whgNqLq2tJiri3GdY", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 80, "baseScore": 104, "extendedScore": null, "score": 0.000234, "legacy": true, "legacyId": "17058", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 76, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.2.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-20T22:54:42.424Z", "modifiedAt": null, "url": null, "title": "Help me make SI's arguments clear", "slug": "help-me-make-si-s-arguments-clear", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:25.400Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "crazy88", "createdAt": "2011-09-02T04:50:28.550Z", "isAdmin": false, "displayName": "crazy88"}, "userId": "JjaJBbvGJE6D6FYvF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wffMWnGDuH6xFrWnp/help-me-make-si-s-arguments-clear", "pageUrlRelative": "/posts/wffMWnGDuH6xFrWnp/help-me-make-si-s-arguments-clear", "linkUrl": "https://www.lesswrong.com/posts/wffMWnGDuH6xFrWnp/help-me-make-si-s-arguments-clear", "postedAtFormatted": "Wednesday, June 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20me%20make%20SI's%20arguments%20clear&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20me%20make%20SI's%20arguments%20clear%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwffMWnGDuH6xFrWnp%2Fhelp-me-make-si-s-arguments-clear%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20me%20make%20SI's%20arguments%20clear%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwffMWnGDuH6xFrWnp%2Fhelp-me-make-si-s-arguments-clear", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwffMWnGDuH6xFrWnp%2Fhelp-me-make-si-s-arguments-clear", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 539, "htmlBody": "<p>One of the biggest problems with evaluating the plausibility of SI's arguments is that the arguments involve a large number of premises (as any complex argument will) and often these arguments are either not written down or are written down in&nbsp;disparate locations, making it very hard to piece together these claims. SI is aware of this and one of their major aims is to state their argument very clearly. I'm hoping to help with this aim.</p>\r\n<p>My specific plan is as follows: I want to map out the broad structure of SI's arguments in \"standard form\" - that is, as a list of premises that support a conclusion. I then want to write this up into a more readable summary and discussion of SI's views.</p>\r\n<p>The first step to achieving this is making sure that I understand what SI is arguing. Obviously, SI is arguing for a number of different things but I take their principle argument to be the following:</p>\r\n<p>P1. Superintelligent AI (SAI) is highly likely to be developed in the near future (say, next 100 years and probably sooner)<br />P2. Without explicit FAI research, superintelligent AI is likely to pose a global catastrophic risk for humanity.<br />P3. FAI research has a reasonable chance of making it so that superintelligent AI will not pose a global catastrophic risk for humanity.<br />Therefore<br />C1. FAI research has a high expected value for humanity.<br />P4. We currently fund FAI research at a level below that supported by its expected value.<br />Therefore<br />C2. Humanity should expend more effort on FAI research.</p>\r\n<p>Note that P1 in this argument can be weakened to simply say that SAI is a non-trivial possibility but, in response, a stronger version of P2 and P3 are required if the conclusion is still to be viable (that is, if SAI is less likely, it needs to be more dangerous or FAI research needs to be more effective in order for FAI research to have the same expected value). However, if P2 and P3 already seem strong to you, then the argument can be made more forceful by weakening P1. One further note, however, doing so might also make the move from C1 and P4 to C2 more open to criticism - that is, some people think that we shouldn't make decisions based on expected value calculations when we are talking about low probability/high value events.</p>\r\n<p>So I'm asking for a few things from anyone willing to comment:</p>\r\n<p>1.) A sense of whether this is a useful project (I'm very busy and would like to know whether this is a suitable use of my scarce spare time) - I will take upvotes/downvotes as representing votes for or against the idea (so feel free to downvote me if you think this idea isn't worth pursuing even if you wouldn't normally downvote this post). <br />2.) A sense of whether I have the broad structure of SI's basic argument right.</p>\r\n<p>In terms of my commitment to this project: as I said before, I'm very busy so I don't promise to finish this project. However, I will commit to notifying Less Wrong if I give in on it and engaging in handover discussions with anyone that wants to take the project over.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wffMWnGDuH6xFrWnp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 19, "extendedScore": null, "score": 9.267186607235459e-07, "legacy": true, "legacyId": "17112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-21T00:38:09.212Z", "modifiedAt": null, "url": null, "title": "What Would You Like To Read? A Quick Poll", "slug": "what-would-you-like-to-read-a-quick-poll", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:52.688Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H5uu8iWQxAcg7P6wk/what-would-you-like-to-read-a-quick-poll", "pageUrlRelative": "/posts/H5uu8iWQxAcg7P6wk/what-would-you-like-to-read-a-quick-poll", "linkUrl": "https://www.lesswrong.com/posts/H5uu8iWQxAcg7P6wk/what-would-you-like-to-read-a-quick-poll", "postedAtFormatted": "Thursday, June 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Would%20You%20Like%20To%20Read%3F%20A%20Quick%20Poll&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Would%20You%20Like%20To%20Read%3F%20A%20Quick%20Poll%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH5uu8iWQxAcg7P6wk%2Fwhat-would-you-like-to-read-a-quick-poll%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Would%20You%20Like%20To%20Read%3F%20A%20Quick%20Poll%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH5uu8iWQxAcg7P6wk%2Fwhat-would-you-like-to-read-a-quick-poll", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH5uu8iWQxAcg7P6wk%2Fwhat-would-you-like-to-read-a-quick-poll", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 241, "htmlBody": "<p>In our discussion of <a href=\"/lw/d5y/why_academic_papers_are_a_terrible_discussion/\">academic papers</a>, Lukeprog argued that lots of smart people preferred to read ideas in academic paper format. Based on my observations, I mostly disagree. But that's just anecdotal evidence. Let's <a href=\"/lw/3wh/science_do_it_yourself/\">use Science</a>!</p>\n<p>Suppose someone at the Singularity Institute thought up a cool new idea: it could be about rationality, Friendly AI, decision theory, making money, or any of the other topics we discuss here on LW. Explaining it takes about ten pages, and it's nontechnical enough that it can be explained to a general audience of non-mathematicians. Which of the following explanations would you be <em><strong>most likely </strong></em>to actually sit down and read through?</p>\n<ul>\n<li>A <a href=\"/r/discussion/lw/d7e/what_would_you_like_to_read_a_quick_poll/6vcb\">post</a> on Less Wrong or another friendly blog</li>\n<li>A <a href=\"/r/discussion/lw/d7e/what_would_you_like_to_read_a_quick_poll/6vca\">book chapter</a>, available both on Kindle and in physical book form</li>\n<li>A <a href=\"/r/discussion/lw/d7e/what_would_you_like_to_read_a_quick_poll/6vc9\">mailing list</a> post, made available through a public archive</li>\n<li>An <a href=\"/r/discussion/lw/d7e/what_would_you_like_to_read_a_quick_poll/6vc8\">academic paper</a>, downloadable over the Internet as a PDF</li>\n<li>A <a href=\"/r/discussion/lw/d7e/what_would_you_like_to_read_a_quick_poll/6vc7\">static HTML</a> page on the Singularity Institute's website</li>\n<li>A <a href=\"/r/discussion/lw/d7e/what_would_you_like_to_read_a_quick_poll/6vc6\">page</a> on a Singularity Institute or Less Wrong wiki</li>\n<li>A <a href=\"/r/discussion/lw/d7e/what_would_you_like_to_read_a_quick_poll/6vc5\">speech</a>, downloadable as an audio file</li>\n<li>A <a href=\"/r/discussion/lw/d7e/what_would_you_like_to_read_a_quick_poll/6vc4\">PowerPoint</a> or other presentation format</li>\n</ul>\n<p>EDIT: To state the obvious, this poll will be biased in favor of blog postings, since it's on a blog. However, I still think it'll provide data that's much better than anecdotal guessing. I've emailed a few rationalist mailing lists to try and counteract this effect.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H5uu8iWQxAcg7P6wk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -2, "extendedScore": null, "score": 9.267661577028323e-07, "legacy": true, "legacyId": "17114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gZgjqrotKyCJR9zGq", "K82evF2iRAiRWwvyn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-21T03:02:10.907Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Created Already In Motion", "slug": "seq-rerun-created-already-in-motion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hmbRoapL9BahhwbiJ/seq-rerun-created-already-in-motion", "pageUrlRelative": "/posts/hmbRoapL9BahhwbiJ/seq-rerun-created-already-in-motion", "linkUrl": "https://www.lesswrong.com/posts/hmbRoapL9BahhwbiJ/seq-rerun-created-already-in-motion", "postedAtFormatted": "Thursday, June 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Created%20Already%20In%20Motion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Created%20Already%20In%20Motion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhmbRoapL9BahhwbiJ%2Fseq-rerun-created-already-in-motion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Created%20Already%20In%20Motion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhmbRoapL9BahhwbiJ%2Fseq-rerun-created-already-in-motion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhmbRoapL9BahhwbiJ%2Fseq-rerun-created-already-in-motion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Today's post, <a href=\"/lw/rs/created_already_in_motion/\">Created Already In Motion</a> was originally published on 01 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Created_Already_In_Motion\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There is no computer program so persuasive that you can run it on a rock. A mind, in order to be a mind, needs some sort of dynamic rules of inference or action. A mind has to be created already in motion.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d6t/seq_rerun_the_moral_void/\">The Moral Void</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hmbRoapL9BahhwbiJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.268322947860219e-07, "legacy": true, "legacyId": "17116", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CuSTqHgeK4CMpWYTe", "kJFsuJPPcwHWzmitf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-21T04:19:00.267Z", "modifiedAt": null, "url": null, "title": "Minimum viable workout routine", "slug": "minimum-viable-workout-routine", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:36.360Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xAiMk4e7neP6Ah7FG/minimum-viable-workout-routine", "pageUrlRelative": "/posts/xAiMk4e7neP6Ah7FG/minimum-viable-workout-routine", "linkUrl": "https://www.lesswrong.com/posts/xAiMk4e7neP6Ah7FG/minimum-viable-workout-routine", "postedAtFormatted": "Thursday, June 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Minimum%20viable%20workout%20routine&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMinimum%20viable%20workout%20routine%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxAiMk4e7neP6Ah7FG%2Fminimum-viable-workout-routine%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Minimum%20viable%20workout%20routine%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxAiMk4e7neP6Ah7FG%2Fminimum-viable-workout-routine", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxAiMk4e7neP6Ah7FG%2Fminimum-viable-workout-routine", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1710, "htmlBody": "<p>So you want the longevity benefits of regular exercise but you've hit some snags. &nbsp;Every routine pretty much makes you miserable. &nbsp;In addition, because of all the conflicting information out there, you aren't even sure if you're getting the full benefits. &nbsp;This post is for you. &nbsp;And don't worry about your current physical circumstances. &nbsp;It works equally well for the overweight, the underweight, and women (no you will not turn into a gross she hulk the moment you touch a weight. &nbsp;Those women take steroids and train hard for years) &nbsp;</p>\n<p>A sub-optimal plan you stick to is better than the perfect routine you abandon after the first week. &nbsp;This routine is not perfect. &nbsp;This routine is optimized for simplicity and low time/mental effort commitment while still getting excellent results. &nbsp;It is strongly based on the routines from Beyond Brawn by Stuart McRobert, and some of the principles of Starting Strength by Mark Rippetoe both of which have much anecdotal evidence of effectiveness in the training logs of various forums. &nbsp;If you're looking for published research to back up my claims I have some bad news for you, the literature on resistance training is basically worthless. &nbsp;A 5 minute&nbsp;perusal&nbsp;of google scholar will show that atrocious methodology such as having \"subjects act as their own control\" are common, and accepted by the relevant journals. &nbsp;And that's if you're lucky enough to find studies that aren't about diabetics, or elderly japanese women. &nbsp;But I'm not going to spend excessive time trying to justify this routine, anyone can do it for a month and see that the results are significant. (I'm open to arguing about it in the comments however.)</p>\n<p>&nbsp;</p>\n<p><strong>A note about cardio: &nbsp;</strong></p>\n<p>Cardiovascular capacity (V02 max) has shown a high degree of correlation to all cause mortality. &nbsp;Why aren't I recommending cardio? &nbsp;Because the only way to increase V02 max is with high intensity exercise. &nbsp;Between high intensity weight lifting and high intensity cardio, high intensity weightlifting easily wins for a newbie. &nbsp;A newbie, especially a significantly out of shape one, will not be capable of a level of cardio exertion that results in a significant adaptation. &nbsp;This can result in a lot of effort with very little in the way of improvement. &nbsp;This is soul-destroyingly frustrating. &nbsp;They can however lift a weight a few times and this will result in an adaptation that allows them to lift more next time. &nbsp;A few months of a weightlifting routine is going to put any person in a much better position to do longevity affecting cardio if that is their goal. &nbsp;Cardio is also generally a terrible fat burner for the exact same reason. &nbsp;</p>\n<p>Edit: there seems to be some confusion about this. &nbsp;The primary problem of exercise is not the&nbsp;optimality of results but instilling the habit of exercising.&nbsp;I believe that cardio is terrible for overcoming this habit forming stage. &nbsp;</p>\n<p>The point of the below program is to get you in the habit of exercising and give you immediate results. &nbsp;Once you have achieved some basic measure of fitness (~3 month time frame) you can maintain, or use the fact that exercising is now much easier to move on to any program you want.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>The nitty gritty: &nbsp;</strong></p>\n<p>You are going to do three exercises 2-3 times per week. &nbsp;Each session will take ~45 minutes to an hour. &nbsp;The exercises are</p>\n<p>* 3x5 trap bar deadlift</p>\n<p>* 3x5 incline bench press</p>\n<p>* 3x5 bent over row &nbsp;(possible substitution for cable rows see below)</p>\n<p>What does 3x5 mean? &nbsp;</p>\n<p>3 sets of 5 reps each. &nbsp;You will assume the correct form, go through the full range of motion for the exercise 5 times, then rest before repeating twice more. &nbsp;</p>\n<p><strong>What weights do I use? &nbsp;</strong></p>\n<p>You will start with the empty bar and add 5lbs every workout for the trap bar deadlift and 5lbs every other workout for the incline bench press and bent over row. &nbsp;Many are tempted to increase weights faster than this. &nbsp;You can do what you want but don't come crying when your progress stalls more quickly. &nbsp;A slow progression that continues for a long time beats a fast increase followed by a time wasting plateau. &nbsp;</p>\n<p><strong>Why these three exercises? &nbsp;</strong></p>\n<p>This routine hits the most muscle mass possible in the smallest number of exercises. &nbsp;All decent routines include hip extension exercises, pushing exercises, and pulling exercise. &nbsp;This ensures that you don't create an imbalance that messes up your posture or limits you unnecessarily. &nbsp;In addition, these exercises require very little in the way of technique coaching, which is really this routine's primary advantage over more popular programs such as starting strength. &nbsp;It took me 8 months to learn to squat well, but I learned to trap bar deadlift in a single session. &nbsp;Similarly with the incline press, it carries with it a much smaller chance of injury from poor form than either the bench press or overhead press that are the mainstays of many programs. &nbsp;</p>\n<p><strong>I have no idea what these exercises are, how do I do them?</strong></p>\n<p>Here is an article for trap bar deadlift, which is so easy that there aren't really many tutorials online: &nbsp;</p>\n<p><a href=\"http://www.t-nation.com/free_online_article/most_recent/the_trap_bar_deadlift\">http://www.t-nation.com/free_online_article/most_recent/the_trap_bar_deadlift</a>&nbsp;&nbsp;</p>\n<p>The key is a neutral spine. &nbsp;You take a big breath at the bottom, squeeze everything tight, and stand up pushing through your heels while maintaining the lumbar arch. &nbsp;Note not to use the raised handles that many trap bars have which reduces the range of motion.</p>\n<p>Incline is similarly straightforward: &nbsp;</p>\n<p><a href=\"http://www.youtube.com/watch?v=dynoKEIcpoU\">http://www.youtube.com/watch?v=dynoKEIcpoU</a>&nbsp;&nbsp;</p>\n<p>note that you DO want to touch your chest at the bottom, but do not bounce the bar off your chest. &nbsp;The cue that works for most is to imagine touching your shirt but not your chest.</p>\n<p>Bent over row can feel a little weird, but it's not too hard to learn: &nbsp;</p>\n<p><a href=\"http://www.youtube.com/watch?v=boxbOSGwD4U\">http://www.youtube.com/watch?v=boxbOSGwD4U</a>&nbsp;&nbsp;</p>\n<p>Note that after more real world testing bent over rows seem to cause the most issues of the three lifts. &nbsp;As the potential for injury is slightly higher with poor form for this exercise than the others I would recommend seated cable rows for those who find they can not perform bent over rows correctly. &nbsp;I'd additionally strongly recommend that if one is forced to make this substitution they should also do some chinups at the end of each workout. &nbsp;The goal of this substitution should be as a temporary measure. &nbsp;One should strive get back to doing bent over rows once physically able to.</p>\n<p>Cable row form video here:&nbsp;<a href=\"http://www.youtube.com/watch?v=HJSVR_63eKM\">http://www.youtube.com/watch?v=HJSVR_63eKM</a></p>\n<p><strong>How do I warmup/cooldown?</strong></p>\n<p>the best warmup and cooldown is 5 minutes on the rowing machine: &nbsp;</p>\n<p><a href=\"http://www.youtube.com/watch?v=H0r_ZPXJLtg\">http://www.youtube.com/watch?v=H0r_ZPXJLtg</a>&nbsp;&nbsp;</p>\n<p>But you can also do an exercise bike or treadmill. &nbsp;</p>\n<p>After the first couple weeks you should also warmup with the empty bar before jumping to your 3x5 work weight on each exercise. &nbsp;Add additional warmups as the weights get heavier. &nbsp;</p>\n<p>e.g.</p>\n<p>1x5 45lbs &nbsp;</p>\n<p>1x5 75bs &nbsp;</p>\n<p>3x5 105lbs &nbsp;</p>\n<p>don't worry excessively about this, it's hard to screw up. &nbsp;The key is just to prepare yourself, remind yourself of proper form, and get blood flowing. &nbsp;Don't skip warmups, you're increasing your chance of injury and ensuring that you won't get strong as fast. &nbsp;</p>\n<p><strong>Can I do this once week? &nbsp;or sporadically? &nbsp;</strong></p>\n<p>You can but you won't see hardly any benefit other than maintenance of your current fitness level. &nbsp;2 times a week is the bare minimum to disrupt homeostasis to any appreciable degree and 3 is better. &nbsp;Make no mistake, even 2 times a week on this will get you miles ahead of most people fitness wise. &nbsp;You should program it like AxxAxxx or AxAxAxx, where A is a workout session and x is a rest day.</p>\n<p><strong>Can I sub in X exercise? &nbsp;</strong></p>\n<p>No, the bare minimum nature of this program leaves no room for changes. &nbsp;Any change necessitates more complicated programming. &nbsp;If you want to do that just do Starting Strength. &nbsp;Likewise if you want to add stuff, like ab work. &nbsp;It isn't necessary. &nbsp;Edit: cable row substitution for bent row is permissible but only if one finds they absolutely can not maintain good form with barbell rows. &nbsp;</p>\n<p><strong>I didn't complete all my reps this session, what do I do? &nbsp;</strong></p>\n<p>Back off the weights by 10-20% and work your way back up. &nbsp;Make sure you're eating and sleeping right. &nbsp;If you keep hitting a wall over and over again it will be time for a more complex routine. &nbsp;</p>\n<p><strong>My gym doesn't have a trap bar. &nbsp;</strong></p>\n<p>Find a gym that does or do a different program. &nbsp;There is no replacement for the trap bar. &nbsp;One option that is non-obvious is buying a trap bar for your current gym. &nbsp;You might be able to negotiate a free month of membership or something but even if that isn't the case the investment is worth it.</p>\n<p><strong>What sort of results can I expect?</strong></p>\n<p>Most people should expect to be trap bar deadlifting their body weight within 3 months. &nbsp;This will have several effects.</p>\n<p>Strenuous physical activity becomes drastically less taxing. &nbsp;</p>\n<p>Chance of injury during said activity reduced.</p>\n<p>V02 max increased.</p>\n<p>Bone density and joint health improvements.</p>\n<p>Increase in lean body mass.</p>\n<p>Improved insulin sensitivity.</p>\n<p>Improved blood markers and pressure (increases HDL and lowers LDL)</p>\n<p>Decreased chance of back problems.</p>\n<p>Improved posture.</p>\n<p>Mental benefits: &nbsp;Most people find the quality of their sleep improved as well as an increase in general energy levels.</p>\n<p>&nbsp;</p>\n<p><strong>A note on nutrition: &nbsp;</strong></p>\n<p>80% of body composition is diet. &nbsp;This won't do much for your body composition if your diet is crappy. &nbsp;Luckily nutrition is fairly easy, there are only 2 rules to follow: &nbsp;</p>\n<p>*Calories in calories out &nbsp;</p>\n<p>*Eat micronutrient dense foods &nbsp;</p>\n<p>if you follow these rules it's actually <em>surprisingly difficult</em> to mess up. &nbsp;Most people also find that following the 2nd one makes following the 1st one much easier. &nbsp;</p>\n<p>That's about it, I will answer questions about anything I forgot. &nbsp;I hope this gets some fence sitters exercising. &nbsp;</p>\n<p><strong style=\"margin: 0px; padding: 0px; color: #333333; font-family: Arial, Helvetica; font-size: 12px; line-height: 21px;\">&ldquo;No citizen has a right to be an amateur in the matter of physical training&hellip;what a disgrace it is for a man to grow old without ever seeing the beauty and strength of which his body is capable.&rdquo;</strong><br style=\"margin: 0px; padding: 0px; color: #333333; font-family: Arial, Helvetica; font-size: 12px; line-height: 21px;\" /><span style=\"color: #333333; font-family: Arial, Helvetica; font-size: 12px; line-height: 21px;\">-Socrates</span></p>\n<p><span style=\"color: #333333; font-family: Arial, Helvetica; font-size: 12px; line-height: 21px;\"><br /></span></p>\n<p><strong>If anyone is going to do this recording your results and sharing them would be much appreciated.</strong></p>\n<p>As detailed as you want, but even qualitative results would be useful to have.</p>\n<p>&nbsp;</p>\n<p><strong>Habit building:</strong></p>\n<p>Speaking of recording your results, logging is helpful for forming habits. &nbsp;Use this link to join the fitocracy LessWrong group. &nbsp;</p>\n<p><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; color: #6f94db; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13px; line-height: 16px; background-color: #f5f6f7;\" href=\"http://ftcy.me/veXNdz\">http://ftcy.me/veXNdz</a></p>\n<p>Fitocracy is a social website for tracking your workouts. &nbsp;Hat tip to jswan for reminding me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xAiMk4e7neP6Ah7FG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 13, "extendedScore": null, "score": 9.268675747834808e-07, "legacy": true, "legacyId": "17117", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>So you want the longevity benefits of regular exercise but you've hit some snags. &nbsp;Every routine pretty much makes you miserable. &nbsp;In addition, because of all the conflicting information out there, you aren't even sure if you're getting the full benefits. &nbsp;This post is for you. &nbsp;And don't worry about your current physical circumstances. &nbsp;It works equally well for the overweight, the underweight, and women (no you will not turn into a gross she hulk the moment you touch a weight. &nbsp;Those women take steroids and train hard for years) &nbsp;</p>\n<p>A sub-optimal plan you stick to is better than the perfect routine you abandon after the first week. &nbsp;This routine is not perfect. &nbsp;This routine is optimized for simplicity and low time/mental effort commitment while still getting excellent results. &nbsp;It is strongly based on the routines from Beyond Brawn by Stuart McRobert, and some of the principles of Starting Strength by Mark Rippetoe both of which have much anecdotal evidence of effectiveness in the training logs of various forums. &nbsp;If you're looking for published research to back up my claims I have some bad news for you, the literature on resistance training is basically worthless. &nbsp;A 5 minute&nbsp;perusal&nbsp;of google scholar will show that atrocious methodology such as having \"subjects act as their own control\" are common, and accepted by the relevant journals. &nbsp;And that's if you're lucky enough to find studies that aren't about diabetics, or elderly japanese women. &nbsp;But I'm not going to spend excessive time trying to justify this routine, anyone can do it for a month and see that the results are significant. (I'm open to arguing about it in the comments however.)</p>\n<p>&nbsp;</p>\n<p><strong id=\"A_note_about_cardio___\">A note about cardio: &nbsp;</strong></p>\n<p>Cardiovascular capacity (V02 max) has shown a high degree of correlation to all cause mortality. &nbsp;Why aren't I recommending cardio? &nbsp;Because the only way to increase V02 max is with high intensity exercise. &nbsp;Between high intensity weight lifting and high intensity cardio, high intensity weightlifting easily wins for a newbie. &nbsp;A newbie, especially a significantly out of shape one, will not be capable of a level of cardio exertion that results in a significant adaptation. &nbsp;This can result in a lot of effort with very little in the way of improvement. &nbsp;This is soul-destroyingly frustrating. &nbsp;They can however lift a weight a few times and this will result in an adaptation that allows them to lift more next time. &nbsp;A few months of a weightlifting routine is going to put any person in a much better position to do longevity affecting cardio if that is their goal. &nbsp;Cardio is also generally a terrible fat burner for the exact same reason. &nbsp;</p>\n<p>Edit: there seems to be some confusion about this. &nbsp;The primary problem of exercise is not the&nbsp;optimality of results but instilling the habit of exercising.&nbsp;I believe that cardio is terrible for overcoming this habit forming stage. &nbsp;</p>\n<p>The point of the below program is to get you in the habit of exercising and give you immediate results. &nbsp;Once you have achieved some basic measure of fitness (~3 month time frame) you can maintain, or use the fact that exercising is now much easier to move on to any program you want.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"The_nitty_gritty___\">The nitty gritty: &nbsp;</strong></p>\n<p>You are going to do three exercises 2-3 times per week. &nbsp;Each session will take ~45 minutes to an hour. &nbsp;The exercises are</p>\n<p>* 3x5 trap bar deadlift</p>\n<p>* 3x5 incline bench press</p>\n<p>* 3x5 bent over row &nbsp;(possible substitution for cable rows see below)</p>\n<p>What does 3x5 mean? &nbsp;</p>\n<p>3 sets of 5 reps each. &nbsp;You will assume the correct form, go through the full range of motion for the exercise 5 times, then rest before repeating twice more. &nbsp;</p>\n<p><strong id=\"What_weights_do_I_use___\">What weights do I use? &nbsp;</strong></p>\n<p>You will start with the empty bar and add 5lbs every workout for the trap bar deadlift and 5lbs every other workout for the incline bench press and bent over row. &nbsp;Many are tempted to increase weights faster than this. &nbsp;You can do what you want but don't come crying when your progress stalls more quickly. &nbsp;A slow progression that continues for a long time beats a fast increase followed by a time wasting plateau. &nbsp;</p>\n<p><strong id=\"Why_these_three_exercises___\">Why these three exercises? &nbsp;</strong></p>\n<p>This routine hits the most muscle mass possible in the smallest number of exercises. &nbsp;All decent routines include hip extension exercises, pushing exercises, and pulling exercise. &nbsp;This ensures that you don't create an imbalance that messes up your posture or limits you unnecessarily. &nbsp;In addition, these exercises require very little in the way of technique coaching, which is really this routine's primary advantage over more popular programs such as starting strength. &nbsp;It took me 8 months to learn to squat well, but I learned to trap bar deadlift in a single session. &nbsp;Similarly with the incline press, it carries with it a much smaller chance of injury from poor form than either the bench press or overhead press that are the mainstays of many programs. &nbsp;</p>\n<p><strong id=\"I_have_no_idea_what_these_exercises_are__how_do_I_do_them_\">I have no idea what these exercises are, how do I do them?</strong></p>\n<p>Here is an article for trap bar deadlift, which is so easy that there aren't really many tutorials online: &nbsp;</p>\n<p><a href=\"http://www.t-nation.com/free_online_article/most_recent/the_trap_bar_deadlift\">http://www.t-nation.com/free_online_article/most_recent/the_trap_bar_deadlift</a>&nbsp;&nbsp;</p>\n<p>The key is a neutral spine. &nbsp;You take a big breath at the bottom, squeeze everything tight, and stand up pushing through your heels while maintaining the lumbar arch. &nbsp;Note not to use the raised handles that many trap bars have which reduces the range of motion.</p>\n<p>Incline is similarly straightforward: &nbsp;</p>\n<p><a href=\"http://www.youtube.com/watch?v=dynoKEIcpoU\">http://www.youtube.com/watch?v=dynoKEIcpoU</a>&nbsp;&nbsp;</p>\n<p>note that you DO want to touch your chest at the bottom, but do not bounce the bar off your chest. &nbsp;The cue that works for most is to imagine touching your shirt but not your chest.</p>\n<p>Bent over row can feel a little weird, but it's not too hard to learn: &nbsp;</p>\n<p><a href=\"http://www.youtube.com/watch?v=boxbOSGwD4U\">http://www.youtube.com/watch?v=boxbOSGwD4U</a>&nbsp;&nbsp;</p>\n<p>Note that after more real world testing bent over rows seem to cause the most issues of the three lifts. &nbsp;As the potential for injury is slightly higher with poor form for this exercise than the others I would recommend seated cable rows for those who find they can not perform bent over rows correctly. &nbsp;I'd additionally strongly recommend that if one is forced to make this substitution they should also do some chinups at the end of each workout. &nbsp;The goal of this substitution should be as a temporary measure. &nbsp;One should strive get back to doing bent over rows once physically able to.</p>\n<p>Cable row form video here:&nbsp;<a href=\"http://www.youtube.com/watch?v=HJSVR_63eKM\">http://www.youtube.com/watch?v=HJSVR_63eKM</a></p>\n<p><strong id=\"How_do_I_warmup_cooldown_\">How do I warmup/cooldown?</strong></p>\n<p>the best warmup and cooldown is 5 minutes on the rowing machine: &nbsp;</p>\n<p><a href=\"http://www.youtube.com/watch?v=H0r_ZPXJLtg\">http://www.youtube.com/watch?v=H0r_ZPXJLtg</a>&nbsp;&nbsp;</p>\n<p>But you can also do an exercise bike or treadmill. &nbsp;</p>\n<p>After the first couple weeks you should also warmup with the empty bar before jumping to your 3x5 work weight on each exercise. &nbsp;Add additional warmups as the weights get heavier. &nbsp;</p>\n<p>e.g.</p>\n<p>1x5 45lbs &nbsp;</p>\n<p>1x5 75bs &nbsp;</p>\n<p>3x5 105lbs &nbsp;</p>\n<p>don't worry excessively about this, it's hard to screw up. &nbsp;The key is just to prepare yourself, remind yourself of proper form, and get blood flowing. &nbsp;Don't skip warmups, you're increasing your chance of injury and ensuring that you won't get strong as fast. &nbsp;</p>\n<p><strong id=\"Can_I_do_this_once_week___or_sporadically___\">Can I do this once week? &nbsp;or sporadically? &nbsp;</strong></p>\n<p>You can but you won't see hardly any benefit other than maintenance of your current fitness level. &nbsp;2 times a week is the bare minimum to disrupt homeostasis to any appreciable degree and 3 is better. &nbsp;Make no mistake, even 2 times a week on this will get you miles ahead of most people fitness wise. &nbsp;You should program it like AxxAxxx or AxAxAxx, where A is a workout session and x is a rest day.</p>\n<p><strong id=\"Can_I_sub_in_X_exercise___\">Can I sub in X exercise? &nbsp;</strong></p>\n<p>No, the bare minimum nature of this program leaves no room for changes. &nbsp;Any change necessitates more complicated programming. &nbsp;If you want to do that just do Starting Strength. &nbsp;Likewise if you want to add stuff, like ab work. &nbsp;It isn't necessary. &nbsp;Edit: cable row substitution for bent row is permissible but only if one finds they absolutely can not maintain good form with barbell rows. &nbsp;</p>\n<p><strong id=\"I_didn_t_complete_all_my_reps_this_session__what_do_I_do___\">I didn't complete all my reps this session, what do I do? &nbsp;</strong></p>\n<p>Back off the weights by 10-20% and work your way back up. &nbsp;Make sure you're eating and sleeping right. &nbsp;If you keep hitting a wall over and over again it will be time for a more complex routine. &nbsp;</p>\n<p><strong id=\"My_gym_doesn_t_have_a_trap_bar___\">My gym doesn't have a trap bar. &nbsp;</strong></p>\n<p>Find a gym that does or do a different program. &nbsp;There is no replacement for the trap bar. &nbsp;One option that is non-obvious is buying a trap bar for your current gym. &nbsp;You might be able to negotiate a free month of membership or something but even if that isn't the case the investment is worth it.</p>\n<p><strong id=\"What_sort_of_results_can_I_expect_\">What sort of results can I expect?</strong></p>\n<p>Most people should expect to be trap bar deadlifting their body weight within 3 months. &nbsp;This will have several effects.</p>\n<p>Strenuous physical activity becomes drastically less taxing. &nbsp;</p>\n<p>Chance of injury during said activity reduced.</p>\n<p>V02 max increased.</p>\n<p>Bone density and joint health improvements.</p>\n<p>Increase in lean body mass.</p>\n<p>Improved insulin sensitivity.</p>\n<p>Improved blood markers and pressure (increases HDL and lowers LDL)</p>\n<p>Decreased chance of back problems.</p>\n<p>Improved posture.</p>\n<p>Mental benefits: &nbsp;Most people find the quality of their sleep improved as well as an increase in general energy levels.</p>\n<p>&nbsp;</p>\n<p><strong id=\"A_note_on_nutrition___\">A note on nutrition: &nbsp;</strong></p>\n<p>80% of body composition is diet. &nbsp;This won't do much for your body composition if your diet is crappy. &nbsp;Luckily nutrition is fairly easy, there are only 2 rules to follow: &nbsp;</p>\n<p>*Calories in calories out &nbsp;</p>\n<p>*Eat micronutrient dense foods &nbsp;</p>\n<p>if you follow these rules it's actually <em>surprisingly difficult</em> to mess up. &nbsp;Most people also find that following the 2nd one makes following the 1st one much easier. &nbsp;</p>\n<p>That's about it, I will answer questions about anything I forgot. &nbsp;I hope this gets some fence sitters exercising. &nbsp;</p>\n<p><strong style=\"margin: 0px; padding: 0px; color: #333333; font-family: Arial, Helvetica; font-size: 12px; line-height: 21px;\">\u201cNo citizen has a right to be an amateur in the matter of physical training\u2026what a disgrace it is for a man to grow old without ever seeing the beauty and strength of which his body is capable.\u201d</strong><br style=\"margin: 0px; padding: 0px; color: #333333; font-family: Arial, Helvetica; font-size: 12px; line-height: 21px;\"><span style=\"color: #333333; font-family: Arial, Helvetica; font-size: 12px; line-height: 21px;\">-Socrates</span></p>\n<p><span style=\"color: #333333; font-family: Arial, Helvetica; font-size: 12px; line-height: 21px;\"><br></span></p>\n<p><strong id=\"If_anyone_is_going_to_do_this_recording_your_results_and_sharing_them_would_be_much_appreciated_\">If anyone is going to do this recording your results and sharing them would be much appreciated.</strong></p>\n<p>As detailed as you want, but even qualitative results would be useful to have.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Habit_building_\">Habit building:</strong></p>\n<p>Speaking of recording your results, logging is helpful for forming habits. &nbsp;Use this link to join the fitocracy LessWrong group. &nbsp;</p>\n<p><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; color: #6f94db; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13px; line-height: 16px; background-color: #f5f6f7;\" href=\"http://ftcy.me/veXNdz\">http://ftcy.me/veXNdz</a></p>\n<p>Fitocracy is a social website for tracking your workouts. &nbsp;Hat tip to jswan for reminding me.</p>", "sections": [{"title": "A note about cardio: \u00a0", "anchor": "A_note_about_cardio___", "level": 1}, {"title": "The nitty gritty: \u00a0", "anchor": "The_nitty_gritty___", "level": 1}, {"title": "What weights do I use? \u00a0", "anchor": "What_weights_do_I_use___", "level": 1}, {"title": "Why these three exercises? \u00a0", "anchor": "Why_these_three_exercises___", "level": 1}, {"title": "I have no idea what these exercises are, how do I do them?", "anchor": "I_have_no_idea_what_these_exercises_are__how_do_I_do_them_", "level": 1}, {"title": "How do I warmup/cooldown?", "anchor": "How_do_I_warmup_cooldown_", "level": 1}, {"title": "Can I do this once week? \u00a0or sporadically? \u00a0", "anchor": "Can_I_do_this_once_week___or_sporadically___", "level": 1}, {"title": "Can I sub in X exercise? \u00a0", "anchor": "Can_I_sub_in_X_exercise___", "level": 1}, {"title": "I didn't complete all my reps this session, what do I do? \u00a0", "anchor": "I_didn_t_complete_all_my_reps_this_session__what_do_I_do___", "level": 1}, {"title": "My gym doesn't have a trap bar. \u00a0", "anchor": "My_gym_doesn_t_have_a_trap_bar___", "level": 1}, {"title": "What sort of results can I expect?", "anchor": "What_sort_of_results_can_I_expect_", "level": 1}, {"title": "A note on nutrition: \u00a0", "anchor": "A_note_on_nutrition___", "level": 1}, {"title": "If anyone is going to do this recording your results and sharing them would be much appreciated.", "anchor": "If_anyone_is_going_to_do_this_recording_your_results_and_sharing_them_would_be_much_appreciated_", "level": 1}, {"title": "Habit building:", "anchor": "Habit_building_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "114 comments"}], "headingsCount": 16}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 114, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-21T13:42:29.475Z", "modifiedAt": null, "url": null, "title": "The Power of Reinforcement", "slug": "the-power-of-reinforcement", "viewCount": null, "lastCommentedAt": "2018-10-23T09:33:49.205Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GGn8MBiY8Xz6NdNdH/the-power-of-reinforcement", "pageUrlRelative": "/posts/GGn8MBiY8Xz6NdNdH/the-power-of-reinforcement", "linkUrl": "https://www.lesswrong.com/posts/GGn8MBiY8Xz6NdNdH/the-power-of-reinforcement", "postedAtFormatted": "Thursday, June 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Power%20of%20Reinforcement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Power%20of%20Reinforcement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGGn8MBiY8Xz6NdNdH%2Fthe-power-of-reinforcement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Power%20of%20Reinforcement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGGn8MBiY8Xz6NdNdH%2Fthe-power-of-reinforcement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGGn8MBiY8Xz6NdNdH%2Fthe-power-of-reinforcement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1263, "htmlBody": "<p><small>Part of the sequence:&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">The Science of Winning at Life</a></small></p>\n<p><small>Also see: <a href=\"/lw/6iu/basics_of_animal_reinforcement/\">Basics of Animal Reinforcement</a>, <a href=\"/lw/6ja/basics_of_human_reinforcement/\">Basics of Human Reinforcement</a>, <a href=\"/lw/6l6/physical_and_mental_behavior/\">Physical and Mental Behavior</a>, <a href=\"/lw/6kx/wanting_vs_liking_revisited/\">Wanting vs. Liking Revisited</a>, <a href=\"/lw/6nz/approving_reinforces_loweffort_behaviors/\">Approving reinforces low-effort behaviors</a>, <a href=\"/lw/2dg/applying_behavioral_psychology_on_myself/\">Applying Behavioral Psychology on Myself</a>.</small></p>\n<p>&nbsp;</p>\n<p><strong>Story 1</strong>:</p>\n<p>On Skype with Eliezer, I said: \"Eliezer, you've been unusually <em>pleasant</em>&nbsp;these past three weeks. I'm really happy to see that, and moreover, it increases my probability than an Eliezer-led FAI research team will <em>work</em>. What caused this change, do you think?\"</p>\n<p>Eliezer replied: \"Well, three weeks ago I was working with Anna and Alicorn, and every time I said something nice they fed me an M&amp;M.\"</p>\n<p>&nbsp;</p>\n<p><strong>Story 2</strong>:</p>\n<p>I once witnessed a worker who <em>hated </em>keeping a work log because it was only used \"against\" him. His supervisor would call to say \"Why did you spend so much time on <em>that</em>?\" or \"Why isn't <em>this</em>&nbsp;done yet?\" but never \"I saw you handled <em>X</em>, great job!\" Not surprisingly, he often \"forgot\" to fill out his worklog.</p>\n<p>Ever since I got everyone at the <a href=\"http://intelligence.org/\">Singularity Institute</a> to keep work logs, I've tried to avoid connections between \"concerned\" feedback and staff work logs, and instead take time to comment positively on things I see in those work logs.</p>\n<p>&nbsp;</p>\n<p><strong>Story 3</strong>:</p>\n<p>Chatting with Eliezer, I said, \"Eliezer, I get the sense that I've inadvertently caused you to be slightly averse to talking to me. Maybe because we disagree on so many things, or something?\"</p>\n<p>Eliezer's reply was: \"No, it's much simpler. Our conversations usually run longer than our previously set deadline, so whenever I finish talking with you I feel drained and slightly cranky.\"</p>\n<p>Now I finish our conversations on time.</p>\n<p>&nbsp;</p>\n<p><strong>Story 4</strong>:</p>\n<p>A major Singularity Institute donor recently said to me: \"By the way, I decided that every time I donate to the Singularity Institute, I'll set aside an additional 5% for myself to do fun things with, as a motivation to donate.\"</p>\n<p><a id=\"more\"></a></p>\n<h4><br /></h4>\n<h4>The power of reinforcement</h4>\n<p>It's amazing to me how consistently we fail to take advantage of <a href=\"http://www.youtube.com/watch?v=HqbVbPvlDoM\">the power of reinforcement</a>.</p>\n<p>Maybe it's because behaviorist techniques like reinforcement feel like they don't respect human agency enough.&nbsp;But if you aren't treating humans more like animals than <em>most</em> people are, then you're <em>modeling humans poorly.</em></p>\n<p>You are not an <a href=\"/lw/5i8/the_power_of_agency/\">agenty</a> <a href=\"http://commonsenseatheism.com/?p=8844\">homunculus</a> \"corrupted\" by heuristics and biases. You just <em>are</em>&nbsp;heuristics and biases.&nbsp;And <a href=\"http://www.amazon.com/Power-Reinforcement-Alternatives-Psychology-Series/dp/0791459160/\">you respond to reinforcement</a>,&nbsp;because most of <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">your motivation systems</a> still work like the motivation systems of other animals.</p>\n<p>&nbsp;</p>\n<h4>A quick reminder of what you learned in high school</h4>\n<ul>\n<li>A <em>reinforcer</em>&nbsp;is anything that, when it occurs in conjunction with an act, increases the probability that the act will occur again.</li>\n<li>A <em>positive reinforcer</em>&nbsp;is something the subject wants, such as food, petting, or praise. <em>Positive reinforcement</em>&nbsp;occurs when a target behavior is followed by something the subject wants, and this increases the probability that the behavior will occur again.</li>\n<li>A <em>negative reinforcer</em>&nbsp;is something the subject wants to avoid, such as a blow, a frown, or an unpleasant sound. <em>Negative reinforcement</em>&nbsp;occurs when a target behavior is followed by some <em>relief</em>&nbsp;from something the subject <em>doesn't want</em>, and this increases the probability that the behavior will happen again.</li>\n</ul>\n<p>&nbsp;</p>\n<h4>What works</h4>\n<ol>\n<li><strong>Small reinforcers are fine</strong>, as long as there is a strong correlation between the behavior and the reinforcer (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1334171/pdf/jeabehav00121-0154.pdf\">Schneider 1973</a>; <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1348029/pdf/jeabehav00059-0043.pdf\">Todorov et al. 1984</a>). All else equal, a large reinforcer is more effective than a small one (<a href=\"http://books.google.com/books?id=SWy3uoXJw_4C&amp;lpg=PA400&amp;ots=4mqS11WYXZ&amp;dq=%22Predisposition%20versus%20experiential%20models%20of%20compulsive%20gambling%22&amp;pg=PA400#v=onepage&amp;q=%22Predisposition%20versus%20experiential%20models%20of%20compulsive%20gambling%22&amp;f=false\">Christopher 1988</a>; <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1832167/pdf/jeab-87-02-201.pdf\">Ludvig et al. 2007</a>; <a href=\"http://www.amazon.co.uk/Effectiveness-token-rewards-chimpanzees-Comparative-psychology/dp/B00085DEL4\">Wolfe 1936</a>), but the more you increase the reinforcer magnitude, the less benefit you get from the increase (<a href=\"http://www.tandfonline.com/doi/abs/10.1300/J075v11n01_03\">Frisch &amp; Dickinson 1990</a>).</li>\n<li><strong>The reinforcer should <em>immediately</em>&nbsp;follow the target behavior</strong>&nbsp;(<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1918083/pdf/jeab-88-01-29.pdf\">Escobar &amp; Bruner 2007</a>; <a href=\"http://www.calstatela.edu/academic/psych/html/Graduate/ABA/PDF%20articles/Effects%20of%20Delayed%20Rfmt.pdf\">Schlinger &amp; Blakely 1994</a>; <a href=\"http://scholar.google.com/scholar?cluster=11020848922119008463&amp;hl=en&amp;as_sdt=1,5\">Schneider 1990</a>). <a href=\"http://www.amazon.com/Dont-Shoot-Dog-Teaching-Training/dp/1860542387/\">Pryor (2007)</a>&nbsp;notes that when the reward is food, small bits (like M&amp;Ms) are best because they can be consumed <em>instantly</em>&nbsp;instead of being consumed over an extended period of time.</li>\n<li><strong>Any <em>feature</em> of a behavior can be strengthened</strong> (e.g., its intensity, frequency, rate, duration, persistence, its shape or form), so long as a reinforcer can be made contingent on <em>that particular feature</em> (<a href=\"http://psyc.csustan.edu/bhesse/psy4725/Articles/Variability/Neuringer2002evidencefunctionstheory.pdf\">Neuringer 2002</a>).</li>\n</ol>\n<p>&nbsp;</p>\n<h4>Example applications</h4>\n<ul>\n<li>If you want someone to call you, then when they <em>do</em> call, don't nag them about how they never call you. Instead, be engaging and positive.</li>\n<li>When trying to maintain order in a class, ignore unruly behavior and praise good behavior (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1310990/pdf/jaba00084-0043.pdf\">Madsen et al. 1968</a>; <a href=\"http://books.google.com/books?id=6w58X9a5U3gC&amp;lpg=PA50&amp;ots=jnqGMYFbTz&amp;dq=Behavioural%20approaches%20in%20the%20secondary%20school&amp;lr&amp;pg=PA50#v=onepage&amp;q&amp;f=false\">McNamara 1987</a>).</li>\n<li>Reward originality to encourage creativity (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1338662/pdf/jeabehav00148-0145.pdf\">Pryor et al. 1969</a>; <a href=\"http://scholar.google.com/scholar?cluster=17390870671691473343&amp;hl=en&amp;as_sdt=1,5\">Chambers et al. 1977</a>;&nbsp;<a href=\"http://eisenberger.psych.udel.edu/files/14_Can_Salient_Reward_Increase_Creative_Performance.pdf\">Eisenberger &amp; Armeli 1997</a>; <a href=\"http://www.psychology.uh.edu/faculty/Eisenberger/files/03_Incremental_effects_of_reward.pdf\">Eisenberger &amp; Rhoades 2001</a>).</li>\n<li>If you want students to <em>understand</em>&nbsp;the material, don't get excited when they <a href=\"/lw/iq/guessing_the_teachers_password/\">guess the teacher's password</a> but instead when they demonstrate a&nbsp;<a href=\"http://yudkowsky.net/rational/technical\">technical understanding</a>.</li>\n<li>To help someone improve at dance or sport, ignore poor performance but reward good performance immediately, for example by shouting \"Good!\" (<a href=\"http://bmo.sagepub.com/content/5/3/372.short\">Buzas &amp; Allyon 1981</a>) The reason you should ignore poor performance if you say \"No, you're doing it wrong!\" you are inadvertently punishing the <em>effort</em>. A better response to a mistake would be to reinforce the effort: \"Good effort! You're almost there! Try once more.\"&nbsp;</li>\n<li>Reward honesty to help people be more honest with you (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1347815/pdf/jeabehav00068-0087.pdf\">Lanza et al 1982</a>).</li>\n<li>Reward opinion-expressing to get people to express their opinions more often (<a href=\"http://cogprints.org/602/1/biblio18.html\">Verplanck 1955</a>).</li>\n<li>You may even be able to reinforce-away annoying <em>involuntary</em>&nbsp;behaviors, such as twitches (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1348176/pdf/jeabehav00050-0049.pdf\">Laurenti-Lions et al. 1985</a>) or vomiting (<a href=\"http://scholar.google.com/scholar?cluster=6842890837888786430&amp;hl=en&amp;as_sdt=1,5\">Wolf et al. 1965</a>).</li>\n<li>Want a young infant to learn to speak more quickly? Reinforce their attempts at vocalization (<a href=\"http://jpepsy.oxfordjournals.org/content/3/2/89.short\">Ramely &amp; Finkelstein 1978</a>).</li>\n<li>More training should occur via video games like <a href=\"http://www.wired.com/geekdad/2012/06/dragonbox/all/\">DragonBox</a>, because computer programs can easily provide <em>instant</em>&nbsp;reinforcement&nbsp;<em>many times a minute</em>&nbsp;for <em>very specific behaviors</em>&nbsp;(<a href=\"http://scholar.google.com/scholar?cluster=9612997544810074285&amp;hl=en&amp;as_sdt=1,5\">Fletcher-Flinn &amp; Gravatt 1995</a>).</li>\n</ul>\n<p>For additional examples and studies, see&nbsp;<a href=\"http://www.amazon.com/Power-Reinforcement-Alternatives-Psychology-Series/dp/0791459160/\"><em>The Power of Reinforcement</em> (2004)</a>, <a href=\"http://www.amazon.com/Dont-Shoot-Dog-Teaching-Training/dp/1860542387/\"><em>Don't Shoot the Dog</em> (2006)</a>, and&nbsp;<a href=\"http://www.amazon.com/Learning-Behavior-Active-Edition/dp/0495095648/\"><em>Learning and Behavior</em> (2008)</a>.</p>\n<ul>\n</ul>\n<p>&nbsp;</p>\n<p>I close with <strong>Story 5</strong>, from <a href=\"http://www.nytimes.com/2006/06/25/fashion/25love.html?pagewanted=all\">Amy Sutherland</a>:</p>\n<blockquote>\n<p>For a book I was writing about a school for exotic animal trainers, I started commuting from Maine to California, where I spent my days watching students do the seemingly impossible: teaching hyenas to pirouette on command, cougars to offer their paws for a nail clipping, and baboons to skateboard.</p>\n<p>I listened, rapt, as professional trainers explained how they taught dolphins to flip and elephants to paint. Eventually it hit me that the same techniques might work on that stubborn but lovable species, the American husband.</p>\n<p>The central lesson I learned from exotic animal trainers is that I should reward behavior I like and ignore behavior I don't. After all, you don't get a sea lion to balance a ball on the end of its nose by nagging. The same goes for the American husband.</p>\n<p>Back in Maine, I began thanking Scott if he threw one dirty shirt into the hamper. If he threw in two, I'd kiss him. Meanwhile, I would step over any soiled clothes on the floor without one sharp word, though I did sometimes kick them under the bed. But as he basked in my appreciation, the piles became smaller.</p>\n<p>I was using what trainers call \"approximations,\" rewarding the small steps toward learning a whole new behavior...</p>\n<p>Once I started thinking this way, I couldn't stop. At the school in California, I'd be scribbling notes on how to walk an emu or have a wolf accept you as a pack member, but I'd be thinking, \"I can't wait to try this on Scott.\"</p>\n<p>...After two years of exotic animal training, my marriage is far smoother, my husband much easier to love.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p align=\"right\">Next post: <a href=\"/lw/63i/rational_romantic_relationships_part_1/\">Rational Romantic Relationships Part 1</a></p>\n<p align=\"right\">Previous post: <a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">The Good News of Situationist Psychology</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><small>My thanks to Erica Edelman for doing much of the research for this post.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ipJwbLxhR83ZksN6Z": 1, "3ee9k6NJfcGzL6kMS": 1, "ZXFpyQWPB5ideFbEG": 1, "Wi3EopKJ2aNdtxSWg": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GGn8MBiY8Xz6NdNdH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 120, "baseScore": 161, "extendedScore": null, "score": 0.000342, "legacy": true, "legacyId": "16634", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 161, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Part of the sequence:&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">The Science of Winning at Life</a></small></p>\n<p><small>Also see: <a href=\"/lw/6iu/basics_of_animal_reinforcement/\">Basics of Animal Reinforcement</a>, <a href=\"/lw/6ja/basics_of_human_reinforcement/\">Basics of Human Reinforcement</a>, <a href=\"/lw/6l6/physical_and_mental_behavior/\">Physical and Mental Behavior</a>, <a href=\"/lw/6kx/wanting_vs_liking_revisited/\">Wanting vs. Liking Revisited</a>, <a href=\"/lw/6nz/approving_reinforces_loweffort_behaviors/\">Approving reinforces low-effort behaviors</a>, <a href=\"/lw/2dg/applying_behavioral_psychology_on_myself/\">Applying Behavioral Psychology on Myself</a>.</small></p>\n<p>&nbsp;</p>\n<p><strong>Story 1</strong>:</p>\n<p>On Skype with Eliezer, I said: \"Eliezer, you've been unusually <em>pleasant</em>&nbsp;these past three weeks. I'm really happy to see that, and moreover, it increases my probability than an Eliezer-led FAI research team will <em>work</em>. What caused this change, do you think?\"</p>\n<p>Eliezer replied: \"Well, three weeks ago I was working with Anna and Alicorn, and every time I said something nice they fed me an M&amp;M.\"</p>\n<p>&nbsp;</p>\n<p><strong>Story 2</strong>:</p>\n<p>I once witnessed a worker who <em>hated </em>keeping a work log because it was only used \"against\" him. His supervisor would call to say \"Why did you spend so much time on <em>that</em>?\" or \"Why isn't <em>this</em>&nbsp;done yet?\" but never \"I saw you handled <em>X</em>, great job!\" Not surprisingly, he often \"forgot\" to fill out his worklog.</p>\n<p>Ever since I got everyone at the <a href=\"http://intelligence.org/\">Singularity Institute</a> to keep work logs, I've tried to avoid connections between \"concerned\" feedback and staff work logs, and instead take time to comment positively on things I see in those work logs.</p>\n<p>&nbsp;</p>\n<p><strong>Story 3</strong>:</p>\n<p>Chatting with Eliezer, I said, \"Eliezer, I get the sense that I've inadvertently caused you to be slightly averse to talking to me. Maybe because we disagree on so many things, or something?\"</p>\n<p>Eliezer's reply was: \"No, it's much simpler. Our conversations usually run longer than our previously set deadline, so whenever I finish talking with you I feel drained and slightly cranky.\"</p>\n<p>Now I finish our conversations on time.</p>\n<p>&nbsp;</p>\n<p><strong>Story 4</strong>:</p>\n<p>A major Singularity Institute donor recently said to me: \"By the way, I decided that every time I donate to the Singularity Institute, I'll set aside an additional 5% for myself to do fun things with, as a motivation to donate.\"</p>\n<p><a id=\"more\"></a></p>\n<h4><br></h4>\n<h4 id=\"The_power_of_reinforcement\">The power of reinforcement</h4>\n<p>It's amazing to me how consistently we fail to take advantage of <a href=\"http://www.youtube.com/watch?v=HqbVbPvlDoM\">the power of reinforcement</a>.</p>\n<p>Maybe it's because behaviorist techniques like reinforcement feel like they don't respect human agency enough.&nbsp;But if you aren't treating humans more like animals than <em>most</em> people are, then you're <em>modeling humans poorly.</em></p>\n<p>You are not an <a href=\"/lw/5i8/the_power_of_agency/\">agenty</a> <a href=\"http://commonsenseatheism.com/?p=8844\">homunculus</a> \"corrupted\" by heuristics and biases. You just <em>are</em>&nbsp;heuristics and biases.&nbsp;And <a href=\"http://www.amazon.com/Power-Reinforcement-Alternatives-Psychology-Series/dp/0791459160/\">you respond to reinforcement</a>,&nbsp;because most of <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">your motivation systems</a> still work like the motivation systems of other animals.</p>\n<p>&nbsp;</p>\n<h4 id=\"A_quick_reminder_of_what_you_learned_in_high_school\">A quick reminder of what you learned in high school</h4>\n<ul>\n<li>A <em>reinforcer</em>&nbsp;is anything that, when it occurs in conjunction with an act, increases the probability that the act will occur again.</li>\n<li>A <em>positive reinforcer</em>&nbsp;is something the subject wants, such as food, petting, or praise. <em>Positive reinforcement</em>&nbsp;occurs when a target behavior is followed by something the subject wants, and this increases the probability that the behavior will occur again.</li>\n<li>A <em>negative reinforcer</em>&nbsp;is something the subject wants to avoid, such as a blow, a frown, or an unpleasant sound. <em>Negative reinforcement</em>&nbsp;occurs when a target behavior is followed by some <em>relief</em>&nbsp;from something the subject <em>doesn't want</em>, and this increases the probability that the behavior will happen again.</li>\n</ul>\n<p>&nbsp;</p>\n<h4 id=\"What_works\">What works</h4>\n<ol>\n<li><strong>Small reinforcers are fine</strong>, as long as there is a strong correlation between the behavior and the reinforcer (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1334171/pdf/jeabehav00121-0154.pdf\">Schneider 1973</a>; <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1348029/pdf/jeabehav00059-0043.pdf\">Todorov et al. 1984</a>). All else equal, a large reinforcer is more effective than a small one (<a href=\"http://books.google.com/books?id=SWy3uoXJw_4C&amp;lpg=PA400&amp;ots=4mqS11WYXZ&amp;dq=%22Predisposition%20versus%20experiential%20models%20of%20compulsive%20gambling%22&amp;pg=PA400#v=onepage&amp;q=%22Predisposition%20versus%20experiential%20models%20of%20compulsive%20gambling%22&amp;f=false\">Christopher 1988</a>; <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1832167/pdf/jeab-87-02-201.pdf\">Ludvig et al. 2007</a>; <a href=\"http://www.amazon.co.uk/Effectiveness-token-rewards-chimpanzees-Comparative-psychology/dp/B00085DEL4\">Wolfe 1936</a>), but the more you increase the reinforcer magnitude, the less benefit you get from the increase (<a href=\"http://www.tandfonline.com/doi/abs/10.1300/J075v11n01_03\">Frisch &amp; Dickinson 1990</a>).</li>\n<li><strong>The reinforcer should <em>immediately</em>&nbsp;follow the target behavior</strong>&nbsp;(<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1918083/pdf/jeab-88-01-29.pdf\">Escobar &amp; Bruner 2007</a>; <a href=\"http://www.calstatela.edu/academic/psych/html/Graduate/ABA/PDF%20articles/Effects%20of%20Delayed%20Rfmt.pdf\">Schlinger &amp; Blakely 1994</a>; <a href=\"http://scholar.google.com/scholar?cluster=11020848922119008463&amp;hl=en&amp;as_sdt=1,5\">Schneider 1990</a>). <a href=\"http://www.amazon.com/Dont-Shoot-Dog-Teaching-Training/dp/1860542387/\">Pryor (2007)</a>&nbsp;notes that when the reward is food, small bits (like M&amp;Ms) are best because they can be consumed <em>instantly</em>&nbsp;instead of being consumed over an extended period of time.</li>\n<li><strong>Any <em>feature</em> of a behavior can be strengthened</strong> (e.g., its intensity, frequency, rate, duration, persistence, its shape or form), so long as a reinforcer can be made contingent on <em>that particular feature</em> (<a href=\"http://psyc.csustan.edu/bhesse/psy4725/Articles/Variability/Neuringer2002evidencefunctionstheory.pdf\">Neuringer 2002</a>).</li>\n</ol>\n<p>&nbsp;</p>\n<h4 id=\"Example_applications\">Example applications</h4>\n<ul>\n<li>If you want someone to call you, then when they <em>do</em> call, don't nag them about how they never call you. Instead, be engaging and positive.</li>\n<li>When trying to maintain order in a class, ignore unruly behavior and praise good behavior (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1310990/pdf/jaba00084-0043.pdf\">Madsen et al. 1968</a>; <a href=\"http://books.google.com/books?id=6w58X9a5U3gC&amp;lpg=PA50&amp;ots=jnqGMYFbTz&amp;dq=Behavioural%20approaches%20in%20the%20secondary%20school&amp;lr&amp;pg=PA50#v=onepage&amp;q&amp;f=false\">McNamara 1987</a>).</li>\n<li>Reward originality to encourage creativity (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1338662/pdf/jeabehav00148-0145.pdf\">Pryor et al. 1969</a>; <a href=\"http://scholar.google.com/scholar?cluster=17390870671691473343&amp;hl=en&amp;as_sdt=1,5\">Chambers et al. 1977</a>;&nbsp;<a href=\"http://eisenberger.psych.udel.edu/files/14_Can_Salient_Reward_Increase_Creative_Performance.pdf\">Eisenberger &amp; Armeli 1997</a>; <a href=\"http://www.psychology.uh.edu/faculty/Eisenberger/files/03_Incremental_effects_of_reward.pdf\">Eisenberger &amp; Rhoades 2001</a>).</li>\n<li>If you want students to <em>understand</em>&nbsp;the material, don't get excited when they <a href=\"/lw/iq/guessing_the_teachers_password/\">guess the teacher's password</a> but instead when they demonstrate a&nbsp;<a href=\"http://yudkowsky.net/rational/technical\">technical understanding</a>.</li>\n<li>To help someone improve at dance or sport, ignore poor performance but reward good performance immediately, for example by shouting \"Good!\" (<a href=\"http://bmo.sagepub.com/content/5/3/372.short\">Buzas &amp; Allyon 1981</a>) The reason you should ignore poor performance if you say \"No, you're doing it wrong!\" you are inadvertently punishing the <em>effort</em>. A better response to a mistake would be to reinforce the effort: \"Good effort! You're almost there! Try once more.\"&nbsp;</li>\n<li>Reward honesty to help people be more honest with you (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1347815/pdf/jeabehav00068-0087.pdf\">Lanza et al 1982</a>).</li>\n<li>Reward opinion-expressing to get people to express their opinions more often (<a href=\"http://cogprints.org/602/1/biblio18.html\">Verplanck 1955</a>).</li>\n<li>You may even be able to reinforce-away annoying <em>involuntary</em>&nbsp;behaviors, such as twitches (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1348176/pdf/jeabehav00050-0049.pdf\">Laurenti-Lions et al. 1985</a>) or vomiting (<a href=\"http://scholar.google.com/scholar?cluster=6842890837888786430&amp;hl=en&amp;as_sdt=1,5\">Wolf et al. 1965</a>).</li>\n<li>Want a young infant to learn to speak more quickly? Reinforce their attempts at vocalization (<a href=\"http://jpepsy.oxfordjournals.org/content/3/2/89.short\">Ramely &amp; Finkelstein 1978</a>).</li>\n<li>More training should occur via video games like <a href=\"http://www.wired.com/geekdad/2012/06/dragonbox/all/\">DragonBox</a>, because computer programs can easily provide <em>instant</em>&nbsp;reinforcement&nbsp;<em>many times a minute</em>&nbsp;for <em>very specific behaviors</em>&nbsp;(<a href=\"http://scholar.google.com/scholar?cluster=9612997544810074285&amp;hl=en&amp;as_sdt=1,5\">Fletcher-Flinn &amp; Gravatt 1995</a>).</li>\n</ul>\n<p>For additional examples and studies, see&nbsp;<a href=\"http://www.amazon.com/Power-Reinforcement-Alternatives-Psychology-Series/dp/0791459160/\"><em>The Power of Reinforcement</em> (2004)</a>, <a href=\"http://www.amazon.com/Dont-Shoot-Dog-Teaching-Training/dp/1860542387/\"><em>Don't Shoot the Dog</em> (2006)</a>, and&nbsp;<a href=\"http://www.amazon.com/Learning-Behavior-Active-Edition/dp/0495095648/\"><em>Learning and Behavior</em> (2008)</a>.</p>\n<ul>\n</ul>\n<p>&nbsp;</p>\n<p>I close with <strong>Story 5</strong>, from <a href=\"http://www.nytimes.com/2006/06/25/fashion/25love.html?pagewanted=all\">Amy Sutherland</a>:</p>\n<blockquote>\n<p>For a book I was writing about a school for exotic animal trainers, I started commuting from Maine to California, where I spent my days watching students do the seemingly impossible: teaching hyenas to pirouette on command, cougars to offer their paws for a nail clipping, and baboons to skateboard.</p>\n<p>I listened, rapt, as professional trainers explained how they taught dolphins to flip and elephants to paint. Eventually it hit me that the same techniques might work on that stubborn but lovable species, the American husband.</p>\n<p>The central lesson I learned from exotic animal trainers is that I should reward behavior I like and ignore behavior I don't. After all, you don't get a sea lion to balance a ball on the end of its nose by nagging. The same goes for the American husband.</p>\n<p>Back in Maine, I began thanking Scott if he threw one dirty shirt into the hamper. If he threw in two, I'd kiss him. Meanwhile, I would step over any soiled clothes on the floor without one sharp word, though I did sometimes kick them under the bed. But as he basked in my appreciation, the piles became smaller.</p>\n<p>I was using what trainers call \"approximations,\" rewarding the small steps toward learning a whole new behavior...</p>\n<p>Once I started thinking this way, I couldn't stop. At the school in California, I'd be scribbling notes on how to walk an emu or have a wolf accept you as a pack member, but I'd be thinking, \"I can't wait to try this on Scott.\"</p>\n<p>...After two years of exotic animal training, my marriage is far smoother, my husband much easier to love.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p align=\"right\">Next post: <a href=\"/lw/63i/rational_romantic_relationships_part_1/\">Rational Romantic Relationships Part 1</a></p>\n<p align=\"right\">Previous post: <a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">The Good News of Situationist Psychology</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><small>My thanks to Erica Edelman for doing much of the research for this post.</small></p>", "sections": [{"title": "The power of reinforcement", "anchor": "The_power_of_reinforcement", "level": 1}, {"title": "A quick reminder of what you learned in high school", "anchor": "A_quick_reminder_of_what_you_learned_in_high_school", "level": 1}, {"title": "What works", "anchor": "What_works", "level": 1}, {"title": "Example applications", "anchor": "Example_applications", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "473 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 474, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EMJ3egz48BtZS8Pws", "BfaAADSQ88cuxLQoD", "5dhWhjfxn4tPfFQdi", "eaczwARbFnrisFx8E", "yDRX2fdkm3HqfTpav", "EJuZcWnk8j7eNQPqq", "vbcjYg6h3XzuqaaN8", "fa5o2tg9EfJE77jEQ", "NMoLJuDJEms7Ku9XS", "JYckkCqhZPrdScjBx", "Q5CjE8pRiACqTvhRM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-21T16:44:26.218Z", "modifiedAt": null, "url": null, "title": "Leading Discussions: Addressing Time Monopolizers", "slug": "leading-discussions-addressing-time-monopolizers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.222Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SwingDancerMike", "createdAt": "2012-06-20T19:35:15.919Z", "isAdmin": false, "displayName": "SwingDancerMike"}, "userId": "JjcjwJCRa4fRdFFC3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/faFZ8iLrz6HshZizu/leading-discussions-addressing-time-monopolizers", "pageUrlRelative": "/posts/faFZ8iLrz6HshZizu/leading-discussions-addressing-time-monopolizers", "linkUrl": "https://www.lesswrong.com/posts/faFZ8iLrz6HshZizu/leading-discussions-addressing-time-monopolizers", "postedAtFormatted": "Thursday, June 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Leading%20Discussions%3A%20Addressing%20Time%20Monopolizers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALeading%20Discussions%3A%20Addressing%20Time%20Monopolizers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfaFZ8iLrz6HshZizu%2Fleading-discussions-addressing-time-monopolizers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Leading%20Discussions%3A%20Addressing%20Time%20Monopolizers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfaFZ8iLrz6HshZizu%2Fleading-discussions-addressing-time-monopolizers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfaFZ8iLrz6HshZizu%2Fleading-discussions-addressing-time-monopolizers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1007, "htmlBody": "<blockquote>\n<p><em>\"Having been that monologuist, I fully endorse this article.\" -David Gerard</em></p>\n<p><em>This is part of a series on leading discussions, aimed at teachers and event organizers.</em></p>\n</blockquote>\n<p>Have you ever been in a discussion (in a class or a workshop) when another attendee delivered a monolog? It lasts for several minutes, drains the energy from the group, and gets the whole event off schedule.<br /><br />As the meetup / workshop / class leader, it's your responsibility to address these time monopolizers and keep everything moving. This article will give you some tips on identifying when you should intervene, and on how to intervene politely.</p>\n<h1>Identifying Time Monopolizers</h1>\n<p>If one person is talking for an extended period, they are probably monopolizing the conversation. Here are some specific signs to look out for:</p>\n<ul>\n<li>The person has covered multiple topics without anyone else speaking. If they are moving from topic to topic without other people chiming in, it's a monolog, not a discussion.</li>\n<li>You (the leader) feel bored or annoyed. Most of the time, everyone else will feel it 10x as much, because they are already less engaged in the conversation than you are. Tip: Don't wait until you're going to snap. Instead, as soon as you notice yourself getting bored / annoyed, shut the monolog down. (Note: I mean \"annoyed they are taking up so much time,\" not \"annoyed they disagree with me.\")</li>\n<li>Other attendees are disengaged. This often manifests in reading handouts or emails, looking out the window or down at their hands, and otherwise not paying attention to the speaker.</li>\n</ul>\n<p>If only one of these items is true -- if the person is crossing multiple topics, but you and the class are engaged, for example -- then feel free to let them speak. But if two or more of these items are true, you should probably intervene. (See the next section for how to do that.)</p>\n<p>If the event ends and you're not sure if someone was monopolizing the conversation, ask the quiet people how it went, and ask them if anyone in the group slowed the class down or monopolized the conversation. They might be too polite to bring the issue up, but once you ask directly, they'll let you know what's going on. Make sure to thank them for their honesty, and keep their feedback anonymous when you talk to the time monopolizer.<br /><br />One note: While a monolog that covers multiple topics is usually bad, a discussion where multiple people cover multiple topics is usually good, even if it prevents you (the leader) from hitting all the topics you'd planned for the event. After all, most people show up at events to meet like-minded people and make new friends, not to cover each of the items on the handout.<br /><br />But, do pay attention to the rest of the attendees: Are they engaged or checked out? Is this a discussion involving half the group, or a dialog between only two people? As the leader, are you interested in the discussion? If two people are having a dialog, and the rest of the group is checking out, it's your job to shut that dialog down, too.</p>\n<h1>How to Intervene</h1>\n<p><em>Politely</em>. This is the key. As the leader of the event, any reprimand you give will be felt more deeply than a reprimand given by a peer.<br /><br />Wait for the person to take a breath, then speak up. (If you don't get that chance after 20 seconds or so, raise your hand like you're stopping traffic, wait a second, then start speaking.) Say something like:<br /><br /><em>That's really interesting. I'm just going to pause you for a second so we can get back to the topic. Grab me afterward and we can discuss that some more.</em><br /><br />Note that we started with praise (\"That's really interesting.\") Don't worry, they'll get the message. Then, without saying anything negative about the person or his statements, simply say that you need to get back on topic.<br /><br />Also, avoid saying \"but\" here: \"That's really interesting, <em>but</em> I'm going to pause you&hellip;\" The \"but\" construction negates the first sentence, and could make it sound sarcastic.<br /><br />Here are some other phrases you can try:</p>\n<ul>\n<li>That's really interesting. I'd like to hear from some of the other folks. (Then ask someone else what they think.)</li>\n<li>I'd love to explore this more with you offline. Could you grab me after (the class / workshop / whatever event this is)?</li>\n</ul>\n<p>[Note: Please add to this list.]<br /><br />What if you don't stop a monolog during the event, but realize afterward that you should have? That's just fine. The point isn't to have every event run perfectly, but rather, to make sure that no one repeatedly monopolizes the conversation month after month. Just talk to them afterward, and say something like:<br /><br /><em>I noticed you were speaking a lot during the event. I love how passionate you are about these topics. I have a favor, though: Some of the people are a little more shy and reserved than you are, and I want to make sure everyone gets a chance to speak. After all, if there are 10 people here, then ideally, each person would make about 10% of the remarks. Do you think you could talk a little less at the next meetup, to give everyone else more of a chance?</em></p>\n<p>(Note: It's not actually important that everyone speak for the same amount of time, but it's a good way to explain what isn't working. Feel free to use your own judgement on how to help your particular time monopolizer understand what's not working.)</p>\n<p>In all these interactions, make sure to smile and be friendly. Remember, he probably doesn't realize he was monopolizing the conversation. And keep in mind that no one adjusts instantly, and you might still need to jump in and direct the conversation at the next event, too.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 1, "T57Qd9J3AfxmwhQtY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "faFZ8iLrz6HshZizu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 36, "extendedScore": null, "score": 9.272097314919604e-07, "legacy": true, "legacyId": "17138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p><em>\"Having been that monologuist, I fully endorse this article.\" -David Gerard</em></p>\n<p><em>This is part of a series on leading discussions, aimed at teachers and event organizers.</em></p>\n</blockquote>\n<p>Have you ever been in a discussion (in a class or a workshop) when another attendee delivered a monolog? It lasts for several minutes, drains the energy from the group, and gets the whole event off schedule.<br><br>As the meetup / workshop / class leader, it's your responsibility to address these time monopolizers and keep everything moving. This article will give you some tips on identifying when you should intervene, and on how to intervene politely.</p>\n<h1 id=\"Identifying_Time_Monopolizers\">Identifying Time Monopolizers</h1>\n<p>If one person is talking for an extended period, they are probably monopolizing the conversation. Here are some specific signs to look out for:</p>\n<ul>\n<li>The person has covered multiple topics without anyone else speaking. If they are moving from topic to topic without other people chiming in, it's a monolog, not a discussion.</li>\n<li>You (the leader) feel bored or annoyed. Most of the time, everyone else will feel it 10x as much, because they are already less engaged in the conversation than you are. Tip: Don't wait until you're going to snap. Instead, as soon as you notice yourself getting bored / annoyed, shut the monolog down. (Note: I mean \"annoyed they are taking up so much time,\" not \"annoyed they disagree with me.\")</li>\n<li>Other attendees are disengaged. This often manifests in reading handouts or emails, looking out the window or down at their hands, and otherwise not paying attention to the speaker.</li>\n</ul>\n<p>If only one of these items is true -- if the person is crossing multiple topics, but you and the class are engaged, for example -- then feel free to let them speak. But if two or more of these items are true, you should probably intervene. (See the next section for how to do that.)</p>\n<p>If the event ends and you're not sure if someone was monopolizing the conversation, ask the quiet people how it went, and ask them if anyone in the group slowed the class down or monopolized the conversation. They might be too polite to bring the issue up, but once you ask directly, they'll let you know what's going on. Make sure to thank them for their honesty, and keep their feedback anonymous when you talk to the time monopolizer.<br><br>One note: While a monolog that covers multiple topics is usually bad, a discussion where multiple people cover multiple topics is usually good, even if it prevents you (the leader) from hitting all the topics you'd planned for the event. After all, most people show up at events to meet like-minded people and make new friends, not to cover each of the items on the handout.<br><br>But, do pay attention to the rest of the attendees: Are they engaged or checked out? Is this a discussion involving half the group, or a dialog between only two people? As the leader, are you interested in the discussion? If two people are having a dialog, and the rest of the group is checking out, it's your job to shut that dialog down, too.</p>\n<h1 id=\"How_to_Intervene\">How to Intervene</h1>\n<p><em>Politely</em>. This is the key. As the leader of the event, any reprimand you give will be felt more deeply than a reprimand given by a peer.<br><br>Wait for the person to take a breath, then speak up. (If you don't get that chance after 20 seconds or so, raise your hand like you're stopping traffic, wait a second, then start speaking.) Say something like:<br><br><em>That's really interesting. I'm just going to pause you for a second so we can get back to the topic. Grab me afterward and we can discuss that some more.</em><br><br>Note that we started with praise (\"That's really interesting.\") Don't worry, they'll get the message. Then, without saying anything negative about the person or his statements, simply say that you need to get back on topic.<br><br>Also, avoid saying \"but\" here: \"That's really interesting, <em>but</em> I'm going to pause you\u2026\" The \"but\" construction negates the first sentence, and could make it sound sarcastic.<br><br>Here are some other phrases you can try:</p>\n<ul>\n<li>That's really interesting. I'd like to hear from some of the other folks. (Then ask someone else what they think.)</li>\n<li>I'd love to explore this more with you offline. Could you grab me after (the class / workshop / whatever event this is)?</li>\n</ul>\n<p>[Note: Please add to this list.]<br><br>What if you don't stop a monolog during the event, but realize afterward that you should have? That's just fine. The point isn't to have every event run perfectly, but rather, to make sure that no one repeatedly monopolizes the conversation month after month. Just talk to them afterward, and say something like:<br><br><em>I noticed you were speaking a lot during the event. I love how passionate you are about these topics. I have a favor, though: Some of the people are a little more shy and reserved than you are, and I want to make sure everyone gets a chance to speak. After all, if there are 10 people here, then ideally, each person would make about 10% of the remarks. Do you think you could talk a little less at the next meetup, to give everyone else more of a chance?</em></p>\n<p>(Note: It's not actually important that everyone speak for the same amount of time, but it's a good way to explain what isn't working. Feel free to use your own judgement on how to help your particular time monopolizer understand what's not working.)</p>\n<p>In all these interactions, make sure to smile and be friendly. Remember, he probably doesn't realize he was monopolizing the conversation. And keep in mind that no one adjusts instantly, and you might still need to jump in and direct the conversation at the next event, too.</p>", "sections": [{"title": "Identifying Time Monopolizers", "anchor": "Identifying_Time_Monopolizers", "level": 1}, {"title": "How to Intervene", "anchor": "How_to_Intervene", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-21T19:52:15.674Z", "modifiedAt": null, "url": null, "title": "A free Utopia", "slug": "a-free-utopia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.459Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "zLn6KMM7RY42sBoDu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QMTvthTohhX7Yy3Q2/a-free-utopia", "pageUrlRelative": "/posts/QMTvthTohhX7Yy3Q2/a-free-utopia", "linkUrl": "https://www.lesswrong.com/posts/QMTvthTohhX7Yy3Q2/a-free-utopia", "postedAtFormatted": "Thursday, June 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20free%20Utopia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20free%20Utopia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQMTvthTohhX7Yy3Q2%2Fa-free-utopia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20free%20Utopia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQMTvthTohhX7Yy3Q2%2Fa-free-utopia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQMTvthTohhX7Yy3Q2%2Fa-free-utopia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 691, "htmlBody": "<p>I seem to have a relatively unique utopia idea (I've never seen anyone speak of a similar utopia) which removes the need for fun theory, and with Alicorn's recent <a title=\"Local Ordinances of Fun\" href=\"/lw/d4x/local_ordinances_of_fun/\">post</a> about her uneasiness with Eliezer's <a title=\"31 Laws of Fun\" href=\"/lw/y0/31_laws_of_fun/\">31 Laws of Fun</a>, I think it's a good time to bring it up in the discussion.</p>\n<p>This Utopia is very simple; it has only 4 real laws, as follows (probably phrased better to prevent loopholes).</p>\n<p style=\"padding-left: 30px;\">Thou shalt not physically harm others without their permission</p>\n<p style=\"padding-left: 30px;\">Thou shalt not damage other's property without their permission</p>\n<p style=\"padding-left: 30px;\">Thou shalt not restrict other's freedom</p>\n<p style=\"padding-left: 30px;\">Thou shalt not plot to break the law with the intention that the plot will happen without their permission</p>\n<p>Basically don't harm others, don't damage their property, don't lock up someone in your basement, and don't hire someone else to do any of this for you. You may harm others with their permission (though most people won't give you permission). The reason for allowing harm with permission is to restrict freedom as little as possible (you wouldn't want to make boxing illegal, for example, and I personally believe suicide is an option people should have ifever they really want it). This seems unmaintainable and it sounds like it would lead to chaos, but with good enough AI I expect it wouldn't be.</p>\n<p>To maintain the law, AI police would be set up all over the place to catch people who break these few laws, but they wouldn't interfere unless someone breaks the law, or is about to do so, or someone asks their help.</p>\n<p>Artificially Intelligent overlords would deal with the entire relatively simple economy. It's basically ask the AI for something, and they will give it to you if it doesn't require too many resources relative to the supply and demand, and if what you're asking doesn't break the 4 laws. You could of course pool with a large group of others to get extra resources if the resources of an army of robots who's sole purpose is to provide you with what you're asking for isn't enough for you.</p>\n<p>People would be allowed to make their own sub-societies on their land. These sub-societies would have their own laws, except for one: thou shalt not restrict other's freedom. If someone's society has laws which someone does not want to follow, they are completely free not to follow them, though the sub-society is free to kick that person out. AIs would provide a free service of analyzing laws for potential dangers and loopholes. Sub-societies would also usually create protocols for things like roads and even social interactions to keep things smooth.</p>\n<p>And to do away with fun theory, the AI would provide a free service to recommend things you'll enjoy. And if you've got a problem such as extreme laziness or if you want photographic memory or something, you could just ask the AI to operate you for that. And if you don't want to die, anti-aging technologies would probably be available, so you can live forever if you want to, but if you don't for some reason, you don't have to.</p>\n<p>Most possible utopias would fall as a subset of this utopia, except those which restrict freedom. You could definitely live happily here, whatever a happy life would be like for you, and if you don't know what to do, AI will be there to recommend things. And this utopia avoids the objection that everyone has about every utopia: the disagreement about what matters and what doesn't.</p>\n<p>And to all the people who will say that no one should ever die, I reply that no one should ever die without their consent. Death is only bad if you don't want to die. Maybe all those who want to die in a Utopia are crazy, but I consider forcibly changing them so that they do want to live an imposition of values no better than religions imposing their values on us all. And if you can't agree with me on this, please just ignore this part of my opinion when you comment on my utopia, I would hate for every comment to be on this small aspect of my views.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QMTvthTohhX7Yy3Q2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": -28, "extendedScore": null, "score": -1.5e-05, "legacy": true, "legacyId": "17140", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QSDRgQq5vzPKDSwF7", "qZJBighPrnv9bSqTZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T04:08:34.364Z", "modifiedAt": null, "url": null, "title": "Meetup : Discussion article for the meetup : Melbourne, practical rationality", "slug": "meetup-discussion-article-for-the-meetup-melbourne-practical", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "spiral_shell", "createdAt": "2010-10-15T23:47:52.452Z", "isAdmin": false, "displayName": "spiral_shell"}, "userId": "LfrwnCGvYLyhJC7aX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4KajsGuCn6E4bAKwv/meetup-discussion-article-for-the-meetup-melbourne-practical", "pageUrlRelative": "/posts/4KajsGuCn6E4bAKwv/meetup-discussion-article-for-the-meetup-melbourne-practical", "linkUrl": "https://www.lesswrong.com/posts/4KajsGuCn6E4bAKwv/meetup-discussion-article-for-the-meetup-melbourne-practical", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Discussion%20article%20for%20the%20meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Discussion%20article%20for%20the%20meetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KajsGuCn6E4bAKwv%2Fmeetup-discussion-article-for-the-meetup-melbourne-practical%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Discussion%20article%20for%20the%20meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KajsGuCn6E4bAKwv%2Fmeetup-discussion-article-for-the-meetup-melbourne-practical", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KajsGuCn6E4bAKwv%2Fmeetup-discussion-article-for-the-meetup-melbourne-practical", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bb'>Discussion article for the meetup : Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 July 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Carlton VIC 3053, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHEN: 06 July 2012 07:00:00PM (+1000)</p>\n\n<p>WHERE: Bens House : See the mailing list for the address - or you can call or text 0432 862 932 for the address. Alternatively, you can email shokwave.sf@gmail.com or even message User:shokwave on LessWrong.</p>\n\n<p>Practical rationality is on the 1st friday of each month, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.\nDiscussion article for the meetup :Melbourne, practical rationality</p>\n\n<p>Contact Scott 0432 862 932</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bb'>Discussion article for the meetup : Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4KajsGuCn6E4bAKwv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.275245227043544e-07, "legacy": true, "legacyId": "17143", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/bb\">Discussion article for the meetup : Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 July 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Carlton VIC 3053, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHEN: 06 July 2012 07:00:00PM (+1000)</p>\n\n<p>WHERE: Bens House : See the mailing list for the address - or you can call or text 0432 862 932 for the address. Alternatively, you can email shokwave.sf@gmail.com or even message User:shokwave on LessWrong.</p>\n\n<p>Practical rationality is on the 1st friday of each month, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.\nDiscussion article for the meetup :Melbourne, practical rationality</p>\n\n<p>Contact Scott 0432 862 932</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/bb\">Discussion article for the meetup : Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T04:23:26.400Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Bedrock of Fairness", "slug": "seq-rerun-the-bedrock-of-fairness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.314Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fFGSQYSieaonjT5CZ/seq-rerun-the-bedrock-of-fairness", "pageUrlRelative": "/posts/fFGSQYSieaonjT5CZ/seq-rerun-the-bedrock-of-fairness", "linkUrl": "https://www.lesswrong.com/posts/fFGSQYSieaonjT5CZ/seq-rerun-the-bedrock-of-fairness", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Bedrock%20of%20Fairness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Bedrock%20of%20Fairness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfFGSQYSieaonjT5CZ%2Fseq-rerun-the-bedrock-of-fairness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Bedrock%20of%20Fairness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfFGSQYSieaonjT5CZ%2Fseq-rerun-the-bedrock-of-fairness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfFGSQYSieaonjT5CZ%2Fseq-rerun-the-bedrock-of-fairness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p>Today's post, <a href=\"/lw/ru/the_bedrock_of_fairness/\">The Bedrock of Fairness</a> was originally published on 03 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Bedrock_of_Fairness\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>What does \"fairness\" actually refer to? Why is it \"fair\" to divide a pie into three equal pieces for three different people?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d7g/seq_rerun_created_already_in_motion/\">Created Already In Motion</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fFGSQYSieaonjT5CZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.275313591612893e-07, "legacy": true, "legacyId": "17144", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iAxkfiyG8WizPSPbq", "hmbRoapL9BahhwbiJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T08:53:07.068Z", "modifiedAt": null, "url": null, "title": "Blogs by LWers", "slug": "blogs-by-lwers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:06.301Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h83ZzxEpKiPPjsRKs/blogs-by-lwers", "pageUrlRelative": "/posts/h83ZzxEpKiPPjsRKs/blogs-by-lwers", "linkUrl": "https://www.lesswrong.com/posts/h83ZzxEpKiPPjsRKs/blogs-by-lwers", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Blogs%20by%20LWers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABlogs%20by%20LWers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh83ZzxEpKiPPjsRKs%2Fblogs-by-lwers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Blogs%20by%20LWers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh83ZzxEpKiPPjsRKs%2Fblogs-by-lwers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh83ZzxEpKiPPjsRKs%2Fblogs-by-lwers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 629, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/hee/wikifying_the_blog_list/\">Wikifying the Blog List</a></p>\n<p>LessWrong posters and readers are generally pretty cool people. Maybe they are interesting bloggers too. And I'm not just talking about rationalist material, that we'd ideally like to be cross posted on LessWrong, no gardening blogs are also fair game. I'm making this a discussion level post so more people can see the list. Please share links to blogs by former or current LWers. Surely the authors wouldn't mind, who wouldn't like more readers? Original list <a href=\"/lw/d4s/siai_may_report/6vq8\">here.</a></p>\n<p><strong>Anyone who wants to suggest a new blog for the list please follow <a href=\"/r/discussion/lw/d8t/blogs_by_lwers/6w6h\">this link</a>. </strong></p>\n<p><strong>Blogs by LWers:</strong></p>\n<ul>\n<li>RobinHanson --- <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> (Katja Grace and Robert Wiblin post here as well)</li>\n<li>Katja Grace --- <a href=\"http://meteuphoric.wordpress.com/\">Meteuphoric</a> (very cool old posts and summaries)</li>\n<li>muflax --- <a rel=\"nofollow\" href=\"http://blog.muflax.com/\">muflax' mindstream</a>, <a href=\"http://daily.muflax.com/\">daily</a></li>\n<li>TGGP --- <a rel=\"nofollow\" href=\"http://entitledtoanopinion.wordpress.com/\">Entitled To An Opinion</a></li>\n<li>Yvain --- <a rel=\"nofollow\" href=\"http://squid314.livejournal.com/\">Jackdaws love my big sphinx of quartz</a></li>\n<li>juliawise --- <a href=\"http://www.givinggladly.com/\">Giving Gladly</a>, <a href=\"http://jdwise.blogspot.com/\">Radiant Things</a></li>\n<li>James_G --- <a rel=\"nofollow\" href=\"http://james-g.com/\">Writings</a></li>\n<li>steven0461 --- <a href=\"http://www.acceleratingfuture.com/steven/\">Black Belt Bayesian</a></li>\n<li>James Miller --- <a rel=\"nofollow\" href=\"http://singularitynotes.com/author/singularitynotes/\">Singluarity Notes</a></li>\n<li>Jsalvati --- <a rel=\"nofollow\" href=\"http://goodmorningeconomics.wordpress.com/author/jsalvati/\">Good Morning, Economics</a></li>\n<li>Will Newsome --- <a rel=\"nofollow\" href=\"http://computationaltheology.blogspot.com/\">Computational Theology</a></li>\n<li>clarissethorn --- <a rel=\"nofollow\" href=\"http://clarissethorn.com/blog/\">Clarrise Thorn</a></li>\n<li>Zack M. Davis --- <a rel=\"nofollow\" href=\"http://zackmdavis.net/blog/\">An Algorithmic Lucidity</a></li>\n<li>Kaj_Sotala --- <a rel=\"nofollow\" href=\"http://xuenay.livejournal.com/\">A view to the gallery of my mind</a></li>\n<li>SilasBarta --- <a href=\"http://blog.tyrannyofthemouse.com/\">Setting Things Straight </a></li>\n<li>tommcabe --- <a rel=\"nofollow\" href=\"http://rationalconspiracy.com/\">The Rationalist Conspiracy</a></li>\n<li>Alicorn --- <a rel=\"nofollow\" href=\"http://alicorn24.livejournal.com/\">Irregular Updates By An Irregular Person</a></li>\n<li>MBlume --- <a rel=\"nofollow\" href=\"http://dudley-doright.livejournal.com/\">Baby, check this out; I've got something to say.</a></li>\n<li>ciphergoth --- <a href=\"http://blog.ciphergoth.org/blog/\">Paul Crowley's blog</a> (mostly about cryonics), <a rel=\"nofollow\" href=\"http://ciphergoth.livejournal.com/\">Paul Crowley</a></li>\n<li>XiXiDu --- <a rel=\"nofollow\" href=\"http://kruel.co/\">Alexander Kruel</a></li>\n<li>Aurini --- <a href=\"http://www.staresattheworld.com/\">Stares At The World</a></li>\n<li>jkaufman --- <a href=\"http://www.jefftk.com/news/\">Jeff Kaufman</a></li>\n<li>Bill_McGrath --- <a href=\"http://billmcgrathmusic.wordpress.com/\">billmcgrathmusic</a></li>\n<li>Sister Y --- <a href=\"http://theviewfromhell.blogspot.com/\">the view from hell</a></li>\n<li>PaulWright --- <a class=\"external text\" rel=\"nofollow\" href=\"http://blog.noctua.org.uk/\">Paul Wright's blog</a></li>\n<li>_ozymandias --- <a rel=\"nofollow\" href=\"http://ozyfrantz.com/\">http://ozyfrantz.com/</a> </li>\n<li>mstevens --- <a href=\"http://www.mstevens.org/\">stdout</a></li>\n<li>HughRistik --- <a href=\"http://www.feministcritics.org/blog/\">Feminist Critics</a></li>\n<li>Julia_Galef --- <a href=\"http://measureofdoubt.com/author/measureofdoubt/\">Measure of Doubt</a></li>\n<li>NancyLebovitz --- <a href=\"http://nancylebov.livejournal.com/\">Input Junkie</a></li>\n<li>David Gerard --- <a href=\"http://davidgerard.co.uk/\">a bunch of them</a></li>\n<li>Jayson_Virissimo --- <a href=\"http://jayquantified.blogspot.com/\">Jay, Quantified</a></li>\n<li>kpreid --- <a href=\"http://kpreid.livejournal.com/\">Kevin Reid's blog</a></li>\n<li>hegemonicon --- <a href=\"http://coarsegrained.tumblr.com/\">Coarse Grained&nbsp;</a></li>\n<li>Villiam_Bur --- <a href=\"http://bur.sk/\">bur.sk</a></li>\n<li>Emile --- <a href=\"http://rationalparent.blogspot.com/\">The Rational Parent</a></li>\n<li>lukeprog --- <a href=\"http://commonsenseatheism.com/\">Common Sense Atheism</a> </li>\n<li>Grognor --- <a href=\"http://grognor.blogspot.com/\">Grognor's Blog</a></li>\n<li>CarlShulman --- <a rel=\"nofollow\" href=\"http://reflectivedisequilibrium.blogspot.com/\">Reflective Disequilibrium</a></li>\n<li>OrphanWilde --- <a href=\"http://aretae.blogspot.com/\">Aretae</a></li>\n<li>Alexei --- <a href=\"http://bentspoongames.com/blog/\">Bent Spoon Games Blog</a></li>\n<li>TimS --- <a href=\"http://educationlawgeorgia.com/blog\">Georgia Special Education Law Blog&nbsp;</a></li>\n<li>loup-valliant --- <a href=\"http://loup-vaillant.fr/articles/\">@ Loup's</a></li>\n<li>RolfAndreaseen --- <a href=\"http://ynglingasaga.wordpress.com/\">Yngling Saga</a></li>\n<li>arundelo --- <a href=\"http://arundelo.livejournal.com/\">Aaron Brown</a></li>\n<li>peter_hurtford --- <a href=\"http://www.greatplay.net/\">Greatplay.net</a></li>\n<li>brilee --- <a href=\"http://moderndescartes.com/blog/\">Modern Descartes</a></li>\n<li>gwern --- <a href=\"http://www.gwern.net/\">gwern.net</a></li>\n<li>erratio --- <a rel=\"nofollow\" href=\"http://erratio.livejournal.com/\">The merry-go-round of life</a></li>\n<li>jimmy --- <a href=\"http://cognitiveengineer.blogspot.ca/\">The Art and Science of Cognitive Engineering</a></li>\n<li>alexvermeer --- <a href=\"http://alexvermeer.com/\">alexvermeer.com</a></li>\n<li><span>sark --- <a href=\"http://sarkology.wordpress.com/\">sarkology</a></span></li>\n<li>gjm --- <a rel=\"nofollow\" href=\"http://www.mccaughan.org.uk/g/log/\">Scribble, scribble, scribble</a></li>\n<li>Giles --- <a rel=\"nofollow\" href=\"http://princemm.wordpress.com/\">Prince Mm Mm</a></li>\n<li>Chris Hallquist --- <a rel=\"nofollow\" href=\"http://freethoughtblogs.com/hallq/\">The Uncredible Hallq</a></li>\n<li>EricHerboso --- <a rel=\"nofollow\" href=\"http://www.ericherboso.org/\">EricHerboso.org</a></li>\n<li>Eneasz - <a rel=\"nofollow\" href=\"http://www.deathisbadblog.com/\">Death Is Bad</a></li>\n<li>Tuxedage - <a href=\"https://tuxedage.wordpress.com/\">Essays and other Musings</a></li>\n<li>Federico - <a href=\"http://studiolo.cortediurbino.org/\">studiolo</a></li>\n<li>Trevor Blake - <a rel=\"nofollow\" href=\"http://ovo127.com\">OVO</a>, editor-<a rel=\"nofollow\" href=\"http://doramarsden.tumblr.com\">Dora Marsden</a>, lead judge-<a rel=\"nofollow\" href=\"http://gwiep.net/\">George Walford International Essay Prize</a></li>\n<li>Pablo_Stafforini -- <a rel=\"nofollow\" href=\"http://www.stafforini.com/blog/\">Pablo's miscellany</a></li>\n</ul>\n<p><a href=\"/r/discussion/lw/d92/less_wrong_on_twitter/\"><strong>List of LWers on Twitter</strong></a></p>\n<p><strong><br /></strong></p>\n<p><strong>Note:</strong> Anyone just digging for interesting blogs they would like to read but dosen't care if they are written by LWers or not should check out <a href=\"/lw/7kk/seeking_rationalityrelated_blogs/\">this thread</a> or maybe <a href=\"/lw/6ls/other_useful_sites_lwers_read/\">this one</a>. Did you guys know we have a wiki article with <a href=\"http://wiki.lesswrong.com/wiki/External_resources\">external resources</a>? We do. Check that out as well. Maybe once we figure out which LWer blogs related to rationality on this list are particularly good we can add a few of them there too.</p>\n<p><strong></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "GQyPQcdEQF4zXhJBq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h83ZzxEpKiPPjsRKs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 62, "extendedScore": null, "score": 0.000138, "legacy": true, "legacyId": "17165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related to:</strong> <a href=\"/lw/hee/wikifying_the_blog_list/\">Wikifying the Blog List</a></p>\n<p>LessWrong posters and readers are generally pretty cool people. Maybe they are interesting bloggers too. And I'm not just talking about rationalist material, that we'd ideally like to be cross posted on LessWrong, no gardening blogs are also fair game. I'm making this a discussion level post so more people can see the list. Please share links to blogs by former or current LWers. Surely the authors wouldn't mind, who wouldn't like more readers? Original list <a href=\"/lw/d4s/siai_may_report/6vq8\">here.</a></p>\n<p><strong id=\"Anyone_who_wants_to_suggest_a_new_blog_for_the_list_please_follow_this_link__\">Anyone who wants to suggest a new blog for the list please follow <a href=\"/r/discussion/lw/d8t/blogs_by_lwers/6w6h\">this link</a>. </strong></p>\n<p><strong id=\"Blogs_by_LWers_\">Blogs by LWers:</strong></p>\n<ul>\n<li>RobinHanson --- <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> (Katja Grace and Robert Wiblin post here as well)</li>\n<li>Katja Grace --- <a href=\"http://meteuphoric.wordpress.com/\">Meteuphoric</a> (very cool old posts and summaries)</li>\n<li>muflax --- <a rel=\"nofollow\" href=\"http://blog.muflax.com/\">muflax' mindstream</a>, <a href=\"http://daily.muflax.com/\">daily</a></li>\n<li>TGGP --- <a rel=\"nofollow\" href=\"http://entitledtoanopinion.wordpress.com/\">Entitled To An Opinion</a></li>\n<li>Yvain --- <a rel=\"nofollow\" href=\"http://squid314.livejournal.com/\">Jackdaws love my big sphinx of quartz</a></li>\n<li>juliawise --- <a href=\"http://www.givinggladly.com/\">Giving Gladly</a>, <a href=\"http://jdwise.blogspot.com/\">Radiant Things</a></li>\n<li>James_G --- <a rel=\"nofollow\" href=\"http://james-g.com/\">Writings</a></li>\n<li>steven0461 --- <a href=\"http://www.acceleratingfuture.com/steven/\">Black Belt Bayesian</a></li>\n<li>James Miller --- <a rel=\"nofollow\" href=\"http://singularitynotes.com/author/singularitynotes/\">Singluarity Notes</a></li>\n<li>Jsalvati --- <a rel=\"nofollow\" href=\"http://goodmorningeconomics.wordpress.com/author/jsalvati/\">Good Morning, Economics</a></li>\n<li>Will Newsome --- <a rel=\"nofollow\" href=\"http://computationaltheology.blogspot.com/\">Computational Theology</a></li>\n<li>clarissethorn --- <a rel=\"nofollow\" href=\"http://clarissethorn.com/blog/\">Clarrise Thorn</a></li>\n<li>Zack M. Davis --- <a rel=\"nofollow\" href=\"http://zackmdavis.net/blog/\">An Algorithmic Lucidity</a></li>\n<li>Kaj_Sotala --- <a rel=\"nofollow\" href=\"http://xuenay.livejournal.com/\">A view to the gallery of my mind</a></li>\n<li>SilasBarta --- <a href=\"http://blog.tyrannyofthemouse.com/\">Setting Things Straight </a></li>\n<li>tommcabe --- <a rel=\"nofollow\" href=\"http://rationalconspiracy.com/\">The Rationalist Conspiracy</a></li>\n<li>Alicorn --- <a rel=\"nofollow\" href=\"http://alicorn24.livejournal.com/\">Irregular Updates By An Irregular Person</a></li>\n<li>MBlume --- <a rel=\"nofollow\" href=\"http://dudley-doright.livejournal.com/\">Baby, check this out; I've got something to say.</a></li>\n<li>ciphergoth --- <a href=\"http://blog.ciphergoth.org/blog/\">Paul Crowley's blog</a> (mostly about cryonics), <a rel=\"nofollow\" href=\"http://ciphergoth.livejournal.com/\">Paul Crowley</a></li>\n<li>XiXiDu --- <a rel=\"nofollow\" href=\"http://kruel.co/\">Alexander Kruel</a></li>\n<li>Aurini --- <a href=\"http://www.staresattheworld.com/\">Stares At The World</a></li>\n<li>jkaufman --- <a href=\"http://www.jefftk.com/news/\">Jeff Kaufman</a></li>\n<li>Bill_McGrath --- <a href=\"http://billmcgrathmusic.wordpress.com/\">billmcgrathmusic</a></li>\n<li>Sister Y --- <a href=\"http://theviewfromhell.blogspot.com/\">the view from hell</a></li>\n<li>PaulWright --- <a class=\"external text\" rel=\"nofollow\" href=\"http://blog.noctua.org.uk/\">Paul Wright's blog</a></li>\n<li>_ozymandias --- <a rel=\"nofollow\" href=\"http://ozyfrantz.com/\">http://ozyfrantz.com/</a> </li>\n<li>mstevens --- <a href=\"http://www.mstevens.org/\">stdout</a></li>\n<li>HughRistik --- <a href=\"http://www.feministcritics.org/blog/\">Feminist Critics</a></li>\n<li>Julia_Galef --- <a href=\"http://measureofdoubt.com/author/measureofdoubt/\">Measure of Doubt</a></li>\n<li>NancyLebovitz --- <a href=\"http://nancylebov.livejournal.com/\">Input Junkie</a></li>\n<li>David Gerard --- <a href=\"http://davidgerard.co.uk/\">a bunch of them</a></li>\n<li>Jayson_Virissimo --- <a href=\"http://jayquantified.blogspot.com/\">Jay, Quantified</a></li>\n<li>kpreid --- <a href=\"http://kpreid.livejournal.com/\">Kevin Reid's blog</a></li>\n<li>hegemonicon --- <a href=\"http://coarsegrained.tumblr.com/\">Coarse Grained&nbsp;</a></li>\n<li>Villiam_Bur --- <a href=\"http://bur.sk/\">bur.sk</a></li>\n<li>Emile --- <a href=\"http://rationalparent.blogspot.com/\">The Rational Parent</a></li>\n<li>lukeprog --- <a href=\"http://commonsenseatheism.com/\">Common Sense Atheism</a> </li>\n<li>Grognor --- <a href=\"http://grognor.blogspot.com/\">Grognor's Blog</a></li>\n<li>CarlShulman --- <a rel=\"nofollow\" href=\"http://reflectivedisequilibrium.blogspot.com/\">Reflective Disequilibrium</a></li>\n<li>OrphanWilde --- <a href=\"http://aretae.blogspot.com/\">Aretae</a></li>\n<li>Alexei --- <a href=\"http://bentspoongames.com/blog/\">Bent Spoon Games Blog</a></li>\n<li>TimS --- <a href=\"http://educationlawgeorgia.com/blog\">Georgia Special Education Law Blog&nbsp;</a></li>\n<li>loup-valliant --- <a href=\"http://loup-vaillant.fr/articles/\">@ Loup's</a></li>\n<li>RolfAndreaseen --- <a href=\"http://ynglingasaga.wordpress.com/\">Yngling Saga</a></li>\n<li>arundelo --- <a href=\"http://arundelo.livejournal.com/\">Aaron Brown</a></li>\n<li>peter_hurtford --- <a href=\"http://www.greatplay.net/\">Greatplay.net</a></li>\n<li>brilee --- <a href=\"http://moderndescartes.com/blog/\">Modern Descartes</a></li>\n<li>gwern --- <a href=\"http://www.gwern.net/\">gwern.net</a></li>\n<li>erratio --- <a rel=\"nofollow\" href=\"http://erratio.livejournal.com/\">The merry-go-round of life</a></li>\n<li>jimmy --- <a href=\"http://cognitiveengineer.blogspot.ca/\">The Art and Science of Cognitive Engineering</a></li>\n<li>alexvermeer --- <a href=\"http://alexvermeer.com/\">alexvermeer.com</a></li>\n<li><span>sark --- <a href=\"http://sarkology.wordpress.com/\">sarkology</a></span></li>\n<li>gjm --- <a rel=\"nofollow\" href=\"http://www.mccaughan.org.uk/g/log/\">Scribble, scribble, scribble</a></li>\n<li>Giles --- <a rel=\"nofollow\" href=\"http://princemm.wordpress.com/\">Prince Mm Mm</a></li>\n<li>Chris Hallquist --- <a rel=\"nofollow\" href=\"http://freethoughtblogs.com/hallq/\">The Uncredible Hallq</a></li>\n<li>EricHerboso --- <a rel=\"nofollow\" href=\"http://www.ericherboso.org/\">EricHerboso.org</a></li>\n<li>Eneasz - <a rel=\"nofollow\" href=\"http://www.deathisbadblog.com/\">Death Is Bad</a></li>\n<li>Tuxedage - <a href=\"https://tuxedage.wordpress.com/\">Essays and other Musings</a></li>\n<li>Federico - <a href=\"http://studiolo.cortediurbino.org/\">studiolo</a></li>\n<li>Trevor Blake - <a rel=\"nofollow\" href=\"http://ovo127.com\">OVO</a>, editor-<a rel=\"nofollow\" href=\"http://doramarsden.tumblr.com\">Dora Marsden</a>, lead judge-<a rel=\"nofollow\" href=\"http://gwiep.net/\">George Walford International Essay Prize</a></li>\n<li>Pablo_Stafforini -- <a rel=\"nofollow\" href=\"http://www.stafforini.com/blog/\">Pablo's miscellany</a></li>\n</ul>\n<p><a href=\"/r/discussion/lw/d92/less_wrong_on_twitter/\"><strong>List of LWers on Twitter</strong></a></p>\n<p><strong><br></strong></p>\n<p><strong>Note:</strong> Anyone just digging for interesting blogs they would like to read but dosen't care if they are written by LWers or not should check out <a href=\"/lw/7kk/seeking_rationalityrelated_blogs/\">this thread</a> or maybe <a href=\"/lw/6ls/other_useful_sites_lwers_read/\">this one</a>. Did you guys know we have a wiki article with <a href=\"http://wiki.lesswrong.com/wiki/External_resources\">external resources</a>? We do. Check that out as well. Maybe once we figure out which LWer blogs related to rationality on this list are particularly good we can add a few of them there too.</p>\n<p><strong></strong></p>", "sections": [{"title": "Anyone who wants to suggest a new blog for the list please follow this link. ", "anchor": "Anyone_who_wants_to_suggest_a_new_blog_for_the_list_please_follow_this_link__", "level": 1}, {"title": "Blogs by LWers:", "anchor": "Blogs_by_LWers_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "71 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PeTL97v92LxRJBsrM", "BBsCv9dSfnxCArgi7", "hBW2JxcvNzJwXdMBA", "a9EwEkSkz3wHqcitY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T09:50:05.739Z", "modifiedAt": null, "url": null, "title": "Facebook worries me.", "slug": "facebook-worries-me", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:25.315Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Laoch", "createdAt": "2010-08-03T08:22:01.285Z", "isAdmin": false, "displayName": "Laoch"}, "userId": "jRKuWSW4uPnBJG4xS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aEuSy357c7cvP6Hvh/facebook-worries-me", "pageUrlRelative": "/posts/aEuSy357c7cvP6Hvh/facebook-worries-me", "linkUrl": "https://www.lesswrong.com/posts/aEuSy357c7cvP6Hvh/facebook-worries-me", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Facebook%20worries%20me.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFacebook%20worries%20me.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaEuSy357c7cvP6Hvh%2Ffacebook-worries-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Facebook%20worries%20me.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaEuSy357c7cvP6Hvh%2Ffacebook-worries-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaEuSy357c7cvP6Hvh%2Ffacebook-worries-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<p><a href=\"http://www.worldcrunch.com/why-facebook-must-die-internet-freedom-flourish/3825\">This</a> article let's me know I'm not the only one concerned at the dominance of companies like Google and Facebook. I don't have enough data or information to judge whether my continued use of these services are going to be detrimental to me or the rest of humanity. What I do know is that increasingly I'm feeling like Facebook and Google know too much about me and have too much of my attention. I feel a little bit like a puppet, a little bit strung along. How do others here feel?</p>\n<p>It reminds me or is&nbsp;analogous to <a href=\"http://www.youtube.com/watch?v=iE-H__aIEFE&amp;list=UUKQ8VQctI2xX8U5h8xIJMeA\">this</a>. In my opinion the food industry having a monopoly on what we eat, just as Google and Facebook will have a monopoly web content. Is this a risk on society?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aEuSy357c7cvP6Hvh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -13, "extendedScore": null, "score": 9.276815887296102e-07, "legacy": true, "legacyId": "17166", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T11:58:33.616Z", "modifiedAt": null, "url": null, "title": "IJMC Mind Uploading Special Issue published", "slug": "ijmc-mind-uploading-special-issue-published", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.271Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K6cjx2D5LEhASx4hm/ijmc-mind-uploading-special-issue-published", "pageUrlRelative": "/posts/K6cjx2D5LEhASx4hm/ijmc-mind-uploading-special-issue-published", "linkUrl": "https://www.lesswrong.com/posts/K6cjx2D5LEhASx4hm/ijmc-mind-uploading-special-issue-published", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20IJMC%20Mind%20Uploading%20Special%20Issue%20published&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIJMC%20Mind%20Uploading%20Special%20Issue%20published%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6cjx2D5LEhASx4hm%2Fijmc-mind-uploading-special-issue-published%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=IJMC%20Mind%20Uploading%20Special%20Issue%20published%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6cjx2D5LEhASx4hm%2Fijmc-mind-uploading-special-issue-published", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6cjx2D5LEhASx4hm%2Fijmc-mind-uploading-special-issue-published", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 440, "htmlBody": "<p>The International Journal of Machine Consciousness recently published its <a href=\"http://www.worldscinet.com/ijmc/04/0401/S17938430120401.html\">special issue on mind uploading</a>. The papers are paywalled, but as the editor of the issue, Ben Goertzel has put together a page that <a href=\"http://wp.goertzel.org/?page_id=368\">links to the authors' preprints of the papers</a>. Preprint versions are available for most of the papers.</p>\n<p>Below is a copy of the preprint page as it was at the time that this post was made. Note though that I'll be away for a couple of days, and thus be unable to update this page if new links get added.</p>\n<blockquote>\n<p>In June 2012 the <a href=\"http://www.worldscinet.com/ijmc/\">International Journal of Machine Consciousness</a> (edited by Antonio Chella) published a <a href=\"http://www.worldscinet.com/ijmc/04/0401/S17938430120401.html\">Special Issue on Mind Uploading</a>, edited by Ben Goertzel and Matthew Ikle&rsquo;.</p>\n<p>This page gathers links to informal, &ldquo;preprint&rdquo; versions of the  papers in that Special Issue, hosted on the paper authors&rsquo; websites.&nbsp;&nbsp;  These preprint versions are not guaranteed to be identical to the final  published versions, but the content should be essentially the same.&nbsp;&nbsp;  The list below contains the whole table of contents of the Special  Issue; at the moment links to preprints are still being added to the  list items as authors post them on their sites.</p>\n</blockquote>\n<blockquote>\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"3\" width=\"100%\" align=\"center\">\n<tbody>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://www.worldscinet.com/ijmc/04/0401/free-access/S1793843012020015.pdf\"><strong>INTRODUCTION</strong></a></div>\n<div><em>BEN GOERTZEL</em><em> and MATTHEW IKLE&rsquo;</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://www.worldscinet.com/ijmc/04/0401/free-access/S179384301240001X.pdf\"><strong>FUNDAMENTALS OF WHOLE BRAIN EMULATION: STATE, TRANSITION AND UPDATE REPRESENTATIONS</strong></a></div>\n<div><em>RANDAL A. KOENE</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://www.sim.me.uk/neural/JournalArticles/Bamford2012IJMC.pdf\"><strong>A FRAMEWORK FOR APPROACHES TO TRANSFER OF A MIND&rsquo;S SUBSTRATE</strong></a></div>\n<div><em>SIM BAMFORD</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://carboncopies.org/research-projects/articles-and-essays/koene.IJMC.experimental-research-in-WBE-the-need-for-innovative-in-vivo-measurement-techniques.proofs-20120410.pdf\"><strong>EXPERIMENTAL RESEARCH IN WHOLE BRAIN EMULATION: THE NEED FOR INNOVATIVE <em>IN VIVO</em> MEASUREMENT TECHNIQUES</strong></a></div>\n<div><em>RANDAL A. KOENE</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><strong>AVAILABLE TOOLS FOR WHOLE BRAIN EMULATION</strong></div>\n<div><em>DIANA DECA</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://www.brainpreservation.org/sites/default/files/ElectronImagingTechnologyForWholeBrainNeuralCircuitMapping_Hayworth2012.pdf\"><strong>ELECTRON IMAGING TECHNOLOGY FOR WHOLE BRAIN NEURAL CIRCUIT MAPPING</strong></a></div>\n<div><em>KENNETH J. HAYWORTH</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><strong>NON-DESTRUCTIVE WHOLE-BRAIN MONITORING USING NANOROBOTS: NEURAL ELECTRICAL DATA RATE REQUIREMENTS</strong></div>\n<div><em>NUNO R. B. MARTINS</em><em>, WOLFRAM ERLHAGEN</em><em> and ROBERT A. FREITAS, JR.</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://www.terasemcentral.org/docs/Terasem%20Mind%20Uploading%20Experiment%20IJMC.pdf\"><strong>THE TERASEM MIND UPLOADING EXPERIMENT</strong></a></div>\n<div><em>MARTINE ROTHBLATT</em></div>\n<div>\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"3\" width=\"100%\" align=\"center\">\n<tbody>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><strong>WHOLE-PERSONALITY EMULATION</strong></div>\n<div><em>WILLIAM SIMS BAINBRIDGE</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://goertzel.org/Goertzel_IJMC_Special_Issue.pdf\"><strong>WHEN SHOULD TWO MINDS BE CONSIDERED VERSIONS OF ONE ANOTHER?</strong></a></div>\n<div><em>BEN GOERTZEL</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://exeter.academia.edu/MichaelHauskeller/Papers/1747050/My_Brain_my_Mind_and_I_Some_Philosophical_Problems_of_Mind-Uploading\"><strong>MY BRAIN, MY MIND, AND I: SOME PHILOSOPHICAL ASSUMPTIONS OF MIND-UPLOADING</strong></a></div>\n<div><em>MICHAEL HAUSKELLER</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://degreesofclarity.com/writing/oto_mind_uploading.pdf\"><strong>SEEKING NORMATIVE GUIDELINES FOR NOVEL FUTURE FORMS OF CONSCIOUSNESS</strong></a></div>\n<div><em>BRANDON OTO</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><strong>TRANS-HUMAN COGNITIVE ENHANCEMENT, PHENOMENAL CONSCIOUSNESS AND THE EXTENDED MIND</strong></div>\n<div><em>TADEUSZ WIESLAW ZAWIDZKI</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://home.millsaps.edu/hopkipd/IJMC-Preprint-HopkinsUploading.pdf\"><strong>WHY UPLOADING WILL NOT WORK, OR, THE GHOSTS HAUNTING TRANSHUMANISM</strong></a></div>\n<div><em>PATRICK D. HOPKINS</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><strong>DIGITAL IMMORTALITY: SELF OR 0010110?</strong></div>\n<div><em>LIZ STILLWAGGON SWAN</em><em> and JOSHUA HOWARD</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://faculty.cs.tamu.edu/choe/ftp/publications/choe-ijmc12-preprint.pdf\"><strong>TIME, CONSCIOUSNESS, AND MIND UPLOADING</strong></a></div>\n<div><em>YOONSUCK CHOE</em><em>, JAEROCK KWON</em><em> and JI RYANG CHUNG</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://www.xuenay.net/Papers/DigitalAdvantages.pdf\"><strong>ADVANTAGES OF ARTIFICIAL INTELLIGENCES, UPLOADS, AND DIGITAL MINDS</strong></a></div>\n<div><em>KAJ SOTALA</em></div>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" width=\"100%\">\n<div><a href=\"http://www.xuenay.net/Papers/CoalescingMinds.pdf\"><strong>COALESCING MINDS: BRAIN UPLOADING-RELATED GROUP MIND SCENARIOS</strong></a></div>\n<div><em>KAJ SOTALA</em><em> and HARRI VALPOLA</em></div>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n</td>\n</tr>\n</tbody>\n</table>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "ksdiAMKfgSyEeKMo6": 2, "Wi3EopKJ2aNdtxSWg": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K6cjx2D5LEhASx4hm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 19, "extendedScore": null, "score": 9.277406817939579e-07, "legacy": true, "legacyId": "17172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T14:42:55.010Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Day*", "slug": "meetup-fort-collins-colorado-meetup-thursday-7pm-new-day", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yLmgJ9kjqZL9BrRCy/meetup-fort-collins-colorado-meetup-thursday-7pm-new-day", "pageUrlRelative": "/posts/yLmgJ9kjqZL9BrRCy/meetup-fort-collins-colorado-meetup-thursday-7pm-new-day", "linkUrl": "https://www.lesswrong.com/posts/yLmgJ9kjqZL9BrRCy/meetup-fort-collins-colorado-meetup-thursday-7pm-new-day", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Day*&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Day*%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyLmgJ9kjqZL9BrRCy%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Day*%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyLmgJ9kjqZL9BrRCy%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyLmgJ9kjqZL9BrRCy%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bc'>Fort Collins, Colorado Meetup Thursday 7pm *New Day*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 June 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're trying a new day, Thursday.</p>\n\n<p>Sign up at the Google Group for announcements:\nhttps://groups.google.com/forum/?fromgroups#!forum/less-wrong-fort-collins-co</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bc'>Fort Collins, Colorado Meetup Thursday 7pm *New Day*</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yLmgJ9kjqZL9BrRCy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.278162947041948e-07, "legacy": true, "legacyId": "17173", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Day_\">Discussion article for the meetup : <a href=\"/meetups/bc\">Fort Collins, Colorado Meetup Thursday 7pm *New Day*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 June 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're trying a new day, Thursday.</p>\n\n<p>Sign up at the Google Group for announcements:\nhttps://groups.google.com/forum/?fromgroups#!forum/less-wrong-fort-collins-co</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Day_1\">Discussion article for the meetup : <a href=\"/meetups/bc\">Fort Collins, Colorado Meetup Thursday 7pm *New Day*</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Day*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Day_", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Day*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Day_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T15:51:08.521Z", "modifiedAt": null, "url": null, "title": "Less Wrong on Twitter", "slug": "less-wrong-on-twitter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:04.278Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Grognor", "createdAt": "2011-01-31T02:54:34.463Z", "isAdmin": false, "displayName": "Grognor"}, "userId": "LoykQRMTxJFxwwdPy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BBsCv9dSfnxCArgi7/less-wrong-on-twitter", "pageUrlRelative": "/posts/BBsCv9dSfnxCArgi7/less-wrong-on-twitter", "linkUrl": "https://www.lesswrong.com/posts/BBsCv9dSfnxCArgi7/less-wrong-on-twitter", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20on%20Twitter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20on%20Twitter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBBsCv9dSfnxCArgi7%2Fless-wrong-on-twitter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20on%20Twitter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBBsCv9dSfnxCArgi7%2Fless-wrong-on-twitter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBBsCv9dSfnxCArgi7%2Fless-wrong-on-twitter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 502, "htmlBody": "<p>List of members of Less Wrong who are on Twitter:<a id=\"more\"></a></p>\n<ul>\n<li><a href=\"https://twitter.com/ESYudkowsky\">Eliezer Yudkowsky</a></li>\n<li><a href=\"http://twitter.com/#!/stevenkaas\">Steven Kaas</a></li>\n<li><a href=\"https://twitter.com/vladimirnesov\">Vladimir Nesov</a></li>\n<li><a href=\"https://twitter.com/So8res\">So8res</a></li>\n<li><a href=\"https://twitter.com/alexvermeer\">Alex Vermeer</a></li>\n<li><a href=\"https://twitter.com/robbensinger\">Robby Bensinger</a></li>\n<li><a href=\"https://twitter.com/romanyam\">Roman Yampolskiy</a></li>\n<li><a href=\"https://twitter.com/aarondtucker\">atucker</a></li>\n<li><a href=\"https://twitter.com/#!/spaceandgames\">Peter de Blanc</a></li>\n<li><a href=\"https://twitter.com/#!/nicktarleton\">Nick Tarleton</a></li>\n<li><a href=\"https://twitter.com/#!/michaelvassar\">Michael Vassar</a> (<a href=\"https://twitter.com/#%21/HiFromMichaelV\">alt</a>)<a href=\"https://twitter.com/#!/michaelvassar\"><br /></a></li>\n<li><a href=\"https://twitter.com/#!/FrankAdamek\">Frank Adamek</a></li>\n<li><a href=\"https://twitter.com/#!/xuenay\">Kaj Sotala</a></li>\n<li><a href=\"https://twitter.com/#!/TheZvi\">Zvi Moshowitz</a></li>\n<li><a href=\"https://twitter.com/#!/KatjaGrace\">Katja Grace</a></li>\n<li><a href=\"https://twitter.com/#!/michaelcurzi\">Michael Curzi</a></li>\n<li><a href=\"https://twitter.com/#!/makefoil\">HonoreDB</a></li>\n<li><a href=\"https://twitter.com/#!/kevin143\">kevin</a></li>\n<li><a href=\"https://twitter.com/#!/mikeanissimov\">Michael Anissimov</a></li>\n<li><a href=\"https://twitter.com/#!/ciphergoth\">ciphergoth</a></li>\n<li><a href=\"https://twitter.com/#!/smiley_friedman\">Shannon Friedman</a></li>\n<li><a href=\"https://twitter.com/#!/ggreer\">ggreer</a></li>\n<li><a href=\"https://twitter.com/#!/fiddlemath\">fiddlemath</a></li>\n<li><a href=\"https://twitter.com/#!/quanticle\">quanticle</a></li>\n<li><a href=\"https://twitter.com/#!/patrissimo\">patrissimo</a></li>\n<li><a href=\"https://twitter.com/#!/WilliamAEden\">William Eden</a></li>\n<li><a href=\"https://twitter.com/#%21/diviacaroline\">Divia Eden</a></li>\n<li><a href=\"https://twitter.com/#!/pjeby\">P.J. Eby</a></li>\n<li><a href=\"https://twitter.com/#!/juliagalef\">Julia Galef</a></li>\n<li><a href=\"https://twitter.com/#!/Sniffnoy\">Sniffnoy</a></li>\n<li><a href=\"https://twitter.com/#!/pwgen\">John Maxwell IV</a></li>\n<li><a href=\"https://twitter.com/#!/lukeprog\">lukeprog</a></li>\n<li><a href=\"https://twitter.com/#!/paprmchn\">paper-machine</a></li>\n<li><a href=\"https://twitter.com/#!/lsparrish\">lsparrish</a></li>\n<li><a href=\"https://twitter.com/#!/Morendil\">Morendil</a></li>\n<li><a href=\"https://twitter.com/#!/willnewsome\">Will Newsome</a> (<a href=\"https://twitter.com/willdoingthings\">alt</a>)</li>\n<li><a href=\"https://twitter.com/#!/GabrielDuquette\">Gabriel Duquette</a></li>\n<li><a href=\"https://twitter.com/#!/michaelblume\">MBlume</a></li>\n<li><a href=\"https://twitter.com/#!/thomblake\">thomblake</a></li>\n<li><a href=\"https://twitter.com/#!/SilasBarta\">Silas Barta</a></li>\n<li><a href=\"https://twitter.com/#!/mindspillage\">mindspillage</a></li>\n<li><a href=\"https://twitter.com/#!/Johnicholas\">Johnicholas</a></li>\n<li><a href=\"https://twitter.com/RobertLumley\">RobertLumley</a></li>\n<li><a href=\"https://twitter.com/#!/teageegeepea\">TGGP</a></li>\n<li><a href=\"https://twitter.com/#!/paulorrj\">Paulovsk</a></li>\n<li><a href=\"https://twitter.com/#!/arundelo\">arundelo</a></li>\n<li><a href=\"https://twitter.com/#!/GuySrinivasan/\">GuySrinivasan</a></li>\n<li><a href=\"https://twitter.com/#!/ariarule\">Nic_Smith</a></li>\n<li><a href=\"https://twitter.com/UncredibleHallq\">Chris Hallquist</a></li>\n<li><a href=\"https://twitter.com/KarmaKaiser\">Karmakaiser</a></li>\n<li><a href=\"https://twitter.com/asilentsky\">Konkvistador</a></li>\n<li><a href=\"https://twitter.com/luminousalicorn\">Alicorn</a></li>\n<li><a href=\"https://twitter.com/#!/gwern\">gwern</a></li>\n<li><a href=\"http://twitter.com/aaronsw\">aaronsw</a> (<a href=\"http://tech.mit.edu/V132/N61/swartz.html\">deceased</a>)<a href=\"http://twitter.com/aaronsw\"><br /></a></li>\n<li><a href=\"https://twitter.com/ericherboso\">Eric Herboso</a></li>\n<li><a rel=\"nofollow\" href=\"https://twitter.com/Respa92\">Michelle_Z</a></li>\n<li><a href=\"https://twitter.com/#!/paulfchristiano\">Paul F. Christiano</a></li>\n<li><a href=\"https://twitter.com/#!/CarlShulman\">Carl Shulman</a></li>\n<li><a href=\"https://twitter.com/#!/mherreshoff\">Marcello</a></li>\n<li><a href=\"https://twitter.com/speedprior\">khafra</a></li>\n<li><a href=\"https://twitter.com/aarondtucker\">atucker</a></li>\n<li><a href=\"https://twitter.com/capsmac\">Caspian</a></li>\n<li><a href=\"https://twitter.com/peasantdramatic\">NoisyEmpire</a></li>\n<li><a href=\"http://twitter.com/zandapheri\">ata</a></li>\n<li><a href=\"http://twitter.com/Liron\">Liron</a></li>\n<li><a href=\"https://twitter.com/peterhurford\">peter_hurford</a></li>\n<li><a href=\"https://twitter.com/drethelin\">drethelin</a></li>\n<li><a href=\"https://twitter.com/esynclairs/\">Rubix</a></li>\n<li><a href=\"https://twitter.com/unchoke\">apophenia</a> (<a href=\"https://twitter.com/unchoke_blog\">alt</a>)</li>\n<li><a href=\"https://twitter.com/Corey_Yanofsky\">Cyan</a></li>\n<li><a href=\"http://twitter.com/simplic10\">simplicio</a></li>\n<li><a href=\"http://twitter.com/slatestarcodex\">Yvain</a></li>\n<li><a href=\"http://twitter.com/Ian_HT\">Hariant</a></li>\n<li><a href=\"https://twitter.com/davidmanheim\">Davidmanheim</a></li>\n<li><a href=\"http://twitter.com/JimDMiller\">James_Miller</a></li>\n<li><a href=\"http://twitter.com/nickernst\">nickernst</a></li>\n<li><a href=\"https://twitter.com/LouieHelm/\">Louie</a></li>\n<li><a href=\"https://twitter.com/ValMichaelSmith/\">Valentine</a></li>\n<li><a href=\"https://twitter.com/amywilley\">amywilley</a></li>\n<li><a href=\"https://twitter.com/Mycroft65536\">Andrew Rettek</a></li>\n<li><a href=\"https://twitter.com/stafforini\">Pablo_Stafforini</a></li>\n<li><a href=\"https://twitter.com/Meaningness\">David_Chapman</a></li>\n<li><a href=\"https://twitter.com/m_bourgon\">malo</a></li>\n<li><a href=\"https://twitter.com/juliadwise\">juliawise</a></li>\n<li><a href=\"https://twitter.com/abramdemski\">abramdemski</a></li>\n<li><a href=\"https://twitter.com/hruvulum\">rhollerith_dot_com</a></li>\n<li><a href=\"https://twitter.com/JaysonVirissimo\">Jayson_Virissimo</a></li>\n<li><a href=\"http://twitter.com/nyansandwich\">nyan_sandwich</a></li>\n<li><a href=\"https://twitter.com/Liorgotesman\">pwno</a></li>\n<li><a href=\"https://twitter.com/AlexMennen\">Alex Mennen</a></li>\n<li><a href=\"https://twitter.com/strohl89\">Brienne Strohl</a></li>\n</ul>\n<p>Twitter accounts by people who aren't really \"LWers\" but recommended for LWers anyway:</p>\n<ul>\n<li>\"<a href=\"https://twitter.com/Grognor/lists/weird-sun-twitter/members\">Weird Sun Twitter</a>\"</li>\n<li><a href=\"https://twitter.com/robinhanson/\">Robin Hanson</a></li>\n<li><a href=\"https://twitter.com/miriberkeley\">MIRI</a></li>\n<li><a href=\"https://twitter.com/paulg\">Paul Graham</a></li>\n<li><a href=\"https://twitter.com/aprayerofquiet\">kenzo</a></li>\n<li><a href=\"https://twitter.com/apoddubn\">Alexander Poddubny</a></li>\n<li><a href=\"https://twitter.com/asquidislove\">j</a>.</li>\n<li><a href=\"https://twitter.com/KimKierkegaard\">Kim Kierkegaardashian</a></li>\n<li><a href=\"https://twitter.com/GateOfHeavens\">Gate of Heavens</a> and <a href=\"https://twitter.com/GapOfGods\">Gap of Gods</a> (erstwhile Catharine Evans)</li>\n<li><a href=\"https://twitter.com/St_Rev\">St_Rev</a></li>\n<li><a href=\"https://twitter.com/#%21/ahaspel\">Aaron Haspel</a></li>\n<li><a href=\"http://twitter.com/InstanceOfClass\">Instance of Class</a></li>\n<li><a href=\"https://twitter.com/MemberOfSpecies\">Member of Species</a></li>\n<li><a href=\"https://twitter.com/nydwracu\">nydwracu</a></li>\n<li><a href=\"https://twitter.com/freeshreeda\">Shreeda</a></li>\n<li><a href=\"https://twitter.com/razibkhan\">Razib Khan</a></li>\n<li><a href=\"https://twitter.com/jennyholzer\">Jenny Holzer</a></li>\n<li><a href=\"http://twitter.com/jasonroy\">Jason Roy</a></li>\n<li><a href=\"http://twitter.com/norvig\">Peter Norvig</a></li>\n<li><a href=\"https://twitter.com/afoolswisdom\">Sark Julian</a></li>\n<li><a href=\"https://twitter.com/muflax\">muflax</a> (pbuh)</li>\n<li><a href=\"http://twitter.com/robertwiblin\">Robert Wiblin</a></li>\n<li><a href=\"https://twitter.com/TheViewFromHell\">Sister Y</a>, <a href=\"https://twitter.com/sarahdoingthing\">sarahdoingthings</a></li>\n<li><a href=\"https://twitter.com/anderssandberg\">Anders Sandberg</a></li>\n<li> <a href=\"https://twitter.com/matingmind\">Geoffrey Miller</a></li>\n<li><a href=\"https://twitter.com/admittedlyhuman\">admittedlyhuman</a>, <a href=\"https://twitter.com/fakeysaysthings\">fakeysaysthings</a></li>\n<li><a href=\"https://twitter.com/possibleviews\">Possible Views</a></li>\n<li><a href=\"https://twitter.com/whaaales\">whales</a></li>\n<li><a href=\"https://twitter.com/SilDogood\">Silas Dogood</a></li>\n<li><a href=\"https://twitter.com/tegmark\">Max Tegmark</a></li>\n<li><a href=\"https://twitter.com/byrneseyeview\">Byrne Hobart</a></li>\n<li><a href=\"https://twitter.com/steveom\">Steve Omohundro</a></li>\n<li><a href=\"https://twitter.com/worrydream\">Bret Victor</a></li>\n<li><span style=\"color: #008000;\"><a href=\"https://twitter.com/Grognor\">Grognor</a></span></li>\n</ul>\n<p>If I missed anyone in either category, let me know. I'll keep this updated.</p>\n<p>Last update: May 18, 2015.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GQyPQcdEQF4zXhJBq": 1, "KADf7NFiAr9DAaQxN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BBsCv9dSfnxCArgi7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 29, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "17174", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T16:15:04.618Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Brussels, Cambridge MA, Cambridge UK, Chicago, Helsinki, Phoenix, Sao Paulo, Tucson, Washington DC", "slug": "weekly-lw-meetups-brussels-cambridge-ma-cambridge-uk-chicago", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:23.454Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QsDcjNLFfAXrr2TER/weekly-lw-meetups-brussels-cambridge-ma-cambridge-uk-chicago", "pageUrlRelative": "/posts/QsDcjNLFfAXrr2TER/weekly-lw-meetups-brussels-cambridge-ma-cambridge-uk-chicago", "linkUrl": "https://www.lesswrong.com/posts/QsDcjNLFfAXrr2TER/weekly-lw-meetups-brussels-cambridge-ma-cambridge-uk-chicago", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Brussels%2C%20Cambridge%20MA%2C%20Cambridge%20UK%2C%20Chicago%2C%20Helsinki%2C%20Phoenix%2C%20Sao%20Paulo%2C%20Tucson%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Brussels%2C%20Cambridge%20MA%2C%20Cambridge%20UK%2C%20Chicago%2C%20Helsinki%2C%20Phoenix%2C%20Sao%20Paulo%2C%20Tucson%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQsDcjNLFfAXrr2TER%2Fweekly-lw-meetups-brussels-cambridge-ma-cambridge-uk-chicago%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Brussels%2C%20Cambridge%20MA%2C%20Cambridge%20UK%2C%20Chicago%2C%20Helsinki%2C%20Phoenix%2C%20Sao%20Paulo%2C%20Tucson%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQsDcjNLFfAXrr2TER%2Fweekly-lw-meetups-brussels-cambridge-ma-cambridge-uk-chicago", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQsDcjNLFfAXrr2TER%2Fweekly-lw-meetups-brussels-cambridge-ma-cambridge-uk-chicago", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 455, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/b5\">S&atilde;o Paulo Meet Up 3:&nbsp;<span class=\"date\">15 June 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/9p\">Phoenix, Arizona:&nbsp;<span class=\"date\">15 June 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/av\">Brussels meetup:&nbsp;<span class=\"date\">16 June 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/b0\">Washington DC Meetup:&nbsp;<span class=\"date\">17 June 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/at\">Tucson, Arizona:&nbsp;<span class=\"date\">20 June 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/b6\">Helsinki Meetup:&nbsp;<span class=\"date\">21 June 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/b2\">Berlin Meetup:&nbsp;<span class=\"date\">26 June 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/aq\">First Cali, Colombia meetup:&nbsp;<span class=\"date\">02 July 2012 07:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/az\">(Cambridge UK) Punt Trip:&nbsp;<span class=\"date\">16 June 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/ad\">Less Wrong Cambridge (MA) third-Sundays meetup:&nbsp;<span class=\"date\">17 June 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/b4\">Chicago games at Harold Washington Library (Sun 6/17):&nbsp;<span class=\"date\">17 June 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/ap\">Summer Festival Megameetup at NYC:&nbsp;<span class=\"date\">23 June 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/b1\">Less Wrong Sydney - June:&nbsp;<span class=\"date\">28 June 2012 06:30PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QsDcjNLFfAXrr2TER", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.278586980872587e-07, "legacy": true, "legacyId": "16995", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T17:55:51.917Z", "modifiedAt": null, "url": null, "title": "Regression To The Mean [Draft][Request for Feedback]", "slug": "regression-to-the-mean-draft-request-for-feedback", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.524Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "faul_sname", "createdAt": "2011-07-10T06:35:12.911Z", "isAdmin": false, "displayName": "faul_sname"}, "userId": "uAdCfrYxKKCJT5nK7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GDgfaBoxM9sJKKtJi/regression-to-the-mean-draft-request-for-feedback", "pageUrlRelative": "/posts/GDgfaBoxM9sJKKtJi/regression-to-the-mean-draft-request-for-feedback", "linkUrl": "https://www.lesswrong.com/posts/GDgfaBoxM9sJKKtJi/regression-to-the-mean-draft-request-for-feedback", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Regression%20To%20The%20Mean%20%5BDraft%5D%5BRequest%20for%20Feedback%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARegression%20To%20The%20Mean%20%5BDraft%5D%5BRequest%20for%20Feedback%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDgfaBoxM9sJKKtJi%2Fregression-to-the-mean-draft-request-for-feedback%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Regression%20To%20The%20Mean%20%5BDraft%5D%5BRequest%20for%20Feedback%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDgfaBoxM9sJKKtJi%2Fregression-to-the-mean-draft-request-for-feedback", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDgfaBoxM9sJKKtJi%2Fregression-to-the-mean-draft-request-for-feedback", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 474, "htmlBody": "<p>\"Rewarding good performance leads to faster improvement than punishing bad performance\"</p>\n<p>\"In general, unusually bad performance improves after punishment, but good performance tends not to improve and sometimes even gets worse after praise is administered.\"</p>\n<p>&nbsp;</p>\n<p>These statements seem contradictory, yet both describe real effects. The apparent contradiction is caused by a phenomenon known as \"regression to the mean,\" which states that the measurement after an exceptional measurement will be closer to average. The improvement after a reprimand is caused not by any effect that reprimand had, nor was the worsening after praise due to the praise. Both observations were due to regression to the mean.</p>\n<p>&nbsp;</p>\n<p>Regression to the mean is caused by two things.</p>\n<p>1. Exceptionally good performance is far above average, and exceptionally bad performance is far below average.</p>\n<p>2. Most performance is about average.</p>\n<p>&nbsp;</p>\n<p>Let's put this in concrete terms. Let's say we are trying to teach our friend Bob to play darts. He's not very good yet, and while he almost always hits the board, he can't really get a higher level of accuracy than that.</p>\n<p>&nbsp;</p>\n<p>On his 12th throw, Bob misses the dartboard entirely. This is&nbsp;extraordinarily bad, even for him. On his next throw, he gets an 8, which is fairly typical, and much better.&nbsp;</p>\n<p>&nbsp;</p>\n<p>On his 57th throw, your friend manages to get a bullseye. You slap him on the back, congratulating him on his improvement. It seems your friend really is getting the hang of this after all. Proud of his accomplishment, Bob lines up his next attempt. He cocks his arm, throws, and...</p>\n<p>Gets a 12.&nbsp;</p>\n<p>&nbsp;</p>\n<p>This is regression to the mean. Since there is a large random factor in darts, especially for unskilled players, a good throw will probably not be followed up with another good throw (since good throws are rare, and one shot is independent of another). This effect shows up whenever there is a large random component in performance.</p>\n<p>&nbsp;</p>\n<p>It's important to be aware of when an exceptional observation was likely due to random variation in measurement, not an exceptional characteristic of the thing being measured. For example, stock performance is mostly random. If one of your stocks does extremely well this year, you should expect it to perform much closer to average next year. If your kid scores 160 on his/her first IQ test, you should expect lower performance on later tests. If it took you 3 hours to get to work yesterday because of traffic instead of the usual 1, you shouldn't worry too much about it taking 3 hours again today.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>This is a very rough draft of a post on regression to the mean. I wrote it because the sequences didn't cover this particular bias blind spot, and it's an important one. I appreciate any feedback, both in terms of providing examples and making it easy to understand and in terms of cleaning up any errors or awkward phrasings.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GDgfaBoxM9sJKKtJi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 9.279050752967436e-07, "legacy": true, "legacyId": "17176", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T19:35:38.437Z", "modifiedAt": null, "url": null, "title": "Questions for shminux", "slug": "questions-for-shminux", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:25.508Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v9nNciAiXubeWhxAS/questions-for-shminux", "pageUrlRelative": "/posts/v9nNciAiXubeWhxAS/questions-for-shminux", "linkUrl": "https://www.lesswrong.com/posts/v9nNciAiXubeWhxAS/questions-for-shminux", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Questions%20for%20shminux&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestions%20for%20shminux%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9nNciAiXubeWhxAS%2Fquestions-for-shminux%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Questions%20for%20shminux%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9nNciAiXubeWhxAS%2Fquestions-for-shminux", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9nNciAiXubeWhxAS%2Fquestions-for-shminux", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<p>As mister shminux mentioned somewhere, he is happy and qualified to answer questions in the field of the Relativity. Here is mine:</p>\n<p>A long rod (a cylinder) could have a large escape velocity in the direction of its main axe. From its end, to the \"infinity\". Larger than the speed of light. While the perpendicular escape velocity is lesser than the speed of light.</p>\n<p>Is this rod then an asymmetric black hole?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v9nNciAiXubeWhxAS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": -10, "extendedScore": null, "score": 9.279509904269052e-07, "legacy": true, "legacyId": "17177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-22T22:31:55.112Z", "modifiedAt": null, "url": null, "title": "Balancing Costs to Ourselves with Benefits to Others", "slug": "balancing-costs-to-ourselves-with-benefits-to-others", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:37.546Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xjufPJmEsN7d9dmkf/balancing-costs-to-ourselves-with-benefits-to-others", "pageUrlRelative": "/posts/xjufPJmEsN7d9dmkf/balancing-costs-to-ourselves-with-benefits-to-others", "linkUrl": "https://www.lesswrong.com/posts/xjufPJmEsN7d9dmkf/balancing-costs-to-ourselves-with-benefits-to-others", "postedAtFormatted": "Friday, June 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Balancing%20Costs%20to%20Ourselves%20with%20Benefits%20to%20Others&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABalancing%20Costs%20to%20Ourselves%20with%20Benefits%20to%20Others%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjufPJmEsN7d9dmkf%2Fbalancing-costs-to-ourselves-with-benefits-to-others%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Balancing%20Costs%20to%20Ourselves%20with%20Benefits%20to%20Others%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjufPJmEsN7d9dmkf%2Fbalancing-costs-to-ourselves-with-benefits-to-others", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjufPJmEsN7d9dmkf%2Fbalancing-costs-to-ourselves-with-benefits-to-others", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 415, "htmlBody": "<p>Once you accept the idea that we have some obligation to try to help other people you are faced with the question of how to trade off costs for yourself against benefits for others. Questions like <a href=\"http://www.jefftk.com/news/2011-08-09.html\">how much should I give?</a>, <a href=\"http://www.jefftk.com/news/2010-07-22.html\">should I avoid slave-created chocolate?</a>, and <a href=\"/lw/70s/career_choice_for_a_utilitarian_giver/\">should I become a doctor?</a> are facets of this broader question of \"how much should I give up to help others?\"</p>\n<p>The simplest approach to this, which comes pretty naturally to me and to most people, is to compartmentalize. You figure out how much you can afford to donate, whether there are foods you're willing to give up, how often to give blood or other body parts, and how much time you can spend volunteering. Within each category you do the best you can to find the right balance. For example, Alexander Berger <a href=\"/r/discussion/lw/d4v/altruistic_kidney_donation/6vpw\">writes</a>:</p>\n<blockquote>I have a policy on how much money I give to charity, and I decided to donate a kidney, but both decisions depended on the specific circumstances of what would be asked of me.</blockquote>\n<p>The problem with this approach is that sometimes you can do better, both for yourself and for others, by trading off between categories. Say the positive effect of giving $1200 to the <a href=\"http://givewell.org/international/top-charities/AMF\">AMF</a> is <a href=\"/r/discussion/lw/d4v/altruistic_kidney_donation/\">about the same</a> as of donating a kidney. Perhaps if I looked at things in terms of having a policy for monetary donation and another for organ donation I might decide to give $X and also donate a kidney, but I'm really not that excited about donating the kidney. I might be happier if I gave $X+$1200 but kept my kidney, which would be neutral from the perspective of benefit to others.</p>\n<p>I'm think the right framework for thinking about this sort of thing is to decide that there's a certain amount of happiness you're willing to forgo for the sake of others, and then do the most good you can within that bound. [1] This doesn't have to be even a very large amount of happiness; you can do a lot of good by <a href=\"http://www.givingwhatwecan.org/our-pledge/give-more-later.php\">giving future income raises</a> to effective charity.</p>\n<p>(This still doesn't answer the question of <a href=\"http://www.jefftk.com/news/2011-08-09.html\">how much</a> you should be willing to sacrifice for others. I don't have a good answer for that yet.)</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/news/2012-06-22.html\">on my blog</a>.</em></small></p>\n<p>&nbsp;</p>\n<p>[1] Technically, this is the <a href=\"http://en.wikipedia.org/wiki/Knapsack_problem\">knapsack problem</a>, which is <a href=\"http://en.wikipedia.org/wiki/NP-hard\">NP-hard</a>. But in practice the actually difficult bit is getting good estimates for the value of all the competing good choices and their likely effects on your happiness.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xjufPJmEsN7d9dmkf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 9.280321207493542e-07, "legacy": true, "legacyId": "17179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hFR59Wc9NwWWppJ7R", "wzdjAmeoPRmBE8v8o"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-23T05:48:16.443Z", "modifiedAt": null, "url": null, "title": "Draft of Edwin Jaynes' \"Probability Theory: The Logic of Science\" online, with lost chapter 30", "slug": "draft-of-edwin-jaynes-probability-theory-the-logic-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:35.503Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "buybuydandavis", "createdAt": "2011-09-04T00:02:35.971Z", "isAdmin": false, "displayName": "buybuydandavis"}, "userId": "sQqaueY24d7xa3PXD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xon5AFbKcbwoQ5RWR/draft-of-edwin-jaynes-probability-theory-the-logic-of", "pageUrlRelative": "/posts/Xon5AFbKcbwoQ5RWR/draft-of-edwin-jaynes-probability-theory-the-logic-of", "linkUrl": "https://www.lesswrong.com/posts/Xon5AFbKcbwoQ5RWR/draft-of-edwin-jaynes-probability-theory-the-logic-of", "postedAtFormatted": "Saturday, June 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Draft%20of%20Edwin%20Jaynes'%20%22Probability%20Theory%3A%20The%20Logic%20of%20Science%22%20online%2C%20with%20lost%20chapter%2030&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADraft%20of%20Edwin%20Jaynes'%20%22Probability%20Theory%3A%20The%20Logic%20of%20Science%22%20online%2C%20with%20lost%20chapter%2030%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXon5AFbKcbwoQ5RWR%2Fdraft-of-edwin-jaynes-probability-theory-the-logic-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Draft%20of%20Edwin%20Jaynes'%20%22Probability%20Theory%3A%20The%20Logic%20of%20Science%22%20online%2C%20with%20lost%20chapter%2030%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXon5AFbKcbwoQ5RWR%2Fdraft-of-edwin-jaynes-probability-theory-the-logic-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXon5AFbKcbwoQ5RWR%2Fdraft-of-edwin-jaynes-probability-theory-the-logic-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p><a href=\"http://thiqaruni.org/mathpdf9/(86).pdf\">http://thiqaruni.org/mathpdf9/(86).pdf</a></p>\n<p>The book didn't include Chapter 30 - \"MAXIMUM ENTROPY: MATRIX FORMULATION\"</p>\n<p>Opening in adobe seems to work out better for me.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksWkwyKRrj582WqpN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xon5AFbKcbwoQ5RWR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 14, "extendedScore": null, "score": 9.282330034564679e-07, "legacy": true, "legacyId": "17139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-23T05:53:41.592Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Moral Complexities", "slug": "seq-rerun-moral-complexities", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MQvxn8KYya4xsfpcP/seq-rerun-moral-complexities", "pageUrlRelative": "/posts/MQvxn8KYya4xsfpcP/seq-rerun-moral-complexities", "linkUrl": "https://www.lesswrong.com/posts/MQvxn8KYya4xsfpcP/seq-rerun-moral-complexities", "postedAtFormatted": "Saturday, June 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Moral%20Complexities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Moral%20Complexities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQvxn8KYya4xsfpcP%2Fseq-rerun-moral-complexities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Moral%20Complexities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQvxn8KYya4xsfpcP%2Fseq-rerun-moral-complexities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQvxn8KYya4xsfpcP%2Fseq-rerun-moral-complexities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<p>Today's post, <a href=\"/lw/rw/moral_complexities/\">Moral Complexities</a> was originally published on 04 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Moral_Complexities\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Key questions for two different moral intuitions: morality-as-preference, and morality-as-given.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d88/seq_rerun_the_bedrock_of_fairness/\">The Bedrock of Fairness</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MQvxn8KYya4xsfpcP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.282354987106929e-07, "legacy": true, "legacyId": "17180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SbdCX6A5AGyyfhdmh", "fFGSQYSieaonjT5CZ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-23T07:04:43.203Z", "modifiedAt": null, "url": null, "title": "Wes Weimer from Udacity presents his list of things you should learn", "slug": "wes-weimer-from-udacity-presents-his-list-of-things-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.509Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Dcb5BHccGDGctWELm/wes-weimer-from-udacity-presents-his-list-of-things-you", "pageUrlRelative": "/posts/Dcb5BHccGDGctWELm/wes-weimer-from-udacity-presents-his-list-of-things-you", "linkUrl": "https://www.lesswrong.com/posts/Dcb5BHccGDGctWELm/wes-weimer-from-udacity-presents-his-list-of-things-you", "postedAtFormatted": "Saturday, June 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wes%20Weimer%20from%20Udacity%20presents%20his%20list%20of%20things%20you%20should%20learn&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWes%20Weimer%20from%20Udacity%20presents%20his%20list%20of%20things%20you%20should%20learn%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDcb5BHccGDGctWELm%2Fwes-weimer-from-udacity-presents-his-list-of-things-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wes%20Weimer%20from%20Udacity%20presents%20his%20list%20of%20things%20you%20should%20learn%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDcb5BHccGDGctWELm%2Fwes-weimer-from-udacity-presents-his-list-of-things-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDcb5BHccGDGctWELm%2Fwes-weimer-from-udacity-presents-his-list-of-things-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 271, "htmlBody": "<p>I've just gotten to the end of Udacity's CS262 course in programming languages. It's been pretty good. Wes Weimer, the lecturer, seems to be a really cool guy. There's a quote from HPMOR in the final exam, which I thought was pretty cool.</p>\n<p>In the last part of the last lecture, Weimer gives advice on what we should learn next. You can watch it <a href=\"http://www.youtube.com/watch?feature=player_embedded&amp;v=pVNjSVPC-yQ\">here. <br /></a></p>\n<p>He advises that you learn the following (paraphrased):</p>\n<blockquote>\n<p>Philosophy until you've covered epistemology, formal logic, free will, the philosophy of science, and what it's like to be a bat.</p>\n<p>Cognitive psychology until you've covered perception, consciousness, and the Flynn effect.</p>\n<p>Speech or rhetoric until you've covered persuasion.</p>\n<p>Anthropology and gender studies, to get an idea of what behaviors are socially constructed and which are essential</p>\n<p>Statistics, until you can avoid being fooled by either others or yourself</p>\n<p>Religion or ethics until you've covered the relationship between unhappiness and unrealized desires</p>\n<p>Physics and engineering until you can explain how a microphone, speaker, and radio all work</p>\n<p>Government until you have an opinion about legislating morality and the relative importance of freedom and equality.</p>\n<p>History until you are not condemned to make the mistakes of the past.</p>\n<p>Life until you are happy. They say ignorance is bliss, but they are wrong all but finitely often.</p>\n</blockquote>\n<p>I thought that was all really useful (except maybe the last two). I've learned up to his required level of philosophy, cognitive psychology, and religion and ethics. I'm working on the physics and gender studies.</p>\n<p>(Incidentally, I strongly recommend Udacity for learning programming. It's really good.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Dcb5BHccGDGctWELm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 9.282682043856565e-07, "legacy": true, "legacyId": "17181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-23T09:57:48.545Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh: Belief in Belief", "slug": "meetup-pittsburgh-belief-in-belief", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tjhgrx6nLsRWZXZxr/meetup-pittsburgh-belief-in-belief", "pageUrlRelative": "/posts/tjhgrx6nLsRWZXZxr/meetup-pittsburgh-belief-in-belief", "linkUrl": "https://www.lesswrong.com/posts/tjhgrx6nLsRWZXZxr/meetup-pittsburgh-belief-in-belief", "postedAtFormatted": "Saturday, June 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%3A%20Belief%20in%20Belief&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%3A%20Belief%20in%20Belief%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftjhgrx6nLsRWZXZxr%2Fmeetup-pittsburgh-belief-in-belief%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%3A%20Belief%20in%20Belief%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftjhgrx6nLsRWZXZxr%2Fmeetup-pittsburgh-belief-in-belief", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftjhgrx6nLsRWZXZxr%2Fmeetup-pittsburgh-belief-in-belief", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bd'>Pittsburgh: Belief in Belief</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 June 2012 11:30:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">EatUnique, Craig St, Pittsburgh, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come and eat lunch and talk about those things you don't believe in believing in believing.</p>\n\n<p>Suggested reading: <a href=\"http://lesswrong.com/lw/i4/belief_in_belief/\" rel=\"nofollow\">http://lesswrong.com/lw/i4/belief_in_belief/</a></p>\n\n<p>Eatunique serves salads, wraps, sandwiches, drinks, and that kind of thing. Call 412-304-6258 if you can't find us.</p>\n\n<p>Our mailing list is at https://groups.google.com/group/lw-pgh?pli=1</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bd'>Pittsburgh: Belief in Belief</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tjhgrx6nLsRWZXZxr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.283479151370675e-07, "legacy": true, "legacyId": "17182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Belief_in_Belief\">Discussion article for the meetup : <a href=\"/meetups/bd\">Pittsburgh: Belief in Belief</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 June 2012 11:30:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">EatUnique, Craig St, Pittsburgh, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come and eat lunch and talk about those things you don't believe in believing in believing.</p>\n\n<p>Suggested reading: <a href=\"http://lesswrong.com/lw/i4/belief_in_belief/\" rel=\"nofollow\">http://lesswrong.com/lw/i4/belief_in_belief/</a></p>\n\n<p>Eatunique serves salads, wraps, sandwiches, drinks, and that kind of thing. Call 412-304-6258 if you can't find us.</p>\n\n<p>Our mailing list is at https://groups.google.com/group/lw-pgh?pli=1</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Belief_in_Belief1\">Discussion article for the meetup : <a href=\"/meetups/bd\">Pittsburgh: Belief in Belief</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh: Belief in Belief", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Belief_in_Belief", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh: Belief in Belief", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Belief_in_Belief1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CqyJzDZWvGhhFJ7dY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-23T13:51:21.039Z", "modifiedAt": null, "url": null, "title": "[Link] RSA Animate: extremely entertaining LW-relevant cartoons", "slug": "link-rsa-animate-extremely-entertaining-lw-relevant-cartoons", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:26.618Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Multiheaded", "createdAt": "2011-07-02T10:10:20.692Z", "isAdmin": false, "displayName": "Multiheaded"}, "userId": "moGiw35FowgiAnzfg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zc64B8jghzr5GLQE2/link-rsa-animate-extremely-entertaining-lw-relevant-cartoons", "pageUrlRelative": "/posts/zc64B8jghzr5GLQE2/link-rsa-animate-extremely-entertaining-lw-relevant-cartoons", "linkUrl": "https://www.lesswrong.com/posts/zc64B8jghzr5GLQE2/link-rsa-animate-extremely-entertaining-lw-relevant-cartoons", "postedAtFormatted": "Saturday, June 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20RSA%20Animate%3A%20extremely%20entertaining%20LW-relevant%20cartoons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20RSA%20Animate%3A%20extremely%20entertaining%20LW-relevant%20cartoons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzc64B8jghzr5GLQE2%2Flink-rsa-animate-extremely-entertaining-lw-relevant-cartoons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20RSA%20Animate%3A%20extremely%20entertaining%20LW-relevant%20cartoons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzc64B8jghzr5GLQE2%2Flink-rsa-animate-extremely-entertaining-lw-relevant-cartoons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzc64B8jghzr5GLQE2%2Flink-rsa-animate-extremely-entertaining-lw-relevant-cartoons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<p>It's a brilliant idea: a lecture by a cool modern thinker, illustrated by word-by-word doodles on a whiteboard. Excellent at pulling you along the train of thought and absolutely disallowing boredom.</p>\n<p>The lectures' content is pretty great too, although there's a definite left-wing, populist bent that's exploting today's post-crisis hot button issues (they got Zizek, for god's sake) - some might not like it. Regardless, it's all very amusing and enlightening. Been linked to before in a comment or to, but it deserves a headline.</p>\n<p>You can start here: http://www.youtube.com/watch?v=dFs9WO2B8uI&amp;feature=relmfu (But they're all worth watching!)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zc64B8jghzr5GLQE2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 7, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "17183", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-23T16:28:05.536Z", "modifiedAt": null, "url": null, "title": "Web of Trust lists Singularity.org as having a bad reputation", "slug": "web-of-trust-lists-singularity-org-as-having-a-bad", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.391Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y7hvsr8hvwcgCN8xG/web-of-trust-lists-singularity-org-as-having-a-bad", "pageUrlRelative": "/posts/Y7hvsr8hvwcgCN8xG/web-of-trust-lists-singularity-org-as-having-a-bad", "linkUrl": "https://www.lesswrong.com/posts/Y7hvsr8hvwcgCN8xG/web-of-trust-lists-singularity-org-as-having-a-bad", "postedAtFormatted": "Saturday, June 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Web%20of%20Trust%20lists%20Singularity.org%20as%20having%20a%20bad%20reputation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeb%20of%20Trust%20lists%20Singularity.org%20as%20having%20a%20bad%20reputation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7hvsr8hvwcgCN8xG%2Fweb-of-trust-lists-singularity-org-as-having-a-bad%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Web%20of%20Trust%20lists%20Singularity.org%20as%20having%20a%20bad%20reputation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7hvsr8hvwcgCN8xG%2Fweb-of-trust-lists-singularity-org-as-having-a-bad", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7hvsr8hvwcgCN8xG%2Fweb-of-trust-lists-singularity-org-as-having-a-bad", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 273, "htmlBody": "<p>I'm not too sure who is familiar with <a href=\"http://www.mywot.com/\">Web of Trust</a>, so I'll start with a brief description. It's basically a browser app that inserts a circle next to text links in websites. The color of the circle indicates whether or not it's average rating by users rates it as having a \"good reputation\" (green) or a \"bad reputation\" (red). There are four criteria: Trustworthiness, Vendor Reliability, Privacy, and Child Safety.</p>\n<p>Singularity.org's printout is <a href=\"http://www.mywot.com/en/scorecard/singularity.org/comment?utm_source=addon&amp;utm_content=rw-viewsc\">here</a>.&nbsp;As you may have guessed from the title, Web of Trust lists Singularity.org as \"poor\" in trustworthiness, vendor reliability, and privacy. There's a comment that, when translated (via Google translate) says \"Mass mailing of non-thematic Forums\". It's also commented under the category \"malicious content/viruses\".</p>\n<p>I'm not entirely sure how these ratings are generated, (<a href=\"http://www.mywot.com/en/faq/website/rating-websites\">How Ratings Work</a>, related)&nbsp;but I've used it for several years, and this is only the second time I've disagreed with a rating.&nbsp;I've always found WOT to be very reliable, and a decent way of warning me if a site is unsafe so I don't have to think about it. So I was fairly alarmed when I saw the red circle there, since I'd imagine it's turning away people that don't know any better. If LW had a red circle, I never would have come here. I'm not sure what SI or LW can do about it, but there's a \"click here if you are the owner of this site\" button,&nbsp;although&nbsp;I don't know what that does.&nbsp;I've left my own rating on there, but it didn't seem to change the overall rankings.</p>\n<p>&nbsp;</p>\n<p>Edit: When I made this post, the scorecard read Trustworthiness 30, Vendor Reliability 31, Privacy 31, Child Safety 100.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y7hvsr8hvwcgCN8xG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 29, "extendedScore": null, "score": 9.285276923058769e-07, "legacy": true, "legacyId": "17184", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-24T04:36:23.833Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup", "slug": "meetup-berkeley-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zf5D4tQv2MzcZgk4c/meetup-berkeley-meetup-0", "pageUrlRelative": "/posts/zf5D4tQv2MzcZgk4c/meetup-berkeley-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/zf5D4tQv2MzcZgk4c/meetup-berkeley-meetup-0", "postedAtFormatted": "Sunday, June 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzf5D4tQv2MzcZgk4c%2Fmeetup-berkeley-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzf5D4tQv2MzcZgk4c%2Fmeetup-berkeley-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzf5D4tQv2MzcZgk4c%2Fmeetup-berkeley-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/be'>Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>See the mailing list for location of Zendo.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/be'>Berkeley meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zf5D4tQv2MzcZgk4c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.288633381359543e-07, "legacy": true, "legacyId": "17185", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup\">Discussion article for the meetup : <a href=\"/meetups/be\">Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>See the mailing list for location of Zendo.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup1\">Discussion article for the meetup : <a href=\"/meetups/be\">Berkeley meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup", "level": 1}, {"title": "Discussion article for the meetup : Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-24T04:46:58.978Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Is Morality Preference?", "slug": "seq-rerun-is-morality-preference", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:26.539Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PHLfkoRwjpCPyqbZQ/seq-rerun-is-morality-preference", "pageUrlRelative": "/posts/PHLfkoRwjpCPyqbZQ/seq-rerun-is-morality-preference", "linkUrl": "https://www.lesswrong.com/posts/PHLfkoRwjpCPyqbZQ/seq-rerun-is-morality-preference", "postedAtFormatted": "Sunday, June 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Is%20Morality%20Preference%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Is%20Morality%20Preference%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPHLfkoRwjpCPyqbZQ%2Fseq-rerun-is-morality-preference%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Is%20Morality%20Preference%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPHLfkoRwjpCPyqbZQ%2Fseq-rerun-is-morality-preference", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPHLfkoRwjpCPyqbZQ%2Fseq-rerun-is-morality-preference", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>Today's post, <a href=\"/lw/rx/is_morality_preference/\">Is Morality Preference?</a> was originally published on 05 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Is_Morality_Preference.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A dialogue on the idea that morality is a subset of our desires.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d98/seq_rerun_moral_complexities/\">Moral Complexities</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PHLfkoRwjpCPyqbZQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.28868218240744e-07, "legacy": true, "legacyId": "17186", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["F5WLc7hCxkB4X4yD4", "MQvxn8KYya4xsfpcP", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-24T13:02:42.468Z", "modifiedAt": null, "url": null, "title": "Minimum Viable Workout Routine is Dangerously Misinformative", "slug": "minimum-viable-workout-routine-is-dangerously-misinformative", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.376Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "betterthanwell", "createdAt": "2009-02-28T00:39:39.875Z", "isAdmin": false, "displayName": "betterthanwell"}, "userId": "W9LCYGeCRvhd5aREo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ehAXv5cANo5hYKChu/minimum-viable-workout-routine-is-dangerously-misinformative", "pageUrlRelative": "/posts/ehAXv5cANo5hYKChu/minimum-viable-workout-routine-is-dangerously-misinformative", "linkUrl": "https://www.lesswrong.com/posts/ehAXv5cANo5hYKChu/minimum-viable-workout-routine-is-dangerously-misinformative", "postedAtFormatted": "Sunday, June 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Minimum%20Viable%20Workout%20Routine%20is%20Dangerously%20Misinformative&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMinimum%20Viable%20Workout%20Routine%20is%20Dangerously%20Misinformative%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FehAXv5cANo5hYKChu%2Fminimum-viable-workout-routine-is-dangerously-misinformative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Minimum%20Viable%20Workout%20Routine%20is%20Dangerously%20Misinformative%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FehAXv5cANo5hYKChu%2Fminimum-viable-workout-routine-is-dangerously-misinformative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FehAXv5cANo5hYKChu%2Fminimum-viable-workout-routine-is-dangerously-misinformative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1837, "htmlBody": "<p>Edit: Dangerously misleading on <em>one</em> crucial point.</p>\n<p>&nbsp;</p>\n<p>This started out as a short reply to the Less Wrong post <a href=\"/r/discussion/lw/d7h/minimum_viable_workout_routine/\">Minimum Viable Workout Routine</a>.&nbsp;Unfortunately I was unable to summarise my points sufficiently,&nbsp;so this reply grew into a post of it's own. I realize that the following is a bit rude to the original poster, and I apologize.&nbsp;Minimum Viable Workout Routine seems to be backed solely by anecdotal personal experience.&nbsp;The majority of the post, which addresses strength training seems decent, and not obviously wrong.&nbsp;However, the following paragraph <em>is</em> dangerously wrong:</p>\n<blockquote>\n<p><em>A note about cardio: Cardiovascular capacity (V02 max) has shown a high degree of correlation to all cause mortality. &nbsp;Why aren't I recommending cardio? &nbsp;Because the only way to increase V02 max is with high intensity exercise. &nbsp;Between high intensity weight lifting and high intensity cardio, high intensity weightlifting easily wins for a newbie. &nbsp;A newbie, especially a significantly out of shape one, <strong>will not be capable of a level of cardio exertion that results in a significant adaptation.</strong> &nbsp;This can result in a lot of effort with very little in the way of improvement. &nbsp;This is soul-destroyingly frustrating.</em></p>\n</blockquote>\n<p>Leaving this uncontested could be dangerous to your health. As we shall see, cardiovascular capacity is indeed important to survival and longevity. It is also<em>&nbsp;quite easily trainable</em>, it needs not be soul crushingly frustrating, and it should not be overlooked.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>A couple of cool findings from the physiology of exercise:</strong></p>\n<p>Instead of having me trying to convince you, we'll take a look at the science, and see what we find.</p>\n<p>You can assume with reasonable confidence that the findings are valid, well-corroborated, and furthermore, the findings quoted in this post have been found to apply to the general population, to the untrained, the athlete, the elderly, and the ill, even though I'm only able to present here a selection of these results.</p>\n<p>Human physiology, and its response to exercise is surprisingly stable across genders, race, age and physical fitness. What works for the well trained athlete will also work for the utter newbie, the obese, and patients with heart disease.</p>\n<p>Strength training is also important to health, increases in maximal strength of the large muscle groups tends to cause large increases in endurance, as measured in time to exhaustion. Why? Because if your muscles have to work slightly less hard at each movement, relative to the one repetition maximum of the muscle, you can work at moderate to high intensities for much longer.</p>\n<p>So, let's first see that a brief strength training intervention can dramatically increase endurance:</p>\n<p>&nbsp;</p>\n<p><strong><a href=\"http://www.ntnu.no/c/document_library/get_file?uuid=fc46ad21-5655-4528-8ede-634cf070b745&amp;groupId=10301 \">Maximal Strength Training Improves Running&nbsp;Economy in Distance Runners</a></strong></p>\n<blockquote>\n<p><em>Purpose: The present study investigated the effect of&nbsp;maximal strength training on running economy (RE) at 70% of maximal oxygen consumption (VO2max) and time to exhaustion&nbsp;at maximal aerobic speed (MAS). Responses in one repetition maximum (1RM) and rate of force development (RFD) in half-squats,&nbsp;maximal oxygen consumption, RE, and time to exhaustion at MAS were examined.</em></p>\n<p><em>Methods: Seventeen well-trained (nine male and&nbsp;eight female) runners were randomly assigned into either an intervention or a control group. The intervention group (four males and&nbsp;four females) performed half-squats, four sets of four repetitions maximum, three times per week for 8 wk, as a supplement to their&nbsp;normal endurance training. The control group continued their normal endurance training during the same period.</em></p>\n<p><em>Results: The&nbsp;intervention manifested significant improvements in 1RM (33.2%), RFD (26.0%), RE (5.0%), and time to exhaustion at MAS (21.3%).&nbsp;No changes were found in VO2max&nbsp;or body weight. The control group exhibited no changes from pre to post values in any of the&nbsp;parameters.</em></p>\n<p><em>Conclusion: Maximal strength training for 8 wk improved RE and increased time to exhaustion at MAS among well trained, long-distance runners, without change in maximal oxygen uptake or body weight</em></p>\n</blockquote>\n<p>&nbsp;</p>\n<p>So a maximal strength training exercise of maybe 10 minutes, 3 times per week, for eight weeks, resulted in an increase in time to exhaustion of ~20% in well trained runners. So increases in strength can lead to large gains in endurance.</p>\n<p>Cool, because endurance training is important. Endurance training makes you die less often, on average:</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.ntnu.no/c/document_library/get_file?uuid=d5a58c79-135e-4e47-affa-6b9aac3c6051&amp;groupId=10301\">Exercise Capactity And Mortality Among Men Referred For Exercise Testing</a></p>\n<blockquote>\n<p><em> </em></p>\n<p><em>Background:&nbsp;</em>\n<p style=\"display: inline !important;\"><em><em>Exercise capacity is known to be an&nbsp;<em></em></em></em></p>\n<em><em>\n<p style=\"display: inline !important;\"><em>important prognostic factor in patients with cardiovascular disease, but it is uncertain whether it predicts&nbsp;<em></em></em></p>\n<p style=\"display: inline !important;\"><em><em>mortality equally well among healthy persons. There&nbsp;<em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>is also uncertainty regarding the predictive power of&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>exercise capacity relative to other clinical and exercise test variables.&nbsp;</em></em></em></p>\n</em></em></p>\n<em><em> <em><em>\n<p style=\"display: inline !important;\"><em><em><em>Methods: We studied a total of 6213 consecutive&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>men referred for treadmill exercise testing for clinical&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>reasons during a mean (&plusmn;SD) of 6.2&plusmn;3.7 years of follow-up. Subjects were classified into two groups: 3679&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>had an abnormal exercise-test result or a history of&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>cardiovascular disease, or both, and 2534 had a normal exercise-test result and no history of cardiovascular disease. Overall mortality was the end point.&nbsp;</em></em></em></p>\n<p><em> </em></p>\n<p style=\"display: inline !important;\"><em>Results: There were a total of 1256 deaths during the&nbsp;<em></em></em></p>\n<em>\n<p style=\"display: inline !important;\"><em>follow-up period, resulting in an average annual mortality of 2.6 percent. Men who died were older than&nbsp;<em></em></em></p>\n<p style=\"display: inline !important;\"><em><em>those who survived and had a lower maximal heart&nbsp;<em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>rate, lower maximal systolic and diastolic blood pressure, and lower exercise capacity. After adjustment for&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>age, the peak exercise capacity measured in metabolic&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>equivalents (MET) was the strongest predictor of the&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>risk of death among both normal subjects and those&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>with cardiovascular disease. Absolute peak exercise capacity was a stronger predictor of the risk of death than&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>the percentage of the age-predicted value achieved,&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>and there was no interaction between the use or nonuse of beta-blockade and the predictive power of exercise capacity. <strong>Each 1-MET increase in exercise capacity&nbsp;<em></em></strong></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em><strong>conferred a 12 percent improvement in survival.&nbsp;</strong></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em><br /></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em><br /></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>Conclusions: Exercise capacity is a more powerful&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>predictor of mortality among men than other established risk factors for cardiovascular disease. (N Engl&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>J Med 2002;346:793-801.)</em></em></em></p>\n</em></em></em></em></em></blockquote>\n<p>&nbsp;</p>\n<p>If you want to live long and prosper, you should train your body, most importantly your heart, to be able to work hard when you can. So that it can work hard for you when it must fight for your life. With that, let us have a look at&nbsp;endurance training in the elderly:</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.ntnu.no/c/document_library/get_file?uuid=bd275193-8b41-47ba-8bdf-4f01cb2a2ec0&amp;groupId=10301\">Effects of High-Intensity Endurance Training on Maximal Oxygen Consumption in Healthy Elderly People</a></p>\n<blockquote>\n<p><em>Each 60-minute training session included four repetitions of exercise at approximately 85% to 95% of maximal heart rate separated by 4-minute rest periods. The control group was encouraged to perform no additive strength or endurance training during the study period. Maximal oxygen consumption increased significantly (p &lt; .05) (13.2%) in the TG compared to the CG. Walking economy and maximal walking speed were unchanged after the training intervention. This training study demonstrates that high-intensity endurance training significantly improves VO2 max in older adults.</em></p>\n</blockquote>\n<p>Why is this interesting? Well, it seems that old people respond the same to vigorous exercise, as do young people.&nbsp;</p>\n<p>But! They tend to do less of it, and consequently they gradually suffer worse health.&nbsp;I'm oversimplifying, but there is a causal path from physical inactivity with old age,&nbsp;(not because of old age) which leads to&nbsp;deterioration&nbsp;of health which leads to death.&nbsp;If you are reaching retirement age, or you know people who are, try to get them to move their butts before it's too late.&nbsp;And don't stop moving.&nbsp;Don't stop moving even if you suffer heart failure. Let's have a look at the effects of vigorous training in patients with heart disease:</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.ntnu.no/c/document_library/get_file?uuid=106a47da-1b71-4dfe-a6c0-e24322708c78&amp;groupId=10301\">Interval and Strength Training in CAD Patients</a>:</p>\n<blockquote>\n<p><em>This study sought to study the effect of high intensity aerobic interval endurance training on peak stroke volume and maximal strength training on mechanical efficiency in coronary artery disease (CAD) patients. 8 CAD patients (age 61.4 &plusmn; 3.7 years) trained 30 interval training sessions with 4 &times; 4 min intervals at 85-95% of peak heart rate while 10 CAD patients (age 66.5 &plusmn; 5.5 years) trained 24 sessions of maximal horizontal leg press.</em></p>\n<p><em>In the interval training group peak stroke volume increased significantly by 23% from 94.1 &plusmn; 23.0 mL &middot; beat (-1) to 115.8 &plusmn; 22.4 mL &middot; beat (-1) (p&lt;0.05). Peak oxygen uptake increased significantly by 17% from 27.2 &plusmn; 4.5 mL &middot; kg (-1) &middot; min (-1) to 31.8 &plusmn; 5.0 mL &middot; kg (-1) min (-1) (p&lt;0.05) in the same group. In contrast, there was no such exercise training-induced change in peak stroke volume or peak oxygen uptake in the maximal strength training group, despite a 35% improvement in sub maximal walking performance.</em></p>\n</blockquote>\n<p>So this is what heart patients get in return for sixteen minutes, three times a week for two and a half months. If&nbsp;you're still breathing, and able to move by your own power, you <em>are</em> capable of the level of cardio exertion that leads to significant adaptation. You don't necessarily have to run fast, you just need to be able to get your heart rate to 85-95% of your maximum heart rate, repeatedly. For a CAD patient or an overweight person, this could mean a brisk walk. If you're out of shape, &nbsp;you don't have to fly across the terrain at amazing speed for your heart to get all excited.</p>\n<p><strong>How, exactly, does endurance training keep people healthy and alive?</strong></p>\n<p>Repeated intervals of brief, but high intensity endurace training causes increased stroke volume of the heart, when working at peak capacity. This carries over into the resting state, and lowers your resting pulse, your blood pressure, increases your maximal oxygen uptake, increases your working capacity when healthy, makes you less likely to get ill. If you get ill, your heart will be better at keeping you alive.</p>\n<p><strong>Closing thoughts:</strong></p>\n<p>Should you trust my opinion? I've only taken a single university course in exercise physiology.&nbsp;You should not trust my opinion, and this is not professional advice.&nbsp;But my opinion is backed up with well-corroborated scientific findings, and you should probably trust those.</p>\n<p>The course i took was excellent. The teaching professors were were former national team coaches, they have applied their research to great effect on healthy, untrained students, CAD patients, COPD patients, youth athletes, elite athletes and billion dollar soccer teams.</p>\n<p>Pursuing a regime of maximal strength training and high intensity interval training, you should see evident and mutually reinforcing gains in strength and endurance, and you'll not stop seeing benefits, even at the level of 100 million dollar soccer players. (<a href=\"http://www.ntnu.no/c/document_library/get_file?uuid=ffe28646-4fa7-4618-a577-e6965dbe8a15&amp;groupId=10301\">Endurance and Strength Training for&nbsp;Soccer Players</a>)</p>\n<p><strong>An final anecdote of my own:</strong></p>\n<p>I used to find physical exercise&nbsp;dis-congruent&nbsp;with my geek identity, I found it boring, painful, useless.&nbsp;Learning the very basics of exercise physiology, along with some nerdy details makes working out seem important.&nbsp;It helps to know why I'm doing it, I know what the expected effects are, and the expected sizes of the effects, how to avoid training fatigue, track my heart rate, track my progress, and on the whole, see that I'm on track to results like those mentioned in the studies above. I find it fun to try and figure out what is happening, and approach working out as something of a puzzle to be solved.&nbsp;Exercise, or the lack of it, is an experiment in health.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8nAXyYLu8eT72Hwuh": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ehAXv5cANo5hYKChu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 29, "extendedScore": null, "score": 9.290968023572624e-07, "legacy": true, "legacyId": "17187", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Edit: Dangerously misleading on <em>one</em> crucial point.</p>\n<p>&nbsp;</p>\n<p>This started out as a short reply to the Less Wrong post <a href=\"/r/discussion/lw/d7h/minimum_viable_workout_routine/\">Minimum Viable Workout Routine</a>.&nbsp;Unfortunately I was unable to summarise my points sufficiently,&nbsp;so this reply grew into a post of it's own. I realize that the following is a bit rude to the original poster, and I apologize.&nbsp;Minimum Viable Workout Routine seems to be backed solely by anecdotal personal experience.&nbsp;The majority of the post, which addresses strength training seems decent, and not obviously wrong.&nbsp;However, the following paragraph <em>is</em> dangerously wrong:</p>\n<blockquote>\n<p><em>A note about cardio: Cardiovascular capacity (V02 max) has shown a high degree of correlation to all cause mortality. &nbsp;Why aren't I recommending cardio? &nbsp;Because the only way to increase V02 max is with high intensity exercise. &nbsp;Between high intensity weight lifting and high intensity cardio, high intensity weightlifting easily wins for a newbie. &nbsp;A newbie, especially a significantly out of shape one, <strong>will not be capable of a level of cardio exertion that results in a significant adaptation.</strong> &nbsp;This can result in a lot of effort with very little in the way of improvement. &nbsp;This is soul-destroyingly frustrating.</em></p>\n</blockquote>\n<p>Leaving this uncontested could be dangerous to your health. As we shall see, cardiovascular capacity is indeed important to survival and longevity. It is also<em>&nbsp;quite easily trainable</em>, it needs not be soul crushingly frustrating, and it should not be overlooked.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"A_couple_of_cool_findings_from_the_physiology_of_exercise_\">A couple of cool findings from the physiology of exercise:</strong></p>\n<p>Instead of having me trying to convince you, we'll take a look at the science, and see what we find.</p>\n<p>You can assume with reasonable confidence that the findings are valid, well-corroborated, and furthermore, the findings quoted in this post have been found to apply to the general population, to the untrained, the athlete, the elderly, and the ill, even though I'm only able to present here a selection of these results.</p>\n<p>Human physiology, and its response to exercise is surprisingly stable across genders, race, age and physical fitness. What works for the well trained athlete will also work for the utter newbie, the obese, and patients with heart disease.</p>\n<p>Strength training is also important to health, increases in maximal strength of the large muscle groups tends to cause large increases in endurance, as measured in time to exhaustion. Why? Because if your muscles have to work slightly less hard at each movement, relative to the one repetition maximum of the muscle, you can work at moderate to high intensities for much longer.</p>\n<p>So, let's first see that a brief strength training intervention can dramatically increase endurance:</p>\n<p>&nbsp;</p>\n<p><strong id=\"Maximal_Strength_Training_Improves_Running_Economy_in_Distance_Runners\"><a href=\"http://www.ntnu.no/c/document_library/get_file?uuid=fc46ad21-5655-4528-8ede-634cf070b745&amp;groupId=10301 \">Maximal Strength Training Improves Running&nbsp;Economy in Distance Runners</a></strong></p>\n<blockquote>\n<p><em>Purpose: The present study investigated the effect of&nbsp;maximal strength training on running economy (RE) at 70% of maximal oxygen consumption (VO2max) and time to exhaustion&nbsp;at maximal aerobic speed (MAS). Responses in one repetition maximum (1RM) and rate of force development (RFD) in half-squats,&nbsp;maximal oxygen consumption, RE, and time to exhaustion at MAS were examined.</em></p>\n<p><em>Methods: Seventeen well-trained (nine male and&nbsp;eight female) runners were randomly assigned into either an intervention or a control group. The intervention group (four males and&nbsp;four females) performed half-squats, four sets of four repetitions maximum, three times per week for 8 wk, as a supplement to their&nbsp;normal endurance training. The control group continued their normal endurance training during the same period.</em></p>\n<p><em>Results: The&nbsp;intervention manifested significant improvements in 1RM (33.2%), RFD (26.0%), RE (5.0%), and time to exhaustion at MAS (21.3%).&nbsp;No changes were found in VO2max&nbsp;or body weight. The control group exhibited no changes from pre to post values in any of the&nbsp;parameters.</em></p>\n<p><em>Conclusion: Maximal strength training for 8 wk improved RE and increased time to exhaustion at MAS among well trained, long-distance runners, without change in maximal oxygen uptake or body weight</em></p>\n</blockquote>\n<p>&nbsp;</p>\n<p>So a maximal strength training exercise of maybe 10 minutes, 3 times per week, for eight weeks, resulted in an increase in time to exhaustion of ~20% in well trained runners. So increases in strength can lead to large gains in endurance.</p>\n<p>Cool, because endurance training is important. Endurance training makes you die less often, on average:</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.ntnu.no/c/document_library/get_file?uuid=d5a58c79-135e-4e47-affa-6b9aac3c6051&amp;groupId=10301\">Exercise Capactity And Mortality Among Men Referred For Exercise Testing</a></p>\n<blockquote>\n<p><em> </em></p>\n<p><em>Background:&nbsp;</em>\n</p><p style=\"display: inline !important;\"><em><em>Exercise capacity is known to be an&nbsp;<em></em></em></em></p>\n<em><em>\n<p style=\"display: inline !important;\"><em>important prognostic factor in patients with cardiovascular disease, but it is uncertain whether it predicts&nbsp;<em></em></em></p>\n<p style=\"display: inline !important;\"><em><em>mortality equally well among healthy persons. There&nbsp;<em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>is also uncertainty regarding the predictive power of&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>exercise capacity relative to other clinical and exercise test variables.&nbsp;</em></em></em></p>\n</em></em><p></p>\n<em><em> <em><em>\n<p style=\"display: inline !important;\"><em><em><em>Methods: We studied a total of 6213 consecutive&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>men referred for treadmill exercise testing for clinical&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>reasons during a mean (\u00b1SD) of 6.2\u00b13.7 years of follow-up. Subjects were classified into two groups: 3679&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>had an abnormal exercise-test result or a history of&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>cardiovascular disease, or both, and 2534 had a normal exercise-test result and no history of cardiovascular disease. Overall mortality was the end point.&nbsp;</em></em></em></p>\n<p><em> </em></p>\n<p style=\"display: inline !important;\"><em>Results: There were a total of 1256 deaths during the&nbsp;<em></em></em></p>\n<em>\n<p style=\"display: inline !important;\"><em>follow-up period, resulting in an average annual mortality of 2.6 percent. Men who died were older than&nbsp;<em></em></em></p>\n<p style=\"display: inline !important;\"><em><em>those who survived and had a lower maximal heart&nbsp;<em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>rate, lower maximal systolic and diastolic blood pressure, and lower exercise capacity. After adjustment for&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>age, the peak exercise capacity measured in metabolic&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>equivalents (MET) was the strongest predictor of the&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>risk of death among both normal subjects and those&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>with cardiovascular disease. Absolute peak exercise capacity was a stronger predictor of the risk of death than&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>the percentage of the age-predicted value achieved,&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>and there was no interaction between the use or nonuse of beta-blockade and the predictive power of exercise capacity. <strong>Each 1-MET increase in exercise capacity&nbsp;<em></em></strong></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em><strong>conferred a 12 percent improvement in survival.&nbsp;</strong></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em><br></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em><br></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>Conclusions: Exercise capacity is a more powerful&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>predictor of mortality among men than other established risk factors for cardiovascular disease. (N Engl&nbsp;<em></em></em></em></em></p>\n<p style=\"display: inline !important;\"><em><em><em>J Med 2002;346:793-801.)</em></em></em></p>\n</em></em></em></em></em></blockquote>\n<p>&nbsp;</p>\n<p>If you want to live long and prosper, you should train your body, most importantly your heart, to be able to work hard when you can. So that it can work hard for you when it must fight for your life. With that, let us have a look at&nbsp;endurance training in the elderly:</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.ntnu.no/c/document_library/get_file?uuid=bd275193-8b41-47ba-8bdf-4f01cb2a2ec0&amp;groupId=10301\">Effects of High-Intensity Endurance Training on Maximal Oxygen Consumption in Healthy Elderly People</a></p>\n<blockquote>\n<p><em>Each 60-minute training session included four repetitions of exercise at approximately 85% to 95% of maximal heart rate separated by 4-minute rest periods. The control group was encouraged to perform no additive strength or endurance training during the study period. Maximal oxygen consumption increased significantly (p &lt; .05) (13.2%) in the TG compared to the CG. Walking economy and maximal walking speed were unchanged after the training intervention. This training study demonstrates that high-intensity endurance training significantly improves VO2 max in older adults.</em></p>\n</blockquote>\n<p>Why is this interesting? Well, it seems that old people respond the same to vigorous exercise, as do young people.&nbsp;</p>\n<p>But! They tend to do less of it, and consequently they gradually suffer worse health.&nbsp;I'm oversimplifying, but there is a causal path from physical inactivity with old age,&nbsp;(not because of old age) which leads to&nbsp;deterioration&nbsp;of health which leads to death.&nbsp;If you are reaching retirement age, or you know people who are, try to get them to move their butts before it's too late.&nbsp;And don't stop moving.&nbsp;Don't stop moving even if you suffer heart failure. Let's have a look at the effects of vigorous training in patients with heart disease:</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.ntnu.no/c/document_library/get_file?uuid=106a47da-1b71-4dfe-a6c0-e24322708c78&amp;groupId=10301\">Interval and Strength Training in CAD Patients</a>:</p>\n<blockquote>\n<p><em>This study sought to study the effect of high intensity aerobic interval endurance training on peak stroke volume and maximal strength training on mechanical efficiency in coronary artery disease (CAD) patients. 8 CAD patients (age 61.4 \u00b1 3.7 years) trained 30 interval training sessions with 4 \u00d7 4 min intervals at 85-95% of peak heart rate while 10 CAD patients (age 66.5 \u00b1 5.5 years) trained 24 sessions of maximal horizontal leg press.</em></p>\n<p><em>In the interval training group peak stroke volume increased significantly by 23% from 94.1 \u00b1 23.0 mL \u00b7 beat (-1) to 115.8 \u00b1 22.4 mL \u00b7 beat (-1) (p&lt;0.05). Peak oxygen uptake increased significantly by 17% from 27.2 \u00b1 4.5 mL \u00b7 kg (-1) \u00b7 min (-1) to 31.8 \u00b1 5.0 mL \u00b7 kg (-1) min (-1) (p&lt;0.05) in the same group. In contrast, there was no such exercise training-induced change in peak stroke volume or peak oxygen uptake in the maximal strength training group, despite a 35% improvement in sub maximal walking performance.</em></p>\n</blockquote>\n<p>So this is what heart patients get in return for sixteen minutes, three times a week for two and a half months. If&nbsp;you're still breathing, and able to move by your own power, you <em>are</em> capable of the level of cardio exertion that leads to significant adaptation. You don't necessarily have to run fast, you just need to be able to get your heart rate to 85-95% of your maximum heart rate, repeatedly. For a CAD patient or an overweight person, this could mean a brisk walk. If you're out of shape, &nbsp;you don't have to fly across the terrain at amazing speed for your heart to get all excited.</p>\n<p><strong id=\"How__exactly__does_endurance_training_keep_people_healthy_and_alive_\">How, exactly, does endurance training keep people healthy and alive?</strong></p>\n<p>Repeated intervals of brief, but high intensity endurace training causes increased stroke volume of the heart, when working at peak capacity. This carries over into the resting state, and lowers your resting pulse, your blood pressure, increases your maximal oxygen uptake, increases your working capacity when healthy, makes you less likely to get ill. If you get ill, your heart will be better at keeping you alive.</p>\n<p><strong id=\"Closing_thoughts_\">Closing thoughts:</strong></p>\n<p>Should you trust my opinion? I've only taken a single university course in exercise physiology.&nbsp;You should not trust my opinion, and this is not professional advice.&nbsp;But my opinion is backed up with well-corroborated scientific findings, and you should probably trust those.</p>\n<p>The course i took was excellent. The teaching professors were were former national team coaches, they have applied their research to great effect on healthy, untrained students, CAD patients, COPD patients, youth athletes, elite athletes and billion dollar soccer teams.</p>\n<p>Pursuing a regime of maximal strength training and high intensity interval training, you should see evident and mutually reinforcing gains in strength and endurance, and you'll not stop seeing benefits, even at the level of 100 million dollar soccer players. (<a href=\"http://www.ntnu.no/c/document_library/get_file?uuid=ffe28646-4fa7-4618-a577-e6965dbe8a15&amp;groupId=10301\">Endurance and Strength Training for&nbsp;Soccer Players</a>)</p>\n<p><strong id=\"An_final_anecdote_of_my_own_\">An final anecdote of my own:</strong></p>\n<p>I used to find physical exercise&nbsp;dis-congruent&nbsp;with my geek identity, I found it boring, painful, useless.&nbsp;Learning the very basics of exercise physiology, along with some nerdy details makes working out seem important.&nbsp;It helps to know why I'm doing it, I know what the expected effects are, and the expected sizes of the effects, how to avoid training fatigue, track my heart rate, track my progress, and on the whole, see that I'm on track to results like those mentioned in the studies above. I find it fun to try and figure out what is happening, and approach working out as something of a puzzle to be solved.&nbsp;Exercise, or the lack of it, is an experiment in health.</p>", "sections": [{"title": "A couple of cool findings from the physiology of exercise:", "anchor": "A_couple_of_cool_findings_from_the_physiology_of_exercise_", "level": 1}, {"title": "Maximal Strength Training Improves Running\u00a0Economy in Distance Runners", "anchor": "Maximal_Strength_Training_Improves_Running_Economy_in_Distance_Runners", "level": 1}, {"title": "How, exactly, does endurance training keep people healthy and alive?", "anchor": "How__exactly__does_endurance_training_keep_people_healthy_and_alive_", "level": 1}, {"title": "Closing thoughts:", "anchor": "Closing_thoughts_", "level": 1}, {"title": "An final anecdote of my own:", "anchor": "An_final_anecdote_of_my_own_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "32 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xAiMk4e7neP6Ah7FG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-24T17:51:18.000Z", "modifiedAt": null, "url": null, "title": "Your existence is informative", "slug": "your-existence-is-informative", "viewCount": null, "lastCommentedAt": "2020-01-22T01:41:49.142Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qAqbR9WTghEsSkTRL/your-existence-is-informative", "pageUrlRelative": "/posts/qAqbR9WTghEsSkTRL/your-existence-is-informative", "linkUrl": "https://www.lesswrong.com/posts/qAqbR9WTghEsSkTRL/your-existence-is-informative", "postedAtFormatted": "Sunday, June 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Your%20existence%20is%20informative&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYour%20existence%20is%20informative%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqAqbR9WTghEsSkTRL%2Fyour-existence-is-informative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Your%20existence%20is%20informative%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqAqbR9WTghEsSkTRL%2Fyour-existence-is-informative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqAqbR9WTghEsSkTRL%2Fyour-existence-is-informative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1226, "htmlBody": "<p><em>Cross posted from <a href=\"http://www.overcomingbias.com/2012/06/your-existence-is-informative.html\">Overcoming Bias</a>. Comments there.</em></p>\n<p style=\"text-align:center;\">***</p>\n<p><em>Warning: this post is technical.</em></p>\n<p>Suppose you know that there are a certain number of planets, N. You are unsure about the truth of a statement Q. If Q is true, you put a high probability on life forming on a given arbitrary planet. If Q is false, you put a low probability on this. You have a prior probability for Q. So far you have not taken into account your observation that the planet you are on has life. How do you update on this evidence, to get a posterior probability for Q? Since you don&#8217;t know which is &#8216;this&#8217; planet, with respect to the model, you can&#8217;t update directly on &#8216;there is life on this planet&#8217;, by excluding worlds where this planet doesn&#8217;t have life. And you can&#8217;t necessarily treat &#8216;this&#8217; as an arbitrary planet, since you wouldn&#8217;t have seen it if it didn&#8217;t have life.</p>\n<p>I have an ongoing disagreement with an associate who suggests that you should take &#8216;this planet has life&#8217; into account by conditioning on &#8216;there exists a planet with life&#8217;. That is,</p>\n<p style=\"text-align:center;\">P(Q|there is life on this planet) = P(Q|there exists a planet with life).</p>\n<p>Here I shall explain my disagreement.</p>\n<p>Nick Bostrom <a href=\"http://www.anthropic-principle.com/preprints/cos/big.html\">argues</a> persuasively that much science would be impossible if we treated &#8216;I observe X&#8217; as &#8216;someone observes X&#8217;. This is basically because in a big world of scientists making measurements, at some point somebody will make most mistaken measurements. So if all you know when you measure the temperature of a solution to be 15 degrees is that you are not in a world where nobody ever measures its temperature to be 15 degrees, this doesn&#8217;t tell you much about the temperature.</p>\n<p>You can add other apparently irrelevant observations you make at the same time &#8211; e.g. that the table is blue chipboard &#8211; in order to make your total observations less likely to arise once in a given world (at its limit, this is the suggestion of <a href=\"http://philsci-archive.pitt.edu/2888/\">FNC</a>). However it seems implausible that you should make different inferences from taking a measurement when you can also see a detailed but irrelevant picture at the same time than those you make with limited sensory input. Also the same problem re-emerges if the universe is supposed to be larger. Given that the universe is thought to be very, very large, this is a problem. Not to mention, it seems implausible that the size of the universe should greatly affect probabilistic judgements made about entities which are close to independent from most of the universe.</p>\n<p>So I think Bostrom&#8217;s case is good. However I&#8217;m not completely comfortable arguing from the acceptability of something that we do (science) back to the truth of the principles that justify it. So I&#8217;d like to make another case against taking &#8216;this planet has life&#8217; as equivalent evidence to &#8216;there exists a planet with life&#8217;.</p>\n<p>Evidence is what excludes possibilities. Seeing the sun shining is evidence against rain, because it excludes the possible worlds where the sky is grey, which include most of those where it is raining. Seeing a picture of the sun shining is not much evidence against rain, because it excludes worlds where you don&#8217;t see such a picture, which are about as likely to be rainy or sunny as those that remain are.</p>\n<p>Receiving the evidence &#8216;there exists a planet with life&#8217; means excluding all worlds where all planets are lifeless, and not excluding any other worlds. At first glance, this must be different from &#8216;this planet has life&#8217;. Take any possible world where some other planet has life, and this planet has no life. &#8216;There exists a planet with life&#8217; doesn&#8217;t exclude that world, while &#8216;this planet has life&#8217; does. Therefore they are different evidence.</p>\n<p>At this point however, note that the planets in the model have no distinguishing characteristics. How do we even decide which planet is &#8216;this planet&#8217; in another possible world? There needs to be some kind of mapping between planets in each world, saying which planet in world A corresponds to which planet in world B, etc. As far as I can tell, any mapping will do, as long as a given planet in one possible world maps to at most one planet in another possible world. This mapping is basically a definition choice.</p>\n<p>So suppose we use a mapping where in every possible world where at least one planet has life, &#8216;this planet&#8217; corresponds to one of the planets that has life. See the below image.</p>\n<div data-shortcode=\"caption\" id=\"attachment_29836\" style=\"width: 310px\" class=\"wp-caption aligncenter\"><a href=\"http://www.overcomingbias.com/wp-content/uploads/2012/06/which-world-is-this-world1.jpg\"><img class=\"size-medium wp-image-29836\" title=\"which planet is which\" src=\"https://i0.wp.com/www.overcomingbias.com/wp-content/uploads/2012/06/which-world-is-this-world1-300x91.jpg\" alt=\"Which planet is which?\" width=\"300\" height=\"91\" /></a><p class=\"wp-caption-text\">Squares are possible worlds, each with two planets. Pink planets have life, blue do not. Define &#8216;this planet&#8217; as the circled one in each case. Learning that there is life on this planet is equal to learning that there is life on some planet.</p></div>\n<p>Now learning that there exists a planet with life is the same as learning that this planet has life. Both exclude the far righthand possible world, and none of the other possible worlds. What&#8217;s more, since we can change the probability distribution we end up with, just by redefining which planets are &#8216;the same planet&#8217; across worlds, indexical evidence such as &#8216;this planet has life&#8217; must be horseshit.</p>\n<p>Actually the last paragraph was false. If in every possible world which contains life, you pick one of the planets with life to be &#8216;this planet&#8217;, you can no longer know whether you are in &#8216;this planet&#8217;. From your observations alone, you could be on the other planet, which only has life when both planets do. The one that is not circled in each of the above worlds. Whichever planet you are on, you know that there exists a planet with life. But because there&#8217;s some probability of you being on the planet which only rarely has life, you have more information than that. Redefining which planet was which didn&#8217;t change that.</p>\n<p>Perhaps a different definition of &#8216;this planet&#8217; would get what my associate wants? The problem with the last was that it no longer necessarily included the planet we are on. So what about we define &#8216;this planet&#8217; to be the one you are on, plus a life-containing planet in all of the other possible worlds that contain at least one life-containing planet. A strange, half-indexical definition, but why not? One thing remains to be specified &#8211; which is &#8216;this&#8217; planet when you don&#8217;t exist? Let&#8217;s say it is chosen randomly.</p>\n<p>Now is learning that &#8216;this planet&#8217; has life any different from learning that some planet has life? Yes. Now again there are cases where some planet has life, but it&#8217;s not the one you are on. This is because the definition only picks out planets with life across other possible worlds, not this one. In this one, &#8216;this planet&#8217; refers to the one you are on. If you don&#8217;t exist, this planet may not have life. Even if there are other planets that do. So again, &#8216;this planet has life&#8217; gives more information than &#8216;there exists a planet with life&#8217;.</p>\n<p>You either have to accept that someone else might exist when you do not, or you have to define &#8216;yourself&#8217; as something that always exists, in which case you no longer know whether you are &#8216;yourself&#8217;. Either way, changing definitions doesn&#8217;t change the evidence. Observing that you are alive tells you more than learning that &#8216;someone is alive&#8217;.</p><br />  <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gocomments/meteuphoric.wordpress.com/4836/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/comments/meteuphoric.wordpress.com/4836/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/godelicious/meteuphoric.wordpress.com/4836/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/delicious/meteuphoric.wordpress.com/4836/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gofacebook/meteuphoric.wordpress.com/4836/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/facebook/meteuphoric.wordpress.com/4836/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gotwitter/meteuphoric.wordpress.com/4836/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/twitter/meteuphoric.wordpress.com/4836/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gostumble/meteuphoric.wordpress.com/4836/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/stumble/meteuphoric.wordpress.com/4836/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/godigg/meteuphoric.wordpress.com/4836/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/digg/meteuphoric.wordpress.com/4836/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/goreddit/meteuphoric.wordpress.com/4836/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/reddit/meteuphoric.wordpress.com/4836/\" /></a> <img alt=\"\" border=\"0\" src=\"https://pixel.wp.com/b.gif?host=meteuphoric.wordpress.com&#038;blog=8643840&#038;post=4836&#038;subd=meteuphoric&#038;ref=&#038;feed=1\" width=\"1\" height=\"1\" />", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qAqbR9WTghEsSkTRL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 6, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "17337", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-24T19:09:29.002Z", "modifiedAt": null, "url": null, "title": "Empirical Sleep Time", "slug": "empirical-sleep-time", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:22.279Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rlpowell", "createdAt": "2009-03-05T00:57:26.519Z", "isAdmin": false, "displayName": "rlpowell"}, "userId": "nFgyJtHMChgKrhnvt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jKr4u9DBiQMQnu3R9/empirical-sleep-time", "pageUrlRelative": "/posts/jKr4u9DBiQMQnu3R9/empirical-sleep-time", "linkUrl": "https://www.lesswrong.com/posts/jKr4u9DBiQMQnu3R9/empirical-sleep-time", "postedAtFormatted": "Sunday, June 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Empirical%20Sleep%20Time&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEmpirical%20Sleep%20Time%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjKr4u9DBiQMQnu3R9%2Fempirical-sleep-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Empirical%20Sleep%20Time%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjKr4u9DBiQMQnu3R9%2Fempirical-sleep-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjKr4u9DBiQMQnu3R9%2Fempirical-sleep-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p>I'm thinking that it should be possible to decide when to sleep based on reduced performance.</p>\n<p>Can anyone suggest a tool for that purpose? &nbsp;Perhaps some reaction time testing software?</p>\n<p>I guess I would have to track myself during the day to make a baseline, which is fine.</p>\n<p>But without some sort of test I end up staying up way pass effectiveness, which is a waste of my time.</p>\n<p>-Robin</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jKr4u9DBiQMQnu3R9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 9.292659906446829e-07, "legacy": true, "legacyId": "17190", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-25T04:30:45.809Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Is Morality Given?", "slug": "seq-rerun-is-morality-given", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.225Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AhcgcctWkZoqwa3z3/seq-rerun-is-morality-given", "pageUrlRelative": "/posts/AhcgcctWkZoqwa3z3/seq-rerun-is-morality-given", "linkUrl": "https://www.lesswrong.com/posts/AhcgcctWkZoqwa3z3/seq-rerun-is-morality-given", "postedAtFormatted": "Monday, June 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Is%20Morality%20Given%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Is%20Morality%20Given%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAhcgcctWkZoqwa3z3%2Fseq-rerun-is-morality-given%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Is%20Morality%20Given%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAhcgcctWkZoqwa3z3%2Fseq-rerun-is-morality-given", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAhcgcctWkZoqwa3z3%2Fseq-rerun-is-morality-given", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>Today's post, <a href=\"/lw/ry/is_morality_given/\">Is Morality Given?</a> was originally published on 06 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Is_Morality_Given.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A dialogue on the idea that morality is an absolute external truth.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d9e/seq_rerun_is_morality_preference/\">Is Morality Preference?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AhcgcctWkZoqwa3z3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.295250065865278e-07, "legacy": true, "legacyId": "17194", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iQNKfYb7aRYopojTX", "PHLfkoRwjpCPyqbZQ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-25T04:58:47.681Z", "modifiedAt": null, "url": null, "title": "Four Short Stories on Error Checking", "slug": "four-short-stories-on-error-checking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:26.368Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "brilee", "createdAt": "2009-11-24T14:36:56.816Z", "isAdmin": false, "displayName": "brilee"}, "userId": "bbMiGjzXWpEqRMwe6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7HpAPDEMQTj77jktg/four-short-stories-on-error-checking", "pageUrlRelative": "/posts/7HpAPDEMQTj77jktg/four-short-stories-on-error-checking", "linkUrl": "https://www.lesswrong.com/posts/7HpAPDEMQTj77jktg/four-short-stories-on-error-checking", "postedAtFormatted": "Monday, June 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Four%20Short%20Stories%20on%20Error%20Checking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFour%20Short%20Stories%20on%20Error%20Checking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HpAPDEMQTj77jktg%2Ffour-short-stories-on-error-checking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Four%20Short%20Stories%20on%20Error%20Checking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HpAPDEMQTj77jktg%2Ffour-short-stories-on-error-checking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HpAPDEMQTj77jktg%2Ffour-short-stories-on-error-checking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 488, "htmlBody": "<p>&nbsp;</p>\n<p>(Cross-posted from my blag)</p>\n<p>1. My microwave clock has been broken since a power outage. It now reads something like 7 hours behind the real time. I've been too lazy to fix it, but the bright side is that it's 7 hours behind, and not 15 minutes behind. If it were 15 minutes behind, then who knows - I might mistake it to be the correct time, and end up fifteen minutes late to an appointment.</p>\n<p>&nbsp;</p>\n<p>2. During the early years of computing, scientists and engineers were responsible for running numerical simulations of various sorts. To do this, they needed randomized starting conditions. Von Neumann favored the <a href=\"http://en.wikipedia.org/wiki/Middle-square_method\">\"middle square\" method</a>. While this method was not a very good pseudo-random number generator, its speed made up for its shortcomings in those early days. Additionally, a useful property of the middle-square method was that when it fell into a short cycle, it was immediately obvious. While other methods may have had undetectable cycles of intermediate length, the middle-square method would invariably output legitimate pseudorandom numbers for some time, then fall into a cycle of length 1, 2 or 4. (1)</p>\n<p>&nbsp;</p>\n<p>3. One day, I was playing with some traffic models. My goal was to be able to correctly model the behavior of a line of cars as they accelerated from a standstill when the light turned from red to green. (2) I had collected some actual data at an intersection, and was planning to test my model against the data, as well as to fit some parameters. I ran a short program to fit these parameters and plot the actual times vs. my program's predicted times. To my astonishment, I had almost a perfect fit! Upon deeper inspection, it turned out that my program had merely gotten the times right by coincidence - while the cars behaved nicely long enough to get to the intersection, afterwards, their velocities oscillated with exponentially growing amplitude. I would have missed this if I had not insisted on checking the raw numbers from the simulation. I recoded my simulation and got a worse fit, but at least it didn't blow up as it had before.</p>\n<p>&nbsp;</p>\n<p>(This last story is fiction. Or so, I hope)</p>\n<p>4. The US News and World Report was doing its annual ranking of universities. They had recently changed the weightings on some of the subscores. Upon running their algorithm, an unexpected candidate rose to the top - Caltech! (Zing.) They concluded that there must have been a mistake with their algorithm, readjusted their weightings, and reran their algorithm. This time, Harvard rose to the top, as it should have. Happy with their results, US News and World Report published their university rankings and raked in a lot of dough.</p>\n<p>&nbsp;</p>\n<p>Notes:</p>\n<p>(1) Test the middle-square story. Is it actually true that all cycles are short? My <a href=\"https://dl.dropbox.com/u/34547557/middle-square.py\">code</a>, if you want to play with it.</p>\n<p>(2) The time for the nth car to reach the intersection is about 2*n seconds</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7HpAPDEMQTj77jktg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 9, "extendedScore": null, "score": 9.295379455887843e-07, "legacy": true, "legacyId": "17195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-25T11:10:22.098Z", "modifiedAt": null, "url": null, "title": "Seeking a \"Seeking Whence 'Seek Whence'\" Sequence", "slug": "seeking-a-seeking-whence-seek-whence-sequence", "viewCount": null, "lastCommentedAt": "2021-10-29T09:05:09.788Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c7KyL6G3482f9eGZ5/seeking-a-seeking-whence-seek-whence-sequence", "pageUrlRelative": "/posts/c7KyL6G3482f9eGZ5/seeking-a-seeking-whence-seek-whence-sequence", "linkUrl": "https://www.lesswrong.com/posts/c7KyL6G3482f9eGZ5/seeking-a-seeking-whence-seek-whence-sequence", "postedAtFormatted": "Monday, June 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeking%20a%20%22Seeking%20Whence%20'Seek%20Whence'%22%20Sequence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeking%20a%20%22Seeking%20Whence%20'Seek%20Whence'%22%20Sequence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7KyL6G3482f9eGZ5%2Fseeking-a-seeking-whence-seek-whence-sequence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeking%20a%20%22Seeking%20Whence%20'Seek%20Whence'%22%20Sequence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7KyL6G3482f9eGZ5%2Fseeking-a-seeking-whence-seek-whence-sequence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7KyL6G3482f9eGZ5%2Fseeking-a-seeking-whence-seek-whence-sequence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 899, "htmlBody": "<p>One of the sharpest and most important tools in the LessWrong cognitive toolkit is the idea of <em>going meta</em>, also called <em>seeking whence </em>or <em>jumping out of the system,</em>&nbsp;all terms crafted by Douglas Hofstadter<em>. </em>Though popularized by Hofstadter and repeatedly emphasized by Eliezer in posts like <a href=\"/lw/le/lost_purposes/\">\"Lost Purposes\"</a>&nbsp;and <a href=\"/lw/nu/taboo_your_words/\">\"Taboo Your Words\"</a>, Wikipedia indicates that similar ideas have been around in philosophy since at least Anaximander in the form of the <a href=\"http://en.wikipedia.org/wiki/Principle_of_sufficient_reason\">Principle of Sufficient Reason</a> (PSR). I think it'd be only appropriate to seek whence this idea of seeking whence, taking a history of ideas perspective. I'd also like analyses of where the theme shows up and why it's appealing and so on, since again it seems pretty important to LessWrong epistemology. Topics that I'd like to see discussed are:</p>\n<ol>\n<li>How <a href=\"http://yudkowsky.net/rational/bayes\">conservation of probability</a> in Bayesian probability theory and <a href=\"http://en.wikipedia.org/wiki/Liouville's_theorem_(Hamiltonian)\">conservation of phase space volume</a> in statistical mechanics are related&mdash;a summary of <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">Eliezer's post</a>s on the topic would be great.</li>\n<li>How conservation of probability &amp;c. are related to other physical/mathematical laws, e.g. <a href=\"http://en.wikipedia.org/wiki/Noether%27s_theorem\">Noether's theorem</a>&nbsp;and quantum mechanics' <a href=\"http://en.wikipedia.org/wiki/Continuity_equation#Quantum_mechanics\">continuity equation</a>.</li>\n<li>The history of the idea of conservation laws; whether the discovery of conservation laws was fueled by PSR-like philosophical-like concerns (e.g. Leibniz?), by lower level intuitive concerns, or other means.</li>\n<li>How conservation of probability &amp;c. are related to the idea of <a href=\"http://www.econ.tuwien.ac.at/hanappi/Lehre/EvoEco/hofstadter.pdf\">seeking whence</a>&nbsp;[pdf]&nbsp;(e.g.,&nbsp;<a href=\"/lw/pa/gazp_vs_glut/\">\"follow the improbability\"</a>).</li>\n<li>How the PSR relates to conservation of probability &amp;c. and to seeking whence.</li>\n<li>How going meta and seeking whence are related/equivalent.</li>\n<li>Which philosophers have used something like the PSR (e.g. <a href=\"http://plato.stanford.edu/entries/sufficient-reason/#Spi\">Spinoza</a>, <a href=\"http://plato.stanford.edu/entries/sufficient-reason/#Lei\">Leibniz</a>) and which haven't; those who haven't, what their reasons were for not using it.</li>\n<li>What kinds of conclusions are typically reached via the PSR or have historically been justified by the PSR, and whether those conclusions fit with LW's standard conclusions. If it disagrees with LW's standard conclusions, where does the PSR not apply or not apply as strongly; alternatively, why standard LW conclusions might be mistaken.</li>\n<li>Whether Schopenhauer's <a href=\"http://en.wikipedia.org/wiki/On_the_Fourfold_Root_of_the_Principle_of_Sufficient_Reason#The_Four_Classes\">four-fold division of the PSR</a> makes sense. (Schopenhauer's a relatively LW-friendly continentalesque philosopher.) A summary of any criticisms of his four-fold division. </li>\n<li>What makes the PSR, going meta, \"JOOTS\"-ing and seeking whence appealing, from a metaphysical, epistemological, pragmatic, and psychological perspective. What sorts of environments or problem sets select for it. (The <a href=\"http://en.wikipedia.org/wiki/Baldwin_effect\">Baldwin effect</a> and similar phenomena might be relevant.)</li>\n<li>What going meta / seeking whence looks like at different levels of organization; how one jumps out of systems at varying levels.</li>\n<li>Eliezer's <a href=\"http://intelligence.org/ourresearch/publications/CFAI/design/structure/causal.html\">rule of derivative validity from CFAI</a> and how it relates to the PSR; an analysis of how the (moral, or perhaps UDT-like decision-policy-centric) PSR might be relevant to <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">Friendliness philosophy</a>, e.g. as compared with <a href=\"http://intelligence.org/files/CEV.pdf\">CEV</a>-like proposals [pdf].</li>\n<li>How latent Platonic nodes in&nbsp;<a href=\"http://intelligence.org/files/TDT.pdf\">TDT</a>&nbsp;[pdf] (p. 78) relate to the PSR.</li>\n<li>A generalization of CFAI's causal validity semantics to&nbsp;<em>timeless</em>&nbsp;validity semantics in the spirit of the generalization of CDT to TDT, or perhaps even further generalizations of causal validity semantics in the spirit of <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless Decision Theory</a> or eXceptionless Decision Theory. (ETA: Whoops, Eliezer already discussed <a href=\"http://intelligence.org/ourresearch/publications/CFAI/design/structure/causal.html#acausal\">the acausal level</a>, but seems to have only mentioned Platonic forms as an afterthought. Maybe ignore this bullet point.)</li>\n<li>How the PSR and the rule of derivative validity relate to Robin Hanson's idea of <a href=\"http://www.overcomingbias.com/2006/12/why_common_prio.html\">pre-rationality</a> and Wei Dai's <a href=\"/lw/1e8/reflections_on_prerationality/\">questions about extending pre-rationality</a> to include past selves' utility functions&mdash;whether this elucidates <a href=\"/lw/15m/towards_a_new_decision_theory/11js\">the relation between XDT and UDT</a>.</li>\n<li>Where Hofstadter picked up the idea of \"going meta\" and what led him to think it was important. What led Eliezer to rely on it so much and emphasize the importance of avoiding lost purposes.</li>\n</ol>\n<div>Does anyone have the mathematical, historical, philosophical, and research skills necessary to write a very long post or two or a sequence on this? How about just the skills required to tackle one of the above questions, or a question like those?</div>\n<div>Might people with relevant knowledge share it in comments on this post? I'll try to write a few comments to seed discussion of some of the above questions.</div>\n<div>(Maybe best discussed elsewhere, but: What other cognitive tools or themes often used on LessWrong have a long history that we don't all know even exists?)</div>\n<div>I'd also just generally appreciate posts on going meta, especially with regards to epistemic rationality&mdash;discussion of \"going meta\" in the domain of instrumental rationality tends to devolve into boring 'experiment more!' 'no, go meta more!' back-and-forths without any accompanying explicit &nbsp;<a href=\"/lw/85x/value_of_information_four_examples/\">VOI calculations</a>. With epistemic rationality the benefits are clearer cut. When people reflect and try to go meta discussions tend to go way better&mdash;e.g., when people are explicitly aware that \"politics is the mind-killer\" they're less likely to start a distracting flame war, and more likely to discuss interesting factual issues. (Please, let's avoid discussion of \"politics is the mind killer\" politics here. ;P ) I think this sort of effect could be increased across the board, especially for mildly political subjects like whether to donate to x-risk charities, and that this would increase the quality of most arguments on LessWrong. Although much of the relevant knowledge needed to go meta there is knowledge of signaling game theory and social psychology, and can't be replaced &nbsp;<em>simply</em>&nbsp; by applying a skill like 'go meta', going meta and staying reflective can still mitigate many obvious failure modes. Posts giving examples of (hypothetical) debates and how the debaters could go meta could be really valuable.</div>\n<div>Thanks all!</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5oDii8KKW53n4HSx4": 2, "QPt5ECwTCAg63mbNu": 2, "xgpBASEThXPuKRhbS": 2, "Xw6pxiicjuv6NJWjf": 2, "DHZhAxgE5edGxxh8d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c7KyL6G3482f9eGZ5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 37, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "17210", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sP2Hg6uPwpfp3jZJN", "WBdvyyHLdxZSAMmoz", "QkX2bAkwG2EpGvNug", "k6EPphHiBH4WWYFCj", "J8LTE8CTQfyEMkhnc", "vADtvr9iDeYsCDfxd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-25T17:25:55.048Z", "modifiedAt": null, "url": null, "title": "Framing a problem in a foreign language seems to reduce decision biases", "slug": "framing-a-problem-in-a-foreign-language-seems-to-reduce", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:26.642Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oohAJqJx6fSito3ja/framing-a-problem-in-a-foreign-language-seems-to-reduce", "pageUrlRelative": "/posts/oohAJqJx6fSito3ja/framing-a-problem-in-a-foreign-language-seems-to-reduce", "linkUrl": "https://www.lesswrong.com/posts/oohAJqJx6fSito3ja/framing-a-problem-in-a-foreign-language-seems-to-reduce", "postedAtFormatted": "Monday, June 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Framing%20a%20problem%20in%20a%20foreign%20language%20seems%20to%20reduce%20decision%20biases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFraming%20a%20problem%20in%20a%20foreign%20language%20seems%20to%20reduce%20decision%20biases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoohAJqJx6fSito3ja%2Fframing-a-problem-in-a-foreign-language-seems-to-reduce%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Framing%20a%20problem%20in%20a%20foreign%20language%20seems%20to%20reduce%20decision%20biases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoohAJqJx6fSito3ja%2Fframing-a-problem-in-a-foreign-language-seems-to-reduce", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoohAJqJx6fSito3ja%2Fframing-a-problem-in-a-foreign-language-seems-to-reduce", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<blockquote>\n<p><span style=\"font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 14px; line-height: 19px;\">The researchers aren't entirely sure why speaking in a less familiar tongue makes people more \"rational\", in the sense of not being affected by framing effects or loss aversion. But they think it may have to do with creating psychological distance, encouraging systematic rather than automatic thinking, and with reducing the emotional impact of decisions. This would certainly fit with past research that's shown the emotional impact of swear words, expressions of love and adverts is diminished when they're presented in a less familiar language.</span></p>\n</blockquote>\n<p>Paywalled article (can someone with access throw a PDF up on dropbox or something?): http://pss.sagepub.com/content/early/2012/04/18/0956797611432178</p>\n<p>Blog summary:&nbsp;http://bps-research-digest.blogspot.co.uk/2012/06/we-think-more-rationally-in-foreign.html</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oohAJqJx6fSito3ja", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -8, "extendedScore": null, "score": 9.29882929565166e-07, "legacy": true, "legacyId": "17211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-26T01:19:59.083Z", "modifiedAt": null, "url": null, "title": "\"Extortionate\" strategy beats tit-for-tat in iterated Prisoner's Dilemma", "slug": "extortionate-strategy-beats-tit-for-tat-in-iterated-prisoner", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:34.614Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "teageegeepea", "createdAt": "2009-03-03T02:22:06.078Z", "isAdmin": false, "displayName": "teageegeepea"}, "userId": "vAWMkXNGgqcyFohPZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J94tzjjtwQjifbm2W/extortionate-strategy-beats-tit-for-tat-in-iterated-prisoner", "pageUrlRelative": "/posts/J94tzjjtwQjifbm2W/extortionate-strategy-beats-tit-for-tat-in-iterated-prisoner", "linkUrl": "https://www.lesswrong.com/posts/J94tzjjtwQjifbm2W/extortionate-strategy-beats-tit-for-tat-in-iterated-prisoner", "postedAtFormatted": "Tuesday, June 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Extortionate%22%20strategy%20beats%20tit-for-tat%20in%20iterated%20Prisoner's%20Dilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Extortionate%22%20strategy%20beats%20tit-for-tat%20in%20iterated%20Prisoner's%20Dilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ94tzjjtwQjifbm2W%2Fextortionate-strategy-beats-tit-for-tat-in-iterated-prisoner%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Extortionate%22%20strategy%20beats%20tit-for-tat%20in%20iterated%20Prisoner's%20Dilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ94tzjjtwQjifbm2W%2Fextortionate-strategy-beats-tit-for-tat-in-iterated-prisoner", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ94tzjjtwQjifbm2W%2Fextortionate-strategy-beats-tit-for-tat-in-iterated-prisoner", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<p>Less Wrong had a <a href=\"http://wiki.lesswrong.com/wiki/Prisoner's_dilemma\">Prisoner's Dilemma</a> contest <a href=\"/lw/cnc/another_iterated_prisoners_dilemma_tournament/\">some time back</a>, whose results I've forgotten. Perhaps it should be rerun with <a href=\"http://edge.org/conversation/on-iterated-prisoner-dilemma\">William H. Press &amp; Freeman Dyson's proposed extortionate strategies</a>.</p>\n<p>I hope Pinker gives a response at Edge.org, since P.D played a significant role his book \"The Better Angels of Our Nature\" as a source of morality <a href=\"http://entitledtoanopinion.wordpress.com/2012/06/03/better-angels-of-our-nature/#prisonersdilemma\">embedded in the nature of logic/reality</a>.</p>\n<p>Hat-tip to <a href=\"http://marginalrevolution.com/marginalrevolution/2012/06/assorted-links-489.html\">Marginal Revolution</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J94tzjjtwQjifbm2W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 4, "extendedScore": null, "score": 9.301019477805782e-07, "legacy": true, "legacyId": "17213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gdEA4oZX5xKRmScJ8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-26T03:41:43.743Z", "modifiedAt": null, "url": null, "title": "[META] [POLL] Comment author/Post author?", "slug": "meta-poll-comment-author-post-author", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dorikka", "createdAt": "2010-12-11T03:34:20.472Z", "isAdmin": false, "displayName": "Dorikka"}, "userId": "HJB33ckc8NzPbvJYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZwrHi9erjmLYxJW2B/meta-poll-comment-author-post-author", "pageUrlRelative": "/posts/ZwrHi9erjmLYxJW2B/meta-poll-comment-author-post-author", "linkUrl": "https://www.lesswrong.com/posts/ZwrHi9erjmLYxJW2B/meta-poll-comment-author-post-author", "postedAtFormatted": "Tuesday, June 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20%5BPOLL%5D%20Comment%20author%2FPost%20author%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20%5BPOLL%5D%20Comment%20author%2FPost%20author%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZwrHi9erjmLYxJW2B%2Fmeta-poll-comment-author-post-author%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20%5BPOLL%5D%20Comment%20author%2FPost%20author%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZwrHi9erjmLYxJW2B%2Fmeta-poll-comment-author-post-author", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZwrHi9erjmLYxJW2B%2Fmeta-poll-comment-author-post-author", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<p>I'm noticing that comments are now shown with 'Comment author:' besides the commenter's name while posts appear to be shown with 'Post author:' beside the author's name (on the actual page of the post, not the 'New' page). This is mildly aesthetically displeasing to me. I'm curious whether other people share my opinion and, if so, whether there was another motivation for the change.</p>\n<p>I've posted a poll below. I'd appreciate if everyone who views this post votes on something, even if it's the 'No preference.' option, so that the information is more accurate.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZwrHi9erjmLYxJW2B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "17215", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-26T04:23:28.016Z", "modifiedAt": null, "url": null, "title": "Some simple existential risk math", "slug": "some-simple-existential-risk-math", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:54.138Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tPuZEnFnNJ5RmP3b8/some-simple-existential-risk-math", "pageUrlRelative": "/posts/tPuZEnFnNJ5RmP3b8/some-simple-existential-risk-math", "linkUrl": "https://www.lesswrong.com/posts/tPuZEnFnNJ5RmP3b8/some-simple-existential-risk-math", "postedAtFormatted": "Tuesday, June 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20simple%20existential%20risk%20math&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20simple%20existential%20risk%20math%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtPuZEnFnNJ5RmP3b8%2Fsome-simple-existential-risk-math%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20simple%20existential%20risk%20math%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtPuZEnFnNJ5RmP3b8%2Fsome-simple-existential-risk-math", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtPuZEnFnNJ5RmP3b8%2Fsome-simple-existential-risk-math", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 689, "htmlBody": "<p>What's the best way to allocate resources towards decreasing existential risk?</p>\n<p>Let's say we have n different risks.&nbsp;Define the functions r1(s1), r2(s2), ... , rn(sn), one for each risk. Each takes USD spent mitigating that particular risk as its input and outputs the probability that particular catastrophic scenario occurs. Probability of survival (\"win probability\") as a function of spending levels is given by</p>\n<p>w(s1, s2, ... , sn) = (1 -&nbsp;r1(s1)) *&nbsp;(1 -&nbsp;r2(s2)) * ... *&nbsp;(1 -&nbsp;rn(sn))</p>\n<p>In other words, we win if none of the catastrophic scenarios occur.</p>\n<p>What are the characteristics of&nbsp;r1(s1), r2(s2), ... , rn(sn)?</p>\n<p>Well obviously their output has to be between 0 and 1, since they are returning probabilities. And, assuming that spending money on this risk actually helps, their output values will go down as their input values go up.</p>\n<p>Furthermore: For any given existential risk, we could imagine a number of different interventions. Each intervention will decrease the risk by a certain amount, and each has an associated price tag. Those interventions that are \"efficient\" have an especially high risk reduction to cost ratio. Assuming the people doing the existential risk reduction spending are good at picking efficient interventions, we should expect them to encounter diminishing returns as they spend more.</p>\n<p>And: No risk's probability can go below zero, so if we assume the value of a risk function decreases monotonically, then the risk function must approach zero asymptotically, suggesting that heavily diminishing returns will be reached at some point.</p>\n<p>What are the characteristics of&nbsp;w(s1, s2, ... , sn)?</p>\n<p>It's pretty easy to prove that if you can decrease any risk by a constant amount, you are best off applying the decrease to the catastrophe that has the highest probability of occurring.</p>\n<p>A quick example calculation: Let's say you have three risks which you are 80%, 90%, and 90% likely to avert. Decreasing the last risk by 10% yields a win probability of 72%. Decreasing the first risk by 10% yields a win probability of 72.9%, almost a 1% increase.</p>\n<p>Of course, cutting the probability of a risk that's 20% likely to occur in half is more realistic than completely eliminating a risk that's 10% likely to occur. So this favors working to reduce the risk that's 20% likely to occur even more.</p>\n<p><strong>Diagram</strong>:&nbsp;This is a two-risk model. The graph is z=(1-x)(1-y). At the yellow point, the green risk is more or less under control and a step of constant size is best taken to reduce the blue risk.</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://i.imgur.com/lPjP1.png\" alt=\"\" width=\"231\" height=\"216\" /></p>\n<h2>Uncertainty</h2>\n<p>All this analysis has assumed that risk probabilities can be estimated perfectly. But clearly this isn't the case.</p>\n<p>If our prior is the same for all risks, and available evidence doesn't give us much reason to move away from our priors, this analysis suggests that available existential risk reduction funds should be divided between different risks roughly evenly if there's a large quantity of such funds.</p>\n<h2>AI</h2>\n<p>In my mind, the best argument that existential risks are significant is the Great Filter. In a nutshell: our galaxy has tons of planets and it's been around for a long time, but Earth was uncolonized when we evolved. So clearly there are one or more very substantial sticking points between \"lifeless rock\" and \"resource-gobbling spacefaring civilization\". Maybe these sticking points are mostly behind us, maybe they are mostly ahead of us.</p>\n<p>Katja Grace <a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">argues</a> that the Great Filter is ahead of us, and that <a href=\"http://meteuphoric.wordpress.com/2010/11/11/sia-says-ai-is-no-big%C2%A0threat/\">AI is unlikely to be responsible for the great filter</a>.</p>\n<p>On the other hand, Nick Bostrom has argued (not sure where) that superintelligent FAI could trivially deal with existential risks if created. It may be that shooting for FAI is a better strategy than combating the great filter directly, especially if this would count as an unconventional strategy compared to what previous civilizations have tried.</p>\n<h2>Potential problems with my model</h2>\n<p>My model ignores interventions that could decrease the probability of multiple existential risks at once, e.g. funding a popular book about the Great Filter or a TV show that raises the sanity waterline.</p>\n<p>I also implicitly assumed that interventions operate independently from one another at the single-risk level.</p>\n<p>In general, it may be more useful to think about existential risk on the level of possible interventions then on the level of potential catastrophes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tPuZEnFnNJ5RmP3b8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "17214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p>What's the best way to allocate resources towards decreasing existential risk?</p>\n<p>Let's say we have n different risks.&nbsp;Define the functions r1(s1), r2(s2), ... , rn(sn), one for each risk. Each takes USD spent mitigating that particular risk as its input and outputs the probability that particular catastrophic scenario occurs. Probability of survival (\"win probability\") as a function of spending levels is given by</p>\n<p>w(s1, s2, ... , sn) = (1 -&nbsp;r1(s1)) *&nbsp;(1 -&nbsp;r2(s2)) * ... *&nbsp;(1 -&nbsp;rn(sn))</p>\n<p>In other words, we win if none of the catastrophic scenarios occur.</p>\n<p>What are the characteristics of&nbsp;r1(s1), r2(s2), ... , rn(sn)?</p>\n<p>Well obviously their output has to be between 0 and 1, since they are returning probabilities. And, assuming that spending money on this risk actually helps, their output values will go down as their input values go up.</p>\n<p>Furthermore: For any given existential risk, we could imagine a number of different interventions. Each intervention will decrease the risk by a certain amount, and each has an associated price tag. Those interventions that are \"efficient\" have an especially high risk reduction to cost ratio. Assuming the people doing the existential risk reduction spending are good at picking efficient interventions, we should expect them to encounter diminishing returns as they spend more.</p>\n<p>And: No risk's probability can go below zero, so if we assume the value of a risk function decreases monotonically, then the risk function must approach zero asymptotically, suggesting that heavily diminishing returns will be reached at some point.</p>\n<p>What are the characteristics of&nbsp;w(s1, s2, ... , sn)?</p>\n<p>It's pretty easy to prove that if you can decrease any risk by a constant amount, you are best off applying the decrease to the catastrophe that has the highest probability of occurring.</p>\n<p>A quick example calculation: Let's say you have three risks which you are 80%, 90%, and 90% likely to avert. Decreasing the last risk by 10% yields a win probability of 72%. Decreasing the first risk by 10% yields a win probability of 72.9%, almost a 1% increase.</p>\n<p>Of course, cutting the probability of a risk that's 20% likely to occur in half is more realistic than completely eliminating a risk that's 10% likely to occur. So this favors working to reduce the risk that's 20% likely to occur even more.</p>\n<p><strong>Diagram</strong>:&nbsp;This is a two-risk model. The graph is z=(1-x)(1-y). At the yellow point, the green risk is more or less under control and a step of constant size is best taken to reduce the blue risk.</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://i.imgur.com/lPjP1.png\" alt=\"\" width=\"231\" height=\"216\"></p>\n<h2 id=\"Uncertainty\">Uncertainty</h2>\n<p>All this analysis has assumed that risk probabilities can be estimated perfectly. But clearly this isn't the case.</p>\n<p>If our prior is the same for all risks, and available evidence doesn't give us much reason to move away from our priors, this analysis suggests that available existential risk reduction funds should be divided between different risks roughly evenly if there's a large quantity of such funds.</p>\n<h2 id=\"AI\">AI</h2>\n<p>In my mind, the best argument that existential risks are significant is the Great Filter. In a nutshell: our galaxy has tons of planets and it's been around for a long time, but Earth was uncolonized when we evolved. So clearly there are one or more very substantial sticking points between \"lifeless rock\" and \"resource-gobbling spacefaring civilization\". Maybe these sticking points are mostly behind us, maybe they are mostly ahead of us.</p>\n<p>Katja Grace <a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">argues</a> that the Great Filter is ahead of us, and that <a href=\"http://meteuphoric.wordpress.com/2010/11/11/sia-says-ai-is-no-big%C2%A0threat/\">AI is unlikely to be responsible for the great filter</a>.</p>\n<p>On the other hand, Nick Bostrom has argued (not sure where) that superintelligent FAI could trivially deal with existential risks if created. It may be that shooting for FAI is a better strategy than combating the great filter directly, especially if this would count as an unconventional strategy compared to what previous civilizations have tried.</p>\n<h2 id=\"Potential_problems_with_my_model\">Potential problems with my model</h2>\n<p>My model ignores interventions that could decrease the probability of multiple existential risks at once, e.g. funding a popular book about the Great Filter or a TV show that raises the sanity waterline.</p>\n<p>I also implicitly assumed that interventions operate independently from one another at the single-risk level.</p>\n<p>In general, it may be more useful to think about existential risk on the level of possible interventions then on the level of potential catastrophes.</p>", "sections": [{"title": "Uncertainty", "anchor": "Uncertainty", "level": 1}, {"title": "AI", "anchor": "AI", "level": 1}, {"title": "Potential problems with my model", "anchor": "Potential_problems_with_my_model", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-26T05:37:30.404Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Where Recursive Justification Hits Bottom", "slug": "seq-rerun-where-recursive-justification-hits-bottom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.809Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QN3syTa8jPksGNbrE/seq-rerun-where-recursive-justification-hits-bottom", "pageUrlRelative": "/posts/QN3syTa8jPksGNbrE/seq-rerun-where-recursive-justification-hits-bottom", "linkUrl": "https://www.lesswrong.com/posts/QN3syTa8jPksGNbrE/seq-rerun-where-recursive-justification-hits-bottom", "postedAtFormatted": "Tuesday, June 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Where%20Recursive%20Justification%20Hits%20Bottom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Where%20Recursive%20Justification%20Hits%20Bottom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQN3syTa8jPksGNbrE%2Fseq-rerun-where-recursive-justification-hits-bottom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Where%20Recursive%20Justification%20Hits%20Bottom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQN3syTa8jPksGNbrE%2Fseq-rerun-where-recursive-justification-hits-bottom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQN3syTa8jPksGNbrE%2Fseq-rerun-where-recursive-justification-hits-bottom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<p>Today's post, <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">Where Recursive Justification Hits Bottom</a> was originally published on 08 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Ultimately, when you reflect on how your mind operates, and consider questions like \"why does occam's razor work?\" and \"why do I expect the future to be like the past?\", you have no other option but to use your own mind. There is no way to jump to an ideal state of pure emptiness and evaluate these claims without using your existing mind.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/d9m/seq_rerun_is_morality_given/\">Is Morality Given?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QN3syTa8jPksGNbrE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 9.302209606739453e-07, "legacy": true, "legacyId": "17221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["C8nEXTcjZb9oauTCW", "AhcgcctWkZoqwa3z3", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-26T08:31:53.427Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 6/25/12", "slug": "group-rationality-diary-6-25-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:35.469Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RpCh8eLjwbN6KFfQG/group-rationality-diary-6-25-12", "pageUrlRelative": "/posts/RpCh8eLjwbN6KFfQG/group-rationality-diary-6-25-12", "linkUrl": "https://www.lesswrong.com/posts/RpCh8eLjwbN6KFfQG/group-rationality-diary-6-25-12", "postedAtFormatted": "Tuesday, June 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%206%2F25%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%206%2F25%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRpCh8eLjwbN6KFfQG%2Fgroup-rationality-diary-6-25-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%206%2F25%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRpCh8eLjwbN6KFfQG%2Fgroup-rationality-diary-6-25-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRpCh8eLjwbN6KFfQG%2Fgroup-rationality-diary-6-25-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 227, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the public group instrumental rationality diary for the week of June 25th. (Based on the rate of participation in prior threads, I thought it might be a good idea to start posting every other week instead of every week. &nbsp;Only so much new stuff happens in a week.) It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">(Previously:&nbsp;<a style=\"color: #8a8a8b; \" href=\"/lw/cew/group_rationality_diary_51412/\">5/14/12</a>,&nbsp;<a style=\"color: #8a8a8b; \" href=\"/r/discussion/lw/ckm/group_rationality_diary_52112/\">5/21/12</a>,&nbsp;<span style=\"color: #8a8a8b; \"><a style=\"color: #8a8a8b; \" href=\"/r/discussion/lw/cpg/group_rationality_diary_52812\">5/28/12</a>,&nbsp;<a style=\"color: #8a8a8b; \" href=\"/lw/cvk/group_rationality_diary_6412/\">6/4/12</a></span>, <a title=\"6/11/12\" href=\"/lw/d0v/group_rationality_diary_61112/\">6/11/12)</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RpCh8eLjwbN6KFfQG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 9.303015666932494e-07, "legacy": true, "legacyId": "17228", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JA2MnwunfZnFXb3by", "aFRC3J3TNmKMf2p4T", "RpeHoK89Ra3hBR26r", "GBxYEuDpyRPvZaN22", "4B6Zn4up5xPHTq5iC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-26T12:36:38.339Z", "modifiedAt": null, "url": null, "title": "A (small) critique of total utilitarianism", "slug": "a-small-critique-of-total-utilitarianism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:00.548Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pyTuR4ZbLfqpS2oMh/a-small-critique-of-total-utilitarianism", "pageUrlRelative": "/posts/pyTuR4ZbLfqpS2oMh/a-small-critique-of-total-utilitarianism", "linkUrl": "https://www.lesswrong.com/posts/pyTuR4ZbLfqpS2oMh/a-small-critique-of-total-utilitarianism", "postedAtFormatted": "Tuesday, June 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20(small)%20critique%20of%20total%20utilitarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20(small)%20critique%20of%20total%20utilitarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpyTuR4ZbLfqpS2oMh%2Fa-small-critique-of-total-utilitarianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20(small)%20critique%20of%20total%20utilitarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpyTuR4ZbLfqpS2oMh%2Fa-small-critique-of-total-utilitarianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpyTuR4ZbLfqpS2oMh%2Fa-small-critique-of-total-utilitarianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3249, "htmlBody": "<p>In total utilitarianism, it is a morally neutral act to kill someone (in a painless and unexpected manner) and creating/giving birth to another being of comparable happiness (or preference satisfaction or welfare). In fact if one can kill a billion people to create a billion and one, one is morally compelled to do so. And this is true for real people, not just thought experiment people - living people with dreams, aspirations, grudges and annoying or endearing quirks. To avoid causing extra pain to those left behind, it is better that you kill off whole families and communities, so that no one is left to mourn the dead. In fact the most morally compelling act would be to kill off the whole of the human species, and replace it with a slightly larger population.</p>\n<p>We have many real world analogues to this thought experiment. For instance, it seems that there is only a small difference between the happiness of richer nations and poorer nations, while the first consume many more&nbsp;resources&nbsp;than the second. Hence to increase utility we should simply kill off all the rich, and let the poor multiply to take their place (continually bumping off any of the poor that gets too rich). Of course, the rich world also produces most of the farming surplus and the technology&nbsp;innovation, which allow us to support a larger population. So we should aim to kill everyone in the rich world apart from farmers and scientists - and enough support staff to keep these professions running (Carl Shulman correctly <a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/6wmr\">points out</a>&nbsp;that we may require most of the rest of the economy as \"support staff\". Still, it's very likely that we could kill off a significant segment of the population - those with the highest consumption relative to their impact of farming and science - and still \"improve\" the situation).</p>\n<p>Even if turns out to be problematic to implement in practice, a true total utilitarian should be thinking: \"I really, really wish there was a way to do targeted killing of many people in the USA, Europe and Japan, large parts of Asia and Latin America and some parts of Africa - it makes me sick to the stomach to think that I can't do that!\" Or maybe: \"I really really wish I could make everyone much poorer without affecting the size of the economy - I wake up at night with nightmare because these people remain above the poverty line!\"</p>\n<p>I won't belabour the point. I find those actions personally&nbsp;repellent, and I believe that nearly everyone finds them somewhat repellent or at least did so at some point in their past. This doesn't mean that it's the wrong thing to do - after all, the accepted answer to the <a href=\"/lw/kn/torture_vs_dust_specks/\">torture vs dust speck</a>&nbsp;dilemma&nbsp;feels intuitively wrong, at least the first time. It does mean, however, that there must be very strong&nbsp;countervailing&nbsp;arguments to balance out this initial repulsion (maybe even a mathematical theorem). For without that... how to justify all this killing?</p>\n<p>Hence for the rest of this post, I'll be arguing that total utilitarianism is built on a foundation of dust, and thus provides no reason to go against your initial intuitive judgement in these problems. The points will be:<a id=\"more\"></a></p>\n<ol>\n<li>Bayesianism and the fact that you should follow a&nbsp;<em>utility function</em> in no way compel you towards total <em>utilitarianism</em>. The similarity in names does not mean the concepts are on similarly rigorous foundations.</li>\n<li>Total utilitarianism is neither a simple, nor an elegant theory. In fact, it is under-defined and arbitrary.</li>\n<li>The most&nbsp;compelling&nbsp;argument for total utilitarianism (basically the one that establishes the <a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">repugnant conclusion</a>), is a very long chain of imperfect reasoning, so there is no reason for the conclusion to be solid.</li>\n<li>Considering the preferences of non-existent beings does not establish total&nbsp;utilitarianism.</li>\n<li>When considering competing moral theories, total utilitarianism does not \"win by default\" thanks to its large values as the population increases.</li>\n<li>Population ethics is <em>hard</em>, just as normal ethics is.</li>\n</ol>\n<p>&nbsp;</p>\n<h2>A utility function does not compel total (or average) utilitarianism</h2>\n<p>There are strong reasons to suspect that the best decision process is one that maximises expected utility for a particular utility function. Any process that does not do so, <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">leaves itself open</a> to be <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/\">money pumped</a> or taken advantage of. This point has been reiterated again and again on Less Wrong, and rightly so.</p>\n<p>Your utility function must be over states of the universe - but that's the only restriction. The theorem says nothing further about the content of your utility function. If you prefer a world with a trillion&nbsp;ecstatic&nbsp;super-humans to one with a septillion subsistence farmers - or vice versa - then as long you maximise your expected utility, the money pumps can't touch you, and the standard Bayesian arguments don't influence you to change your mind. Your values are fully rigorous.</p>\n<p>For instance, in the&nbsp;<a href=\"/lw/kn/torture_vs_dust_specks/\">torture vs dust speck</a>&nbsp;scenario, average utilitarianism also compels you to take torture, as do a host of other possible utility functions. A lot of arguments around this subject,&nbsp;that may&nbsp;implicitly&nbsp;feel to be in favour of total utilitarianism, turn out to be nothing of the sort. For instance, avoiding <a href=\"/lw/hw/scope_insensitivity/\">scope&nbsp;insensitivity</a>&nbsp;does not compel you to total utilitarianism, and you can&nbsp;perfectly&nbsp;allow birth-death asymmetries or similar intuitions, while remaining an expected utility maximiser.</p>\n<p>&nbsp;</p>\n<h2>Total utilitarianism is not simple nor elegant, but is arbitrary</h2>\n<p>Total utilitarianism is defined as maximising the sum of everyone's individual utility function. That's a simple definition. But what are these individual utility functions? Do people act like expected utility maximisers? In a word... <a href=\"http://en.wikipedia.org/wiki/Behavioral_economics\">no</a>. In another word... <a href=\"http://www.mind.org.uk/help/diagnoses_and_conditions/depression\">NO</a>. In yet another word... <a href=\"http://www.darwinawards.com/\">NO</a>!</p>\n<p>So what are these utilities? Are they the utility that the individuals \"should have\"?&nbsp;According&nbsp;to what and who's criteria?&nbsp;Is it \"welfare\"? How is that defined? Is it happiness? Again, how is that defined? Is it preferences? On what scale? And what if the individual disagrees with the utility they are supposed to have? What if their revealed preferences are different again?</p>\n<p>There are (various different) ways to start resolving these problems, and philosophers have spent a lot of ink and time doing so. The point remains that total utilitarianism cannot claim to be a simple theory, if the objects that it sums over are so poorly and&nbsp;controversially&nbsp;defined.</p>\n<p>And the sum itself is a huge problem. There is no natural scale on which to compare utility functions. Divide one utility function by a billion, multiply the other by e<sup>&pi;</sup>, and they are still perfectly valid utility functions. In a study group at the FHI, we've been looking at various ways of combining utility functions - equivalently, of doing <a href=\"http://en.wikipedia.org/wiki/Social_choice_theory\">interpersonal utility comparisons</a>&nbsp;(IUC). Turns out it's very hard, there seems no natural way of doing this, and a lot has also been written about this, concluding little. Unless your theory comes with a particular IUC method, the only way of summing these utilities is to do an essentially arbitrary choice for each individual before summing. Thus standard total utilitarianism is an arbitrary sum of ill defined, non-natural&nbsp;objects.</p>\n<p>Why then is it so popular? Well, one reason is that there are models that make use of something like total utilitarianism to great effect. Classical economic theory, for instance, models everyone as perfectly rational expected utility maximisers. It gives good predictions - but it remains a model, with a domain of validity. You wouldn't conclude from that economic model that, say, mental illnesses don't exist. Similarly,&nbsp;modelling&nbsp;each life as having the same value and maximising expected lives saved is sensible and intuitive in many <a href=\"/lw/n3/circular_altruism/\">scenarios</a>&nbsp;- but not necessarily all.</p>\n<p>Maybe if we had a bit more information about the affected populations, we could use a more sophisticated model, such as one incorporating <a href=\"http://en.wikipedia.org/wiki/Quality-adjusted_life_year\">quality adjusted life years</a>&nbsp;(QALY). Or maybe we could let other factors affect our thinking - what if we had to choose between saving a population of 1000 versus a population of 1001, of same average QALYs, but where the first set contained the entire&nbsp;<a href=\"http://en.wikipedia.org/wiki/Aw%C3%A1-Guaj%C3%A1_people\">Aw&aacute;</a> tribe/culture of 300 people, and the second is made up of representatives from much larger ethnic groups, much more culturally replaceable? Should we let that influence our decision? Well maybe we should, maybe we shouldn't, but it would be wrong to say \"well, I would really like to save the&nbsp;Aw&aacute;, but the model I settled on earlier won't allow me to, so I best follow the model\". The models are there precisely to model our moral&nbsp;intuitions (the clue is in the name), not freeze them.</p>\n<p>&nbsp;</p>\n<h2>The repugnant conclusion is at the end of a flimsy chain</h2>\n<p>There is a seemingly sound argument for the <a href=\"http://en.wikipedia.org/wiki/Repugnant_Conclusion\">repugnant conclusion</a>, which goes some way towards making total utilitarianism plausible. It goes like this:</p>\n<ol>\n<li>Start with a population of very happy/utilitied/welfared/preference satisfied people.</li>\n<li>Add other people whose lives are worth living, but whose average \"utility\" is less than that of the initial population.</li>\n<li>Redistribute \"utility\" in an egalitarian way across the whole population, increasing the average a little as you do so (but making sure the top rank have their utility lowered).</li>\n<li>Repeat as often as required.</li>\n<li>End up with a huge population whose lives are barely worth living.</li>\n</ol>\n<p>If all these steps increase the quality of the outcome (and it seems intuitively that they do), then the end state much be better than the&nbsp;starting&nbsp;state, agreeing with total&nbsp;utilitarianism. So, what could go wrong with this reasoning? Well, as seen before, the term \"utility\" is very much undefined, as is its scale - hence&nbsp;egalitarian&nbsp;is extremely undefined. So this argument is not&nbsp;mathematically&nbsp;precise, its rigour is illusionary. And when you recast the argument in qualitative terms, as you must, it become much weaker.</p>\n<p>Going through the&nbsp;iteration, there will come a point when the human world is going to lose its last anime, its&nbsp;last opera, its last copy of the Lord of the Rings, its last&nbsp;mathematics, its last online discussion board, its last football game - anything that might cause more-than-appropriate enjoyment. At that stage, would you be entirely sure that the loss was worthwhile, in exchange of a weakly defined \"more equal\" society? More to the point, would you be sure that when&nbsp;iterating&nbsp;this process billions of times, <em>every</em> redistribution will be an improvement? This is a&nbsp;conjunctive&nbsp;statement, so you have to be nearly&nbsp;entirely&nbsp;certain of every link in the chain, if you want to believe the outcome. And, to reiterate, these links cannot be reduced to simple mathematical statements - you have to be certain that each step is qualitatively better than the previous one.</p>\n<p>And you also have to be certain that your theory does not allow path dependency. One can take the&nbsp;perfectly&nbsp;valid position that \"If there were an existing poorer population, then the right thing to do would be to redistribute wealth, and thus lose the last copy of Akira. However, currently there is no&nbsp;existing&nbsp;poor population, hence I would oppose it coming into being, precisely because it would result in the lose of Akira.\" You can reject this type of reasoning, and a variety of others that block the repugnant conclusion at some stage of the chain (the Stanford&nbsp;Encyclopaedia&nbsp;of Philosophy has a good <a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">entry</a> on the&nbsp;Repugnant Conclusion and the arguments&nbsp;surrounding&nbsp;it). But most reasons for doing so&nbsp;already&nbsp;pre-suppose total utilitarianism. In that case, you cannot use the above as an argument for your theory.</p>\n<p>&nbsp;</p>\n<h2>Hypothetical beings have&nbsp;hypothetical (and complicated) things&nbsp;say to you</h2>\n<p>There is another major strand of argument for total utilitarianism, which claims that we owe it to non-existent beings to satisfy their preferences, that they would prefer to exist rather than remain non-existent, and hence we should bring them into existence. How does this argument fare?</p>\n<p>First of all, it should be emphasised that one is free to accept or reject that argument without any fear of inconsistency. If one maintains that never-existent beings have no relevant preferences, then one will never stumble over a problem. They don't exist, they can't make decisions, they can't contradict anything. In order to raise them to the point where their decisions are relevant, one has to raise them to existence, in reality or in simulation. By the time they can answer \"would you like to exist?\", they&nbsp;already&nbsp;do, so you are talking about whether or not to kill them, not whether or not to let them exist.</p>\n<p>But secondly, it seems that the \"non-existent beings\" argument is often advanced for the sole purpose of arguing for total utilitarianism, rather than as a defensible position in its own right. Rarely are its implication analysed. What would&nbsp;a proper theory of non-existent beings look like?</p>\n<p>Well, for a start the whole happiness/utility/preference problem comes back with extra sting. It's hard enough to make a utility function out of real world people, but how to do so with hypothetical people? Is it an essentially arbitrary process (dependent&nbsp;entirely&nbsp;on \"which types of people we think of first\"), or is it done properly, teasing out the \"choices\" and \"life experiences\" of the hypotheticals? In that last case, if we do it in too much detail, we could argue that we've already created the being in simulation, so it comes back to the death issue.</p>\n<p>But imagine that we've somehow extracted a utility function from the preferences of non-existent beings. Apparently, they would prefer to exist rather than not exist. But is this true? There are many people in the world who would prefer not to commit suicide, but would not mind much if external events ended their lives - they cling to life as a habit. Presumably non-existent versions of them \"would not mind\" remaining non-existent.</p>\n<p>Even for those that would prefer to exist, we can ask questions about the intensity of that desire, and how it compares with their other desires. For instance, among these hypothetical beings, some would be mothers of hypothetical infants, leaders of hypothetical religions, inmates of hypothetical prisons, and would only prefer to exist if they could bring/couldn't bring the rest of their hypothetical world with them. But this is ridiculous - we can't bring the hypothetical world with them, they would grow up in ours - so are we only really talking about the preferences of hypothetical babies, or hypothetical (and non-conscious) foetuses?</p>\n<p>If we do look at adults, bracketing the issue above, then we get some that&nbsp;would prefer that they not exist, as long as certain others do - or conversely that they not exist, as long as others also not exist. How should we take that into account?&nbsp;Assuming the universe infinite, any hypothetical being would exist somewhere. Is mere existence enough, or do we have to have a large measure or density of existence? Do we need them to exist close to us? Are their own preferences relevant - ie we only have a duty to bring into the world, those beings that would desire to exist in multiple copies everywhere? Or do we feel these have already \"enough existence\" and select the under-counted beings? What if very few hypothetical beings are total utilitarians - is that relevant?</p>\n<p>On a more personal note, every time we make a decision, we&nbsp;eliminate&nbsp;a particular being. We can not longer be the person who took the other job offer, or read the other book at that time and place. As these differences accumulate, we diverge quite a bit from what we could have been. When we do so, do we feel that we're killing off these extra hypothetical beings? Why not? Should we be compelled to lead double lives, assuming two (or more) completely&nbsp;separate&nbsp;identities, to increase the number of beings in the world? If not, why not?</p>\n<p>These are some of the questions that a theory of non-existent beings would have to grapple with, before it can become an \"obvious\" argument for total utilitarianism.</p>\n<p>&nbsp;</p>\n<h2>Moral uncertainty: total utilitarianism doesn't win by default</h2>\n<p>An argument that I have met occasionally is that while other ethical theories such as&nbsp;average utilitarianism, birth-death asymmetry, path dependence, preferences of non-loss of culture, etc... may have some validity, total utilitarianism wins as the population increases because the others don't scale in the same way. By the time we reach the trillion trillion trillion mark, total utilitarianism will completely dominate, even if we gave it little weight at the&nbsp;beginning.</p>\n<p>But this is the wrong way to compare competing moral theories. Just as different people's utilities don't have a common scale, different moral utilities don't have a common scale. For instance, would you say that square-total utilitarianism&nbsp;is certainly wrong? This theory is simply total utilitarianism further multiplied by the population; it would correspond roughly to the number of connections between people. Or what about exponential-square-total&nbsp;utilitarianism? This would correspond&nbsp;roughly&nbsp;to the set of possible connections between people. As long as we think that&nbsp;exponential-square-total&nbsp;utilitarianism is not certainly&nbsp;completely&nbsp;wrong, then the same argument as above would show it quickly dominating as population increases.</p>\n<p>Or what about 3^^^3&nbsp;average&nbsp;utilitarianism - which is simply average&nbsp;utilitarianism, multiplied by 3^^^3? Obviously that example is silly - we know that rescaling shouldn't change anything about the theory. But similarly, dividing total&nbsp;utilitarianism&nbsp;by 3^^^3 shouldn't change anything, so&nbsp;total&nbsp;utilitarianism's scaling advantage is&nbsp;illusory.</p>\n<p>As mentioned before, comparing different utility functions is a hard and subtle process. One method that seems to have surprisingly nice properties (to such an extent that I recommend always using as a first try) is to normalise the lowest possible attainable utility to zero, the highest attainable utility to one, multiply by the weight you give to the theory, and then add the normalised utilities together.</p>\n<p>For instance, assume you equally valued&nbsp;average utilitarianism and total utilitarianism, giving them both weights of one (and you had solved all the definitional problems above). Among the choices you were facing, the worst outcome for both theories is an empty world. The best outcome for average utilitarianism would be ten people with an average \"utility\" of 100. The best outcome for total&nbsp;utilitarianism&nbsp;would be a quadrillion people with an average \"utility\" of 1. Then how would either of those compare to ten trillion people with an average utility of 60? Well, the normalised utility of this for the&nbsp;average&nbsp;utilitarian is 0.6, while for the total utilitarian it's also 60/100=0.6, and 0.6+0.6=1.2. This is better that the utility for the small world (1+10<sup>-9</sup>) or the large world (0.01+1), so it beats&nbsp;either of the&nbsp;extremal&nbsp;choices.</p>\n<p>Extending this method, we can bring in such theories as exponential-square-total&nbsp;utilitarianism (probably with small weights!), without needing to fear that it will swamp all other moral theories. And with this normalisation (or similar ones), even small weights to moral theories such as \"culture has some intrinsic value\" will often prevent total utilitarianism from walking away with <em>all</em> of the marbles.</p>\n<p>&nbsp;</p>\n<h2>(Population) ethics is still hard</h2>\n<p>What is the conclusion? At Less Wrong, we're used to realising that ethics is hard, that value is fragile, that there is no single easy moral theory to safely program the AI with. But it seemed for a while that population ethics might be different - that there may be natural and easy ways to determine what to do with large groups, even though we couldn't decide what to do with individuals. I've argued strongly here that it's not the case - that population ethics remain hard, that we have to figure out what theory we want to have without access to easy shortcuts.</p>\n<p>But in another way it's liberating. To those who are mainly total&nbsp;utilitarians&nbsp;but internally doubt that a world with infinitely many barely happy people surrounded by nothing but \"<a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">muzak and potatoes</a>\" is really among the best of the best - well, you don't have to convince yourself of that. You may choose to believe it, or you may choose not to. No voice <a href=\"/lw/ky/fake_morality/\">in the sky</a> or in the math will force you either way. You can start putting together a moral theory that&nbsp;incorporates&nbsp;<em>all</em> your moral intuitions - those that drove you to total utilitarianism, and those that don't quite fit in that framework.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 1, "ZTRNmvQGgoYiymYnq": 1, "uG75MELqjCEfciaRp": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pyTuR4ZbLfqpS2oMh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": 47, "extendedScore": null, "score": 0.000172, "legacy": true, "legacyId": "17171", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In total utilitarianism, it is a morally neutral act to kill someone (in a painless and unexpected manner) and creating/giving birth to another being of comparable happiness (or preference satisfaction or welfare). In fact if one can kill a billion people to create a billion and one, one is morally compelled to do so. And this is true for real people, not just thought experiment people - living people with dreams, aspirations, grudges and annoying or endearing quirks. To avoid causing extra pain to those left behind, it is better that you kill off whole families and communities, so that no one is left to mourn the dead. In fact the most morally compelling act would be to kill off the whole of the human species, and replace it with a slightly larger population.</p>\n<p>We have many real world analogues to this thought experiment. For instance, it seems that there is only a small difference between the happiness of richer nations and poorer nations, while the first consume many more&nbsp;resources&nbsp;than the second. Hence to increase utility we should simply kill off all the rich, and let the poor multiply to take their place (continually bumping off any of the poor that gets too rich). Of course, the rich world also produces most of the farming surplus and the technology&nbsp;innovation, which allow us to support a larger population. So we should aim to kill everyone in the rich world apart from farmers and scientists - and enough support staff to keep these professions running (Carl Shulman correctly <a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/6wmr\">points out</a>&nbsp;that we may require most of the rest of the economy as \"support staff\". Still, it's very likely that we could kill off a significant segment of the population - those with the highest consumption relative to their impact of farming and science - and still \"improve\" the situation).</p>\n<p>Even if turns out to be problematic to implement in practice, a true total utilitarian should be thinking: \"I really, really wish there was a way to do targeted killing of many people in the USA, Europe and Japan, large parts of Asia and Latin America and some parts of Africa - it makes me sick to the stomach to think that I can't do that!\" Or maybe: \"I really really wish I could make everyone much poorer without affecting the size of the economy - I wake up at night with nightmare because these people remain above the poverty line!\"</p>\n<p>I won't belabour the point. I find those actions personally&nbsp;repellent, and I believe that nearly everyone finds them somewhat repellent or at least did so at some point in their past. This doesn't mean that it's the wrong thing to do - after all, the accepted answer to the <a href=\"/lw/kn/torture_vs_dust_specks/\">torture vs dust speck</a>&nbsp;dilemma&nbsp;feels intuitively wrong, at least the first time. It does mean, however, that there must be very strong&nbsp;countervailing&nbsp;arguments to balance out this initial repulsion (maybe even a mathematical theorem). For without that... how to justify all this killing?</p>\n<p>Hence for the rest of this post, I'll be arguing that total utilitarianism is built on a foundation of dust, and thus provides no reason to go against your initial intuitive judgement in these problems. The points will be:<a id=\"more\"></a></p>\n<ol>\n<li>Bayesianism and the fact that you should follow a&nbsp;<em>utility function</em> in no way compel you towards total <em>utilitarianism</em>. The similarity in names does not mean the concepts are on similarly rigorous foundations.</li>\n<li>Total utilitarianism is neither a simple, nor an elegant theory. In fact, it is under-defined and arbitrary.</li>\n<li>The most&nbsp;compelling&nbsp;argument for total utilitarianism (basically the one that establishes the <a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">repugnant conclusion</a>), is a very long chain of imperfect reasoning, so there is no reason for the conclusion to be solid.</li>\n<li>Considering the preferences of non-existent beings does not establish total&nbsp;utilitarianism.</li>\n<li>When considering competing moral theories, total utilitarianism does not \"win by default\" thanks to its large values as the population increases.</li>\n<li>Population ethics is <em>hard</em>, just as normal ethics is.</li>\n</ol>\n<p>&nbsp;</p>\n<h2 id=\"A_utility_function_does_not_compel_total__or_average__utilitarianism\">A utility function does not compel total (or average) utilitarianism</h2>\n<p>There are strong reasons to suspect that the best decision process is one that maximises expected utility for a particular utility function. Any process that does not do so, <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">leaves itself open</a> to be <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/\">money pumped</a> or taken advantage of. This point has been reiterated again and again on Less Wrong, and rightly so.</p>\n<p>Your utility function must be over states of the universe - but that's the only restriction. The theorem says nothing further about the content of your utility function. If you prefer a world with a trillion&nbsp;ecstatic&nbsp;super-humans to one with a septillion subsistence farmers - or vice versa - then as long you maximise your expected utility, the money pumps can't touch you, and the standard Bayesian arguments don't influence you to change your mind. Your values are fully rigorous.</p>\n<p>For instance, in the&nbsp;<a href=\"/lw/kn/torture_vs_dust_specks/\">torture vs dust speck</a>&nbsp;scenario, average utilitarianism also compels you to take torture, as do a host of other possible utility functions. A lot of arguments around this subject,&nbsp;that may&nbsp;implicitly&nbsp;feel to be in favour of total utilitarianism, turn out to be nothing of the sort. For instance, avoiding <a href=\"/lw/hw/scope_insensitivity/\">scope&nbsp;insensitivity</a>&nbsp;does not compel you to total utilitarianism, and you can&nbsp;perfectly&nbsp;allow birth-death asymmetries or similar intuitions, while remaining an expected utility maximiser.</p>\n<p>&nbsp;</p>\n<h2 id=\"Total_utilitarianism_is_not_simple_nor_elegant__but_is_arbitrary\">Total utilitarianism is not simple nor elegant, but is arbitrary</h2>\n<p>Total utilitarianism is defined as maximising the sum of everyone's individual utility function. That's a simple definition. But what are these individual utility functions? Do people act like expected utility maximisers? In a word... <a href=\"http://en.wikipedia.org/wiki/Behavioral_economics\">no</a>. In another word... <a href=\"http://www.mind.org.uk/help/diagnoses_and_conditions/depression\">NO</a>. In yet another word... <a href=\"http://www.darwinawards.com/\">NO</a>!</p>\n<p>So what are these utilities? Are they the utility that the individuals \"should have\"?&nbsp;According&nbsp;to what and who's criteria?&nbsp;Is it \"welfare\"? How is that defined? Is it happiness? Again, how is that defined? Is it preferences? On what scale? And what if the individual disagrees with the utility they are supposed to have? What if their revealed preferences are different again?</p>\n<p>There are (various different) ways to start resolving these problems, and philosophers have spent a lot of ink and time doing so. The point remains that total utilitarianism cannot claim to be a simple theory, if the objects that it sums over are so poorly and&nbsp;controversially&nbsp;defined.</p>\n<p>And the sum itself is a huge problem. There is no natural scale on which to compare utility functions. Divide one utility function by a billion, multiply the other by e<sup>\u03c0</sup>, and they are still perfectly valid utility functions. In a study group at the FHI, we've been looking at various ways of combining utility functions - equivalently, of doing <a href=\"http://en.wikipedia.org/wiki/Social_choice_theory\">interpersonal utility comparisons</a>&nbsp;(IUC). Turns out it's very hard, there seems no natural way of doing this, and a lot has also been written about this, concluding little. Unless your theory comes with a particular IUC method, the only way of summing these utilities is to do an essentially arbitrary choice for each individual before summing. Thus standard total utilitarianism is an arbitrary sum of ill defined, non-natural&nbsp;objects.</p>\n<p>Why then is it so popular? Well, one reason is that there are models that make use of something like total utilitarianism to great effect. Classical economic theory, for instance, models everyone as perfectly rational expected utility maximisers. It gives good predictions - but it remains a model, with a domain of validity. You wouldn't conclude from that economic model that, say, mental illnesses don't exist. Similarly,&nbsp;modelling&nbsp;each life as having the same value and maximising expected lives saved is sensible and intuitive in many <a href=\"/lw/n3/circular_altruism/\">scenarios</a>&nbsp;- but not necessarily all.</p>\n<p>Maybe if we had a bit more information about the affected populations, we could use a more sophisticated model, such as one incorporating <a href=\"http://en.wikipedia.org/wiki/Quality-adjusted_life_year\">quality adjusted life years</a>&nbsp;(QALY). Or maybe we could let other factors affect our thinking - what if we had to choose between saving a population of 1000 versus a population of 1001, of same average QALYs, but where the first set contained the entire&nbsp;<a href=\"http://en.wikipedia.org/wiki/Aw%C3%A1-Guaj%C3%A1_people\">Aw\u00e1</a> tribe/culture of 300 people, and the second is made up of representatives from much larger ethnic groups, much more culturally replaceable? Should we let that influence our decision? Well maybe we should, maybe we shouldn't, but it would be wrong to say \"well, I would really like to save the&nbsp;Aw\u00e1, but the model I settled on earlier won't allow me to, so I best follow the model\". The models are there precisely to model our moral&nbsp;intuitions (the clue is in the name), not freeze them.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_repugnant_conclusion_is_at_the_end_of_a_flimsy_chain\">The repugnant conclusion is at the end of a flimsy chain</h2>\n<p>There is a seemingly sound argument for the <a href=\"http://en.wikipedia.org/wiki/Repugnant_Conclusion\">repugnant conclusion</a>, which goes some way towards making total utilitarianism plausible. It goes like this:</p>\n<ol>\n<li>Start with a population of very happy/utilitied/welfared/preference satisfied people.</li>\n<li>Add other people whose lives are worth living, but whose average \"utility\" is less than that of the initial population.</li>\n<li>Redistribute \"utility\" in an egalitarian way across the whole population, increasing the average a little as you do so (but making sure the top rank have their utility lowered).</li>\n<li>Repeat as often as required.</li>\n<li>End up with a huge population whose lives are barely worth living.</li>\n</ol>\n<p>If all these steps increase the quality of the outcome (and it seems intuitively that they do), then the end state much be better than the&nbsp;starting&nbsp;state, agreeing with total&nbsp;utilitarianism. So, what could go wrong with this reasoning? Well, as seen before, the term \"utility\" is very much undefined, as is its scale - hence&nbsp;egalitarian&nbsp;is extremely undefined. So this argument is not&nbsp;mathematically&nbsp;precise, its rigour is illusionary. And when you recast the argument in qualitative terms, as you must, it become much weaker.</p>\n<p>Going through the&nbsp;iteration, there will come a point when the human world is going to lose its last anime, its&nbsp;last opera, its last copy of the Lord of the Rings, its last&nbsp;mathematics, its last online discussion board, its last football game - anything that might cause more-than-appropriate enjoyment. At that stage, would you be entirely sure that the loss was worthwhile, in exchange of a weakly defined \"more equal\" society? More to the point, would you be sure that when&nbsp;iterating&nbsp;this process billions of times, <em>every</em> redistribution will be an improvement? This is a&nbsp;conjunctive&nbsp;statement, so you have to be nearly&nbsp;entirely&nbsp;certain of every link in the chain, if you want to believe the outcome. And, to reiterate, these links cannot be reduced to simple mathematical statements - you have to be certain that each step is qualitatively better than the previous one.</p>\n<p>And you also have to be certain that your theory does not allow path dependency. One can take the&nbsp;perfectly&nbsp;valid position that \"If there were an existing poorer population, then the right thing to do would be to redistribute wealth, and thus lose the last copy of Akira. However, currently there is no&nbsp;existing&nbsp;poor population, hence I would oppose it coming into being, precisely because it would result in the lose of Akira.\" You can reject this type of reasoning, and a variety of others that block the repugnant conclusion at some stage of the chain (the Stanford&nbsp;Encyclopaedia&nbsp;of Philosophy has a good <a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">entry</a> on the&nbsp;Repugnant Conclusion and the arguments&nbsp;surrounding&nbsp;it). But most reasons for doing so&nbsp;already&nbsp;pre-suppose total utilitarianism. In that case, you cannot use the above as an argument for your theory.</p>\n<p>&nbsp;</p>\n<h2 id=\"Hypothetical_beings_have_hypothetical__and_complicated__things_say_to_you\">Hypothetical beings have&nbsp;hypothetical (and complicated) things&nbsp;say to you</h2>\n<p>There is another major strand of argument for total utilitarianism, which claims that we owe it to non-existent beings to satisfy their preferences, that they would prefer to exist rather than remain non-existent, and hence we should bring them into existence. How does this argument fare?</p>\n<p>First of all, it should be emphasised that one is free to accept or reject that argument without any fear of inconsistency. If one maintains that never-existent beings have no relevant preferences, then one will never stumble over a problem. They don't exist, they can't make decisions, they can't contradict anything. In order to raise them to the point where their decisions are relevant, one has to raise them to existence, in reality or in simulation. By the time they can answer \"would you like to exist?\", they&nbsp;already&nbsp;do, so you are talking about whether or not to kill them, not whether or not to let them exist.</p>\n<p>But secondly, it seems that the \"non-existent beings\" argument is often advanced for the sole purpose of arguing for total utilitarianism, rather than as a defensible position in its own right. Rarely are its implication analysed. What would&nbsp;a proper theory of non-existent beings look like?</p>\n<p>Well, for a start the whole happiness/utility/preference problem comes back with extra sting. It's hard enough to make a utility function out of real world people, but how to do so with hypothetical people? Is it an essentially arbitrary process (dependent&nbsp;entirely&nbsp;on \"which types of people we think of first\"), or is it done properly, teasing out the \"choices\" and \"life experiences\" of the hypotheticals? In that last case, if we do it in too much detail, we could argue that we've already created the being in simulation, so it comes back to the death issue.</p>\n<p>But imagine that we've somehow extracted a utility function from the preferences of non-existent beings. Apparently, they would prefer to exist rather than not exist. But is this true? There are many people in the world who would prefer not to commit suicide, but would not mind much if external events ended their lives - they cling to life as a habit. Presumably non-existent versions of them \"would not mind\" remaining non-existent.</p>\n<p>Even for those that would prefer to exist, we can ask questions about the intensity of that desire, and how it compares with their other desires. For instance, among these hypothetical beings, some would be mothers of hypothetical infants, leaders of hypothetical religions, inmates of hypothetical prisons, and would only prefer to exist if they could bring/couldn't bring the rest of their hypothetical world with them. But this is ridiculous - we can't bring the hypothetical world with them, they would grow up in ours - so are we only really talking about the preferences of hypothetical babies, or hypothetical (and non-conscious) foetuses?</p>\n<p>If we do look at adults, bracketing the issue above, then we get some that&nbsp;would prefer that they not exist, as long as certain others do - or conversely that they not exist, as long as others also not exist. How should we take that into account?&nbsp;Assuming the universe infinite, any hypothetical being would exist somewhere. Is mere existence enough, or do we have to have a large measure or density of existence? Do we need them to exist close to us? Are their own preferences relevant - ie we only have a duty to bring into the world, those beings that would desire to exist in multiple copies everywhere? Or do we feel these have already \"enough existence\" and select the under-counted beings? What if very few hypothetical beings are total utilitarians - is that relevant?</p>\n<p>On a more personal note, every time we make a decision, we&nbsp;eliminate&nbsp;a particular being. We can not longer be the person who took the other job offer, or read the other book at that time and place. As these differences accumulate, we diverge quite a bit from what we could have been. When we do so, do we feel that we're killing off these extra hypothetical beings? Why not? Should we be compelled to lead double lives, assuming two (or more) completely&nbsp;separate&nbsp;identities, to increase the number of beings in the world? If not, why not?</p>\n<p>These are some of the questions that a theory of non-existent beings would have to grapple with, before it can become an \"obvious\" argument for total utilitarianism.</p>\n<p>&nbsp;</p>\n<h2 id=\"Moral_uncertainty__total_utilitarianism_doesn_t_win_by_default\">Moral uncertainty: total utilitarianism doesn't win by default</h2>\n<p>An argument that I have met occasionally is that while other ethical theories such as&nbsp;average utilitarianism, birth-death asymmetry, path dependence, preferences of non-loss of culture, etc... may have some validity, total utilitarianism wins as the population increases because the others don't scale in the same way. By the time we reach the trillion trillion trillion mark, total utilitarianism will completely dominate, even if we gave it little weight at the&nbsp;beginning.</p>\n<p>But this is the wrong way to compare competing moral theories. Just as different people's utilities don't have a common scale, different moral utilities don't have a common scale. For instance, would you say that square-total utilitarianism&nbsp;is certainly wrong? This theory is simply total utilitarianism further multiplied by the population; it would correspond roughly to the number of connections between people. Or what about exponential-square-total&nbsp;utilitarianism? This would correspond&nbsp;roughly&nbsp;to the set of possible connections between people. As long as we think that&nbsp;exponential-square-total&nbsp;utilitarianism is not certainly&nbsp;completely&nbsp;wrong, then the same argument as above would show it quickly dominating as population increases.</p>\n<p>Or what about 3^^^3&nbsp;average&nbsp;utilitarianism - which is simply average&nbsp;utilitarianism, multiplied by 3^^^3? Obviously that example is silly - we know that rescaling shouldn't change anything about the theory. But similarly, dividing total&nbsp;utilitarianism&nbsp;by 3^^^3 shouldn't change anything, so&nbsp;total&nbsp;utilitarianism's scaling advantage is&nbsp;illusory.</p>\n<p>As mentioned before, comparing different utility functions is a hard and subtle process. One method that seems to have surprisingly nice properties (to such an extent that I recommend always using as a first try) is to normalise the lowest possible attainable utility to zero, the highest attainable utility to one, multiply by the weight you give to the theory, and then add the normalised utilities together.</p>\n<p>For instance, assume you equally valued&nbsp;average utilitarianism and total utilitarianism, giving them both weights of one (and you had solved all the definitional problems above). Among the choices you were facing, the worst outcome for both theories is an empty world. The best outcome for average utilitarianism would be ten people with an average \"utility\" of 100. The best outcome for total&nbsp;utilitarianism&nbsp;would be a quadrillion people with an average \"utility\" of 1. Then how would either of those compare to ten trillion people with an average utility of 60? Well, the normalised utility of this for the&nbsp;average&nbsp;utilitarian is 0.6, while for the total utilitarian it's also 60/100=0.6, and 0.6+0.6=1.2. This is better that the utility for the small world (1+10<sup>-9</sup>) or the large world (0.01+1), so it beats&nbsp;either of the&nbsp;extremal&nbsp;choices.</p>\n<p>Extending this method, we can bring in such theories as exponential-square-total&nbsp;utilitarianism (probably with small weights!), without needing to fear that it will swamp all other moral theories. And with this normalisation (or similar ones), even small weights to moral theories such as \"culture has some intrinsic value\" will often prevent total utilitarianism from walking away with <em>all</em> of the marbles.</p>\n<p>&nbsp;</p>\n<h2 id=\"_Population__ethics_is_still_hard\">(Population) ethics is still hard</h2>\n<p>What is the conclusion? At Less Wrong, we're used to realising that ethics is hard, that value is fragile, that there is no single easy moral theory to safely program the AI with. But it seemed for a while that population ethics might be different - that there may be natural and easy ways to determine what to do with large groups, even though we couldn't decide what to do with individuals. I've argued strongly here that it's not the case - that population ethics remain hard, that we have to figure out what theory we want to have without access to easy shortcuts.</p>\n<p>But in another way it's liberating. To those who are mainly total&nbsp;utilitarians&nbsp;but internally doubt that a world with infinitely many barely happy people surrounded by nothing but \"<a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">muzak and potatoes</a>\" is really among the best of the best - well, you don't have to convince yourself of that. You may choose to believe it, or you may choose not to. No voice <a href=\"/lw/ky/fake_morality/\">in the sky</a> or in the math will force you either way. You can start putting together a moral theory that&nbsp;incorporates&nbsp;<em>all</em> your moral intuitions - those that drove you to total utilitarianism, and those that don't quite fit in that framework.</p>", "sections": [{"title": "A utility function does not compel total (or average) utilitarianism", "anchor": "A_utility_function_does_not_compel_total__or_average__utilitarianism", "level": 1}, {"title": "Total utilitarianism is not simple nor elegant, but is arbitrary", "anchor": "Total_utilitarianism_is_not_simple_nor_elegant__but_is_arbitrary", "level": 1}, {"title": "The repugnant conclusion is at the end of a flimsy chain", "anchor": "The_repugnant_conclusion_is_at_the_end_of_a_flimsy_chain", "level": 1}, {"title": "Hypothetical beings have\u00a0hypothetical (and complicated) things\u00a0say to you", "anchor": "Hypothetical_beings_have_hypothetical__and_complicated__things_say_to_you", "level": 1}, {"title": "Moral uncertainty: total utilitarianism doesn't win by default", "anchor": "Moral_uncertainty__total_utilitarianism_doesn_t_win_by_default", "level": 1}, {"title": "(Population) ethics is still hard", "anchor": "_Population__ethics_is_still_hard", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "237 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 237, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN", "ZTN6bLWqpwWn2i4qZ", "2ftJ38y9SRBCBsCzy", "4ZzefKQwAtMo5yp99", "fATPBv4pnHC33EmJ2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-26T16:02:29.102Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:41.046Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s6Zf5C9hK3nSwcpDL/meetup-brussels-meetup-12", "pageUrlRelative": "/posts/s6Zf5C9hK3nSwcpDL/meetup-brussels-meetup-12", "linkUrl": "https://www.lesswrong.com/posts/s6Zf5C9hK3nSwcpDL/meetup-brussels-meetup-12", "postedAtFormatted": "Tuesday, June 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs6Zf5C9hK3nSwcpDL%2Fmeetup-brussels-meetup-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs6Zf5C9hK3nSwcpDL%2Fmeetup-brussels-meetup-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs6Zf5C9hK3nSwcpDL%2Fmeetup-brussels-meetup-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bf'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 July 2012 12:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Museum of Natural Sciences Rue Vautier 29 B-1000 Brussels</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll meet in the lobby as usual, I'll have a sign, should you arrive late we will have moved to the cafeteria (just go straight once you're past the entrance, you can't miss it) If you are in the neighborhood, consider dropping by. (getting there: <a href=\"http://www.naturalsciences.be/information/visitor/access\" rel=\"nofollow\">http://www.naturalsciences.be/information/visitor/access</a>)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bf'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s6Zf5C9hK3nSwcpDL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.30509904278219e-07, "legacy": true, "legacyId": "17229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/bf\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 July 2012 12:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Museum of Natural Sciences Rue Vautier 29 B-1000 Brussels</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll meet in the lobby as usual, I'll have a sign, should you arrive late we will have moved to the cafeteria (just go straight once you're past the entrance, you can't miss it) If you are in the neighborhood, consider dropping by. (getting there: <a href=\"http://www.naturalsciences.be/information/visitor/access\" rel=\"nofollow\">http://www.naturalsciences.be/information/visitor/access</a>)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/bf\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-27T01:31:14.685Z", "modifiedAt": null, "url": null, "title": "How to read a book", "slug": "how-to-read-a-book", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.475Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Blackened", "createdAt": "2012-05-09T10:50:12.050Z", "isAdmin": false, "displayName": "Blackened"}, "userId": "qNeT4N9rP7tSxwquv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cxpDhWtxY6d6idi4y/how-to-read-a-book", "pageUrlRelative": "/posts/cxpDhWtxY6d6idi4y/how-to-read-a-book", "linkUrl": "https://www.lesswrong.com/posts/cxpDhWtxY6d6idi4y/how-to-read-a-book", "postedAtFormatted": "Wednesday, June 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20read%20a%20book&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20read%20a%20book%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcxpDhWtxY6d6idi4y%2Fhow-to-read-a-book%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20read%20a%20book%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcxpDhWtxY6d6idi4y%2Fhow-to-read-a-book", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcxpDhWtxY6d6idi4y%2Fhow-to-read-a-book", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 245, "htmlBody": "<p>I'm considering reading the book by the title <a href=\"http://en.wikipedia.org/wiki/How_to_Read_a_Book\">How to read a book</a>. A friend of mine (his critical thinking is quite good, but certainly not as good as it could be, so I can't trust his opinion too much) said he has read it and that it helped him a lot. He said it had advice on reading comprehension, critical thinking (\"don't automatically accept what you read\") and that when people read something, they tend to forget it quite easily (and that the book addresses this issue). But he also quoted a part of the book, which said that only reading hard things will improve your reading - it might be true, but it doesn't sound intuitive to me (according to my rationalist intuition, obviously :D). Also, the book is written in 1940 and revised in 1972. Additionally, the author is religious (I think he's even highly religious). And <strong>if I remember correctly</strong>, it's not based on research - there is a quite high chance that I don't remember correctly. I checked its Amazon page, nothing said anything about research (browsed through all the low ratings to see if they complain about that, nobody did).</p>\n<p>&nbsp;</p>\n<p>Should I bother reading it? If it delivers what it promises, it will obviously be so cost-effective that most rationalists should abandon reading whatever they're reading and switch to this book. But is there a version that is entirely based on research, with references or sound theory behind most claims?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cxpDhWtxY6d6idi4y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 1, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "17232", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-27T03:14:05.720Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Fear of Common Knowledge", "slug": "seq-rerun-the-fear-of-common-knowledge", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YymNfPBduRqbaG3Sw/seq-rerun-the-fear-of-common-knowledge", "pageUrlRelative": "/posts/YymNfPBduRqbaG3Sw/seq-rerun-the-fear-of-common-knowledge", "linkUrl": "https://www.lesswrong.com/posts/YymNfPBduRqbaG3Sw/seq-rerun-the-fear-of-common-knowledge", "postedAtFormatted": "Wednesday, June 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Fear%20of%20Common%20Knowledge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Fear%20of%20Common%20Knowledge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYymNfPBduRqbaG3Sw%2Fseq-rerun-the-fear-of-common-knowledge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Fear%20of%20Common%20Knowledge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYymNfPBduRqbaG3Sw%2Fseq-rerun-the-fear-of-common-knowledge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYymNfPBduRqbaG3Sw%2Fseq-rerun-the-fear-of-common-knowledge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>Today's post, <a href=\"/lw/s1/the_fear_of_common_knowledge/\">The Fear of Common Knowledge</a> was originally published on 09 July 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>A discussion of an interesting kind of lie, in which someone tells a lie that the person they're speaking to knows is a lie, but doesn't know that the person who told the lie knows that they know it's a lie.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/dad/seq_rerun_where_recursive_justification_hits/\">Where Recursive Justification Hits Bottom</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YymNfPBduRqbaG3Sw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 9.308205842617874e-07, "legacy": true, "legacyId": "17233", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YcRZbgRbZGpu9xFox", "QN3syTa8jPksGNbrE", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-27T03:29:24.648Z", "modifiedAt": null, "url": null, "title": "Cat or Rabbit", "slug": "cat-or-rabbit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.359Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benvolio", "createdAt": "2010-07-13T06:27:07.741Z", "isAdmin": false, "displayName": "Benvolio"}, "userId": "ceFSmHTF5vAxLDoAM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ghiuvQmfjKo7KWo2f/cat-or-rabbit", "pageUrlRelative": "/posts/ghiuvQmfjKo7KWo2f/cat-or-rabbit", "linkUrl": "https://www.lesswrong.com/posts/ghiuvQmfjKo7KWo2f/cat-or-rabbit", "postedAtFormatted": "Wednesday, June 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cat%20or%20Rabbit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACat%20or%20Rabbit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FghiuvQmfjKo7KWo2f%2Fcat-or-rabbit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cat%20or%20Rabbit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FghiuvQmfjKo7KWo2f%2Fcat-or-rabbit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FghiuvQmfjKo7KWo2f%2Fcat-or-rabbit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1728, "htmlBody": "<p>&nbsp;</p>\n<p>First go at this went terrible, but best to revise.&nbsp;</p>\n<p>This is a post containing dark arts. It is an entirely irrational emotional appeal as to why rationality is a better way to be. It uses biased and terrible personal anecdotes based on the lives of people living in some of the poorest of the world's slums, ones that were the first to ever receive housing projects and how they bettered their lives by throwing off the&nbsp;shackles&nbsp;of their faith and reviewing their cached thoughts and seeing what is in front of them. This now continues, annotated for the LW audience.&nbsp;</p>\n<p>So I have very little maths training, I've mostly been a storyteller all of my life. I have tried to live as a rationalist, mostly by accident and it is enheartening to know that a community exists. After this most recent minicamp I figured I might do well to explore how I became a rationalist. Everyone kept asking about origin stories. My origin is in stories. I have the words of my father, perhaps a bit edited, chopped, and otherwise warped by the vagarities of time and memory, but something we do not have that is had by many other tribes is a litany, a set of stories and parables passed down with the sounds of crackling fire and whispering winds.</p>\n<p>This is a piece of fertilizer it is bullshit meant to help things grow. It may not be useful but people do forget just how much effort there is in going from 0 to 1.&nbsp;</p>\n<p>This is a story told me on a warm night on an old beach as my father and I walked the sands looking for old clay pipes left by sailors in previous centuries. He said to me, I can never get used to how fat the pigeons are in DC. It shows a lack of motivation on the part of them that lives on the street. My gran wouldn't have had it. It never mattered what &nbsp;folks told her was right or not, she knew by trying and watching folks try and picking up the pieces afterwards. If the world was fair she'd ha been in charge &nbsp;of a college somewhere. I&rsquo;ve learned a lot from Grandma. She&rsquo;s like a good witch. She showed me how to use different parts of the cumfrey plant (she calls it knit-bone) to make tea, poultices and dressings to help cure all kinds of stuff; how to make beer or soup from stinging nettles; how to get food, wine and slide-whistles from an elderberry bush; how to make dandelion salad and dandelion and burrdock pop. Because of her, I feel safe wherever I go. I know all the birds, trees and plants and how they can help me. I know all the poisons. I can trap rats, pigeons and rabbits, too. Rats are pretty straightforward, though nowadays, rats are left to our Ian. He&rsquo;s a crack shot with his rifle and he has the patience of Job. Seeing the world as it is around you means you eat well, seeing the world as the storybooks tell you makes you starve.&nbsp;</p>\n<p>Trapping pigeons is fun. You get a big cardboard box, a long piece of string, a short stick, and some bread. You go underneath a railway bridge and tie one end of the string to the stick, then turn the box upside down and prop one end of it up with the stick. You break the bread into pieces and sprinkle it on the ground around and under the box. You hold on to the other end of the string and hide as far away as you can. Then you wait quietly. The pigeons come down for the bread and when they go under the box, you pull the string.&nbsp;</p>\n<p>Eating a pigeon you&rsquo;ve caught yourself is better than eating tripe from a cow you&rsquo;ve never met. You can't fool yourself into forgetting. You can't think of it like a hamburger or an existing food thing. You have taken part of the world around you and converted it into fuel for yourself.&nbsp;Pigeon meat is darker than any meat I know, and its taste is strong, but it&rsquo;s very good. What puzzles me is why so many of them have club feet and twisted beaks. Maybe that&rsquo;s why they didn&rsquo;t evolve into eagles. I wonder what they are evolving into. They don&rsquo;t look fit, like you&rsquo;d expect in the survival of the fittest, yet they don&rsquo;t seem to be dying out, either, since the only color that&rsquo;s not black around here is pigeon shit. My dad says, &ldquo;Don&rsquo;t think so much. That&rsquo;s what makes you sad all the time&rdquo;.&nbsp;</p>\n<p>The story breaks for a moment \"This is a fight we still have, and yes, thinking makes you sad for a bit as you have to look and see that the problems are, but it can make you happy if you like solving problems.\"</p>\n<p>Rabbits are a different story. Our Ian hunts them with his dog and his gun and his ferret. A ferret can turn around inside a rabbit burrow, but a rabbit can&rsquo;t. This strikes me as another failure of evolution. But rabbits make up for it by having so many babies. A lot of people think ferrets are cruel because they&rsquo;re such good hunters. I think rabbits are crueler, for feeding their babies to ferrets, rather than digging wider burrows or turning places they can defend. Ian sets nets at all the entrances, then sends in the ferret. Any rabbits that escape the nets get shot by Ian or bit by the Jack Russel. The ferret terrifies the rabbits. They scream like human babies. It upsets me, but I still eat them, because they taste good. Believe it or not, I once chased down a rabbit. Our Steven shone a light at it (it was evening) and it paused for a second. When it saw me move, it snapped out of its trance and bolted. We ran all over the bloody field, but I wouldn&rsquo;t give up. We came to a fence which, at the rabbit&rsquo;s level, was only a single strand of wire, nine inches off the ground. It froze and I picked it up. <em>That taught me that all your obstacles are inside your own head. Knowing this does not mean that I put it to good use.</em> The rabbit was young and I let it go, hoping it would not expect other humans to be so gentle. It troubles me that if you&rsquo;re kind to an animal, or even a person, you are responsible for it trusting the next person it meets. And if you&rsquo;re not kind, then you are the person you fear it might meet next. Once I found a family of hedgehogs and fed the babies every day, till none of them curled up when I approached. I had lots of nightmares about them trusting bad people.</p>\n<p>When I talk to Grandma about rabbits, she tells me about Uncle Luke, another of her brothers, who was a butcher and when The Great War broke out, nobody could get meat, because the good stuff was going to the upper classes.&nbsp;Anyhow, Uncle Luke found a way out of his predicament when he realized that, headless, footless, tailless and skinless, cats look just like rabbits. Business really picked up when him and his wife started selling rabbit/cats and horsemeat pies. The point about this story that interests me, is that people paid according to what they believed they were buying. &lsquo;Rabbit pie&rsquo; was expensive, while cat pie (only if you knew enough to ask for it) was cheap, even though they were the same thing. I thought this was funny and showed people are stupid, until my Sunday School teacher told me that I would only go to heaven if I believed in it. Now I&rsquo;m not sure what to think.</p>\n<p>The importance of knowing the difference between things that work if you don't believe in them and things that only work if you do can not be understated.&nbsp;</p>\n<p>Now of course at this time my dad tousles my hair and helps sort through the things we have that may be lay or shells or a hundred other things. I asked him if he's figured it out now. He tells me, \"If there is a heaven its weirder than anything we can imagine. We're men and make men gods but tomorrow you should go to the library and try to find out who talked about triangle gods. The only thing you can be sure of is anyone who tells you they know what happens thereafter is out to steal your watch. Never trust anyone in a silly hat or expensive outfit who tries to tell you what a god thinks. They are your enemy and the enemy of everyone who works an honest day. Now they have people who don't have such trappings who are good and kind and simple folk and you should pity them for being in the hands of such wicked men. Help them how you can but they might just do something crazy on account their preacher said so so always keep a good escape route. and how do we keep a good escape route?\"</p>\n<p>\"Always watch the way you came in, always be loud so they'll never know you can be quiet, and always keep a holdout on you.\"</p>\n<p>\"Good lad.\"</p>\n<div>Now a moment aside dear reader. Remember you don't have to be smart to be right. You don't even have to be smart to do things the right way. If we are to be rational we should value intellect&nbsp;appropriately, which is to say, highly, but not infinitely so. Being simple and honest is a good tool for rationalists to have. Not seeing the difference between a skinned cat and a skinned rabbit is an advantage. Thinking about whether your moments of kindness might do more harm than good is an advantage. Questioning all of your impulses whether they are selfish or charitable is a good habit, and knowing the world around you is a worthy thing. I am sorry for phrasing this so obliquely, it is my first day and I have revised it. If I need to revise it again please say and it will change. To make this clear, the purpose of this story is to seep good habits into the mind by whatever means are to hand.&nbsp;</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ghiuvQmfjKo7KWo2f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -16, "extendedScore": null, "score": -7e-06, "legacy": true, "legacyId": "17235", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-27T13:44:57.860Z", "modifiedAt": null, "url": null, "title": "Science and the Supernatural", "slug": "science-and-the-supernatural", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YK7pBGSvzCABfvcjz/science-and-the-supernatural", "pageUrlRelative": "/posts/YK7pBGSvzCABfvcjz/science-and-the-supernatural", "linkUrl": "https://www.lesswrong.com/posts/YK7pBGSvzCABfvcjz/science-and-the-supernatural", "postedAtFormatted": "Wednesday, June 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Science%20and%20the%20Supernatural&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScience%20and%20the%20Supernatural%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYK7pBGSvzCABfvcjz%2Fscience-and-the-supernatural%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Science%20and%20the%20Supernatural%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYK7pBGSvzCABfvcjz%2Fscience-and-the-supernatural", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYK7pBGSvzCABfvcjz%2Fscience-and-the-supernatural", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 401, "htmlBody": "<p><a href=\"http://whyevolutionistrue.wordpress.com/2012/06/27/can-science-test-the-supernatural-yes\"></a>Can science test the supernatural? Yes!! is an interesting article about why there are not \"two nonoverlapping magisteria\".  The core of the article is based on the paper <a href=\"http://www.naturalism.org/Can%20Science%20Test%20Supernatural%20Worldviews-%20Final%20Author%27s%20Copy%20%28Fishman%202007%29.pdf\">Can Science Test Supernatural Worldviews?</a> Abstract of the latter paper:</p>\n<blockquote>Several prominent scientists, philosophers, and scientific institutions have argued that science cannot test supernatural worldviews on the grounds that (1) science presupposes a naturalistic worldview (Naturalism) or that (2) claims involving supernatural phenomena are inherently beyond the scope of scientific investigation. The present paper argues that these assumptions are questionable and that indeed science can test supernatural claims. While scientific evidence may ultimately support a naturalistic worldview, science does not presuppose Naturalism as an a priori commitment, and supernatural claims are amenable to scientific evaluation. This conclusion challenges the rationale behind a recent judicial ruling in the United States concerning the teaching of &ldquo;Intelligent Design&rdquo; in public schools as an alternative to evolution and the official statements of two major scientific institutions that exert a substantial influence on science educational policies in the United States. Given that science does have implications concerning the probable truth of supernatural worldviews, claims should not be excluded a priori from science education simply because they might be characterized as supernatural, paranormal, or religious. Rather, claims should be excluded from science education when the evidence does not support them, regardless of whether they are designated as &lsquo;natural&rsquo; or &lsquo;supernatural&rsquo;.</blockquote>\n<p>Three more specific quotes from one or both papers:</p>\n<p>- Claims about the supernatural should be prohibited from science classes not because they&rsquo;re religious, but because science has &ldquo;proven&rdquo; them wrong.</p>\n<p>- To exclude, a priori, the supernatural would validate the complaint voiced by some ID adherents and other creationists that science is dogmatically committed to Naturalism and thus opposed in principle to considering supernatural explanations (Johnson, 1999; see Stenger, 2006a). On the other hand, if there is no fundamental barrier preventing science from evaluating supernatural claims, then to declare the study of supernatural phenomena out of bounds to scientific investigation imposes artificial constraints on scientific inquiry, which potentially would deny science the noble task of purging false beliefs from the public sphere or the opportunity to discover aspects of reality that may have significant worldview implications.</p>\n<p>- Naturalism is not a presupposition of doing science, but a conclusion from doing science.</p>\n<p>And the second paper has a large section titled, <strong>Science Can Test Supernatural Claims: A Bayesian Perspective</strong></p>\n<p>They are both well worth reading.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YK7pBGSvzCABfvcjz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "17253", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-27T18:28:04.744Z", "modifiedAt": null, "url": null, "title": "Bounded versions of G\u00f6del's and L\u00f6b's theorems", "slug": "bounded-versions-of-goedel-s-and-loeb-s-theorems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:01.884Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z7SuGwxTBnQm8uFq4/bounded-versions-of-goedel-s-and-loeb-s-theorems", "pageUrlRelative": "/posts/z7SuGwxTBnQm8uFq4/bounded-versions-of-goedel-s-and-loeb-s-theorems", "linkUrl": "https://www.lesswrong.com/posts/z7SuGwxTBnQm8uFq4/bounded-versions-of-goedel-s-and-loeb-s-theorems", "postedAtFormatted": "Wednesday, June 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bounded%20versions%20of%20G%C3%B6del's%20and%20L%C3%B6b's%20theorems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABounded%20versions%20of%20G%C3%B6del's%20and%20L%C3%B6b's%20theorems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz7SuGwxTBnQm8uFq4%2Fbounded-versions-of-goedel-s-and-loeb-s-theorems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bounded%20versions%20of%20G%C3%B6del's%20and%20L%C3%B6b's%20theorems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz7SuGwxTBnQm8uFq4%2Fbounded-versions-of-goedel-s-and-loeb-s-theorems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz7SuGwxTBnQm8uFq4%2Fbounded-versions-of-goedel-s-and-loeb-s-theorems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 550, "htmlBody": "<p>While writing up decision theory results, I had to work out bounded versions of <a href=\"http://en.wikipedia.org/wiki/G%C3%B6del's_incompleteness_theorems#Second_incompleteness_theorem\">G&ouml;del's second incompleteness theorem</a> and <a href=\"http://en.wikipedia.org/wiki/L%C3%B6b's_theorem\">L&ouml;b's theorem</a>. Posting the tentative proofs here to let people find any major errors before adding formality. Any comments are welcome!</p>\n<p><strong>Bounded G&ouml;del's theorem</strong></p>\n<p>Informal meaning: if a theory T has a short proof that any proof of T's inconsistency must be long, then T is inconsistent.</p>\n<p>Formal statement: there exists a total computable function f such that for any effectively generated theory T that includes Peano arithmetic, and for any integer n, if T proves in n symbols that \"T can't prove a falsehood in f(n) symbols\", then T is inconsistent.</p>\n<p><strong>Proof</strong></p>\n<p>If the theory T takes more than n symbols to describe, the condition of the theorem can't hold, so let's assume T is smaller than that.</p>\n<p>Let's take g(n)&gt;&gt;n and f(n)&gt;&gt;exp(g(n)). For any integer n, let's define a program R that enumerates all proofs in the theory T up to length g(n), looking for either a proof that R prints something (in which case R immediately halts without printing anything), or a proof that R doesn't print anything (in which case R prints something and halts). If no proof is found, R halts without printing anything.</p>\n<p>Assume that the theory T is consistent. Note that if the program R finds some proof, then it makes the proved statement false. Also note that the theory T can completely simulate the execution of the program R in exp(g(n)) symbols. Therefore if the program R finds any proof, the resulting contradiction in the theory T can be \"exposed\" using exp(g(n)) symbols. Since f(n)&gt;&gt;exp(g(n)), and the theory T proves in n symbols that \"T can't prove a falsehood in f(n) symbols\", we see that T proves in slightly more than n symbols that the program R won't find any proofs. Therefore the theory T proves in slightly more than n symbols that the program R won't print anything. Since g(n)&gt;&gt;n, the program R will find that proof and print something, making the theory T inconsistent. QED.</p>\n<p>(The main idea of the proof is taken from&nbsp;<a href=\"http://mathoverflow.net/questions/72062/what-are-some-proofs-of-godels-theorem-which-are-essentially-different-from-th/72151#72151\">Ron Maimon</a>, originally due to Rosser, and adapted to the bounded case.)</p>\n<p><strong>Bounded L&ouml;b's theorem</strong></p>\n<p>Informal meaning: if having a bounded-size proof of statement S would imply the truth of S, and that fact has a short proof, then S is provable.</p>\n<p>Formal statement: there exists a total computable function f such that for any effectively generated theory T that includes Peano arithmetic, any statement S in that theory, and any integer n, if T proves in n symbols that \"if T proves S in f(n) symbols, then S is true\", then T proves S.</p>\n<p><strong>Proof</strong></p>\n<p>Taking the contrapositive, the theory T proves in n symbols that \"if the statement S is false, then T doesn't prove S in f(n) symbols\". Therefore the theory T+&not;S&nbsp;proves in n symbols that \"T+&not;S&nbsp;doesn't prove a falsehood in f(n) symbols\" (I'm glossing over the tiny changes to n and f(n) here). Taking the function f from the bounded G&ouml;del's theorem, we obtain that the theory T+&not;S is inconsistent. That means T proves S. QED.</p>\n<p>(The main idea of the proof is taken from&nbsp;<a href=\"http://www.scottaaronson.com/democritus/lec3.html\">Scott Aaronson</a>, originally due to Kripke or Kreisel.)</p>\n<p>As a sanity check, the regular versions of the theorems seem to be easy corollaries of the bounded versions. But of course there could still be errors...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2, "AJDHQ4mFnsNbBzPhT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z7SuGwxTBnQm8uFq4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 52, "extendedScore": null, "score": 9.312436782717876e-07, "legacy": true, "legacyId": "17254", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>While writing up decision theory results, I had to work out bounded versions of <a href=\"http://en.wikipedia.org/wiki/G%C3%B6del's_incompleteness_theorems#Second_incompleteness_theorem\">G\u00f6del's second incompleteness theorem</a> and <a href=\"http://en.wikipedia.org/wiki/L%C3%B6b's_theorem\">L\u00f6b's theorem</a>. Posting the tentative proofs here to let people find any major errors before adding formality. Any comments are welcome!</p>\n<p><strong id=\"Bounded_G_del_s_theorem\">Bounded G\u00f6del's theorem</strong></p>\n<p>Informal meaning: if a theory T has a short proof that any proof of T's inconsistency must be long, then T is inconsistent.</p>\n<p>Formal statement: there exists a total computable function f such that for any effectively generated theory T that includes Peano arithmetic, and for any integer n, if T proves in n symbols that \"T can't prove a falsehood in f(n) symbols\", then T is inconsistent.</p>\n<p><strong id=\"Proof\">Proof</strong></p>\n<p>If the theory T takes more than n symbols to describe, the condition of the theorem can't hold, so let's assume T is smaller than that.</p>\n<p>Let's take g(n)&gt;&gt;n and f(n)&gt;&gt;exp(g(n)). For any integer n, let's define a program R that enumerates all proofs in the theory T up to length g(n), looking for either a proof that R prints something (in which case R immediately halts without printing anything), or a proof that R doesn't print anything (in which case R prints something and halts). If no proof is found, R halts without printing anything.</p>\n<p>Assume that the theory T is consistent. Note that if the program R finds some proof, then it makes the proved statement false. Also note that the theory T can completely simulate the execution of the program R in exp(g(n)) symbols. Therefore if the program R finds any proof, the resulting contradiction in the theory T can be \"exposed\" using exp(g(n)) symbols. Since f(n)&gt;&gt;exp(g(n)), and the theory T proves in n symbols that \"T can't prove a falsehood in f(n) symbols\", we see that T proves in slightly more than n symbols that the program R won't find any proofs. Therefore the theory T proves in slightly more than n symbols that the program R won't print anything. Since g(n)&gt;&gt;n, the program R will find that proof and print something, making the theory T inconsistent. QED.</p>\n<p>(The main idea of the proof is taken from&nbsp;<a href=\"http://mathoverflow.net/questions/72062/what-are-some-proofs-of-godels-theorem-which-are-essentially-different-from-th/72151#72151\">Ron Maimon</a>, originally due to Rosser, and adapted to the bounded case.)</p>\n<p><strong id=\"Bounded_L_b_s_theorem\">Bounded L\u00f6b's theorem</strong></p>\n<p>Informal meaning: if having a bounded-size proof of statement S would imply the truth of S, and that fact has a short proof, then S is provable.</p>\n<p>Formal statement: there exists a total computable function f such that for any effectively generated theory T that includes Peano arithmetic, any statement S in that theory, and any integer n, if T proves in n symbols that \"if T proves S in f(n) symbols, then S is true\", then T proves S.</p>\n<p><strong id=\"Proof1\">Proof</strong></p>\n<p>Taking the contrapositive, the theory T proves in n symbols that \"if the statement S is false, then T doesn't prove S in f(n) symbols\". Therefore the theory T+\u00acS&nbsp;proves in n symbols that \"T+\u00acS&nbsp;doesn't prove a falsehood in f(n) symbols\" (I'm glossing over the tiny changes to n and f(n) here). Taking the function f from the bounded G\u00f6del's theorem, we obtain that the theory T+\u00acS is inconsistent. That means T proves S. QED.</p>\n<p>(The main idea of the proof is taken from&nbsp;<a href=\"http://www.scottaaronson.com/democritus/lec3.html\">Scott Aaronson</a>, originally due to Kripke or Kreisel.)</p>\n<p>As a sanity check, the regular versions of the theorems seem to be easy corollaries of the bounded versions. But of course there could still be errors...</p>", "sections": [{"title": "Bounded G\u00f6del's theorem", "anchor": "Bounded_G_del_s_theorem", "level": 1}, {"title": "Proof", "anchor": "Proof", "level": 1}, {"title": "Bounded L\u00f6b's theorem", "anchor": "Bounded_L_b_s_theorem", "level": 1}, {"title": "Proof", "anchor": "Proof1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-28T01:15:56.765Z", "modifiedAt": null, "url": null, "title": "Anybody know some good Modafinil suppliers?", "slug": "anybody-know-some-good-modafinil-suppliers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.552Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "EidDACeeaYXPADM2d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CpemMhCxeyWGCWs4J/anybody-know-some-good-modafinil-suppliers", "pageUrlRelative": "/posts/CpemMhCxeyWGCWs4J/anybody-know-some-good-modafinil-suppliers", "linkUrl": "https://www.lesswrong.com/posts/CpemMhCxeyWGCWs4J/anybody-know-some-good-modafinil-suppliers", "postedAtFormatted": "Thursday, June 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anybody%20know%20some%20good%20Modafinil%20suppliers%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnybody%20know%20some%20good%20Modafinil%20suppliers%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCpemMhCxeyWGCWs4J%2Fanybody-know-some-good-modafinil-suppliers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anybody%20know%20some%20good%20Modafinil%20suppliers%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCpemMhCxeyWGCWs4J%2Fanybody-know-some-good-modafinil-suppliers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCpemMhCxeyWGCWs4J%2Fanybody-know-some-good-modafinil-suppliers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>I've been interested in purchasing some Modafinil but I don't know any trustworthy sources. Any suggestions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CpemMhCxeyWGCWs4J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -9, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "17256", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-28T03:32:29.950Z", "modifiedAt": null, "url": null, "title": "Introduction to Game Theory: Sequence Guide", "slug": "introduction-to-game-theory-sequence-guide", "viewCount": null, "lastCommentedAt": "2012-07-05T12:28:08.482Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QxZs5Za4qXBegXCgu/introduction-to-game-theory-sequence-guide", "pageUrlRelative": "/posts/QxZs5Za4qXBegXCgu/introduction-to-game-theory-sequence-guide", "linkUrl": "https://www.lesswrong.com/posts/QxZs5Za4qXBegXCgu/introduction-to-game-theory-sequence-guide", "postedAtFormatted": "Thursday, June 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Introduction%20to%20Game%20Theory%3A%20Sequence%20Guide&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntroduction%20to%20Game%20Theory%3A%20Sequence%20Guide%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxZs5Za4qXBegXCgu%2Fintroduction-to-game-theory-sequence-guide%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Introduction%20to%20Game%20Theory%3A%20Sequence%20Guide%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxZs5Za4qXBegXCgu%2Fintroduction-to-game-theory-sequence-guide", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxZs5Za4qXBegXCgu%2Fintroduction-to-game-theory-sequence-guide", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>This sequence of posts is a primer on game theory intended at an introductory level. Because it is introductory, Less Wrong veterans may find some parts boring, obvious, or simplistic - although hopefully nothing is so simplistic as to be outright wrong.<br /><br />Parts of this sequence draw heavily upon material from <em><a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393062430\">The Art of Strategy</a> </em>by Avinash Dixit and Barry Nalebuff, and it may in part be considered a (very favorable) review of the book accompanied by an exploration of its content. I have tried to include enough material to be useful, but not so much material that it becomes a plagiarism rather than a review (it's probably a bad idea to pick a legal fight with people who write books called <em>The Art of Strategy</em>.) Therefore, for the most complete and engaging presentation of this material, I highly recommend the original book. <br /><br />All posts will be linked from here as they go up:<br /><br />1. <a href=\"/lw/dbe/introduction_to_game_theory_sequence_guide/\">Introduction to Game Theory: Sequence Guide</a><br />2. <a href=\"/lw/dbj/backward_reasoning_over_decision_trees/\">Backward Reasoning Over Decision Trees</a><br />3. <a href=\"/lw/dc7/nash_equilibria_and_schelling_points/\">Nash Equilibria and Schelling Points</a><br />4. <a href=\"/lw/dd3/introduction_to_prisoners_dilemma/\">Introduction to Prisoners' Dilemma</a><br />5. <a href=\"/lw/del/real_world_solutions_to_prisoners_dilemmas/\">Real World Solutions to Prisoners' Dilemmas</a><br />6. <a href=\"/lw/dgc/interlude_for_behavioral_economics/\">Interlude for Behavioral Economics</a><br />7. <a href=\"/lw/did/what_is_signaling_really/\">What Is Signaling, Really?</a><br />8. <a href=\"/lw/dm8/bargaining_and_auctions/\">Bargaining and Auctions</a><br />9. <a href=\"/lw/dp6/imperfect_voting_systems/\">Imperfect Voting Systems</a><br />10. <a href=\"/lw/dr9/game_theory_as_a_dark_art/\">Game Theory As A Dark Art</a></p>\n<p>Special thanks to Luke for his book recommendation and his strong encouragement to write this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 13}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QxZs5Za4qXBegXCgu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 72, "extendedScore": null, "score": 0.000155, "legacy": true, "legacyId": "17258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 72, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QxZs5Za4qXBegXCgu", "EhEZoTFzys9EDmEXn", "yJfBzcDL9fBHJfZ6P", "QdXrkWoK2Pp6XhNuQ", "BroeiXGh9PrKZEkJ5", "Yy7mgec8tsbTAuTqb", "KheBaeW8Pi7LwewoF", "um7w5RogAHhxGy8Ti", "xNBRkPNHAGQ6EQaLS", "A2Qam9Bd9xpbb2wLQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-28T04:28:52.384Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] My Kind of Reflection", "slug": "seq-rerun-my-kind-of-reflection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.837Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZeoSNJ6QovbrWzckH/seq-rerun-my-kind-of-reflection", "pageUrlRelative": "/posts/ZeoSNJ6QovbrWzckH/seq-rerun-my-kind-of-reflection", "linkUrl": "https://www.lesswrong.com/posts/ZeoSNJ6QovbrWzckH/seq-rerun-my-kind-of-reflection", "postedAtFormatted": "Thursday, June 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20My%20Kind%20of%20Reflection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20My%20Kind%20of%20Reflection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZeoSNJ6QovbrWzckH%2Fseq-rerun-my-kind-of-reflection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20My%20Kind%20of%20Reflection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZeoSNJ6QovbrWzckH%2Fseq-rerun-my-kind-of-reflection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZeoSNJ6QovbrWzckH%2Fseq-rerun-my-kind-of-reflection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>Today's post, <a href=\"/lw/s2/my_kind_of_reflection/\">My Kind of Reflection</a> was originally published on 10 July 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>A few key differences between Eliezer Yudkowsky's ideas on reflection and the ideas of other philosophers.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/dap/seq_rerun_the_fear_of_common_knowledge/\">The Fear of Common Knowledge</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZeoSNJ6QovbrWzckH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.315219785353557e-07, "legacy": true, "legacyId": "17264", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TynBiYt6zg42StRbb", "YymNfPBduRqbaG3Sw", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-28T06:06:24.237Z", "modifiedAt": null, "url": null, "title": "Help create an instrumental rationality \"stack ranking\"?", "slug": "help-create-an-instrumental-rationality-stack-ranking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.850Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "internety", "createdAt": "2011-02-01T00:45:14.606Z", "isAdmin": false, "displayName": "internety"}, "userId": "d72GEz82JYvQxnkoH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K9nAbPgKkXRqQmgv2/help-create-an-instrumental-rationality-stack-ranking", "pageUrlRelative": "/posts/K9nAbPgKkXRqQmgv2/help-create-an-instrumental-rationality-stack-ranking", "linkUrl": "https://www.lesswrong.com/posts/K9nAbPgKkXRqQmgv2/help-create-an-instrumental-rationality-stack-ranking", "postedAtFormatted": "Thursday, June 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20create%20an%20instrumental%20rationality%20%22stack%20ranking%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20create%20an%20instrumental%20rationality%20%22stack%20ranking%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9nAbPgKkXRqQmgv2%2Fhelp-create-an-instrumental-rationality-stack-ranking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20create%20an%20instrumental%20rationality%20%22stack%20ranking%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9nAbPgKkXRqQmgv2%2Fhelp-create-an-instrumental-rationality-stack-ranking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9nAbPgKkXRqQmgv2%2Fhelp-create-an-instrumental-rationality-stack-ranking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 283, "htmlBody": "<p>I recently heard about SIAI's<a href=\"/lw/5ec/minicamp_on_rationality_awesomeness_and/\" target=\"_blank\"> Rationality Minicamp</a> and thought it sounded cool, but for logistical/expense reasons I won't be going to one.&nbsp;</p>\n<p>There are probably lots of people who are interested in improving their instrumental rationality, know about and like LessWrong, but haven't read the vast majority of content because there is just so much material, and the practical payoff is uncertain.&nbsp;</p>\n<p>It would be cool if it was much easier for people to find the highest ROI material on LessWrong.</p>\n<p>My rough idea for how this new instrumental rationality tool might work:</p>\n<p>&nbsp;</p>\n<ul>\n<li>It starts off as a simple wiki focused on instrumental rationality. People only add things to the wiki (often just links to existing LessWrong articles) if they have tried them and found them very useful for achieving their goals.</li>\n<li>People are encouraged to add \"exercises\" that help you develop the skill represented by the article, of the type that are presumably done at the Rationality Minicamps.</li>\n<li>Only people who have tried the specific thing in question should add comments about their experiences with it.</li>\n<li><strong>Long Term Goal</strong>: Every LessWrong user can define their own private stack rank of the most important concepts/techniques/habits for instrumental rationality. These stack ranks are globally merged by some LessWrong software to create an overall stack rank of the highest ROI ideas/behaviors/techniques as judged by the LessWrong community at any given time. People looking to improve their instrumental rationality can then just visit this global stack rank and pick the highest item that they haven't tried yet to experiment with, and work backwards from there if there are any prerequisites.</li>\n</ul>\n<p>&nbsp;</p>\n<p>Do you think others would find this useful? Anyone have suggested improvements?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K9nAbPgKkXRqQmgv2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 3, "extendedScore": null, "score": 9.315671706578277e-07, "legacy": true, "legacyId": "17270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9vBasHrBtCmC6zAzD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-28T06:33:14.976Z", "modifiedAt": null, "url": null, "title": "Meetup : St. Peterburg meetup", "slug": "meetup-st-peterburg-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HHov7QbfaEAgpTTs9/meetup-st-peterburg-meetup", "pageUrlRelative": "/posts/HHov7QbfaEAgpTTs9/meetup-st-peterburg-meetup", "linkUrl": "https://www.lesswrong.com/posts/HHov7QbfaEAgpTTs9/meetup-st-peterburg-meetup", "postedAtFormatted": "Thursday, June 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20St.%20Peterburg%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20St.%20Peterburg%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHHov7QbfaEAgpTTs9%2Fmeetup-st-peterburg-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20St.%20Peterburg%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHHov7QbfaEAgpTTs9%2Fmeetup-st-peterburg-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHHov7QbfaEAgpTTs9%2Fmeetup-st-peterburg-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bg'>St. Peterburg meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 July 2012 07:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Saint Petersburg, Nevsky Prospect 20</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant at 19:00. Look for a table with \u201cLW\u201d banner, I will put it somewhere.</p>\n\n<p>Topics will be:</p>\n\n<ul>\n<li><p>Rationality in everyday life and in fiction.</p></li>\n<li><p>Transhumanism movement in Russia.</p></li>\n<li><p>Projects to begin by rationalists commumity.</p></li>\n</ul>\n\n<p>Please comment here, if you are going to participate.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bg'>St. Peterburg meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HHov7QbfaEAgpTTs9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 9.315796102719581e-07, "legacy": true, "legacyId": "17278", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___St__Peterburg_meetup\">Discussion article for the meetup : <a href=\"/meetups/bg\">St. Peterburg meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 July 2012 07:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Saint Petersburg, Nevsky Prospect 20</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant at 19:00. Look for a table with \u201cLW\u201d banner, I will put it somewhere.</p>\n\n<p>Topics will be:</p>\n\n<ul>\n<li><p>Rationality in everyday life and in fiction.</p></li>\n<li><p>Transhumanism movement in Russia.</p></li>\n<li><p>Projects to begin by rationalists commumity.</p></li>\n</ul>\n\n<p>Please comment here, if you are going to participate.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___St__Peterburg_meetup1\">Discussion article for the meetup : <a href=\"/meetups/bg\">St. Peterburg meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : St. Peterburg meetup", "anchor": "Discussion_article_for_the_meetup___St__Peterburg_meetup", "level": 1}, {"title": "Discussion article for the meetup : St. Peterburg meetup", "anchor": "Discussion_article_for_the_meetup___St__Peterburg_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-28T15:08:39.443Z", "modifiedAt": null, "url": null, "title": "Call for volunteers: Publishing the Sequences", "slug": "call-for-volunteers-publishing-the-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:32.383Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wedrifid", "createdAt": "2009-07-04T22:18:20.822Z", "isAdmin": false, "displayName": "wedrifid"}, "userId": "FqKohKFRCZnbfbbcS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GmBepDxQx9PNaNz6v/call-for-volunteers-publishing-the-sequences", "pageUrlRelative": "/posts/GmBepDxQx9PNaNz6v/call-for-volunteers-publishing-the-sequences", "linkUrl": "https://www.lesswrong.com/posts/GmBepDxQx9PNaNz6v/call-for-volunteers-publishing-the-sequences", "postedAtFormatted": "Thursday, June 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Call%20for%20volunteers%3A%20Publishing%20the%20Sequences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACall%20for%20volunteers%3A%20Publishing%20the%20Sequences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGmBepDxQx9PNaNz6v%2Fcall-for-volunteers-publishing-the-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Call%20for%20volunteers%3A%20Publishing%20the%20Sequences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGmBepDxQx9PNaNz6v%2Fcall-for-volunteers-publishing-the-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGmBepDxQx9PNaNz6v%2Fcall-for-volunteers-publishing-the-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 222, "htmlBody": "<p><img style=\"float: right; border: 1px solid black; margin-left: 10px; margin-right: 10px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2012/06/The-Sequences-cover.png\" alt=\"\" width=\"300\" />The Singularity Institute is in the process of publishing Eliezer Yudkowsky&rsquo;s <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a> of rationality posts as an electronic book. The Sequences are made up of multiple hundreds of posts. These are being downloaded and converted to LaTeX for publishing programmatically and that&rsquo;s where the human tasks begin. These will entail:</p>\n<ul>\n<li>Verifying that all the content has all been transferred, including all text, equations and images.</li>\n<li>Proofreading for any typographical errors that may have escaped attention thus far.</li>\n<li>Verifying that all external links are still alive (and replacing any that are not).</li>\n<li>Creating a bibliography for all material referenced in the chapters (posts).</li>\n</ul>\n<p>The recent <a href=\"http://intelligence.org/research/\">document</a> <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">publishing</a> efforts at SIAI would not have been possible without the assistance of dedicated volunteers. This new project is the perfect opportunity to <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">help out</a> lesswrong while giving you an excuse to catch up on (or revisit) your reading of some foundational rational thinking material. As an added bonus every post reviewed will save the world with 3.5*epsilon probability.</p>\n<p>We need volunteers who are willing to read some sequence posts and have an eye for detail. Anyone interested in contributing should contact me at cameron.taylor [at] singinst [dot] org.</p>\n<p>For those more interested in academic papers we also have regular publications (and re-publications) that need proofreading and editing before they are released.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GmBepDxQx9PNaNz6v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 22, "extendedScore": null, "score": 9.318185011938173e-07, "legacy": true, "legacyId": "17284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qMuAazqwJvkvo8teR", "3p3CYauiX8oLjmwRF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-29T00:49:38.820Z", "modifiedAt": null, "url": null, "title": "Meetup : A Game of Nomic", "slug": "meetup-a-game-of-nomic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:52.725Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SRa84RZjtGtTMHzM9/meetup-a-game-of-nomic", "pageUrlRelative": "/posts/SRa84RZjtGtTMHzM9/meetup-a-game-of-nomic", "linkUrl": "https://www.lesswrong.com/posts/SRa84RZjtGtTMHzM9/meetup-a-game-of-nomic", "postedAtFormatted": "Friday, June 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20A%20Game%20of%20Nomic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20A%20Game%20of%20Nomic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRa84RZjtGtTMHzM9%2Fmeetup-a-game-of-nomic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20A%20Game%20of%20Nomic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRa84RZjtGtTMHzM9%2Fmeetup-a-game-of-nomic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRa84RZjtGtTMHzM9%2Fmeetup-a-game-of-nomic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bh'>A Game of Nomic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 July 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Midtown Manhattan, New York, NY 10010</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi everyone. I'll be holding a Saturday meetup at my apartment to play Nomic, not this Saturday, but next Saturday (July 7th, nine days from now). For those not familiar with Nomic, it's a game where playing the game is about changing the rules of the game. The last time we played with the NYC rationalist group was super-awesome, and I'm looking forward to doing it again.</p>\n\n<p>Meetup will start at 3 PM, and will be followed by pizza or other forms of dinner (depending on interest).</p>\n\n<p>NOTE: Due to conflict with other events, this has been moved to Saturday, July 21st.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bh'>A Game of Nomic</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SRa84RZjtGtTMHzM9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.32087919400114e-07, "legacy": true, "legacyId": "17286", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___A_Game_of_Nomic\">Discussion article for the meetup : <a href=\"/meetups/bh\">A Game of Nomic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 July 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Midtown Manhattan, New York, NY 10010</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi everyone. I'll be holding a Saturday meetup at my apartment to play Nomic, not this Saturday, but next Saturday (July 7th, nine days from now). For those not familiar with Nomic, it's a game where playing the game is about changing the rules of the game. The last time we played with the NYC rationalist group was super-awesome, and I'm looking forward to doing it again.</p>\n\n<p>Meetup will start at 3 PM, and will be followed by pizza or other forms of dinner (depending on interest).</p>\n\n<p>NOTE: Due to conflict with other events, this has been moved to Saturday, July 21st.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___A_Game_of_Nomic1\">Discussion article for the meetup : <a href=\"/meetups/bh\">A Game of Nomic</a></h2>", "sections": [{"title": "Discussion article for the meetup : A Game of Nomic", "anchor": "Discussion_article_for_the_meetup___A_Game_of_Nomic", "level": 1}, {"title": "Discussion article for the meetup : A Game of Nomic", "anchor": "Discussion_article_for_the_meetup___A_Game_of_Nomic1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-29T02:06:13.524Z", "modifiedAt": "2020-08-21T23:23:53.935Z", "url": null, "title": "Nash Equilibria and Schelling Points", "slug": "nash-equilibria-and-schelling-points", "viewCount": null, "lastCommentedAt": "2021-03-09T16:36:45.040Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Scott Alexander", "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yJfBzcDL9fBHJfZ6P/nash-equilibria-and-schelling-points", "pageUrlRelative": "/posts/yJfBzcDL9fBHJfZ6P/nash-equilibria-and-schelling-points", "linkUrl": "https://www.lesswrong.com/posts/yJfBzcDL9fBHJfZ6P/nash-equilibria-and-schelling-points", "postedAtFormatted": "Friday, June 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nash%20Equilibria%20and%20Schelling%20Points&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANash%20Equilibria%20and%20Schelling%20Points%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyJfBzcDL9fBHJfZ6P%2Fnash-equilibria-and-schelling-points%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nash%20Equilibria%20and%20Schelling%20Points%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyJfBzcDL9fBHJfZ6P%2Fnash-equilibria-and-schelling-points", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyJfBzcDL9fBHJfZ6P%2Fnash-equilibria-and-schelling-points", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2208, "htmlBody": "<p>A Nash equilibrium is an outcome in which neither player is willing to unilaterally change her strategy, and they are often applied to games in which both players move simultaneously and where decision trees are less useful.</p><p>Suppose my girlfriend and I have both lost our cell phones and cannot contact each other. Both of us would really like to spend more time at home with each other (utility 3). But both of us also have a slight preference in favor of working late and earning some overtime (utility 2). If I go home and my girlfriend's there and I can spend time with her, great. If I stay at work and make some money, that would be pretty okay too. But if I go home and my girlfriend's not there and I have to sit around alone all night, that would be the worst possible outcome (utility 1). Meanwhile, my girlfriend has the same set of preferences: she wants to spend time with me, she'd be okay with working late, but she doesn't want to sit at home alone.</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/31240ed1fb069da8c99f86e3d154232a458061b49c85abf4.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/31240ed1fb069da8c99f86e3d154232a458061b49c85abf4.png/w_128 128w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/31240ed1fb069da8c99f86e3d154232a458061b49c85abf4.png/w_208 208w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/31240ed1fb069da8c99f86e3d154232a458061b49c85abf4.png/w_288 288w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/31240ed1fb069da8c99f86e3d154232a458061b49c85abf4.png/w_368 368w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/31240ed1fb069da8c99f86e3d154232a458061b49c85abf4.png/w_448 448w\"></figure><p>This \u201cgame\u201d has two Nash equilibria. If we both go home, neither of us regrets it: we can spend time with each other and we've both got our highest utility. If we both stay at work, again, neither of us regrets it: since my girlfriend is at work, I am glad I stayed at work instead of going home, and since I am at work, my girlfriend is glad she stayed at work instead of going home. Although we both may wish that we had both gone home, neither of us specifically regrets our own choice, given our knowledge of how the other acted.</p><p>When all players in a game are reasonable, the (apparently) rational choice will be to go for a Nash equilibrium (why would you want to make a choice you'll regret when you know what the other player chose?) And since John Nash (remember that movie <i>A Beautiful Mind</i>?) proved that every game has at least one, all games between well-informed rationalists (who are not also being superrational in a sense to be discussed later) should end in one of these.</p><p>What if the game seems specifically designed to thwart Nash equilibria? Suppose you are a general invading an enemy country's heartland. You can attack one of two targets, East City or West City (you declared war on them because you were offended by their uncreative toponyms). The enemy general only has enough troops to defend one of the two cities. If you attack an undefended city, you can capture it easily, but if you attack the city with the enemy army, they will successfully fight you off.</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/292baf9e304831b2f3bb208b31363df2ecc59e2dd4de0306.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/292baf9e304831b2f3bb208b31363df2ecc59e2dd4de0306.png/w_136 136w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/292baf9e304831b2f3bb208b31363df2ecc59e2dd4de0306.png/w_216 216w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/292baf9e304831b2f3bb208b31363df2ecc59e2dd4de0306.png/w_296 296w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/292baf9e304831b2f3bb208b31363df2ecc59e2dd4de0306.png/w_376 376w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/292baf9e304831b2f3bb208b31363df2ecc59e2dd4de0306.png/w_456 456w\"></figure><p>Here there is no Nash equilibrium without introducing randomness. If both you and your enemy choose to go to East City, you will regret your choice - you should have gone to West and taken it undefended. If you go to East and he goes to West, he will regret his choice - he should have gone East and stopped you in your tracks. Reverse the names, and the same is true of the branches where you go to West City. So every option has someone regretting their choice, and there is no simple Nash equilibrium. What do you do?</p><p>Here the answer should be obvious: it doesn't matter. Flip a coin. If you flip a coin, and your opponent flips a coin, neither of you will regret your choice. Here we see a \"mixed Nash equilibrium\", an equilibrium reached with the help of randomness.</p><p>We can formalize this further. Suppose you are attacking a different country with two new potential targets: Metropolis and Podunk. Metropolis is a rich and strategically important city (utility: 10); Podunk is an out of the way hamlet barely worth the trouble of capturing it (utility: 1).</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c0b6fefd812fe627f8d52b840513f76146e8bf6616518de4.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c0b6fefd812fe627f8d52b840513f76146e8bf6616518de4.png/w_136 136w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c0b6fefd812fe627f8d52b840513f76146e8bf6616518de4.png/w_216 216w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c0b6fefd812fe627f8d52b840513f76146e8bf6616518de4.png/w_296 296w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c0b6fefd812fe627f8d52b840513f76146e8bf6616518de4.png/w_376 376w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c0b6fefd812fe627f8d52b840513f76146e8bf6616518de4.png/w_456 456w\"></figure><p>A so-called first-level player thinks: \u201cWell, Metropolis is a better prize, so I might as well attack that one. That way, if I win I get 10 utility instead of 1\u201d</p><p>A second-level player thinks: \u201cObviously Metropolis is a better prize, so my enemy expects me to attack that one. So if I attack Podunk, he'll never see it coming and I can take the city undefended.\u201d</p><p>A third-level player thinks: \u201cObviously Metropolis is a better prize, so anyone clever would never do something as obvious as attack there. They'd attack Podunk instead. But my opponent knows that, so, seeking to stay one step ahead of me, he has defended Podunk. He will never expect me to attack Metropolis, because that would be too obvious. Therefore, the city will actually be undefended, so I should take Metropolis.\u201d</p><p>And so on ad infinitum, until you become hopelessly confused and have no choice but to spend years developing a resistance to iocane powder.</p><p>But surprisingly, there is a single best solution to this problem, even if you are playing against an opponent who, like Professor Quirrell, plays \u201cone level higher than you.\u201d</p><p>When the two cities were equally valuable, we solved our problem by flipping a coin. That won't be the best choice this time. Suppose we flipped a coin and attacked Metropolis when we got heads, and Podunk when we got tails. Since my opponent can predict my strategy, he would defend Metropolis every time; I am equally likely to attack Podunk and Metropolis, but taking Metropolis would cost them much more utility. My total expected utility from flipping the coin is 0.5: half the time I successfully take Podunk and gain 1 utility, and half the time I am defeated at Metropolis and gain 0.And this is not a Nash equilibrium: if I had known my opponent's strategy was to defend Metropolis every time, I would have skipped the coin flip and gone straight for Podunk.</p><p>So how can I find a Nash equilibrium? In a Nash equilibrium, I don't regret my strategy when I learn my opponent's action. If I can come up with a strategy that pays exactly the same utility whether my opponent defends Podunk or Metropolis, it will have this useful property. We'll start by supposing I am flipping a <i>biased</i> coin that lands on Metropolis x percent of the time, and therefore on Podunk (1-x) percent of the time. To be truly indifferent which city my opponent defends, 10x (the utility my strategy earns when my opponent leaves Metropolis undefended) should equal 1(1-x) (the utility my strategy earns when my opponent leaves Podunk undefended). Some quick algebra finds that 10x = 1(1-x) is satisfied by x = 1/11. So I should attack Metropolis 1/11 of the time and Podunk 10/11 of the time.</p><p>My opponent, going through a similar process, comes up with the suspiciously similar result that he should defend Metropolis 10/11 of the time, and Podunk 1/11 of the time.</p><p>If we both pursue our chosen strategies, I gain an average 0.9090... utility each round, soundly beating my previous record of 0.5, and my opponent <a href=\"http://en.wikipedia.org/wiki/Minimax_theorem#Minimax_theorem\">suspiciously</a> loses an average -.9090 utility. It turns out there is no other strategy I can use to consistently do better than this when my opponent is playing optimally, and that even if I knew my opponent's strategy I would not be able to come up with a better strategy to beat it. It also turns out that there is no other strategy my opponent can use to consistently do better than this if I am playing optimally, and that my opponent, upon learning my strategy, doesn't regret his strategy either.</p><p>In <a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393062430\"><i>The Art of Strategy</i></a>, Dixit and Nalebuff cite a real-life application of the same principle in, of all things, penalty kicks in soccer. A right-footed kicker has a better chance of success if he kicks to the right, but a smart goalie can predict that and will defend to the right; a player expecting this can accept a less spectacular kick to the left if he thinks the left will be undefended, but a very smart goalie can predict this too, and so on. Economist Ignacio Palacios-Huerta laboriously analyzed the success rates of various kickers and goalies on the field, <a href=\"https://www.lesswrong.com/Ignacio%20Palacios-Huerta\">and found</a> that they actually pursued a mixed strategy generally within 2% of the game theoretic ideal, proving that people are pretty good at doing these kinds of calculations unconsciously.</p><p>So every game really does have at least one Nash equilibrium, even if it's only a mixed strategy. But some games can have many, many more. Recall the situation between me and my girlfriend:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/cfd517bdba494bb76180fd34742342ac8bd1171d6d116f08.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/cfd517bdba494bb76180fd34742342ac8bd1171d6d116f08.png/w_128 128w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/cfd517bdba494bb76180fd34742342ac8bd1171d6d116f08.png/w_208 208w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/cfd517bdba494bb76180fd34742342ac8bd1171d6d116f08.png/w_288 288w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/cfd517bdba494bb76180fd34742342ac8bd1171d6d116f08.png/w_368 368w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/cfd517bdba494bb76180fd34742342ac8bd1171d6d116f08.png/w_448 448w\"></figure><p>There are two Nash equilibria: both of us working late, and both of us going home. If there were only one equilibrium, and we were both confident in each other's rationality, we could choose that one and there would be no further problem. But in fact this game does present a problem: intuitively it seems like we might still make a mistake and end up in different places.</p><p>Here we might be tempted to just leave it to chance; after all, there's a 50% probability we'll both end up choosing the same activity. But other games might have thousands or millions of possible equilibria and so will require a more refined approach.</p><p><i>Art of Strategy</i> describes a game show in which two strangers were separately taken to random places in New York and promised a prize if they could successfully meet up; they had no communication with one another and no clues about how such a meeting was to take place. Here there are a nearly infinite number of possible choices: they could both meet at the corner of First Street and First Avenue at 1 PM, they could both meet at First Street and Second Avenue at 1:05 PM, etc. Since neither party would regret their actions (if I went to First and First at 1 and found you there, I would be thrilled) these are all Nash equilibria.</p><p>Despite this mind-boggling array of possibilities, in fact all six episodes of this particular game ended with the two contestants meeting successfully after only a few days. The most popular meeting site was the Empire State Building at noon.</p><p>How did they do it? The world-famous Empire State Building is what game theorists call focal: it stands out as a natural and obvious target for coordination. Likewise noon, classically considered the very middle of the day, is a focal point in time. These focal points, also called Schelling points after theorist Thomas Schelling who discovered them, provide an obvious target for coordination attempts.</p><p>What makes a Schelling point? The most important factor is that it be special. The Empire State Building, depending on when the show took place, may have been the tallest building in New York; noon is the only time that fits the criteria of \u201cexactly in the middle of the day\u201d, except maybe midnight when people would be expected to be too sleepy to meet up properly.</p><p>Of course, specialness, like beauty, is in the eye of the beholder. David Friedman writes:</p><blockquote><p><i>Two people are separately confronted with the list of numbers [2, 5, 9, 25, 69, 73, 82, 96, 100, 126, 150 ] and offered a reward if they independently choose the same number. If the two are mathematicians, it is likely that they will both choose 2\u2014the only even prime. Non-mathematicians are likely to choose 100\u2014a number which seems, to the mathematicians, no more unique than the other two exact squares. Illiterates might agree on 69, because of its peculiar symmetry\u2014as would, for a different reason, those whose interest in numbers is more prurient than mathematical.</i></p></blockquote><p>A recent <a href=\"https://www.lesswrong.com/lw/d3h/open_thread_june_1630_2012/6v8u\">open thread comment</a> pointed out that you can justify anything with \u201cfor decision-theoretic reasons\u201d or \u201cdue to meta-level concerns\u201d. I humbly propose adding \u201cas a Schelling point\u201d to this list, except that the list is tongue-in-cheek and Schelling points really do explain almost everything - <a href=\"http://blog.rivast.com/?p=4808\">stock markets</a>, <a href=\"http://iis-db.stanford.edu/evnts/6632/Goemans_-_CISAC_Research_Seminar_Background_Paper_-_New_Borders.pdf\">national borders</a>, <a href=\"http://faculty.lebow.drexel.edu/mccainr/top/eco/game/vow/marriage.html\">marriages</a>, <a href=\"http://www.daviddfriedman.com/Academic/Property/Property.html\">private property</a>, religions, <a href=\"http://www.ccoyne.com/files/Focal_PDF.PDF\">fashion</a>, political parties, peace treaties, social networks, <a href=\"http://www.tinmarine.com/2012/02/schelling-points.html\">software platforms</a> and languages all involve or are based upon Schelling points. In fact, whenever something has \u201csymbolic value\u201d a Schelling point is likely to be involved in some way. I hope to expand on this point a bit more later.</p><p>Sequential games can include one more method of choosing between Nash equilibria: the idea of a <a href=\"http://en.wikipedia.org/wiki/Subgame_perfect_equilibrium\">subgame-perfect equilibrium</a>, a special kind of Nash equlibrium that remains a Nash equilibrium for every subgame of the original game. In more intuitive terms, this equilibrium means that even in a long multiple-move game no one at any point makes a decision that goes against their best interests (remember the example from the last post, where we crossed out the branches in which Clinton made implausible choices that failed to maximize his utility?) Some games have multiple Nash equilibria but only one subgame-perfect one; we'll examine this idea further when we get to the iterated prisoners' dilemma and ultimatum game.</p><p>In conclusion, every game has at least one Nash equilibrium, a point at which neither player regrets her strategy even when she knows the other player's strategy. Some equilibria are simple choices, others involve plans to make choices randomly according to certain criteria. Purely rational players will always end up at a Nash equilibrium, but many games will have multiple possible equilibria. If players are trying to coordinate, they may land at a Schelling point, an equilibria which stands out as special in some way.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 3, "Ng8Gice9KNkncxqcj": 2, "xexCWMyds6QLWognu": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yJfBzcDL9fBHJfZ6P", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 89, "baseScore": 110, "extendedScore": null, "score": 0.000231, "legacy": true, "legacyId": "17287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 110, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-29T02:13:01.306Z", "modifiedAt": null, "url": null, "title": "Scholarship: how to tell good advice from bad advice?", "slug": "scholarship-how-to-tell-good-advice-from-bad-advice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:59.099Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XiN8AusAke5dzX6Mt/scholarship-how-to-tell-good-advice-from-bad-advice", "pageUrlRelative": "/posts/XiN8AusAke5dzX6Mt/scholarship-how-to-tell-good-advice-from-bad-advice", "linkUrl": "https://www.lesswrong.com/posts/XiN8AusAke5dzX6Mt/scholarship-how-to-tell-good-advice-from-bad-advice", "postedAtFormatted": "Friday, June 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scholarship%3A%20how%20to%20tell%20good%20advice%20from%20bad%20advice%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScholarship%3A%20how%20to%20tell%20good%20advice%20from%20bad%20advice%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiN8AusAke5dzX6Mt%2Fscholarship-how-to-tell-good-advice-from-bad-advice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scholarship%3A%20how%20to%20tell%20good%20advice%20from%20bad%20advice%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiN8AusAke5dzX6Mt%2Fscholarship-how-to-tell-good-advice-from-bad-advice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiN8AusAke5dzX6Mt%2Fscholarship-how-to-tell-good-advice-from-bad-advice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p>Lukeprog has done some great posts talking about <a href=\"/lw/9fy/leveling_up_in_rationality_a_personal_journey/\">how scholarship has benefited him personally</a> and <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">how to do it.</a>&nbsp;However, he hasn't talked much about how to tell the good from the bad, and I think this is an especially big problem with it comes to \"how to\"s, rather than standard academic subjects.</p>\n<p>The thing is this: writing on academic subjects tends to be dominated by academics seeking to gain status from their work. <a href=\"/lw/4ba/some_heuristics_for_evaluating_the_soundness_of/\">Not</a> <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">all</a><a href=\"/lw/9sv/diseased_disciplines_the_strange_case_of_the/\"> fields</a> have a strong correlation between status and making actual discoveries, but many (perhaps most) do. So for academic subjects, the big challenge when it comes to making sure you're not reading nonsense is to try to evaluate the soundness of the field in question.</p>\n<p>When it comes to seeking out \"how to\" advice, though, writing on the subjects seems to be dominated by people looking to make money selling advice. That means that if bad advice will sell better than good advice, they'll give the bad advice. Even if their writing includes legitimate advice, they have incentives to give a distorted picture of the subject they're writing on if it will sell better (for example, making whatever they're talking about sound easier than it really is). In some cases, advice books end up trying to nudge you towards buying some more expensive thing the author is selling.</p>\n<p>So: when you're researching a \"how to\" topic, how do you tell the good advice from the bad advice?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XiN8AusAke5dzX6Mt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "17288", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFYWpLNfdwnDwQ3wy", "37sHjeisS9uJufi4u", "fyZBtNB3Ki3fM4a6Y", "FwiPfF8Woe5JrzqEu", "4ACmfJkXQxkYacdLt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-29T05:06:25.645Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Genetic Fallacy", "slug": "seq-rerun-the-genetic-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.587Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ecPeiBSjtznPZTMaF/seq-rerun-the-genetic-fallacy", "pageUrlRelative": "/posts/ecPeiBSjtznPZTMaF/seq-rerun-the-genetic-fallacy", "linkUrl": "https://www.lesswrong.com/posts/ecPeiBSjtznPZTMaF/seq-rerun-the-genetic-fallacy", "postedAtFormatted": "Friday, June 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Genetic%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Genetic%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FecPeiBSjtznPZTMaF%2Fseq-rerun-the-genetic-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Genetic%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FecPeiBSjtznPZTMaF%2Fseq-rerun-the-genetic-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FecPeiBSjtznPZTMaF%2Fseq-rerun-the-genetic-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 201, "htmlBody": "<p>Today's post, <a href=\"/lw/s3/the_genetic_fallacy/\">The Genetic Fallacy</a> was originally published on 11 July 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>The genetic fallacy seems like a strange kind of fallacy. The problem is that the original justification for a belief does not always equal the sum of all the evidence that we currently have available. But, on the other hand, it is very easy for people to still believe untruths from a source that they have since rejected.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/dbk/seq_rerun_my_kind_of_reflection/\">My Kind of Reflection</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ecPeiBSjtznPZTMaF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.322070379613773e-07, "legacy": true, "legacyId": "17299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KZLa74SzyKhSJ3M55", "ZeoSNJ6QovbrWzckH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-29T11:19:58.228Z", "modifiedAt": null, "url": null, "title": "The Fiction Genome Project", "slug": "the-fiction-genome-project", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.721Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oXGTijwhjZB8cnJ3W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ygSvkNd2CmPqiQxLy/the-fiction-genome-project", "pageUrlRelative": "/posts/ygSvkNd2CmPqiQxLy/the-fiction-genome-project", "linkUrl": "https://www.lesswrong.com/posts/ygSvkNd2CmPqiQxLy/the-fiction-genome-project", "postedAtFormatted": "Friday, June 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fiction%20Genome%20Project&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fiction%20Genome%20Project%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FygSvkNd2CmPqiQxLy%2Fthe-fiction-genome-project%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fiction%20Genome%20Project%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FygSvkNd2CmPqiQxLy%2Fthe-fiction-genome-project", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FygSvkNd2CmPqiQxLy%2Fthe-fiction-genome-project", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 677, "htmlBody": "<p>The<strong> Music </strong>Genome Project is what powers&nbsp;<a href=\"http://www.pandora.com/restricted\">Pandora</a>.&nbsp;According&nbsp;to Wikipedia:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>The Music Genome Project was first conceived by Will Glaser and Tim Westergren in late 1999. In January 2000, they joined forces with Jon Kraft to found Pandora Media to bring their idea to market.[1] The Music Genome Project was an effort to \"capture the essence of music at the fundamental level\" using almost 400 attributes to describe songs and a complex mathematical algorithm to organize them. Under the direction of Nolan Gasser, the musical structure and implementation of the Music Genome Project, made up of 5 Genomes (Pop/Rock, Hip-Hop/Electronica, Jazz, World Music, and Classical), was advanced and codified.</p>\n<p>&nbsp;</p>\nA given song is represented by a vector (a list of attributes) containing approximately 400 \"genes\" (analogous to trait-determining genes for organisms in the field of genetics). Each gene corresponds to a characteristic of the music, for example, gender of lead vocalist, level of distortion on the electric guitar, type of background vocals, etc. Rock and pop songs have 150 genes, rap songs have 350, and jazz songs have approximately 400. Other genres of music, such as world and classical music, have 300&ndash;500 genes. The system depends on a sufficient number of genes to render useful results. Each gene is assigned a number between 1 and 5, in half-integer increments.[2]\n<p>&nbsp;</p>\nGiven the vector of one or more songs, a list of other similar songs is constructed using a distance function. Each song is analyzed by a musician in a process that takes 20 to 30 minutes per song.[3] Ten percent of songs are analyzed by more than one technician to ensure conformity with the in-house standards and statistical reliability. The technology is currently used by Pandora to play music for Internet users based on their preferences. Because of licensing restrictions, Pandora is available only to users whose location is reported to be in the USA by Pandora's geolocation software.[4]\n<p>&nbsp;</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Eminent <a href=\"/user/lionhearted/\">lesswronger</a>, strategist, and <a href=\"http://www.sebastianmarshall.com/\">blogger</a>, Sebastian Marshall, &nbsp;<a href=\"http://www.sebastianmarshall.com/why-im-very-skeptical-of-genius\">wonders</a>:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>Personally, I was thinking of doing a sort of &ldquo;DNA analysis&rdquo; of successful writing. Have you heard of the <a class=\"urllink\" href=\"http://en.wikipedia.org/wiki/Music_Genome_Project\">Music Genome Project</a>? It powers <a class=\"urllink\" href=\"http://www.pandora.com/\">Pandora.com</a>.</p>\n<p>&nbsp;</p>\nSo I was thinking, <strong>you could probably do something like that for writing, and then try to craft a written work with elements known to appeal to people. For instance, if you wished to write a best selling detective novel, you might do an analysis of when the antagonist(s) appear in the plot for the first time. You might find that 15% of bestsellers open with the primary antagonist committing their crime, 10% have the antagonist mixed in quickly into the plot, and 75% keep the primary antagonist a vague and shadowy figure until shortly before the climax.</strong>\n<p>&nbsp;</p>\nI don&rsquo;t know if the pattern fits that &ndash; I don&rsquo;t read many detective novels &ndash; but it would be a bit of a surprise if it did. You might think, well, hey, I better either introduce the antagonist right away having them commit their crime, or keep him shadowy for a while.\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Or, to use an easier example &ndash; <strong>perhaps you could wholesale adopt the use of engineering checklists into your chosen discipline?</strong> It seems to me like lots of fields don&rsquo;t use checklists that could benefit tremendously from them. I run this through my mind again and again &ndash; what kind of checklist could be built here? I first came across the concept of checklists being adopted in surgery from engineering, and then having surgical accidents and mistakes go way down.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p><a href=\"http://tvtropes.org/pmwiki/posts.php?discussion=13407002040A24720100&amp;page=1\">Some people at TV Tropes came&nbsp;across&nbsp;that article, and thought that their wiki's database might be a good starting point to make this project a reality.</a> I came here to look for the savvy, intelligence, and level of technical expertise in all things AI and NIT that I've come to expect of this site's user-base, hoping that some of you might be interested in having a look at the discussion, and, perhaps, would feel like joining in, or at least sharing some good advice.</p>\n<p>Thank you. (Also, should I make this post \"Discussion\" or \"Top Level\"?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ygSvkNd2CmPqiQxLy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 19, "extendedScore": null, "score": 9.323803701875209e-07, "legacy": true, "legacyId": "17314", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-29T13:50:48.261Z", "modifiedAt": null, "url": null, "title": "Hedonic vs Preference Utilitarianism in the Context of Wireheading", "slug": "hedonic-vs-preference-utilitarianism-in-the-context-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:04.436Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h9DMcvqhmTeF2MWbd/hedonic-vs-preference-utilitarianism-in-the-context-of", "pageUrlRelative": "/posts/h9DMcvqhmTeF2MWbd/hedonic-vs-preference-utilitarianism-in-the-context-of", "linkUrl": "https://www.lesswrong.com/posts/h9DMcvqhmTeF2MWbd/hedonic-vs-preference-utilitarianism-in-the-context-of", "postedAtFormatted": "Friday, June 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hedonic%20vs%20Preference%20Utilitarianism%20in%20the%20Context%20of%20Wireheading&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHedonic%20vs%20Preference%20Utilitarianism%20in%20the%20Context%20of%20Wireheading%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9DMcvqhmTeF2MWbd%2Fhedonic-vs-preference-utilitarianism-in-the-context-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hedonic%20vs%20Preference%20Utilitarianism%20in%20the%20Context%20of%20Wireheading%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9DMcvqhmTeF2MWbd%2Fhedonic-vs-preference-utilitarianism-in-the-context-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9DMcvqhmTeF2MWbd%2Fhedonic-vs-preference-utilitarianism-in-the-context-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1023, "htmlBody": "<p>We're probably going to develop the technology to directly produce pleasure in the brain with <a href=\"http://en.wikipedia.org/wiki/Brain_stimulation_reward\">electrical stimulation</a>. We <a href=\"http://en.wikipedia.org/wiki/Deep_Brain_Stimulation\">already do this</a> to some extent, though with the goal of restoring people to normal function. This poses a similar question to drugs, but potentially without the primary downsides: <a href=\"http://wiki.lesswrong.com/wiki/Wireheading\">wireheading</a> may cause intense undiminising pleasure. [1]</p>\n<p>Like <a href=\"/lw/65w/not_for_the_sake_of_pleasure_alone/\">many</a> <a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">people</a> my first reaction to the idea is negative. Joy from wireheading strikes me as much less valuable than joy from social interaction, <a href=\"http://en.wikipedia.org/wiki/Flow_(psychology)\">flow</a>, <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">warm fuzzies</a>, or music. But perhaps we discount pleasure from drugs only because their overall effect on people's lives tends to be negative, and there's not actually anything lesser about that kind of happiness. If there's some activity in the brain that corresponds to the joy I value, direct stimulation should if anything be better: we can probably get much more pure, much more intense pleasure with careful application of electricity.</p>\n<p>Maybe wireheading would grow into a role as a kind of special retirement for rich people: once you've saved enough money to pay other people to take care of your physical needs, you plug in and spend the rest of your years euphorically. Perhaps there's a cycle of increasing popularity and decreasing price. If it became cheap enough and made people happy enough, a charity to provide this for people who couldn't otherwise afford it might be more cost-effective at increasing happiness than even the best ones trying to reduce suffering elsewhere.</p>\n<p>Even then, there's something dangerous and strange about a technology that could make people happy even if they didn't want it to. If what matters is happiness it's nearly unimportant whether someone wants to wirehead; even if they hate the idea, the harm of forcing it on them would be much less than the benefit of them being really happy for the rest of their life. Imagine it becomes a pretty standard thing to do at age 30, after fifteen years of hard work to save up the money, but a few holdouts reject wireheading and want to stay unstimulated. A government program to put wires in people's brains and pleasurably stimulate them against their will sounds like dystopic science fiction, but could it be the right thing to do? Morally obligatory, perhaps? Even after accounting for the side effect where other people are unhappy and upset about it?</p>\n<p>Even if I accept the pleasure of wireheading as legitimate, I find the idea of forcing it upon people over their objections repellant. Maybe there's something essentially important about preferences? Instead of trying to maximize joy and minimize suffering, perhaps I should be trying to <a href=\"http://en.wikipedia.org/wiki/Preference_utilitarianism\">best satisfy people's perferences?</a> [2] In most cases valuing preference satisfaction is indistinguishable from valuing happiness: I would prefer to eat chocolate over candy because I think the chocolate would make me happier. I prefer outcomes where I am happier in general, but because I don't like the idea of being forced to do things even if I agree they would make me happier, valuing preferences seems reasonable.</p>\n<p>Preferences and happiness don't always align, however. Consider a small child sitting in front of a fire. They point at a coal and say \"Want! Want!\", being very insistent. They have a strong preference for you to give them the coal, but of course if you do they will experience a lot of pain. Clearly you shouldn't give it to them. Parenting is full of needing to consider the child's long term best interest over their current preferences.</p>\n<p>Or consider fetuses too young to have preferences. I visit a society where it is common to drink a lot, even when pregnant, and everyone sees it as normal. Say they believe me when I describe the effects of large quantities of alcohol <a href=\"http://en.wikipedia.org/wiki/Fetal_alcohol_syndrome\">on fetal development</a> but reject my suggestion that they reduce their consumption: \"the baby doesn't care.\" A fetus early enough in its development not to be capable of preferring anything can still be changed by alcohol. It seems wrong to me to ignore the future suffering of the child on the grounds that it currently has no preferences. [3]</p>\n<p>In regard to death these again disagree. Previously <a href=\"http://www.jefftk.com/news/2012-04-14.html\">it seemed to me</a> that death was bad in that it can be painful to the person, sorrowful for those left behind, and tragic in cutting short a potentially joyful life. But if preferences are what matter then death is much worse: many people have very strong preferences not to die.</p>\n<p>I'm not sure how to reconcile this. Neither preferences nor joy/suffering seem to give answers consistent with my intuitions. (Not that <a href=\"http://www.jefftk.com/news/2012-04-30.html\">I trust them much</a>.) Both come close, though I think the latter comes closer. Treating wireheading-style pleasure as being a lesser kind than real well-being might do it, but if people have strong preferences against something that would give them true happiness there's still an opening for a very ugly paternalism, and I don't see any real reason to discount pleasure from electrical stimulation. Another answer would be that value is more complex than either preferences or happiness and that I just don't fully understand it yet. But when happiness comes so close to fitting I have to consider that it may be right and the ways a value system grounded on happiness differ from my intuitions are problems with my intutions.</p>\n<p><em>(I also posted this <a href=\"http://www.jefftk.com/news/2012-06-29.html\">on my blog</a>)</em></p>\n<p>[1] Yvain <a href=\"/lw/1lb/are_wireheads_happy/\">points out</a> that wireheading experiments may have been stimulating desire instead pleasure. This would mean you'd really want to get more stimulation but aren't actually enjoying it. Still, it's not a stretch to assume that we can figure out what we're stimulating and go for pleasure, or perhaps both pleasure and desire.</p>\n<p>[2] Specifically, current preferences. If I include future preferences then we could just say that while someone currently doesn't want to be wireheaded, after it is forced upon them and they get a taste of it they may have an even stronger preference not to have the stimulation stop.</p>\n<p>[3] Future people make this even stronger. The difference between a future with 10 billion simultaneous happy people and one with several orders of magnitude more seems very important, even though they don't currently exist to have preferences.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb186": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h9DMcvqhmTeF2MWbd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "17316", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["87mdaCvCyo5bkk8hE", "synsRtBKDeAFuo7e3", "3p3CYauiX8oLjmwRF", "HmfxSWnqnK265GEFM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-29T14:10:16.796Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Berlin, Fort Collins, Montreal, NYC, Sydney", "slug": "weekly-lw-meetups-berlin-fort-collins-montreal-nyc-sydney", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8qCG5e5o2vAiTKNd5/weekly-lw-meetups-berlin-fort-collins-montreal-nyc-sydney", "pageUrlRelative": "/posts/8qCG5e5o2vAiTKNd5/weekly-lw-meetups-berlin-fort-collins-montreal-nyc-sydney", "linkUrl": "https://www.lesswrong.com/posts/8qCG5e5o2vAiTKNd5/weekly-lw-meetups-berlin-fort-collins-montreal-nyc-sydney", "postedAtFormatted": "Friday, June 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Berlin%2C%20Fort%20Collins%2C%20Montreal%2C%20NYC%2C%20Sydney&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Berlin%2C%20Fort%20Collins%2C%20Montreal%2C%20NYC%2C%20Sydney%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8qCG5e5o2vAiTKNd5%2Fweekly-lw-meetups-berlin-fort-collins-montreal-nyc-sydney%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Berlin%2C%20Fort%20Collins%2C%20Montreal%2C%20NYC%2C%20Sydney%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8qCG5e5o2vAiTKNd5%2Fweekly-lw-meetups-berlin-fort-collins-montreal-nyc-sydney", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8qCG5e5o2vAiTKNd5%2Fweekly-lw-meetups-berlin-fort-collins-montreal-nyc-sydney", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 413, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/b7\">Montreal First Meetup :&nbsp;<span class=\"date\">23 June 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/b2\">Berlin Meetup:&nbsp;<span class=\"date\">26 June 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/bc\">Fort Collins, Colorado Meetup Thursday 7pm *New Day*:&nbsp;<span class=\"date\">28 June 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/aq\">First Cali, Colombia meetup:&nbsp;<span class=\"date\">02 July 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/b9\">Hyderabad Meetup:&nbsp;<span class=\"date\">08 July 2012 03:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/ap\">Summer Festival Megameetup at NYC:&nbsp;<span class=\"date\">23 June 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/b1\">Less Wrong Sydney - June:&nbsp;<span class=\"date\">28 June 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/bb\">Melbourne, practical rationality:&nbsp;<span class=\"date\">06 July 2012 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8qCG5e5o2vAiTKNd5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.324594165578044e-07, "legacy": true, "legacyId": "17175", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-29T23:24:32.086Z", "modifiedAt": null, "url": null, "title": "Formalising cousin_it's bounded versions of G\u00f6del's theorem", "slug": "formalising-cousin_it-s-bounded-versions-of-goedel-s-theorem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.542Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cW45ADhbckFDomAEv/formalising-cousin_it-s-bounded-versions-of-goedel-s-theorem", "pageUrlRelative": "/posts/cW45ADhbckFDomAEv/formalising-cousin_it-s-bounded-versions-of-goedel-s-theorem", "linkUrl": "https://www.lesswrong.com/posts/cW45ADhbckFDomAEv/formalising-cousin_it-s-bounded-versions-of-goedel-s-theorem", "postedAtFormatted": "Friday, June 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Formalising%20cousin_it's%20bounded%20versions%20of%20G%C3%B6del's%20theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFormalising%20cousin_it's%20bounded%20versions%20of%20G%C3%B6del's%20theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcW45ADhbckFDomAEv%2Fformalising-cousin_it-s-bounded-versions-of-goedel-s-theorem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Formalising%20cousin_it's%20bounded%20versions%20of%20G%C3%B6del's%20theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcW45ADhbckFDomAEv%2Fformalising-cousin_it-s-bounded-versions-of-goedel-s-theorem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcW45ADhbckFDomAEv%2Fformalising-cousin_it-s-bounded-versions-of-goedel-s-theorem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1188, "htmlBody": "<p>In this post, I'll try and put cousin_it's <a href=\"/lw/dba/bounded_versions_of_g%C3%B6dels_and_l%C3%B6bs_theorems/\">bounded version of&nbsp;G&ouml;del's theorem</a> on a rigorous footing. This will be logic-heavy, not programming heavy. The key unproved assumptions are listed at the bottom of the post.</p>\n<p>Notes on terminology:</p>\n<ul>\n<li>T is our deductive system.</li>\n<li>&perp; is a canonical contradiction in T. &not;A is defined to be&nbsp;A&rarr;&perp;.</li>\n<li>T&nbsp;\u22a2 A means that T proves the proposition A.</li>\n<li>T&nbsp;\u22a2<sub>j</sub> A means that T proves A in a proof requiring no more than j symbols.</li>\n<li>\u25a1 A is the statement \"there exists a number that is an encoding in <a href=\"http://en.wikipedia.org/wiki/G%C3%B6del_numbering\">G&ouml;del numbering</a> of a valid proof in T of A\".</li>\n<li>\u25a1<sub>n</sub> A is the statement \"there exists a number that is an encoding in&nbsp;G&ouml;del numbering&nbsp;of a valid proof in A with no more than n symbols in the proof\".</li>\n</ul>\n<p>The first needed fact is that there is a computable function g such that if&nbsp;T&nbsp;\u22a2<sub>n</sub>&nbsp;A, then T&nbsp;\u22a2<sub>g(n)</sub>&nbsp;\u25a1<span style=\"font-size: 11px;\"><sub>n</sub></span>&nbsp;A. In informal language, this means that if T can prove A in j symbols, it can prove that it can do so, in g(j) symbols. This g will be the critical piece of the infrastructure.</p>\n<p>Now, these arguments work with general definitions of proofs, but I like to put exact bounds where I can and move uncertainty to a single place. So I will put some restrictions on the language we use, and what counts as a proof. First, I will assume the symbols \"(\", \")\", \"j\", and \"=\" are part of the language - ( and ) as parentheses, and j as a free variable.</p>\n<p>I will require that proofs start and end with parenthesis. A proof need not state what statement it is proving! As long as there is a computable process that can check that \"p is a proof of A\", p need not mention A. This means that I can make the following&nbsp;assumptions:</p>\n<ul>\n<li>If p is a proof of&nbsp;A&rarr;B&nbsp;and q a proof of A, then&nbsp;(pq) is a proof of B (modus ponens).</li>\n<li>If p is a proof of&nbsp;A&rarr;B&nbsp;and q a proof of &not;B, then&nbsp;(pq) is a proof of &not;A (modus tollens).</li>\n<li>A proof that A&harr;B will be treated as a valid proof of A&rarr;B and B&rarr;A also.</li>\n<li>Well-formed sentences with free variables can also have proofs (as in \"j=0 or j&gt;0\" and \"j&gt;3 &rarr; j<sup>2</sup>&gt;9\").</li>\n<li>If A and B have free variable j, and p is a proof that&nbsp;A&rarr;B, then (p(j=n)) is a proof that&nbsp;A(n)&rarr;B(n).<a id=\"more\"></a></li>\n</ul>\n<p>For a number n,&nbsp;l(n) is defined as the minimum number of symbols in the language of T needed to define n; generally l(n) is proportional to log(n).</p>\n<p>R will be defined using the <a href=\"http://en.wikipedia.org/wiki/Diagonal_lemma\">diagonal lemma</a>, with one extra free variable j, as:</p>\n<ul>\n<li>T&nbsp;\u22a2<sub>a</sub>&nbsp; &nbsp;R &harr; (\u25a1<sub>j</sub>&nbsp;R &rarr;&nbsp;<strong>&perp;</strong>)</li>\n</ul>\n<p>In other words, R(n) claims that there is no proof of length n of R(n). Then:</p>\n<ul>\n<li>T&nbsp;\u22a2&nbsp; &nbsp;\u25a1<sub>j</sub>&nbsp;R &rarr; \u25a1<sub>j+a+2&nbsp;</sub>(\u25a1<span style=\"font-size: 11px;\"><sub>j</sub></span>&nbsp;R &rarr;&nbsp;<strong>&perp;</strong>)</li>\n</ul>\n<p>Here T simply starts with a proof of length j of R, adds the&nbsp;preceding&nbsp;proof of length a that R implies&nbsp;(\u25a1<span style=\"font-size: 11px;\"><sub>j</sub></span>&nbsp;R &rarr;&nbsp;&perp;), plus two extra parentheses, and thus generates a proof of length j+a+2 of (\u25a1<span style=\"font-size: 11px;\"><sub>j</sub></span>&nbsp;R &rarr;&nbsp;&perp;). This is an algorithmic process whose validity T can establish.</p>\n<ul>\n<li>T&nbsp;\u22a2 &nbsp; \u25a1<sub>j+a+2</sub>&nbsp;((\u25a1<span style=\"font-size: 11px;\"><sub>j</sub></span>&nbsp;R)&nbsp;&rarr;&nbsp;<strong>&perp;</strong>) &nbsp; &nbsp;&rarr; &nbsp; &nbsp;(\u25a1<sub>g(j)&nbsp;</sub>(\u25a1<sub>j</sub>&nbsp;R) &rarr; (\u25a1<sub>g(j)+j+a+4&nbsp;</sub><strong>&perp;</strong>))</li>\n</ul>\n<p>This implies that if we can prove in j+a+2 symbols that a certain expression (here&nbsp;(\u25a1<span style=\"font-size: 11px;\"><sub>j</sub></span>&nbsp;R)) implies&nbsp;&perp;, then a proof of that expression in any specific number of symbols (here g(j)) will give you a proof of&nbsp;&perp; in g(j)+j+k+2 symbols. That longer proof will simply be the proof of&nbsp;(\u25a1<span style=\"font-size: 11px;\"><sub>j</sub></span>&nbsp;R)&nbsp;&rarr;&nbsp;&perp; (length j+k), followed by the proof of&nbsp;\u25a1<sub>j</sub>&nbsp;R (length g(j)), with starting and terminating parenthesis (two more symbols). That whole process was an algorithm, and T can show that it works.</p>\n<p>We can put this together with the previous theorem, to get:</p>\n<ul>\n<li>T&nbsp;\u22a2 &nbsp;&nbsp; \u25a1<sub>j</sub>&nbsp;R &rarr;&nbsp;&nbsp;(\u25a1<sub>g(j)&nbsp;</sub>(\u25a1<sub>j</sub>&nbsp;R) &rarr; (\u25a1<sub>g(j)+j+a+4&nbsp;</sub><strong>&perp;</strong>))</li>\n</ul>\n<p>Now we must use:</p>\n<ul>\n<li>T&nbsp;\u22a2 &nbsp; \u25a1<sub>j</sub>&nbsp;R&nbsp;&rarr; \u25a1<sub>g(j)&nbsp;</sub>(\u25a1<sub>j</sub>&nbsp;R)</li>\n</ul>\n<p>This is just the fomalisation of the fact that T can prove that it can check a proof of length j in g(j) steps. Putting that together with the previous expression gives:</p>\n<ul>\n<li>T&nbsp;\u22a2<sub>b</sub>&nbsp; &nbsp;\u25a1<sub>j</sub>&nbsp;R &rarr;&nbsp;&nbsp;\u25a1<sub>g(j)+j+a+4&nbsp;</sub><strong>&perp;</strong></li>\n</ul>\n<p>Here b is some fixed constant that, critically, doesn't depend on n (because we haven't even seen n yet!) Now we are finally ready to insert n instead of j.</p>\n<ul>\n<li>T&nbsp;\u22a2<sub>b+l(n)+6</sub>&nbsp; &nbsp;\u25a1<sub>n</sub>&nbsp;R(n) &rarr;&nbsp;&nbsp;\u25a1<sub>g(n)+n+a+4&nbsp;</sub><strong>&perp;</strong></li>\n</ul>\n<p>This is simply the proof above, followed by the expression (j=n), of 4+l(n) symbols, and two more surrounding brackets. Now we add the <strong>critical assumption</strong>: that there exists a proof of length c that there is no proof of contradiction in T, up to length d:</p>\n<ul>\n<li>T \u22a2<sub>c</sub>&nbsp; &nbsp;(\u25a1<sub>d&nbsp;</sub><strong>&perp;</strong>)&nbsp; &rarr;&nbsp;<strong>&perp;</strong></li>\n</ul>\n<p>For the argument to work, we will require c&nbsp;&le;&nbsp;n-(b+l(n)+6)-(a+l(n)+10) and d = g(n)+n+a+4. Then putting the above two statements together (I seem to say that a lot), and remembering that A&nbsp;&rarr;&nbsp;&perp; is the definition of &not;A and that the proof system allows direct modus tollens at the cost of two extra brackets,</p>\n<ul>\n<li>T&nbsp;\u22a2<sub>n-a-l(n)-8</sub>&nbsp; &nbsp;\u25a1<sub>n</sub>&nbsp;R(n) &rarr; <strong>&perp;</strong></li>\n</ul>\n<p>Now, we can take the original diagonal lemma definition of R (provable in a symbols), and add in j=n, giving</p>\n<ul>\n<li>T&nbsp;\u22a2<sub>a+l(n)+6</sub>&nbsp; &nbsp;R(n) &harr; (\u25a1<sub>n</sub>&nbsp;R(n) &rarr;&nbsp;<strong>&perp;</strong>)</li>\n</ul>\n<p>Again combining the two, we finally reach our goal:</p>\n<ul>\n<li>T&nbsp;\u22a2<sub>n</sub>&nbsp; &nbsp;R(n) </li>\n</ul>\n<p>From now on, keeping track of how many steps T requires is less vital, but we'll do it anyway to see how many steps the it takes for the contradiction to emerge. As before, T can prove in g(n) steps that it can prove R(n) in n steps:</p>\n<ul>\n<li>T&nbsp;\u22a2<sub>g(n)</sub>&nbsp; &nbsp;\u25a1<sub>n</sub>&nbsp;R(n)</li>\n</ul>\n<p>Combining this line with the one where we proved &not;\u25a1<sub>n</sub>&nbsp;R(n) (equivalently,&nbsp;\u25a1<sub>n</sub>&nbsp;R(n)&nbsp;&rarr;&nbsp;&perp;) we get a direct modus ponens:</p>\n<ul>\n<li>T&nbsp;\u22a2<sub>g(n)+</sub><sub>n-a-l(n)-6</sub>&nbsp; &nbsp;<strong>&perp;</strong></li>\n</ul>\n<p>I.e. T proves a contradiction. And an exhausted QED.</p>\n<p>So, the critical assumption that blows everything up was that there was a proof of length less than&nbsp;n-(b+l(n)+6)-(a+l(n)+10) that there were no contradictions in proof of T up to length g(n)+n+a+4. Neglecting the constant terms, this means that there is a proof of length n-2l(n) (with l(n) of order log(n)) of lack of contradictions up to length g(n)+n.</p>\n<p>So how fast does the g(n) term grow? This measures how long it takes T to parse a proof of length n, checking validity. I would hazard a guess that g(n) is no worse that quadratic in n, and possibly of order nlog(n) or linear in n. It depends partially on the efficiency of the proofs we allow. If our proofs can call any previously proved statement in the whole of the proof structure, the complexity of checking may be n<sup>2</sup> or nlog(n). If our proofs are less efficient and longer - only using modus ponens or tollens on adjacent expressions bracketed together, and hence needing a lot of pointless repetition - then the parser has an easier job, and probably can check the proof in a linear amount of steps.</p>\n<p>Assumptions still needing fully rigorous proofs:</p>\n<ul>\n<li>Counting the number of symbols in&nbsp;G&ouml;del numbering of a proof is a primitive recursive operation.</li>\n<li>There is a computable function g&nbsp;such that if&nbsp;T&nbsp;\u22a2<sub>n</sub> A, then&nbsp;T&nbsp;\u22a2<sub>g(n)</sub>&nbsp;\u25a1<sub>n</sub>&nbsp;A, and T can prove this.</li>\n<li>Diagonalisation works with an extra free variable.</li>\n<li>My definition of what constitutes a proof is not problematic.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AJDHQ4mFnsNbBzPhT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cW45ADhbckFDomAEv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 9.327167470864406e-07, "legacy": true, "legacyId": "17315", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["z7SuGwxTBnQm8uFq4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-30T00:51:35.867Z", "modifiedAt": null, "url": null, "title": "Meetup : Tucson: Fundamental Questions", "slug": "meetup-tucson-fundamental-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:35.430Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DeevGrape", "createdAt": "2011-11-02T19:26:47.963Z", "isAdmin": false, "displayName": "DeevGrape"}, "userId": "m9JPq6irpKromu9x9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LNXN3CaZLcLk6cRLK/meetup-tucson-fundamental-questions", "pageUrlRelative": "/posts/LNXN3CaZLcLk6cRLK/meetup-tucson-fundamental-questions", "linkUrl": "https://www.lesswrong.com/posts/LNXN3CaZLcLk6cRLK/meetup-tucson-fundamental-questions", "postedAtFormatted": "Saturday, June 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tucson%3A%20Fundamental%20Questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tucson%3A%20Fundamental%20Questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLNXN3CaZLcLk6cRLK%2Fmeetup-tucson-fundamental-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tucson%3A%20Fundamental%20Questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLNXN3CaZLcLk6cRLK%2Fmeetup-tucson-fundamental-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLNXN3CaZLcLk6cRLK%2Fmeetup-tucson-fundamental-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bi'>Tucson: Fundamental Questions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 July 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2443 North Campbell Avenue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After the moderate success of our last meetup (during which much fun and conversation was had!), there is now a next meetup. The nominal topic we'll be discussing is \"the fundamental question of rationality\":  \"What do you believe and why do you believe it?\" And maybe we'll also talk about \"What are you doing and why are you doing it?\" Of course, discussion will veer.\nHope to see some new faces!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bi'>Tucson: Fundamental Questions</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LNXN3CaZLcLk6cRLK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.327571802732855e-07, "legacy": true, "legacyId": "17318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tucson__Fundamental_Questions\">Discussion article for the meetup : <a href=\"/meetups/bi\">Tucson: Fundamental Questions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 July 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2443 North Campbell Avenue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After the moderate success of our last meetup (during which much fun and conversation was had!), there is now a next meetup. The nominal topic we'll be discussing is \"the fundamental question of rationality\":  \"What do you believe and why do you believe it?\" And maybe we'll also talk about \"What are you doing and why are you doing it?\" Of course, discussion will veer.\nHope to see some new faces!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tucson__Fundamental_Questions1\">Discussion article for the meetup : <a href=\"/meetups/bi\">Tucson: Fundamental Questions</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tucson: Fundamental Questions", "anchor": "Discussion_article_for_the_meetup___Tucson__Fundamental_Questions", "level": 1}, {"title": "Discussion article for the meetup : Tucson: Fundamental Questions", "anchor": "Discussion_article_for_the_meetup___Tucson__Fundamental_Questions1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-30T00:54:56.884Z", "modifiedAt": "2020-08-21T23:24:33.626Z", "url": null, "title": "Introduction to Prisoners' Dilemma", "slug": "introduction-to-prisoners-dilemma", "viewCount": null, "lastCommentedAt": "2021-03-31T19:12:06.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Scott Alexander", "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QdXrkWoK2Pp6XhNuQ/introduction-to-prisoners-dilemma", "pageUrlRelative": "/posts/QdXrkWoK2Pp6XhNuQ/introduction-to-prisoners-dilemma", "linkUrl": "https://www.lesswrong.com/posts/QdXrkWoK2Pp6XhNuQ/introduction-to-prisoners-dilemma", "postedAtFormatted": "Saturday, June 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Introduction%20to%20Prisoners'%20Dilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntroduction%20to%20Prisoners'%20Dilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQdXrkWoK2Pp6XhNuQ%2Fintroduction-to-prisoners-dilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Introduction%20to%20Prisoners'%20Dilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQdXrkWoK2Pp6XhNuQ%2Fintroduction-to-prisoners-dilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQdXrkWoK2Pp6XhNuQ%2Fintroduction-to-prisoners-dilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1376, "htmlBody": "<p><strong>Related to: </strong><a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_dilemma\">Previous posts on the Prisoners' Dilemma</a></p><p>Sometimes Nash equilibria just don't match our intuitive criteria for a good outcome. The classic example is the Prisoners' Dilemma.</p><p>The police arrest two criminals, Alice and Bob, on suspicion of murder. The police admit they don't have enough evidence to convict the pair of murder, but they do have enough evidence to convict them of a lesser offence, possession of a firearm. They place Alice and Bob in separate cells and offer them the following deal:</p><p>\u201cIf neither of you confess, we'll have to charge you with possession, which will land you one year in jail. But if you turn state's witness against your partner, we can convict your partner of murder and give her the full twenty year sentence; in exchange, we will let you go free. Unless, that is, both of you testify against each other; in that case, we'll give you both fifteen years.\u201d</p><p>Alice's decision tree looks like this (note that although Alice and Bob make their decisions simultaneously, I've represented it with Alice's decision first, which is a little sketchy but should illustrate the point):</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/907167717c43de8c6544d16ec60aafb43daf1930359e168a.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/907167717c43de8c6544d16ec60aafb43daf1930359e168a.png/w_110 110w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/907167717c43de8c6544d16ec60aafb43daf1930359e168a.png/w_190 190w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/907167717c43de8c6544d16ec60aafb43daf1930359e168a.png/w_270 270w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/907167717c43de8c6544d16ec60aafb43daf1930359e168a.png/w_350 350w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/907167717c43de8c6544d16ec60aafb43daf1930359e168a.png/w_430 430w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/907167717c43de8c6544d16ec60aafb43daf1930359e168a.png/w_510 510w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/907167717c43de8c6544d16ec60aafb43daf1930359e168a.png/w_590 590w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/907167717c43de8c6544d16ec60aafb43daf1930359e168a.png/w_670 670w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/907167717c43de8c6544d16ec60aafb43daf1930359e168a.png/w_750 750w\"></figure><p>If we use the same strategy we used as a chess player, we can cross out options where Bob decides to spend extra years in jail for no personal benefit, and we're left with this:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3bb4916c58ec9c958d9482be23e238d980c662ea5735625b.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3bb4916c58ec9c958d9482be23e238d980c662ea5735625b.png/w_110 110w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3bb4916c58ec9c958d9482be23e238d980c662ea5735625b.png/w_190 190w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3bb4916c58ec9c958d9482be23e238d980c662ea5735625b.png/w_270 270w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3bb4916c58ec9c958d9482be23e238d980c662ea5735625b.png/w_350 350w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3bb4916c58ec9c958d9482be23e238d980c662ea5735625b.png/w_430 430w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3bb4916c58ec9c958d9482be23e238d980c662ea5735625b.png/w_510 510w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3bb4916c58ec9c958d9482be23e238d980c662ea5735625b.png/w_590 590w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3bb4916c58ec9c958d9482be23e238d980c662ea5735625b.png/w_670 670w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3bb4916c58ec9c958d9482be23e238d980c662ea5735625b.png/w_750 750w\"></figure><p>Seen like this, the choice is clear. If you stay quiet (\u201ccooperate\u201d), Bob turns on you, and you are left in jail alone for twenty years, wailing and gnashing your teeth. So instead, you both turn on each other (\u201cdefect\u201d), and end up with a sentence barely any shorter.</p><p>Another way to \u201cprove\u201d that defection is the \u201cright\u201d choice places Bob's decision first. What if you knew Bob would choose to cooperate with you? Then your choice would be between defecting and walking free, or cooperating and spending a whole year in jail - here defection wins out. But what if you knew Bob would choose to defect against you? Then your choice would be between defecting and losing fifteen years, or cooperating and losing twenty - again defection wins out. Since Bob can only either defect or cooperate, and since defection is better in both branches, \u201cclearly\u201d defection is the best option.</p><p>But a lot of things about this solution seem intuitively stupid. For example, when Bob goes through the same reasoning, your \u201crational\u201d solution ends up with both of you in jail for fifteen years, but if you had both cooperated, you would have been out after a year. Both cooperating is better for both of you than both defecting, but you still both defect.</p><p>And if you still don't find that odd, imagine a different jurisdiction where the sentence for possession is only one day, and the police will only take a single day off your sentence for testifying against an accomplice. Now a pair of cooperators would end up with only a day in jail each, and a pair of defectors would end up with nineteen years, three hundred sixty four days each. Yet the math still tells you to defect!</p><p>Unfortunately, your cooperation only helps Bob, and Bob's cooperation only helps you. We can think of the Prisoner's Dilemma as a problem: both you and Bob prefer (cooperate, cooperate) to (defect, defect), but as it is, you're both going to end out with (defect, defect) and it doesn't seem like there's much you can do about it. To \u201csolve\u201d the Prisoner's Dilemma would be to come up with a way to make you and Bob pick the more desirable (cooperate, cooperate) outcome.</p><p>One proposed solution to the Prisoner's Dilemma is to iterate it - to assume it will happen multiple times in succession, as if Alice and Bob are going to commit new crimes as soon as they both get out of prison. In this case, you can threaten to reciprocate; to promise to reward cooperation with future cooperation and punish defection with future defection. Suppose Alice and Bob plan to commit two crimes, and before the first crime both promise to stay quiet on the second crime if and only if their partner stays quiet on the first. Now your decision tree as Alice looks like this:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083f213a5dd5101c838cc852d3486d4e9a9b26fa50df6b4c.png/w_975 975w\"></figure><p>And your calculation of Bob's thought processes go like this:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66f888fb50beb290660959c8623cdf1e712d79fb1f9406cc.png/w_975 975w\"></figure><p>Remember that, despite how the graph looks, your first choice and Bob's first choice are simultaneous: they can't causally affect each other. So as Alice, you reason like this: On the top, Bob knows that if you testify against him, his choice will be either to testify against you (leading to the branch where you both testify against each other again next time) or to stay quiet (leading to a branch where next time he testifies against you but you stay quiet). So Bob reasons that if you testify against him, he should stay quiet this time.</p><p>On the bottom, Bob knows that if you don't testify against him, he can either testify against you (leading to the branch where you testify against him next time but he stays quiet) or stay quiet (leading to the branch where you both stay quiet again next time). Therefore, if you don't testify against him, Bob won't testify against you.</p><p>So you know that no matter what you do this time, Bob won't testify against you. That means your choice is between branches 2 and 4: Bob testifying against you next time or Bob not testifying against you next time. You prefer Branch 4, so you decide not to testify against Bob. The dilemma ends with neither of you testifying against each other in either crime, and both of you getting away with very light two year sentences.</p><p>The teeny tiny little flaw in this plan is that Bob may be a dirty rotten liar. Maybe he says he'll reciprocate, and so you both stay quiet after the first crime. Upon getting out of jail you continue your crime spree, predictably get re-arrested, and you stay quiet like you said you would to reward Bob's cooperation last time. But at the trial, you get a nasty surprise: Bob defects against you and walks free, and you end up with a twenty year sentence.</p><p>If we ratchet up to sprees of one hundred crimes and subsequent sentences (presumably committed by immortal criminals who stubbornly refuse to be cowed by the police's 100% conviction rate) on first glance it looks like we can successfully ensure Bob's cooperation on 99 of those crimes. After all, Bob won't want to defect on crime 50, because I could punish him on crime 51. He won't want to defect on crime 99, because I could punish him on crime 100. But he <i>will</i> want to defect on crime 100, because he gains either way and there's nothing I can do to punish him.</p><p>Here's where it gets weird. I assume Bob is a rational utility-maximizer and so will always defect on crime 100, since it benefits him and I can't punish him for it. So since I'm also rational, I might as well also defect on crime 100; my previous incentive to cooperate was to ensure Bob's good behavior, but since Bob won't show good behavior on crime 100 no matter what I do, I might as well look after my own interests.</p><p>But if we both know that we're both going to defect on crime 100 no matter what, then there's no incentive to cooperate on crime 99. After all, the only incentive to cooperate on crime 99 was to ensure my rival's cooperation on crime 100, and since that's out of the picture anyway, I might as well shorten my sentence a little.</p><p>Sadly, this generalizes by a sort of proof by induction. If crime N will always be (defect, defect), then crime N-1 should also always be (defect, defect), which means we should defect on all of the hundred crimes in our spree.</p><p>This feat of reasoning has limited value in real life, where perfectly rational immortal criminals rarely plot in smoke-filled rooms to commit exactly one hundred crimes together; criminals who are uncertain exactly when their crime sprees will come to a close still have incentive to cooperate. But it still looks like we're going to need a better solution than simply iterating the dilemma. The next post will discuss possibilities for such a solution.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 3, "be2Mh2bddQ6ZaBcti": 14}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QdXrkWoK2Pp6XhNuQ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 56, "extendedScore": null, "score": 0.000118, "legacy": true, "legacyId": "17319", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 57, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-30T03:06:44.114Z", "modifiedAt": null, "url": null, "title": "Meetup : DC Planning Meetup", "slug": "meetup-dc-planning-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pjeeW3pvzG3ohLKpS/meetup-dc-planning-meetup", "pageUrlRelative": "/posts/pjeeW3pvzG3ohLKpS/meetup-dc-planning-meetup", "linkUrl": "https://www.lesswrong.com/posts/pjeeW3pvzG3ohLKpS/meetup-dc-planning-meetup", "postedAtFormatted": "Saturday, June 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20DC%20Planning%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20DC%20Planning%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpjeeW3pvzG3ohLKpS%2Fmeetup-dc-planning-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20DC%20Planning%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpjeeW3pvzG3ohLKpS%2Fmeetup-dc-planning-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpjeeW3pvzG3ohLKpS%2Fmeetup-dc-planning-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 35, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bj'>DC Planning Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 July 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Portrait Gallery Courtyard</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting to discuss the future of the group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bj'>DC Planning Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pjeeW3pvzG3ohLKpS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 9.328199459013192e-07, "legacy": true, "legacyId": "17325", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___DC_Planning_Meetup\">Discussion article for the meetup : <a href=\"/meetups/bj\">DC Planning Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 July 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Portrait Gallery Courtyard</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting to discuss the future of the group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___DC_Planning_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/bj\">DC Planning Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : DC Planning Meetup", "anchor": "Discussion_article_for_the_meetup___DC_Planning_Meetup", "level": 1}, {"title": "Discussion article for the meetup : DC Planning Meetup", "anchor": "Discussion_article_for_the_meetup___DC_Planning_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-30T03:17:13.241Z", "modifiedAt": "2020-08-21T23:22:42.408Z", "url": null, "title": "Backward Reasoning Over Decision Trees", "slug": "backward-reasoning-over-decision-trees", "viewCount": null, "lastCommentedAt": "2020-07-12T20:27:52.980Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Scott Alexander", "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EhEZoTFzys9EDmEXn/backward-reasoning-over-decision-trees", "pageUrlRelative": "/posts/EhEZoTFzys9EDmEXn/backward-reasoning-over-decision-trees", "linkUrl": "https://www.lesswrong.com/posts/EhEZoTFzys9EDmEXn/backward-reasoning-over-decision-trees", "postedAtFormatted": "Saturday, June 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Backward%20Reasoning%20Over%20Decision%20Trees&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABackward%20Reasoning%20Over%20Decision%20Trees%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhEZoTFzys9EDmEXn%2Fbackward-reasoning-over-decision-trees%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Backward%20Reasoning%20Over%20Decision%20Trees%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhEZoTFzys9EDmEXn%2Fbackward-reasoning-over-decision-trees", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhEZoTFzys9EDmEXn%2Fbackward-reasoning-over-decision-trees", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1341, "htmlBody": "<p>Game theory is the study of how rational actors interact to pursue incentives. It starts with the same questionable premises as economics: that everyone behaves rationally, that everyone is purely self-interested1, and that desires can be exactly quantified - and uses them to investigate situations of conflict and cooperation.</p><p>Here we will begin with some fairly obvious points about decision trees, but by the end we will have the tools necessary to explain a somewhat surprising finding: that giving a US president the additional power of line-item veto may in many cases make the president less able to enact her policies. Starting at the beginning:</p><p>The basic unit of game theory is the choice. Rational agents make choices in order to maximize their utility, which is sort of like a measure of how happy they are. In a one-person game, your choices affect yourself and maybe the natural environment, but nobody else. These are pretty simple to deal with:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8eab992eda83b71a2ed42ee27341b48fc1bb3821526c05ab.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8eab992eda83b71a2ed42ee27341b48fc1bb3821526c05ab.png/w_123 123w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8eab992eda83b71a2ed42ee27341b48fc1bb3821526c05ab.png/w_203 203w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8eab992eda83b71a2ed42ee27341b48fc1bb3821526c05ab.png/w_283 283w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8eab992eda83b71a2ed42ee27341b48fc1bb3821526c05ab.png/w_363 363w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8eab992eda83b71a2ed42ee27341b48fc1bb3821526c05ab.png/w_443 443w\"></figure><p>Here we visualize a choice as a branching tree. At each branch, we choose the option with higher utility; in this case, going to the beach. Since each outcome leads to new choices, sometimes the decision trees can be longer than this:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70770aafc9dd2bd768903386e5d2aee7457b293704178e9f.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70770aafc9dd2bd768903386e5d2aee7457b293704178e9f.png/w_110 110w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70770aafc9dd2bd768903386e5d2aee7457b293704178e9f.png/w_190 190w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70770aafc9dd2bd768903386e5d2aee7457b293704178e9f.png/w_270 270w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70770aafc9dd2bd768903386e5d2aee7457b293704178e9f.png/w_350 350w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70770aafc9dd2bd768903386e5d2aee7457b293704178e9f.png/w_430 430w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70770aafc9dd2bd768903386e5d2aee7457b293704178e9f.png/w_510 510w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70770aafc9dd2bd768903386e5d2aee7457b293704178e9f.png/w_590 590w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70770aafc9dd2bd768903386e5d2aee7457b293704178e9f.png/w_670 670w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70770aafc9dd2bd768903386e5d2aee7457b293704178e9f.png/w_750 750w\"></figure><p>Here's a slightly more difficult decision, denominated in money instead of utility. If you want to make as much money as possible, then your first choice - going to college or starting a minimum wage job right Now - seems to favor the more lucrative minimum wage job. But when you take Later into account, college opens up more lucrative future choices, as measured in the gray totals on the right-hand side. This illustrates the important principle of reasoning backward over decision trees. If you reason forward, taking the best option on the first choice and so on, you end up as a low-level manager. To get the real cash, you've got to start at the end - the total on the right - and then examine what choice at each branch will take you there.</p><p>This is all about as obvious as, well, not hitting yourself on the head with a hammer, so let's move on to where it really gets interesting: two-player games.</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1434a2583038eb0960b4cda201758f29b4b168a6261fa6d.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1434a2583038eb0960b4cda201758f29b4b168a6261fa6d.png/w_140 140w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1434a2583038eb0960b4cda201758f29b4b168a6261fa6d.png/w_220 220w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1434a2583038eb0960b4cda201758f29b4b168a6261fa6d.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1434a2583038eb0960b4cda201758f29b4b168a6261fa6d.png/w_380 380w\"></figure><p>I'm playing White, and it's my move. For simplicity I consider only two options: queen takes knight and queen takes rook. The one chess book I've read values pieces in number of pawns: a knight is worth three pawns, a rook five, a queen nine. So at first glance, it looks like my best move is to take Black's rook. As for Black, I have arbitrarily singled out pawn takes pawn as her preferred move in the current position, but if I play queen takes rook, a new option opens up for her: bishop takes queen. Let's look at the decision tree:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06d27360cb931c681f795c4a175d094881507e3433ed6c6d.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06d27360cb931c681f795c4a175d094881507e3433ed6c6d.png/w_115 115w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06d27360cb931c681f795c4a175d094881507e3433ed6c6d.png/w_195 195w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06d27360cb931c681f795c4a175d094881507e3433ed6c6d.png/w_275 275w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06d27360cb931c681f795c4a175d094881507e3433ed6c6d.png/w_355 355w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06d27360cb931c681f795c4a175d094881507e3433ed6c6d.png/w_435 435w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06d27360cb931c681f795c4a175d094881507e3433ed6c6d.png/w_515 515w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06d27360cb931c681f795c4a175d094881507e3433ed6c6d.png/w_595 595w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06d27360cb931c681f795c4a175d094881507e3433ed6c6d.png/w_675 675w\"></figure><p>If I foolishly play this two player game the same way I played the one-player go-to-college game, I note that the middle branch has the highest utility for White, so I take the choice that leads there: capture the rook. And then Black plays bishop takes queen, and I am left wailing and gnashing my teeth. What did I do wrong?</p><p>I should start by assuming Black will, whenever presented with a choice, take the option with the highest Black utility. Unless Black is stupid, I can cross out any branch that requires Black to play against her own interests. So now the tree looks like this:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f978723ee9f27725b12b1296fbcee7e0753c33d35b6143af.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f978723ee9f27725b12b1296fbcee7e0753c33d35b6143af.png/w_115 115w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f978723ee9f27725b12b1296fbcee7e0753c33d35b6143af.png/w_195 195w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f978723ee9f27725b12b1296fbcee7e0753c33d35b6143af.png/w_275 275w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f978723ee9f27725b12b1296fbcee7e0753c33d35b6143af.png/w_355 355w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f978723ee9f27725b12b1296fbcee7e0753c33d35b6143af.png/w_435 435w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f978723ee9f27725b12b1296fbcee7e0753c33d35b6143af.png/w_515 515w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f978723ee9f27725b12b1296fbcee7e0753c33d35b6143af.png/w_595 595w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f978723ee9f27725b12b1296fbcee7e0753c33d35b6143af.png/w_675 675w\"></figure><p>The two realistic options are me playing queen takes rook and ending up without a queen and -4 utility, or me playing queen takes knight and ending up with a modest gain of 2 utility.</p><p>(my apologies if I've missed some obvious strategic possibility on this particular chessboard; I'm not so good at chess but hopefully the point of the example is clear.)</p><p>This method of alternating moves in a branching tree matches both our intuitive thought processes during a chess game (\u201cOkay, if I do this, then Black's going to do this, and then I'd do this, and then...\u201d) and the foundation of some of the <a href=\"http://en.wikipedia.org/wiki/Alpha-beta_pruning\">algorithms</a> chess computers like Deep Blue use. In fact, it may seem pretty obvious, or even unnecessary. But it can be used to analyze some more complicated games with counterintuitive results.</p><p><a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393062430\">Art of Strategy</a> describes a debate from 1990s US politics revolving around so-called <a href=\"http://en.wikipedia.org/wiki/Line_item_veto\">\u201cline-item veto\u201d</a> power, the ability to veto only one part of a bill. For example, if Congress passed a bill declaring March to be National Game Theory Month and April to be National Branching Tree Awareness Month, the President could veto only the part about April and leave March intact (as politics currently works, the President could only veto or accept the whole bill). During the '90s, President Clinton fought pretty hard for this power, which seems reasonable as it expands his options when dealing with the hostile Republican Congress.</p><p>But Dixit and Nalebuff explain that gaining line-item veto powers might hurt a President. How? Branching trees can explain.</p><p>Imagine Clinton and the Republican Congress are fighting over a budget. We can think of this as a game of sequential moves, much like chess. On its turn, Congress proposes a budget. On Clinton's turn, he either accepts or rejects the budget. A player \u201cwins\u201d if the budget contains their pet projects. In this game, we start with low domestic and military budgets. Clinton really wants to raise domestic spending (utility +10), and has a minor distaste for raised military spending (utility -5). Congress really wants to raise military spending (utility +10), but has a minor distaste for raised domestic spending (utility -5). The status quo is zero utility for both parties; if neither party can come to an agreement, voters get angry at them and they both lose 2 utility. Here's the tree when Clinton lacks line-item veto:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dc5cd83eb332ac6c0852ded2c145309eb0b893b5105b6a7a.png/w_989 989w\"></figure><p>For any particular Republican choice, Clinton will never respond in a way that does not maximize his utility, so the the Republicans reason backward and arrive at something like this:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8e9a485b705fe92d3b2945477b45919363271d13ff415e0.png/w_989 989w\"></figure><p>If Republicans are perfectly rational agents, they choose the second option, high domestic and high military spending, to give them their highest plausibly obtainable utility of 5.</p><p>But what if Clinton has the line-item veto? Now his options look like this:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f5ab682447e557e25e13e4e6e9f0d85a82fef0d96dda2c1.png/w_989 989w\"></figure><p>If the Republicans stick to their previous choice of \u201chigh domestic and high military spending\u201d, Clinton line-item vetoes the military spending, and we end up with a situation identical to the first choice: Clinton sitting on a pile of utility, and the Republicans wailing and gnashing their teeth. The Republicans need to come up with a new strategy, and their thought processes, based on Clinton as a utility-maximizer, look like this:</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ccf9b06d4bac441125575db2a56ca57194fa7090d8019181.png/w_989 989w\"></figure><p>Here Congress's highest utility choice is to propose low domestic spending (it doesn't matter if they give more money to the military or not as this will get line-item vetoed). Let's say they propose low domestic and low military spending, and Clinton accepts. The utilities are (0, 0), and now there is much wailing and gnashing of teeth on both sides (game theorists call this a gnash equilibrium. Maybe you've heard of it.)</p><p>But now Clinton has a utility of 0, instead of a utility of 5. Giving him extra options has cost him utility! Why should this happen, and shouldn't he be able to avoid it?</p><p>This happened because Clinton's new abilities affect not only his own choices, but those of his opponents (compare <a href=\"https://www.lesswrong.com/lw/14a/thomas_schellings_strategy_of_conflict/\">Schelling: Strategies of Conflict</a>). He may be able to deal with this if he can make the Republicans trust him.</p><p>In summary, simple sequential games can often be explored by reasoning backwards over decision trees representing the choices of the players involved. The next post will discuss simultaneous games and the concept of a Nash equilibrium.</p><p><strong>Footnotes:</strong></p><p><strong>1: </strong>Game theory requires self-interest in that all players' are driven solely by their desire to maximize their own payoff in the game currently being played without regard to the welfare of other players or any external standard of fairness. However, it can also be used to describe the behavior of altruistic agents so long as their altruistic concerns are represented in the evaluation of their payoff.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EhEZoTFzys9EDmEXn", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 112, "baseScore": 147, "extendedScore": null, "score": 0.000309, "legacy": true, "legacyId": "17263", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 147, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["tJQsxD34maYw2g5E4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-30T04:13:20.367Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Fundamental Doubts", "slug": "seq-rerun-fundamental-doubts", "viewCount": null, "lastCommentedAt": "2022-05-20T02:56:45.102Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xMMH6nDowkyjzeAQd/seq-rerun-fundamental-doubts", "pageUrlRelative": "/posts/xMMH6nDowkyjzeAQd/seq-rerun-fundamental-doubts", "linkUrl": "https://www.lesswrong.com/posts/xMMH6nDowkyjzeAQd/seq-rerun-fundamental-doubts", "postedAtFormatted": "Saturday, June 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Fundamental%20Doubts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Fundamental%20Doubts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxMMH6nDowkyjzeAQd%2Fseq-rerun-fundamental-doubts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Fundamental%20Doubts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxMMH6nDowkyjzeAQd%2Fseq-rerun-fundamental-doubts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxMMH6nDowkyjzeAQd%2Fseq-rerun-fundamental-doubts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>Today's post, <a href=\"/lw/s4/fundamental_doubts/\">Fundamental Doubts</a> was originally published on 12 July 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Fundamental_Doubts\">LW wiki</a>):</p>\n<blockquote>There are some things that are so fundamental, that you really can't doubt them effectively. Be careful you don't use this as an excuse, but ultimately, you really can't start out by saying that you won't trust anything that is the output of a neuron.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/dcj/seq_rerun_the_genetic_fallacy/\">The Genetic Fallacy</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xMMH6nDowkyjzeAQd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.328508835289156e-07, "legacy": true, "legacyId": "17327", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9EahWKqay6HZcaNTY", "ecPeiBSjtznPZTMaF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-30T06:01:48.565Z", "modifiedAt": null, "url": null, "title": "Thoughts on moral intuitions", "slug": "thoughts-on-moral-intuitions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:09.783Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fEFwHBeatkaKpm8ZB/thoughts-on-moral-intuitions", "pageUrlRelative": "/posts/fEFwHBeatkaKpm8ZB/thoughts-on-moral-intuitions", "linkUrl": "https://www.lesswrong.com/posts/fEFwHBeatkaKpm8ZB/thoughts-on-moral-intuitions", "postedAtFormatted": "Saturday, June 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20on%20moral%20intuitions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20on%20moral%20intuitions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfEFwHBeatkaKpm8ZB%2Fthoughts-on-moral-intuitions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20on%20moral%20intuitions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfEFwHBeatkaKpm8ZB%2Fthoughts-on-moral-intuitions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfEFwHBeatkaKpm8ZB%2Fthoughts-on-moral-intuitions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1850, "htmlBody": "<p>The style in the following article is slightly different from most LW articles, due to the fact that I <a href=\"http://xuenay.livejournal.com/345356.html\">originally posted this</a> on <a href=\"http://xuenay.livejournal.com\">my blog</a>. Some folks on #lesswrong liked it, so I thought it might be liked here as well.</p>\n<hr />\n<p>Our moral reasoning is ultimately grounded in our moral intuitions: instinctive \"black box\" judgements of what is right and wrong. For example, most people would think that needlessly hurting somebody else is wrong, just because. The claim doesn't need further elaboration, and in fact the reasons for it <em>can't</em> be explained, though people can and do construct elaborate rationalizations for why everyone should accept the claim. This makes things interesting when people with different moral intuitions try to debate morality with each other.<br /><br />---<br /><br />Why do modern-day liberals (for example) generally consider it okay to say \"I think everyone should be happy\" without offering an explanation, but not okay to say \"I think I should be free to keep slaves\", regardless of the explanation offered? In an earlier age, the second statement might have been considered acceptable, while the first one would have required an explanation.<br /><br />In general, people accept their favorite intuitions as given and require people to justify any intuitions which contradict those. If people have strongly left-wing intuitions, they tend to consider right-wing intuitions arbitrary and unacceptable, while considering left-wing intuitions so obvious as to not need any explanation. And vice versa.<br /><br />Of course, you will notice that in some cultures specific moral intuitions tend to dominate, while other intuitions dominate in other cultures. People tend to pick up the moral intuitions of their environment: some claims go so strongly against the prevailing moral intuitions of my social environment that if I were to even hypothetically raise the possibility of them being correct, I would be loudly condemned and feel bad for even thinking that way. (Related: Paul Graham's <a href=\"http://paulgraham.com/say.html\">What you can't say</a>.) \"Culture\" here is to be understood as being considerably more fine-grained than just \"the culture in Finland\" or the \"culture in India\" - there are countless of subcultures even within a single country.<br /><br />---<br /><br />Social psychologists distinguish between two kinds of moral rules: ones which people consider absolute, and ones which people consider to be social conventions. For example, if a group of people all bullied and picked on one of them, this would usually be considered wrong, even if everyone in the group (including the bullied person) thought it was okay. But if there's a rule that you should wear a specific kind of clothing while at work, then it's considered okay not to wear those clothes if you get special permission from your boss, or if you switch to another job without that rule.<br /><br />The funny thing is that many people don't realize that the distinction of which is which is by itself a moral intuition which varies from people to people, and from culture to culture. Jonathan Haidt writes in <a href=\"http://www.amazon.com/The-Righteous-Mind-Politics-Religion/dp/0307377903/\"><em>The Righteous Mind: Why Good People Are Divided by Politics and Religion</em></a> of <a href=\"http://faculty.virginia.edu/haidtlab/articles/haidt.koller.1993.affect-culture-morality.pub001.pdf\">his finding</a> that while the upper classes in both Brazil and USA were likely to find violations of harmless taboos to be violations of social convention, lower classes in both countries were more likely to find them violations of absolute moral codes. At the time, moral psychology had mistakenly thought that \"moving on\" to a conception of right and wrong that was only grounded in concrete harms would be the way that children's morality naturally develops, and that children discover morality by themselves instead of learning it from others.<br /><br />So moral psychologists had mistakenly been thinking about some moral intuitions as absolute instead of relative. But we can hardly blame them, for it's common to fail to notice that the distinction between \"social convention\" and \"moral fact\" is variable. Sometimes this is probably done for purpose, for rhetorical reasons - it's a much more convincing speech if you can appeal to ultimate moral truths rather than to social conventions. But just as often people simply don't seem to realize the distinction.<br /><br /><em>(Note to international readers: I have been corrupted by the American blogosphere and literature, and will therefore be using \"liberal\" and \"conservative\" mostly to denote their American meanings. I apologize profusely to my European readers for this terrible misuse of language and for not using the correct terminology like God intended it to be used.)</em><br /><br />For example, social conservatives <a href=\"http://www.rightwingwatch.org/content/labarbera-gays-pathological-homosexualization-schools-military\">sometimes complain</a> that liberals are pushing their morality on them, by requiring things such as not condemning homosexuality. To liberals, this is obviously absurd - nobody is saying that the conservatives should be gay, people are just saying that people shouldn&rsquo;t be denied equal rights simply because of their sexual orientation. From the liberal point of view, it is the conservatives who are pushing their beliefs on others, not vice versa.<br /><br />But let's contrast \"oppressing gays\" to \"banning polluting factories\". Few liberals would be willing to accept the claim that if somebody wants to build a factory that causes a lot of harm to the environment, he should be allowed to do so, and to ban him from doing it would be to push the liberal ideals on the factory-owner. They might, however, protest that to prevent them from banning the factory would be pushing (e.g.) pro-capitalism ideals on them. So, in other words:<br /><br />Conservatives want to prevent people from being gay. They think that this just means upholding morality. They think that if somebody wants to prevent them from doing so, that somebody is pushing their own ideals on them.<br /><br />Liberals want to prevent people from polluting their environment. They think that this just means upholding morality. They think that if somebody wants to prevent them from doing so, that somebody is pushing their own ideals on them.<br /><br />Now my liberal readers (do I even <em>have</em> any socially conservative readers?) will no doubt be rushing to point out the differences in these two examples. Most obviously the fact that pollution hurts other people than just the factory owner, like people on their nearby summer cottages who like seeing nature in a pristine and pure state, so it's justified to do something about it. But conservatives might also argue that openly gay behavior encourages being openly gay, and that this hurts those in nearby suburbs who like seeing people act properly, so it's justified to do something about it.<br /><br />It's easy to say that \"anything that doesn't harm others should be allowed\", but it's much harder to rigorously define harm, and liberals and conservatives differ in when they think it's okay to cause somebody else harm. And even this is probably conceding too much to the liberal point of view, as it accepts a position where the morality of an act is judged primarily in the form of the harms it causes. Some conservatives would be likely to argue that homosexuality <em>just is</em> wrong, the way that killing somebody <em>just is</em> wrong.<br /><br />My point isn't that we should accept the conservative argument. Of course we should reject it - my liberal moral intuitions say so. But we can't in all honestly claim an objective moral high ground. If we are to be honest to ourselves, we will accept that yes, we are pushing our moral beliefs on them - just as they are pushing their moral beliefs on us. And we will hope that our moral beliefs win.<br /><br />Here's another example of \"failing to notice the subjectivity of what counts as social convention\". Many people are annoyed by aggressive vegetarians, who think anyone who eats meat is a bad person, or by religious people who are actively trying to convert others. People often say that it's fine to be vegetarian or religious if that's what you like, but you shouldn't push your ideology to others and require them to act the same.<br /><br />Compare this to saying that it's fine to refuse to send Jews to concentration camps, or to let people die in horrible ways when they could have been saved, but you shouldn't push your ideology to others and require them to act the same. I expect that would sound absurd to most of us. But if you accept a certain vegetarian point of view, then killing animals for food is exactly equivalent to the Holocaust. And if you accept a certain religious view saying that unconverted people will go to Hell for an eternity, then not trying to convert them is even worse than letting people die in horrible ways. To say that these groups shouldn't push their morality to others is to already push your own ideology - which says that decisions about what to eat and what to believe are just social conventions, while decisions about whether to kill humans and save lives are moral facts - on them.<br /><br />So what use is there in debating morality, if we have so divergent moral intuitions? In some cases, people have such widely differing intuitions that there is no point. In other cases, their intuitions are similar enough that they can find common ground, and in that case discussion can be useful. Intuitions can clearly be affected by words, and sometimes people do shift their intuitions as a result of having debated them. But this usually requires appealing to, or at least starting out from, some moral intuition that they already accept. There are <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distances</a> involved in moral claims, just as there are inferential distances involved in factual claims.<br /><br />So what about the cases when the distance is too large, when the gap simply cannot be bridged? Well in those cases, we will simply have to fight to keep pushing our own moral intuitions to as many people as possible, and hope that they will end up having more influence than the unacceptable intuitions. Many liberals probably don't want to admit to themselves that this is what we should do, in order to beat the conservatives - it goes so badly against the liberal rhetoric. It would be much nicer to pretend that we are simply letting everyone live the way they want to, and that we are fighting to defend everyone's right for that.<br /><br />But it would be more honest to admit that we actually want to let everyone live the way they want to, <em>as long as </em>they don't things we consider \"really wrong\", such as discriminating against gays. And that in this regard we're no different from the conservatives, who would likewise let everyone live the way they wanted to, <em>as long as </em>they don't do things the conservatives consider \"really wrong\".<br /><br />Of course, whether or not you'll want to be that honest depends on what your moral intuitions have to say about honesty.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"z95PGFXtPpwakqkTA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fEFwHBeatkaKpm8ZB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 59, "extendedScore": null, "score": 0.000141, "legacy": true, "legacyId": "17285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 202, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-30T06:47:40.455Z", "modifiedAt": null, "url": null, "title": "Aubrey de Grey has responded to his IAMA - Now with Transcript!", "slug": "aubrey-de-grey-has-responded-to-his-iama-now-with-transcript", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.840Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Username", "createdAt": "2010-09-04T16:10:50.550Z", "isAdmin": false, "displayName": "Username"}, "userId": "8iY88evtFECPgCs3s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ReSqnpwWh4K786JPc/aubrey-de-grey-has-responded-to-his-iama-now-with-transcript", "pageUrlRelative": "/posts/ReSqnpwWh4K786JPc/aubrey-de-grey-has-responded-to-his-iama-now-with-transcript", "linkUrl": "https://www.lesswrong.com/posts/ReSqnpwWh4K786JPc/aubrey-de-grey-has-responded-to-his-iama-now-with-transcript", "postedAtFormatted": "Saturday, June 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Aubrey%20de%20Grey%20has%20responded%20to%20his%20IAMA%20-%20Now%20with%20Transcript!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAubrey%20de%20Grey%20has%20responded%20to%20his%20IAMA%20-%20Now%20with%20Transcript!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FReSqnpwWh4K786JPc%2Faubrey-de-grey-has-responded-to-his-iama-now-with-transcript%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Aubrey%20de%20Grey%20has%20responded%20to%20his%20IAMA%20-%20Now%20with%20Transcript!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FReSqnpwWh4K786JPc%2Faubrey-de-grey-has-responded-to-his-iama-now-with-transcript", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FReSqnpwWh4K786JPc%2Faubrey-de-grey-has-responded-to-his-iama-now-with-transcript", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p>Watch the video response here: <a href=\"http://www.youtube.com/watch?v=-tsI_28O3Ws\">http://www.youtube.com/watch?v=-tsI_28O3Ws</a></p>\n<p>This was posted here on lesswrong a while ago, but they recently uploaded a new version of the video and I took the liberty of typing up a transcript.</p>\n<p>The video is fairly long, about 25 minutes. But it's incredibly engaging and I highly recommend watching it. For those who prefer text (because it's faster or because you are a computer), you can read the transcript in <a href=\"https://docs.google.com/document/d/1RKiR7r6h0fmEaEb7d-50RcXVkD_kkdBnQRebYWSiRqc/edit\">this google doc</a>, or <a href=\"/r/discussion/lw/ddh/aubrey_de_grey_has_responded_to_his_iama_now_with/6xee\">below in the comments</a>. Enjoy!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BhfefamXXee6c2CH8": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ReSqnpwWh4K786JPc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 58, "extendedScore": null, "score": 0.000156, "legacy": true, "legacyId": "17333", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-30T20:49:08.313Z", "modifiedAt": null, "url": null, "title": "Summary of \"How to Win Friends and Influence People\"", "slug": "summary-of-how-to-win-friends-and-influence-people", "viewCount": null, "lastCommentedAt": "2021-10-19T16:04:25.022Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cosmos", "createdAt": "2009-04-26T03:18:01.731Z", "isAdmin": false, "displayName": "Cosmos"}, "userId": "c3Ji9Th6jATRyHLFC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mWvKuG2XysMsgX7sd/summary-of-how-to-win-friends-and-influence-people", "pageUrlRelative": "/posts/mWvKuG2XysMsgX7sd/summary-of-how-to-win-friends-and-influence-people", "linkUrl": "https://www.lesswrong.com/posts/mWvKuG2XysMsgX7sd/summary-of-how-to-win-friends-and-influence-people", "postedAtFormatted": "Saturday, June 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Summary%20of%20%22How%20to%20Win%20Friends%20and%20Influence%20People%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASummary%20of%20%22How%20to%20Win%20Friends%20and%20Influence%20People%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmWvKuG2XysMsgX7sd%2Fsummary-of-how-to-win-friends-and-influence-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Summary%20of%20%22How%20to%20Win%20Friends%20and%20Influence%20People%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmWvKuG2XysMsgX7sd%2Fsummary-of-how-to-win-friends-and-influence-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmWvKuG2XysMsgX7sd%2Fsummary-of-how-to-win-friends-and-influence-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<p>In the very back of Kaj's excellent <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\" target=\"_blank\">How to Run a Successful Less Wrong Meetup Group</a> booklet, he has a recommended reading section, including the classic book How to Win Friends and Influence People.</p>\n<p>It just so happens that not only have I read the book myself, but I have <a href=\"http://effectivenessforgeeks.com/summary-of-how-to-win-friends-and-influence-people/\" target=\"_blank\">written up a concise summary of the core advice here</a>. Kaj suggested that I post this on the discussion section because others might find it useful, so here you go!</p>\n<p>I suspect that more people are willing to read a summary of a book from the 1930s than an actual book from the 1930s. What I will say about reading the long-form text is that it can be more useful for <em>internalizing</em> these concepts and giving examples of them. It is far too easy to abstractly know what you need to do, much harder to actually take action on those beliefs...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 1, "8SfkJYYMe75MwjHzN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mWvKuG2XysMsgX7sd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 26, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "17339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qMuAazqwJvkvo8teR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-01T01:20:09.534Z", "modifiedAt": null, "url": null, "title": "New York Less Wrong: Expansion Plans", "slug": "new-york-less-wrong-expansion-plans", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:36.411Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YThw4sodANTDLLWf7/new-york-less-wrong-expansion-plans", "pageUrlRelative": "/posts/YThw4sodANTDLLWf7/new-york-less-wrong-expansion-plans", "linkUrl": "https://www.lesswrong.com/posts/YThw4sodANTDLLWf7/new-york-less-wrong-expansion-plans", "postedAtFormatted": "Sunday, July 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20York%20Less%20Wrong%3A%20Expansion%20Plans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20York%20Less%20Wrong%3A%20Expansion%20Plans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYThw4sodANTDLLWf7%2Fnew-york-less-wrong-expansion-plans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20York%20Less%20Wrong%3A%20Expansion%20Plans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYThw4sodANTDLLWf7%2Fnew-york-less-wrong-expansion-plans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYThw4sodANTDLLWf7%2Fnew-york-less-wrong-expansion-plans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1268, "htmlBody": "<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>Followup To:</strong> <a href=\"/lw/4ul/less_wrong_nyc_case_study_of_a_successful/\">New York City, Case Study of a Successful Rationalist Chapter</a>; <a href=\"/lw/5c0/epistle_to_the_new_york_less_wrongians/\">Epistle to the New York Less Wrongians</a><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Last week, the New York Less Wrong group hosted the <a href=\"/lw/cue/meetup_megameetup_summer_festival_at_nyc/\">Summer Solstice Megameetup</a>. Everyone had a <em>huge</em> amount of fun, and we met a lot of new people who didn't normally come to meetups. I've recently moved to New York, and we all enjoyed the solstice events so much that I'd like to host more of them.&nbsp;</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">So, in addition to the regular Tuesday evening New York meetups, I'm going to start hosting Saturday afternoon meetups - it seems a lot of people are free on the weekends, but can't come to events during the work week. We also plan to host larger parties once a month or so, to just have fun, talk, and blow off steam. Parties are also useful as a Schelling point for those who are interested, but live far away or otherwise can't come every week.&nbsp;</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><a id=\"more\"></a></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">If anyone here lives in New York, but doesn't come to local meetups, we'd like to meet you! You can contact me at<span class=\"Apple-converted-space\">&nbsp;</span><a style=\"color: #1155cc;\" href=\"mailto:pphysics141@gmail.com\" target=\"_blank\">pphysics141@gmail.com</a>.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong><br /></strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>New York events this July, and relevant contact info:</strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>Pool Party, Saturday, July 7th</strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Location: New Jersey, about an hour west of Manhattan by train<br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">(Some of us are meeting up in Manhattan, and taking the train together)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Contact: <strong>Geoff Cameron,&nbsp;<a style=\"color: #1155cc;\" href=\"mailto:gscshoyru@gmail.com\" target=\"_blank\">gscshoyru@gmail.com</a></strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong><br /></strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>Hiking and Camping Trip, Saturday, July 14th and Sunday, July 15th</strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Location: Undecided, but likely the woods of New Jersey or Connecticut</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">(Meet up Saturday in Manhattan before heading out)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Contact: <strong>Raymond Arnold,&nbsp;<a style=\"color: #1155cc;\" href=\"mailto:raemon777@gmail.com\" target=\"_blank\">raemon777@gmail.com</a></strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>Nomic Game and Evening Dance Party, Saturday, July 21st</strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Location: Private residence, midtown Manhattan, New York, NY</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Nomic game and discussion will start at 3 PM, party will start at 7 PM</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Contact: <strong>Tom McCabe,<span class=\"Apple-converted-space\">&nbsp;</span><a style=\"color: #1155cc;\" href=\"mailto:pphysics141@gmail.com\" target=\"_blank\">pphysics141@gmail.com</a></strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>Tuesday Evening Meetups, and General Questions</strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Contact: <strong>Zvi Mowshowitz,<span class=\"Apple-converted-space\">&nbsp;</span><a style=\"color: #1155cc;\" href=\"mailto:thezvi@gmail.com\" target=\"_blank\">thezvi@gmail.com</a></strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">On a longer term basis, we're thinking about the question of how to grow the group well beyond its present size. Once things get big enough, like in the Bay Area, you start to get bigger than <a href=\"/lw/x9/dunbars_function/\">Dunbar's Number</a> and lose the \"everyone knows everyone else\" feel. But that can happen in a good way (smaller groups form, and each one is its own tight-knit community) or a bad way (a swamp of people forms, and people lose close connections to each other). The <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">Less Wrong Meetup Guide</a> has a lot of extremely useful info on how to <em>create</em> and <em>maintain</em> a meetup, but we still need to tackle the question of how to grow a meetup <em>beyond Dunbar size</em> if we want to take over the world.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Some problems that we've had to face in New York, the Bay Area, or both are:</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>Forming close connections. </strong>In a room with five people, it's pretty easy to learn other people's names, faces, interests and personalities. But with fifty people it's difficult, and with five hundred it's impossible. Moreover, since non-distinct memories fade much faster than distinct ones, having regular meetups with five people will result in learning more and more about them, while having regular conferences with five hundred probably won't give you <em>deeper</em> relationships with any of the five hundred. <br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>The limits of the LW audience. </strong>Less Wrong gets a decent amount of traffic, but it's fairly small compared to major websites like Reddit, or even Hacker News. Hence, beyond a certain point, getting bigger will have to mean either growing LW itself, or including non-rationalists as part of the target audience.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>Preventing color politics. </strong>We've <em>mostly</em> avoided this so far, but with size comes more <a href=\"http://www.paulgraham.com/boss.html\">status hierarchy</a>, and with status hierarchy comes more fighting and polarization. (Large events like Burning Man don't have strong status hierarchies, but I think that's mostly by virtue of being temporary, as opposed to permanent communities.) Leadership of a ten-person group is largely about whose apartment is most comfortable to host stuff in, but leadership (especially explicit leadership) of a thousand-person group easily becomes a prize to be fought over. <br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>Group dilution.</strong> Years ago, it was prophesized that the Singularity (if marketed heavily enough) would come to mean a new, more advanced form of <a href=\"http://www.columbian.com/news/2012/jun/21/fisher-heralds-the-singularity/\">toothpaste</a>. We're not quite there yet, but the day seems to be approaching inexorably, just as it did for \"nanotechnology\". Beware degeneration to the lowest common denominator - <a href=\"/lw/c1/wellkept_gardens_die_by_pacifism/\">Well-Kept Gardens Die By Pacifism</a> applies to in-person groups, too.&nbsp; <br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">In the spirit of Paul Graham, here are a bunch of thoughts I have on solutions, but I don't know which ones will work and which won't. Some are probably bad ideas for reasons I just haven't thought of yet, but for others, we'll probably just have to try and see.<br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\"><strong>Idea: Create a private group Wiki, with pages on discussion topics, meetups, and people's faces and contact info.</strong></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">Advantages: Encourages socializing with other group members outside of meetups. Technically, faces, interests, and contact info are usually on Facebook... but there's so much <em>other</em> stuff on Facebook that it's usually impossible to find. Several times, I've wanted to hang out with \"that person who looked like X, and was interested in Y\", but didn't pursue it, because there was no simple way to find them.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff;\">\n<div>Disadvantages: This likely requires someone to actively maintain it, or it'll just become abandoned.</div>\n<div><br /></div>\n<div><strong>Idea: Promote the Less Wrong group within other, related groups, like skeptics, atheists, scientists, and so on.</strong></div>\n<div>Advantages: Will introduce a lot of intelligent people to rationality and other related concepts.</div>\n<div>Disadvantages: Might lead to having less of a community feel, if lots of random people join. <br /></div>\n<div><strong><br /></strong></div>\n<div><strong>Idea: Have lots of members hold their own meetups, at their own apartments or other locations.</strong></div>\n<div>Advantages: Creates a lot more total group activity. Avoids overcrowding, since people might want to go to meetup A but not meetup B. Specialization of labor, since different groups can focus on different things. <br /></div>\n<div>Disadvantages: People usually only have a few hours a week to dedicate to one social group. Hence, if you have too many meetups, the quality of the <em>best</em> meetups might drop, because the other meetups draw away too many good people.</div>\n<div><strong><br /></strong></div>\n<div><strong>Idea: Formally incorporate the group, either as a profit or non-profit.</strong></div>\n<div>Advantages: Makes it easier to collect money from members, and then use that money to advance the group's interests. <br /></div>\n<div>Disadvantages: Probably increases the risk of color politics and other social unpleasantness. Anything formalized is more likely to be fought over, by virtue of being more explicit.</div>\n<div><strong><br /></strong></div>\n<div><strong>Idea: Have formal group membership and membership dues.</strong></div>\n<div>Advantages: Once the group is sufficiently large, it allows you to keep individual meetups below Dunbar's Number, by making them members-only.&nbsp;</div>\n<div>Disadvantages: Might introduce more color politics about the money, or the membership criteria. Might also make it a lot harder to get cool new people, by creating <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivial inconveniences</a>.&nbsp;</div>\n<div><strong><br /></strong></div>\n<div><strong>Idea: Hold joint meetups with other groups.</strong></div>\n<div>Advantages: Lots of new people get introduced to rationality simultaneously.&nbsp;</div>\n<div>Disadvantages: It's distressingly common for people at big meetups to only talk to the other people they know, largely eliminating the point.&nbsp;</div>\n<div><strong><br /></strong></div>\n<div><strong>Idea: Print \"rationality group business cards\" that can be passed out, with a leader's contact info and a brief description of the group.&nbsp;</strong></div>\n<div>Advantages: A quick, easy form of advertising, and an easy followup mechanism for those who have expressed interest.&nbsp;</div>\n<div>Disadvantages: Someone has to answer the emails, and then decide how to allocate time between contacts.</div>\n<div><br /></div>\n<div>I'd like to conclude by emphasizing that a successful large group will probably be <em>different in character</em> than existing groups, not just more of the same. The New York group originally wasn't just bigger or more active, but <em>different in kind</em> than predecessor groups - physical contact? karaoke? the outdoors? What does that have to do with rationality? Genuine progress requires <a href=\"/lw/xm/building_weirdtopia/\">Weirdtopia</a>, not just Eutopia.<br /></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YThw4sodANTDLLWf7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 23, "extendedScore": null, "score": 9.334396665023301e-07, "legacy": true, "legacyId": "17340", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CsKboswS3z5iaiutC", "jP583FwKepjiWbeoQ", "CGAeib9xGEbmjtqBP", "W5PhyEQqEWTcpRpqn", "qMuAazqwJvkvo8teR", "tscc3e5eujrsEeFN4", "reitXJgJXFzKpdKyd", "cWjK3SbRcLkb3gN69"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-01T04:27:17.533Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Rebelling Within Nature", "slug": "seq-rerun-rebelling-within-nature", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BHz3iyLyMwGewtv2g/seq-rerun-rebelling-within-nature", "pageUrlRelative": "/posts/BHz3iyLyMwGewtv2g/seq-rerun-rebelling-within-nature", "linkUrl": "https://www.lesswrong.com/posts/BHz3iyLyMwGewtv2g/seq-rerun-rebelling-within-nature", "postedAtFormatted": "Sunday, July 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Rebelling%20Within%20Nature&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Rebelling%20Within%20Nature%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHz3iyLyMwGewtv2g%2Fseq-rerun-rebelling-within-nature%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Rebelling%20Within%20Nature%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHz3iyLyMwGewtv2g%2Fseq-rerun-rebelling-within-nature", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHz3iyLyMwGewtv2g%2Fseq-rerun-rebelling-within-nature", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"/lw/s5/rebelling_within_nature/\">Rebelling Within Nature</a> was originally published on 13 July 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Rebelling_Within_Nature\">LW wiki</a>):</p>\n<blockquote>When we rebel against our own nature, we act in accordance with our own nature. There isn't any other way it could be.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/ddb/seq_rerun_fundamental_doubts/\">Fundamental Doubts</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BHz3iyLyMwGewtv2g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.33526696717652e-07, "legacy": true, "legacyId": "17342", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YhNGY6ypoNbLJvDBu", "xMMH6nDowkyjzeAQd", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-01T15:39:08.521Z", "modifiedAt": null, "url": null, "title": "[link] Feynman lecture: The Character of Physical Law 1 ", "slug": "link-feynman-lecture-the-character-of-physical-law-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:31.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MartinB", "createdAt": "2009-04-20T11:11:22.800Z", "isAdmin": false, "displayName": "MartinB"}, "userId": "2BGK5dWpTXzCE7iwF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FWd3co4SX58ZnxxX6/link-feynman-lecture-the-character-of-physical-law-1", "pageUrlRelative": "/posts/FWd3co4SX58ZnxxX6/link-feynman-lecture-the-character-of-physical-law-1", "linkUrl": "https://www.lesswrong.com/posts/FWd3co4SX58ZnxxX6/link-feynman-lecture-the-character-of-physical-law-1", "postedAtFormatted": "Sunday, July 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Feynman%20lecture%3A%20The%20Character%20of%20Physical%20Law%201%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Feynman%20lecture%3A%20The%20Character%20of%20Physical%20Law%201%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFWd3co4SX58ZnxxX6%2Flink-feynman-lecture-the-character-of-physical-law-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Feynman%20lecture%3A%20The%20Character%20of%20Physical%20Law%201%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFWd3co4SX58ZnxxX6%2Flink-feynman-lecture-the-character-of-physical-law-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFWd3co4SX58ZnxxX6%2Flink-feynman-lecture-the-character-of-physical-law-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5, "htmlBody": "<p>http://www.youtube.com/watch?v=O-JKAHaml7A&amp;feature=channel&amp;list=UL</p>\n<p>&nbsp;</p>\n<p>You might enjoy it :-)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FWd3co4SX58ZnxxX6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -6, "extendedScore": null, "score": 9.338392726874524e-07, "legacy": true, "legacyId": "17343", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-01T15:40:19.708Z", "modifiedAt": null, "url": null, "title": "Hope Function", "slug": "hope-function", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:29.146Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7gyBiGroJykJvB3eo/hope-function", "pageUrlRelative": "/posts/7gyBiGroJykJvB3eo/hope-function", "linkUrl": "https://www.lesswrong.com/posts/7gyBiGroJykJvB3eo/hope-function", "postedAtFormatted": "Sunday, July 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hope%20Function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHope%20Function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7gyBiGroJykJvB3eo%2Fhope-function%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hope%20Function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7gyBiGroJykJvB3eo%2Fhope-function", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7gyBiGroJykJvB3eo%2Fhope-function", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<p>Yesterday I finished transcribing<a href=\"http://www.gwern.net/docs/1994-falk\"> <strong>\"The Ups and Downs of the Hope Function In a Fruitless Search</strong></a><strong>\"</strong>. This is a statistics &amp; psychology paper describing a simple probabilistic search problem and the sheer difficulty subjects have in producing the correct Bayesian answer. Besides providing a great but simple illustration of the mind projection fallacy in action, the simple search problem maps onto a number of forecasting problems: the problem may be looking in a desk for a letter that may not be there, but we could also look at a problem in which we check every year for the creation of AI and ask <a href=\"/lw/5hq/an_inflection_point_for_probability_estimates_of/428t#body_t1_428t\">how our beliefs change over time</a> - which turns out to defuse a common scoffing criticism of past technological forecasting. (This last problem was why I went back and used it, after I first read of it.)</p>\n<p>The math is all simple - arithmetic and one application of Bayes's law - so I think all LWers can enjoy it, and it has amusing examples to analyze. I have also taken the trouble to annotate it with Wikipedia links, relevant materials, and many PDF links (some jailbroken just for this transcript). I hope everyone finds it as interesting as I did.</p>\n<p>I thank John Salvatier <a href=\"/lw/7hi/free_research_help_editing_and_article_downloads/\">for doing the ILL request</a> which got me a scan of this book chapter.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PJKgSRkXkCqXmCk3M": 1, "ksdiAMKfgSyEeKMo6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7gyBiGroJykJvB3eo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 38, "extendedScore": null, "score": 9.33839824820061e-07, "legacy": true, "legacyId": "17344", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["q3wQGNicZZQPCmJXZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-01T18:06:11.054Z", "modifiedAt": null, "url": null, "title": "July 2012 Media Thread", "slug": "july-2012-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.561Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iFnP6aMpGNLdMyRzP/july-2012-media-thread", "pageUrlRelative": "/posts/iFnP6aMpGNLdMyRzP/july-2012-media-thread", "linkUrl": "https://www.lesswrong.com/posts/iFnP6aMpGNLdMyRzP/july-2012-media-thread", "postedAtFormatted": "Sunday, July 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20July%202012%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJuly%202012%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFnP6aMpGNLdMyRzP%2Fjuly-2012-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=July%202012%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFnP6aMpGNLdMyRzP%2Fjuly-2012-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFnP6aMpGNLdMyRzP%2Fjuly-2012-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the monthly thread for posting media of various types that you've found that you enjoy. I find that exposure to LW ideas makes me less likely to enjoy some entertainment media that is otherwise quite popular, and finding media recommended by LWers is a good way to mitigate this. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the&nbsp;</span><a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/media_thread/\">older threads</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Rules:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres, which&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/94z/january_2012_media_thread/5kos\">I was apparently too dumb to do</a>.</li>\n<li>If you have a thread to add, such as a video game thread or an Anime thread, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iFnP6aMpGNLdMyRzP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 9.33907708279623e-07, "legacy": true, "legacyId": "17345", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-01T20:55:36.230Z", "modifiedAt": null, "url": null, "title": "Wisdom of the Crowd: not always so wise", "slug": "wisdom-of-the-crowd-not-always-so-wise", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:31.270Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tgb", "createdAt": "2011-11-22T01:42:56.795Z", "isAdmin": false, "displayName": "tgb"}, "userId": "ZSMRTvQtfA4eieBPk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9f5qxXDdudfzz4YpC/wisdom-of-the-crowd-not-always-so-wise", "pageUrlRelative": "/posts/9f5qxXDdudfzz4YpC/wisdom-of-the-crowd-not-always-so-wise", "linkUrl": "https://www.lesswrong.com/posts/9f5qxXDdudfzz4YpC/wisdom-of-the-crowd-not-always-so-wise", "postedAtFormatted": "Sunday, July 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wisdom%20of%20the%20Crowd%3A%20not%20always%20so%20wise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWisdom%20of%20the%20Crowd%3A%20not%20always%20so%20wise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9f5qxXDdudfzz4YpC%2Fwisdom-of-the-crowd-not-always-so-wise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wisdom%20of%20the%20Crowd%3A%20not%20always%20so%20wise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9f5qxXDdudfzz4YpC%2Fwisdom-of-the-crowd-not-always-so-wise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9f5qxXDdudfzz4YpC%2Fwisdom-of-the-crowd-not-always-so-wise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 532, "htmlBody": "<p>I have a confession to make: I have been not \"publishing\" my results to an experiment because the results were uninteresting. You may recall some time ago that I <a href=\"/lw/97l/poll_wisdom_of_the_crowd_experiment/\">made a post</a> asking people to take a survey so that I could look at a small variation of the typical \"Wisdom of the Crowds\" experiment where people make estimates on a value and the average of crowd's estimates is better than that of all or almost all of the individual estimates. Since LessWrong is full of people who like to do these kinds of things (thank you!), I got 177 responses - many more than I was hoping for!</p>\n<p>I am now coming back to this since I happened upon <a href=\"/lw/hb/useful_statistical_biases/\">an older post by Eliezer</a> saying the following</p>\n<blockquote>\n<p>When you hear that a classroom gave an average estimate of 871 beans for a jar that contained 850 beans, and that only one individual student did better than the crowd, the astounding notion is not that the crowd can be more accurate than the individual.&nbsp; The astounding notion is that human beings are unbiased estimators of beans in a jar, having no significant directional error on the problem, yet with large variance.&nbsp; It implies that we tend to get the answer wrong but there's no systematic reason why.&nbsp; It requires that there be lots of errors that vary from individual to individual - and this is reliably true, enough so to keep most individuals from guessing the jar correctly. And  yet there are no directional errors that everyone makes, or if there are, they cancel out very precisely in the average case, despite the large individual variations.&nbsp; Which is just plain <em>odd</em>.&nbsp;<strong> I find myself somewhat suspicious of the claim, and wonder whether other experiments that found less amazing accuracy were not as popularly reported. </strong></p>\n</blockquote>\n<p>(Emphasis added.) It turns out that I myself was sitting upon exactly such results.</p>\n<p><strong><a href=\"https://docs.google.com/spreadsheet/ccc?key=0AmyaCjMZ7CSvdHhVX3c1ZU1BZ2pEZENvVXdvY3BvdWc\">The results are here.</a></strong> Sheet 1 shows raw data and Sheet 3 shows some values from those numbers. A few values that were clearly either jokes or mistakes (like not noticing the answer was in millions) were removed. In summary: (according to Wikipedia) 1000 million people in  Africa (as of 2009) whereas the estimate from LessWrong was 781 million  and the first transatlantic telephone call happened in 1926 whereas the  average from the poll was 1899.</p>\n<p>There! I've come clean!</p>\n<p>I had deferred making this public because I thought the result that I was trying to test wasn't really being tested in this experiment, regardless of the results. The idea (see my original post linked about) was to see whether selecting between two choices would still let the crowd average out to the correct value (this two-option choice was meant to reflect the structure of some democracies). But how to interpret the results? It seemed that my selection of values is too important and that the average would change depending on what I picked even if everyone was to make an estimate, then look at the two options and choose the best one. So perhaps the only result of note here is that for the questions given, Less Wrong users were not particularly great at being a wise crowd.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vg4LDxjdwHLotCm8w": 1, "izp6eeJJEg9v5zcur": 1, "kJrjorSx3hXa7q7CJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9f5qxXDdudfzz4YpC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 33, "extendedScore": null, "score": 9.339865696391898e-07, "legacy": true, "legacyId": "17346", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["D2Rzwu6a8syB9tHZT", "Wwq6WFpx9HyzwgCKx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-01T22:45:37.450Z", "modifiedAt": null, "url": null, "title": "Open Thread, July 1-15, 2012", "slug": "open-thread-july-1-15-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.252Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8z95iBTmQDwQWpfzW/open-thread-july-1-15-2012", "pageUrlRelative": "/posts/8z95iBTmQDwQWpfzW/open-thread-july-1-15-2012", "linkUrl": "https://www.lesswrong.com/posts/8z95iBTmQDwQWpfzW/open-thread-july-1-15-2012", "postedAtFormatted": "Sunday, July 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20July%201-15%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20July%201-15%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8z95iBTmQDwQWpfzW%2Fopen-thread-july-1-15-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20July%201-15%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8z95iBTmQDwQWpfzW%2Fopen-thread-july-1-15-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8z95iBTmQDwQWpfzW%2Fopen-thread-july-1-15-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8z95iBTmQDwQWpfzW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 9.340377881771118e-07, "legacy": true, "legacyId": "17347", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 154, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-01T22:55:04.932Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Meetup-Post-Minicamp edition", "slug": "meetup-washington-dc-meetup-post-minicamp-edition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:29.346Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a6tEd66b3Za8aZQZR/meetup-washington-dc-meetup-post-minicamp-edition", "pageUrlRelative": "/posts/a6tEd66b3Za8aZQZR/meetup-washington-dc-meetup-post-minicamp-edition", "linkUrl": "https://www.lesswrong.com/posts/a6tEd66b3Za8aZQZR/meetup-washington-dc-meetup-post-minicamp-edition", "postedAtFormatted": "Sunday, July 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Meetup-Post-Minicamp%20edition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Meetup-Post-Minicamp%20edition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa6tEd66b3Za8aZQZR%2Fmeetup-washington-dc-meetup-post-minicamp-edition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Meetup-Post-Minicamp%20edition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa6tEd66b3Za8aZQZR%2Fmeetup-washington-dc-meetup-post-minicamp-edition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa6tEd66b3Za8aZQZR%2Fmeetup-washington-dc-meetup-post-minicamp-edition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/bk\">Washington DC Meetup-Post-Minicamp edition</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">08 July 2012 03:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Portrait Gallery Courtyard</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We'll be discussing some of the topics covered at the recent rationality minicamp: using fungibility and value of information estimates in particular.</p>\n<p>If you have any feedback on this choice of topic, please post here or to the list. Thanks!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/bk\">Washington DC Meetup-Post-Minicamp edition</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a6tEd66b3Za8aZQZR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 9.34042191466615e-07, "legacy": true, "legacyId": "17348", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Meetup_Post_Minicamp_edition\">Discussion article for the meetup : <a href=\"/meetups/bk\">Washington DC Meetup-Post-Minicamp edition</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">08 July 2012 03:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Portrait Gallery Courtyard</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We'll be discussing some of the topics covered at the recent rationality minicamp: using fungibility and value of information estimates in particular.</p>\n<p>If you have any feedback on this choice of topic, please post here or to the list. Thanks!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Meetup_Post_Minicamp_edition1\">Discussion article for the meetup : <a href=\"/meetups/bk\">Washington DC Meetup-Post-Minicamp edition</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Meetup-Post-Minicamp edition", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Meetup_Post_Minicamp_edition", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Meetup-Post-Minicamp edition", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Meetup_Post_Minicamp_edition1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-01T23:09:04.621Z", "modifiedAt": null, "url": null, "title": "[Event] Symposium on Cryonics and Brain-Threatening Disorders", "slug": "event-symposium-on-cryonics-and-brain-threatening-disorders", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:31.411Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AipRLfPwbLoAJhHTE/event-symposium-on-cryonics-and-brain-threatening-disorders", "pageUrlRelative": "/posts/AipRLfPwbLoAJhHTE/event-symposium-on-cryonics-and-brain-threatening-disorders", "linkUrl": "https://www.lesswrong.com/posts/AipRLfPwbLoAJhHTE/event-symposium-on-cryonics-and-brain-threatening-disorders", "postedAtFormatted": "Sunday, July 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BEvent%5D%20Symposium%20on%20Cryonics%20and%20Brain-Threatening%20Disorders&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BEvent%5D%20Symposium%20on%20Cryonics%20and%20Brain-Threatening%20Disorders%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAipRLfPwbLoAJhHTE%2Fevent-symposium-on-cryonics-and-brain-threatening-disorders%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BEvent%5D%20Symposium%20on%20Cryonics%20and%20Brain-Threatening%20Disorders%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAipRLfPwbLoAJhHTE%2Fevent-symposium-on-cryonics-and-brain-threatening-disorders", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAipRLfPwbLoAJhHTE%2Fevent-symposium-on-cryonics-and-brain-threatening-disorders", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 821, "htmlBody": "<p>Upcoming <a href=\"http://www.cryonicsnw.org/2012/03/06/symposium-on-cryonics-and-brain-threatening-disorders/\">event</a>:</p>\n<blockquote>\n<p style=\"text-align: justify;\">On Saturday July 7, 2012, the Institute  for Evidence Based Cryonics and Cryonics Northwest will organize a  symposium on cryonics and brain-threatening disorders in Portland,  Oregon. The symposium will start at 09:00 am at the offices of Kaos  Softwear. Entrance to the event is free.</p>\n<p style=\"text-align: justify;\">This symposium is the first event of its  kind in the history of cryonics and concerns one of the most important  challenges facing aging cryonicists. Please register for the event on  our <a href=\"https://www.facebook.com/events/341429765900433/\" target=\"_blank\">Facebook</a> page so we know how many attendees to expect.</p>\n<p style=\"text-align: justify;\">The following speakers and presentations are confirmed and more speakers / activities may be added in the future:</p>\n<p style=\"text-align: justify;\"><strong></strong><em></em><strong>Chana de Wolf<em> &ndash; </em>Neurogenesis in the Adult Brain and Alzheimer&rsquo;s Disease</strong></p>\n<p style=\"text-align: justify;\">Early neuroanatomists considered the  adult brain fixed and incapable of neurogenesis. Chana de Wolf will  review the emerging evidence for adult neurogenesis and its implications  for the treatment of Alzheimer&rsquo;s disease and other identity-destroying  disorders.</p>\n<p style=\"text-align: justify;\"><em>Chana de Wolf has a master&rsquo;s degree in Neuroscience and is the President of Advanced Neural Biosciences, Inc.</em></p>\n<p style=\"text-align: justify;\"><strong>Aubrey de Grey, Ph.D. &ndash; Repairing the Aging Brain: The SENS Approach</strong></p>\n<p style=\"text-align: justify;\">Like all other organs, the brain  accumulates molecular and cellular alterations throughout life that are  eventually deleterious to its function. Unlike all other organs, it  cannot be replaced wholesale by a new one created in the lab; the damage  must be repaired piecemeal. In this talk I will survey the current  status of repairing the three major forms of damage seen in the brain of  elderly people: the amyloid plaques that accumulate in the  extracellular space in Alzheimer&rsquo;s disease, the various intracellular  proteinaceous aggregates seen in all the major forms of  neurodegeneration, and the loss of neurons of various types seen in the  advanced stages of Alzheimer&rsquo;s, Parkinson&rsquo;s and in aging in general.  Relevance to revival of cryonics patiends and to certain schemes for  uploading will also be discussed.</p>\n<p style=\"text-align: justify;\"><em>Dr. Aubrey de Grey is a biomedical  gerontologist based in Cambridge, UK, and is the Chief Science Officer  of SENS Foundation, a California-based 501(c)(3) charity dedicated to  combating the aging process. He is also Editor-in-Chief of Rejuvenation  Research, the world&rsquo;s highest-impact peer-reviewed journal focused on  intervention in aging. He received his BA and Ph.D. from the University  of Cambridge in 1985 and 2000 respectively. Dr. de Grey is a Fellow of  both the Gerontological Society of America and the American Aging  Association, and sits on the editorial and scientific advisory boards of  numerous journals and organisations.</em></p>\n<p style=\"text-align: justify;\"><strong>Ben Best &ndash; Drugs, Supplements, and other Treatments to Mitigate and Prevent Alzheimer&rsquo;s Disease</strong></p>\n<p style=\"text-align: justify;\">In the United State over 40% of people  over age 84 develop Alzheimer&rsquo;s Disease. Death by Alzheimer&rsquo;s Disease  for a cryonicist could mean death in the absolute sense, even if  cryopreserved under the best of circumstances. Ben Best will discuss  drugs, supplements, and other treatments to mitigate and prevent  Alzheimer&rsquo;s Disease, discussing the relevance of these  preventative/mitigating agents to probable causes of Alzheimer&rsquo;s  Disease.</p>\n<p style=\"text-align: justify;\"><em>Ben Best has bachelor&rsquo;s degrees in  Pharmacy, Physics, Computing Science and Business (Accounting and  Finance). He is President of the Cryonics Institute and has done  extensive self-study of mechanisms of aging in general and Alzheimer&rsquo;s  Disease in particular.</em></p>\n<p style=\"text-align: justify;\"><em><strong>Mike Perry, Ph.D. &ndash; Early Detection of Alzheimer&rsquo;s Disease: Some Recent Progress<br /> </strong><br /> </em>A new study has doubled the time interval for the first detectable  changes in the brain of a person with Alzheimer&rsquo;s disease (AD): from  five years to ten years before dementia occurs.&nbsp; Mike Perry will report  on this advance and other progress that offers the possibility of both  earlier detection and more effective treatments for AD.<em></em></p>\n<p style=\"text-align: justify;\"><em>Mike Perry has a Ph.D. in computer  science and is the Care Services Manager at Alcor Life Extension  Foundation. His book, Forever for All, offers a moral argument for the  pursuit of life extension through cryonics, with an optimistic  conclusion about the scientific prospects for immortality.</em></p>\n<p style=\"text-align: justify;\"><strong>Max More, Ph.D. &ndash; Survival, Identity, and the Extended Mind</strong></p>\n<p style=\"text-align: justify;\">Can personal identity be reduced to the  brain? If it cannot, does this offer challenges or advantages for  cryonics? And what is the relevance of the concept of the extended mind  for brain-threatening disorders such as Alzheimer&rsquo;s disease? Alcor  President Max More reviews recent theories about the mind and identity  and their implications for personal survival.</p>\n<p style=\"text-align: justify;\"><em>Max More is the President &amp;  Chief Executive Officer of the Alcor Life Extension Foundation. More has  a degree in Philosophy, Politics, and Economics from St. Anne&rsquo;s  College, Oxford University (1984-87). He was awarded a Dean&rsquo;s Fellowship  in Philosophy in 1987 by the University of Southern California. He  studied and taught philosophy at USC with an emphasis on philosophy of  mind, ethics, and personal identity, completing his Ph.D. in 1995, with a  dissertation that examined issues including the nature of death, and  what it is about each individual that continues despite great change  over time.</em><em><br /> </em></p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AipRLfPwbLoAJhHTE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 3, "extendedScore": null, "score": 9.340487069601462e-07, "legacy": true, "legacyId": "17349", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Upcoming <a href=\"http://www.cryonicsnw.org/2012/03/06/symposium-on-cryonics-and-brain-threatening-disorders/\">event</a>:</p>\n<blockquote>\n<p style=\"text-align: justify;\">On Saturday July 7, 2012, the Institute  for Evidence Based Cryonics and Cryonics Northwest will organize a  symposium on cryonics and brain-threatening disorders in Portland,  Oregon. The symposium will start at 09:00 am at the offices of Kaos  Softwear. Entrance to the event is free.</p>\n<p style=\"text-align: justify;\">This symposium is the first event of its  kind in the history of cryonics and concerns one of the most important  challenges facing aging cryonicists. Please register for the event on  our <a href=\"https://www.facebook.com/events/341429765900433/\" target=\"_blank\">Facebook</a> page so we know how many attendees to expect.</p>\n<p style=\"text-align: justify;\">The following speakers and presentations are confirmed and more speakers / activities may be added in the future:</p>\n<p style=\"text-align: justify;\"><strong></strong><em></em><strong>Chana de Wolf<em> \u2013 </em>Neurogenesis in the Adult Brain and Alzheimer\u2019s Disease</strong></p>\n<p style=\"text-align: justify;\">Early neuroanatomists considered the  adult brain fixed and incapable of neurogenesis. Chana de Wolf will  review the emerging evidence for adult neurogenesis and its implications  for the treatment of Alzheimer\u2019s disease and other identity-destroying  disorders.</p>\n<p style=\"text-align: justify;\"><em>Chana de Wolf has a master\u2019s degree in Neuroscience and is the President of Advanced Neural Biosciences, Inc.</em></p>\n<p style=\"text-align: justify;\"><strong id=\"Aubrey_de_Grey__Ph_D____Repairing_the_Aging_Brain__The_SENS_Approach\">Aubrey de Grey, Ph.D. \u2013 Repairing the Aging Brain: The SENS Approach</strong></p>\n<p style=\"text-align: justify;\">Like all other organs, the brain  accumulates molecular and cellular alterations throughout life that are  eventually deleterious to its function. Unlike all other organs, it  cannot be replaced wholesale by a new one created in the lab; the damage  must be repaired piecemeal. In this talk I will survey the current  status of repairing the three major forms of damage seen in the brain of  elderly people: the amyloid plaques that accumulate in the  extracellular space in Alzheimer\u2019s disease, the various intracellular  proteinaceous aggregates seen in all the major forms of  neurodegeneration, and the loss of neurons of various types seen in the  advanced stages of Alzheimer\u2019s, Parkinson\u2019s and in aging in general.  Relevance to revival of cryonics patiends and to certain schemes for  uploading will also be discussed.</p>\n<p style=\"text-align: justify;\"><em>Dr. Aubrey de Grey is a biomedical  gerontologist based in Cambridge, UK, and is the Chief Science Officer  of SENS Foundation, a California-based 501(c)(3) charity dedicated to  combating the aging process. He is also Editor-in-Chief of Rejuvenation  Research, the world\u2019s highest-impact peer-reviewed journal focused on  intervention in aging. He received his BA and Ph.D. from the University  of Cambridge in 1985 and 2000 respectively. Dr. de Grey is a Fellow of  both the Gerontological Society of America and the American Aging  Association, and sits on the editorial and scientific advisory boards of  numerous journals and organisations.</em></p>\n<p style=\"text-align: justify;\"><strong id=\"Ben_Best___Drugs__Supplements__and_other_Treatments_to_Mitigate_and_Prevent_Alzheimer_s_Disease\">Ben Best \u2013 Drugs, Supplements, and other Treatments to Mitigate and Prevent Alzheimer\u2019s Disease</strong></p>\n<p style=\"text-align: justify;\">In the United State over 40% of people  over age 84 develop Alzheimer\u2019s Disease. Death by Alzheimer\u2019s Disease  for a cryonicist could mean death in the absolute sense, even if  cryopreserved under the best of circumstances. Ben Best will discuss  drugs, supplements, and other treatments to mitigate and prevent  Alzheimer\u2019s Disease, discussing the relevance of these  preventative/mitigating agents to probable causes of Alzheimer\u2019s  Disease.</p>\n<p style=\"text-align: justify;\"><em>Ben Best has bachelor\u2019s degrees in  Pharmacy, Physics, Computing Science and Business (Accounting and  Finance). He is President of the Cryonics Institute and has done  extensive self-study of mechanisms of aging in general and Alzheimer\u2019s  Disease in particular.</em></p>\n<p style=\"text-align: justify;\"><em><strong>Mike Perry, Ph.D. \u2013 Early Detection of Alzheimer\u2019s Disease: Some Recent Progress<br> </strong><br> </em>A new study has doubled the time interval for the first detectable  changes in the brain of a person with Alzheimer\u2019s disease (AD): from  five years to ten years before dementia occurs.&nbsp; Mike Perry will report  on this advance and other progress that offers the possibility of both  earlier detection and more effective treatments for AD.<em></em></p>\n<p style=\"text-align: justify;\"><em>Mike Perry has a Ph.D. in computer  science and is the Care Services Manager at Alcor Life Extension  Foundation. His book, Forever for All, offers a moral argument for the  pursuit of life extension through cryonics, with an optimistic  conclusion about the scientific prospects for immortality.</em></p>\n<p style=\"text-align: justify;\"><strong id=\"Max_More__Ph_D____Survival__Identity__and_the_Extended_Mind\">Max More, Ph.D. \u2013 Survival, Identity, and the Extended Mind</strong></p>\n<p style=\"text-align: justify;\">Can personal identity be reduced to the  brain? If it cannot, does this offer challenges or advantages for  cryonics? And what is the relevance of the concept of the extended mind  for brain-threatening disorders such as Alzheimer\u2019s disease? Alcor  President Max More reviews recent theories about the mind and identity  and their implications for personal survival.</p>\n<p style=\"text-align: justify;\"><em>Max More is the President &amp;  Chief Executive Officer of the Alcor Life Extension Foundation. More has  a degree in Philosophy, Politics, and Economics from St. Anne\u2019s  College, Oxford University (1984-87). He was awarded a Dean\u2019s Fellowship  in Philosophy in 1987 by the University of Southern California. He  studied and taught philosophy at USC with an emphasis on philosophy of  mind, ethics, and personal identity, completing his Ph.D. in 1995, with a  dissertation that examined issues including the nature of death, and  what it is about each individual that continues despite great change  over time.</em><em><br> </em></p>\n</blockquote>\n<p>&nbsp;</p>", "sections": [{"title": "Aubrey de Grey, Ph.D. \u2013 Repairing the Aging Brain: The SENS Approach", "anchor": "Aubrey_de_Grey__Ph_D____Repairing_the_Aging_Brain__The_SENS_Approach", "level": 1}, {"title": "Ben Best \u2013 Drugs, Supplements, and other Treatments to Mitigate and Prevent Alzheimer\u2019s Disease", "anchor": "Ben_Best___Drugs__Supplements__and_other_Treatments_to_Mitigate_and_Prevent_Alzheimer_s_Disease", "level": 1}, {"title": "Max More, Ph.D. \u2013 Survival, Identity, and the Extended Mind", "anchor": "Max_More__Ph_D____Survival__Identity__and_the_Extended_Mind", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-02T03:56:20.818Z", "modifiedAt": null, "url": null, "title": "Meetup : Salt Lake City: The Really Getting Bayes Game", "slug": "meetup-salt-lake-city-the-really-getting-bayes-game", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mJtPDPGYXDbobc7fF/meetup-salt-lake-city-the-really-getting-bayes-game", "pageUrlRelative": "/posts/mJtPDPGYXDbobc7fF/meetup-salt-lake-city-the-really-getting-bayes-game", "linkUrl": "https://www.lesswrong.com/posts/mJtPDPGYXDbobc7fF/meetup-salt-lake-city-the-really-getting-bayes-game", "postedAtFormatted": "Monday, July 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Salt%20Lake%20City%3A%20The%20Really%20Getting%20Bayes%20Game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Salt%20Lake%20City%3A%20The%20Really%20Getting%20Bayes%20Game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJtPDPGYXDbobc7fF%2Fmeetup-salt-lake-city-the-really-getting-bayes-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Salt%20Lake%20City%3A%20The%20Really%20Getting%20Bayes%20Game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJtPDPGYXDbobc7fF%2Fmeetup-salt-lake-city-the-really-getting-bayes-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJtPDPGYXDbobc7fF%2Fmeetup-salt-lake-city-the-really-getting-bayes-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bl'>Salt Lake City: The Really Getting Bayes Game</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 July 2012 03:00:53PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1558 Palo Verde Way #12, Cottonwood Heights, Utah 84121</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup will be 3pm Sunday, July 8th at Kip's house: 1558 Palo Verde Way, Cottonwood Heights (or SLC), Utah 84121\nI'll be presenting a basic reintroduction to Bayes Rule, and then we can start playing The Really Getting Bayes Game! There will be a discussion afterwards, with an eye to whether or not this sort of practice is useful in training Real Life bayesian reasoning and worth repeating, and/or potential variations on the game.\nPlease feel free to bring an interesting or unusual snack you think others might like to try. Variety is the spice of life!\nInfo on Really Getting Bayes Game: <a href=\"http://math.berkeley.edu/~critch/mphd/rgb.pdf\" rel=\"nofollow\">http://math.berkeley.edu/~critch/mphd/rgb.pdf</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bl'>Salt Lake City: The Really Getting Bayes Game</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mJtPDPGYXDbobc7fF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "17353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City__The_Really_Getting_Bayes_Game\">Discussion article for the meetup : <a href=\"/meetups/bl\">Salt Lake City: The Really Getting Bayes Game</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 July 2012 03:00:53PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1558 Palo Verde Way #12, Cottonwood Heights, Utah 84121</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup will be 3pm Sunday, July 8th at Kip's house: 1558 Palo Verde Way, Cottonwood Heights (or SLC), Utah 84121\nI'll be presenting a basic reintroduction to Bayes Rule, and then we can start playing The Really Getting Bayes Game! There will be a discussion afterwards, with an eye to whether or not this sort of practice is useful in training Real Life bayesian reasoning and worth repeating, and/or potential variations on the game.\nPlease feel free to bring an interesting or unusual snack you think others might like to try. Variety is the spice of life!\nInfo on Really Getting Bayes Game: <a href=\"http://math.berkeley.edu/~critch/mphd/rgb.pdf\" rel=\"nofollow\">http://math.berkeley.edu/~critch/mphd/rgb.pdf</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City__The_Really_Getting_Bayes_Game1\">Discussion article for the meetup : <a href=\"/meetups/bl\">Salt Lake City: The Really Getting Bayes Game</a></h2>", "sections": [{"title": "Discussion article for the meetup : Salt Lake City: The Really Getting Bayes Game", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City__The_Really_Getting_Bayes_Game", "level": 1}, {"title": "Discussion article for the meetup : Salt Lake City: The Really Getting Bayes Game", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City__The_Really_Getting_Bayes_Game1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-02T04:29:39.000Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Probability is Subjectively Objective", "slug": "seq-rerun-probability-is-subjectively-objective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.894Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wmG5APgjzf7GzQuhH/seq-rerun-probability-is-subjectively-objective", "pageUrlRelative": "/posts/wmG5APgjzf7GzQuhH/seq-rerun-probability-is-subjectively-objective", "linkUrl": "https://www.lesswrong.com/posts/wmG5APgjzf7GzQuhH/seq-rerun-probability-is-subjectively-objective", "postedAtFormatted": "Monday, July 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Probability%20is%20Subjectively%20Objective&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Probability%20is%20Subjectively%20Objective%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwmG5APgjzf7GzQuhH%2Fseq-rerun-probability-is-subjectively-objective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Probability%20is%20Subjectively%20Objective%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwmG5APgjzf7GzQuhH%2Fseq-rerun-probability-is-subjectively-objective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwmG5APgjzf7GzQuhH%2Fseq-rerun-probability-is-subjectively-objective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>Today's post, <a href=\"/lw/s6/probability_is_subjectively_objective/\">Probability is Subjectively Objective</a> was originally published on 14 July 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Probability_is_Subjectively_Objective\">LW wiki</a>):</p>\n<blockquote>Is probability subjective or objective? Better question: what does \"subjective\" and \"objective\" mean?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/ddq/seq_rerun_rebelling_within_nature/\">Rebelling Within Nature</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wmG5APgjzf7GzQuhH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 9.34197976929313e-07, "legacy": true, "legacyId": "17354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XhaKvQyHzeXdNnFKy", "BHz3iyLyMwGewtv2g", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-02T04:49:25.558Z", "modifiedAt": null, "url": null, "title": "E.T. Jaynes and Hugh Everett - includes a previously unpublished review by Jaynes of a published short version of Everett's dissertation", "slug": "e-t-jaynes-and-hugh-everett-includes-a-previously", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.647Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "buybuydandavis", "createdAt": "2011-09-04T00:02:35.971Z", "isAdmin": false, "displayName": "buybuydandavis"}, "userId": "sQqaueY24d7xa3PXD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tPXn6qpWw9BZAikNQ/e-t-jaynes-and-hugh-everett-includes-a-previously", "pageUrlRelative": "/posts/tPXn6qpWw9BZAikNQ/e-t-jaynes-and-hugh-everett-includes-a-previously", "linkUrl": "https://www.lesswrong.com/posts/tPXn6qpWw9BZAikNQ/e-t-jaynes-and-hugh-everett-includes-a-previously", "postedAtFormatted": "Monday, July 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20E.T.%20Jaynes%20and%20Hugh%20Everett%20-%20includes%20a%20previously%20unpublished%20review%20by%20Jaynes%20of%20a%20published%20short%20version%20of%20Everett's%20dissertation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AE.T.%20Jaynes%20and%20Hugh%20Everett%20-%20includes%20a%20previously%20unpublished%20review%20by%20Jaynes%20of%20a%20published%20short%20version%20of%20Everett's%20dissertation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtPXn6qpWw9BZAikNQ%2Fe-t-jaynes-and-hugh-everett-includes-a-previously%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=E.T.%20Jaynes%20and%20Hugh%20Everett%20-%20includes%20a%20previously%20unpublished%20review%20by%20Jaynes%20of%20a%20published%20short%20version%20of%20Everett's%20dissertation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtPXn6qpWw9BZAikNQ%2Fe-t-jaynes-and-hugh-everett-includes-a-previously", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtPXn6qpWw9BZAikNQ%2Fe-t-jaynes-and-hugh-everett-includes-a-previously", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 295, "htmlBody": "<p>&nbsp;</p>\n<p>E.T. Jaynes had a brief exchange of correspondence with Hugh Everett in 1957. The exchange was initiated by Everett, who commented on recently published works by Jaynes. Jaynes responded to Everett's comments, and finally sent Everett a letter reviewing a short version of Everett's thesis published that year.&nbsp;</p>\n<p>Jaynes reaction was extremely positive at first: \"It seems fair to say that your theory is the logical completion of quantum theory, in exactly the same sense that relativity was the logical completion of classical theory.\" High praise. But Jaynes swiftly follows up the praise with fundamental objections:&nbsp;\"This is just the fundamental cause of Einstein's most serious objections to quantum theory, and it seems to me that the things that worried Einstein still cause trouble in your theory, but in an entirely new way.\" His letter goes on to detail his concerns, and insist, wtih Bohm, &nbsp;that \"Einstein's objections to quantum theory have never been satisfactorily answered.</p>\n<p>The Collected Works of Everett has some narrative about their interaction:</p>\n<p>http://books.google.com/books?id=dowpli7i6TgC&amp;lpg=PA261&amp;dq=jaynes%20everett&amp;pg=PA261#v=onepage&amp;q&amp;f=false</p>\n<p>&nbsp;</p>\n<p>Hugh Everett marginal notes on page from E. T. Jaynes' \"Information Theory and Statistical Mechanics\"</p>\n<p>http://ucispace.lib.uci.edu/handle/10575/1140</p>\n<p>&nbsp;</p>\n<p>Hugh Everett handwritten draft letter to E.T. Jaynes, 15-May-1957</p>\n<p>http://ucispace.lib.uci.edu/handle/10575/1186</p>\n<p>&nbsp;</p>\n<p>Hugh Everett letter to E. T. Jaynes, 11-June-1957</p>\n<p>http://ucispace.lib.uci.edu/handle/10575/1124</p>\n<p>&nbsp;</p>\n<p>E.T. Jaynes letter to Hugh Everett, 15-October-1957 -&nbsp;<strong>Never before published</strong></p>\n<p>https://sites.google.com/site/etjaynesstudy/jaynes-documents/Jaynes-Everett_19571015.pdf?</p>\n<p>Directory at Google site with all the links and docs above. Also links to Washington University at St. Louis copyright form for this doc, Everett's thesis, long and short forms, and Jaynes' paper (the papers they were discussing in their correspondence). I hope to be adding the final letter in this exchange, Jaynes to Hewitt 17-June-1957, within a couple of weeks. , and maybe some documents from the Yahoo Group ETJaynesStudy as well.</p>\n<p><a href=\"https://sites.google.com/site/etjaynesstudy/jaynes-documents\">https://sites.google.com/site/etjaynesstudy/jaynes-documents</a></p>\n<p>&nbsp;</p>\n<p>For perspective on Jaynes more recent thoughts on quantum theory:</p>\n<p>Jaynes paper on EPR and Bell's Theorem: http://bayes.wustl.edu/etj/articles/cmystery.pdf</p>\n<p>Jaynes speculations on quantum theory: http://bayes.wustl.edu/etj/articles/scattering.by.free.pdf</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksWkwyKRrj582WqpN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tPXn6qpWw9BZAikNQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 9.342071866804583e-07, "legacy": true, "legacyId": "17355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-02T06:06:36.198Z", "modifiedAt": null, "url": null, "title": "Can anyone explain to me why CDT two-boxes?", "slug": "can-anyone-explain-to-me-why-cdt-two-boxes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:31.528Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Andreas_Giger", "createdAt": "2011-09-12T00:45:35.617Z", "isAdmin": false, "displayName": "Andreas_Giger"}, "userId": "JjKS2qrYMyWoCcjnH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BdwbiCwzGaSaGDbAe/can-anyone-explain-to-me-why-cdt-two-boxes", "pageUrlRelative": "/posts/BdwbiCwzGaSaGDbAe/can-anyone-explain-to-me-why-cdt-two-boxes", "linkUrl": "https://www.lesswrong.com/posts/BdwbiCwzGaSaGDbAe/can-anyone-explain-to-me-why-cdt-two-boxes", "postedAtFormatted": "Monday, July 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20anyone%20explain%20to%20me%20why%20CDT%20two-boxes%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20anyone%20explain%20to%20me%20why%20CDT%20two-boxes%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBdwbiCwzGaSaGDbAe%2Fcan-anyone-explain-to-me-why-cdt-two-boxes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20anyone%20explain%20to%20me%20why%20CDT%20two-boxes%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBdwbiCwzGaSaGDbAe%2Fcan-anyone-explain-to-me-why-cdt-two-boxes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBdwbiCwzGaSaGDbAe%2Fcan-anyone-explain-to-me-why-cdt-two-boxes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 993, "htmlBody": "<p>I have read lots of LW posts on this topic, and everyone seems to take this for granted without giving a proper explanation. So if anyone could explain this to me, I would appreciate that.</p>\n<p>This is a simple question that is in need of a simple answer. Please don't link to pages and pages of theorycrafting.&nbsp;Thank you.</p>\n<p>&nbsp;</p>\n<p>Edit: Since posting this, I have come to the conclusion that CDT doesn't actually play Newcomb. Here's a disagreement with that statement:</p>\n<blockquote>\n<p>If you write up a CDT algorithm and then put it into a Newcomb's problem simulator, it will do something. It's playing the game; maybe not well, but it's playing.</p>\n</blockquote>\n<p>And here's my response:</p>\n<blockquote>\n<p>The thing is, an actual Newcomb simulator can't possibly exist because Omega doesn't exist. There are tons of workarounds, like using coin tosses as a substitution for Omega and ignoring the results whenever the coin was wrong, but that is something fundamentally different from Newcomb.</p>\n<p>You can only simulate Newcomb in theory, and it is perfectly possible to just not play a theoretical game, if you reject the theory it is based on. In theoretical Newcomb, CDT doesn't care about the rule of Omega being right, so CDT does not play Newcomb.</p>\n<p>If you're trying to simulate Newcomb in reality by substituting Omega with someone who has only empirically been proven right, you substitute Newcomb with a problem that consists of little more than simple calculation of priors and payoffs, and that's hardly the point here.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Edit 2: Clarification regarding backwards causality, which seems to confuse people:</p>\n<blockquote>\n<p>Newcomb assumes that Omega is omniscient, which more importantly means that the decision you make right now determines whether Omega has put money in the box or not. Obviously this is backwards causality, and therefore not possible in real life, which is why Nozick doesn't spend too much ink on this.</p>\n<p>But if you rule out the possibility of backwards causality, Omega can only make his prediction of your decision based on all your actions up to the point where it has to decide whether to put money in the box or not. In that case, if you take two people who have so far always acted (decided) identical, but one will one-box while the other one will two-box, Omega <em>cannot</em> make&nbsp;different predictions for them. And no matter what prediction Omega makes, you don't want to be the one who one-boxes.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Edit 3: Further clarification on the possible problems that could be considered Newcomb:</p>\n<blockquote>\n<p>There's four types of Newcomb problems:</p>\n<ol>\n<li>Omniscient Omega (backwards causality) - CDT rejects this case, which cannot exist in reality.</li>\n<li>Fallible Omega, but still backwards causality - CDT rejects this case, which cannot exist in reality.</li>\n<li>Infallible Omega, no backwards causality - CDT correctly two-boxes. To improve payouts, CDT would have to have decided differently <em>in the past</em>, which is not decision theory anymore.</li>\n<li>Fallible Omega, no backwards causality -&nbsp;CDT correctly two-boxes. To improve payouts, CDT would have to have decided differently <em>in the past</em>, which is not decision theory anymore.</li>\n</ol>\n<p>That's all there is to it.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Edit 4: Excerpt from&nbsp;Nozick's \"Newcomb's Problem and Two Principles of Choice\":</p>\n<p><span style=\"background-color: #ffffcc; text-align: justify; line-height: 19px;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"> </span></span></p>\n<blockquote>\n<p>Now, at last, to return to Newcomb's example of the predictor. If one believes, for this case, that there is backwards causality, that your choice causes the money to be there or not, that it causes him to have made the prediction that he made, then there is no problem. One takes only what is in the second box. Or if one believes that the way the predictor works is by looking into the future; he, in some sense, sees what you are doing, and hence is no more likely to be wrong about what you do than someone else who is standing there at the time and watching you, and would normally see you, say, open only one box, then there is no problem. You take only what is in the second box. But suppose we establish or take as given that there is no backwards causality, that what you actually decide to do does not affect what he did in the past, that what you actually decide to do is not part of the explanation of why he made the prediction he made. So let us agree that the predictor works as follows: He observes you sometime before you are faced with the choice, examines you with complicated apparatus, etc., and then uses his theory to predict on the basis of this state you were in, what choice you would make later when faced with the choice. Your deciding to do as you do is not part of the explanation of why he makes the prediction he does, though your being in a certain state earlier, is part of the explanation of why he makes the prediction he does, and why you decide as you do.</p>\n<p>I believe that one should take what is in both boxes. I fear that the considerations I have adduced thus far will not convince those proponents of taking only what is in the second box. Furthermore I suspect that an adequate solution to this problem will go much deeper than I have yet gone or shall go in this paper. So I want to pose one question. I assume that it is clear that in the vaccine example, the person should not be convinced by the probability argument, and should choose the dominant action. I assume also that it is clear that in the case of the two brothers, the brother should not be convinced by the probability argument offered. The question I should like to put to proponents of taking only what is in the second box in Newcomb's example (and hence not performing the dominant action) is: what is the difference between Newcomb's example and the other two examples which make the difference between not following the dominance principle, and following it?</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BdwbiCwzGaSaGDbAe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -18, "extendedScore": null, "score": 9.342431299789945e-07, "legacy": true, "legacyId": "17358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 136, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-02T11:08:02.521Z", "modifiedAt": null, "url": null, "title": "New \"Best\" comment sorting system", "slug": "new-best-comment-sorting-system", "viewCount": null, "lastCommentedAt": "2014-11-22T19:33:16.650Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NwbQFJ8n7rsQkpN8b/new-best-comment-sorting-system", "pageUrlRelative": "/posts/NwbQFJ8n7rsQkpN8b/new-best-comment-sorting-system", "linkUrl": "https://www.lesswrong.com/posts/NwbQFJ8n7rsQkpN8b/new-best-comment-sorting-system", "postedAtFormatted": "Monday, July 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20%22Best%22%20comment%20sorting%20system&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20%22Best%22%20comment%20sorting%20system%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwbQFJ8n7rsQkpN8b%2Fnew-best-comment-sorting-system%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20%22Best%22%20comment%20sorting%20system%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwbQFJ8n7rsQkpN8b%2Fnew-best-comment-sorting-system", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwbQFJ8n7rsQkpN8b%2Fnew-best-comment-sorting-system", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p>Way back in October 2009 Reddit introduced their \"Best\" comment sorting system. We've just pulled those changes into Less Wrong. The changes&nbsp;affect only comments, not stories.</p>\n<p>It's good. It should significantly improve the visibility of good comments posted later in the life of an article. You (yes you) should adopt it.&nbsp;It's the default for new users.</p>\n<p>See&nbsp;<a href=\"http://blog.reddit.com/2009/10/reddits-new-comment-sorting-system.html\">http://blog.reddit.com/2009/10/reddits-new-comment-sorting-system.html</a> for the details.</p>\n<p><a id=\"more\"></a></p>\n<p><img style=\"box-shadow: 2px 2px 5px #888888; border:1px solid #021a40;\" src=\"https://img.skitch.com/20120702-gw2sk1irwa5dxq7gun8rxnrwgf.png\" alt=\"Location of &quot;Best&quot; comment sorting option\" width=\"624\" height=\"300\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NwbQFJ8n7rsQkpN8b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 35, "extendedScore": null, "score": 9.343835408119465e-07, "legacy": true, "legacyId": "17367", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-02T13:18:20.084Z", "modifiedAt": null, "url": null, "title": "Less Wrong Product & Service Recommendations", "slug": "less-wrong-product-and-service-recommendations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:39.475Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3ibmxAsvmFftXmYHX/less-wrong-product-and-service-recommendations", "pageUrlRelative": "/posts/3ibmxAsvmFftXmYHX/less-wrong-product-and-service-recommendations", "linkUrl": "https://www.lesswrong.com/posts/3ibmxAsvmFftXmYHX/less-wrong-product-and-service-recommendations", "postedAtFormatted": "Monday, July 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20Product%20%26%20Service%20Recommendations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20Product%20%26%20Service%20Recommendations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ibmxAsvmFftXmYHX%2Fless-wrong-product-and-service-recommendations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20Product%20%26%20Service%20Recommendations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ibmxAsvmFftXmYHX%2Fless-wrong-product-and-service-recommendations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ibmxAsvmFftXmYHX%2Fless-wrong-product-and-service-recommendations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p>I have often benefited from recommendations for Things I Didn't Know I Wanted.</p>\n<p>Given that Less Wrong is a community of unusually intelligent, critical, and self-improvement-focused people, I suspect we can generate a pretty helpful thread of product recommendations &mdash; perhaps even a <em>monthly</em>&nbsp;thread of product recommendations.</p>\n<p>Rules:</p>\n<ul>\n<li>Post one product your recommend per comment, so they can be discussed and voted on independently.</li>\n<li>Provide a link for purchasing the product.</li>\n<li>No books, movies, TV, games, or music. (These should go in other threads, like <a href=\"/lw/ddt/july_2012_media_thread/\">this one</a> or&nbsp;<a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">this one</a>.)</li>\n</ul>\n<div>I'll post my own recommendations to the comments section, too.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "Tg9aFPFCPBHxGABRr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3ibmxAsvmFftXmYHX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 29, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "17368", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 371, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iFnP6aMpGNLdMyRzP", "xg3hXCYQPJkwHyik2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-02T13:37:16.765Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Sydney 16th July Event for Less Wrong", "slug": "meetup-less-wrong-sydney-16th-july-event-for-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:39.295Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oklord", "createdAt": "2011-03-22T11:37:19.291Z", "isAdmin": false, "displayName": "Oklord"}, "userId": "EusNQMHkhmSq2k9Y9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Be7HowZqr4pYs5tis/meetup-less-wrong-sydney-16th-july-event-for-less-wrong", "pageUrlRelative": "/posts/Be7HowZqr4pYs5tis/meetup-less-wrong-sydney-16th-july-event-for-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/Be7HowZqr4pYs5tis/meetup-less-wrong-sydney-16th-july-event-for-less-wrong", "postedAtFormatted": "Monday, July 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Sydney%2016th%20July%20Event%20for%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Sydney%2016th%20July%20Event%20for%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBe7HowZqr4pYs5tis%2Fmeetup-less-wrong-sydney-16th-july-event-for-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Sydney%2016th%20July%20Event%20for%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBe7HowZqr4pYs5tis%2Fmeetup-less-wrong-sydney-16th-july-event-for-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBe7HowZqr4pYs5tis%2Fmeetup-less-wrong-sydney-16th-july-event-for-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bm'>Less Wrong Sydney 16th July Event for Less Wrong</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 July 2012 06:35:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">565-567 George Street, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><pre><code>Hello everyone - the time has come for another meet up!\n</code></pre>\n\n<p>As per suggestions made at the previous event, the food at Norita's is crap, and Mafia has proven to be more fun than most board games. Hence, the suggestion goes that we might try City of Sydney RSL!</p>\n\n<p>That said, esoteric pursuit is encouraged, and if you want to bring any materials like board games, feel free! Suggestions on time changes, venue changes and topics, as always, are solicited from the floor.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bm'>Less Wrong Sydney 16th July Event for Less Wrong</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Be7HowZqr4pYs5tis", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.344530698050299e-07, "legacy": true, "legacyId": "17369", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney_16th_July_Event_for_Less_Wrong\">Discussion article for the meetup : <a href=\"/meetups/bm\">Less Wrong Sydney 16th July Event for Less Wrong</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 July 2012 06:35:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">565-567 George Street, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><pre><code>Hello everyone - the time has come for another meet up!\n</code></pre>\n\n<p>As per suggestions made at the previous event, the food at Norita's is crap, and Mafia has proven to be more fun than most board games. Hence, the suggestion goes that we might try City of Sydney RSL!</p>\n\n<p>That said, esoteric pursuit is encouraged, and if you want to bring any materials like board games, feel free! Suggestions on time changes, venue changes and topics, as always, are solicited from the floor.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney_16th_July_Event_for_Less_Wrong1\">Discussion article for the meetup : <a href=\"/meetups/bm\">Less Wrong Sydney 16th July Event for Less Wrong</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Sydney 16th July Event for Less Wrong", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney_16th_July_Event_for_Less_Wrong", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Sydney 16th July Event for Less Wrong", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney_16th_July_Event_for_Less_Wrong1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-02T16:37:40.791Z", "modifiedAt": null, "url": null, "title": "Malthusian copying: mass death of unhappy life-loving uploads", "slug": "malthusian-copying-mass-death-of-unhappy-life-loving-uploads", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.192Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ynyemLY8YWX8rQ84f/malthusian-copying-mass-death-of-unhappy-life-loving-uploads", "pageUrlRelative": "/posts/ynyemLY8YWX8rQ84f/malthusian-copying-mass-death-of-unhappy-life-loving-uploads", "linkUrl": "https://www.lesswrong.com/posts/ynyemLY8YWX8rQ84f/malthusian-copying-mass-death-of-unhappy-life-loving-uploads", "postedAtFormatted": "Monday, July 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Malthusian%20copying%3A%20mass%20death%20of%20unhappy%20life-loving%20uploads&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMalthusian%20copying%3A%20mass%20death%20of%20unhappy%20life-loving%20uploads%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FynyemLY8YWX8rQ84f%2Fmalthusian-copying-mass-death-of-unhappy-life-loving-uploads%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Malthusian%20copying%3A%20mass%20death%20of%20unhappy%20life-loving%20uploads%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FynyemLY8YWX8rQ84f%2Fmalthusian-copying-mass-death-of-unhappy-life-loving-uploads", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FynyemLY8YWX8rQ84f%2Fmalthusian-copying-mass-death-of-unhappy-life-loving-uploads", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 595, "htmlBody": "<p>Robin Hanson has done a <a href=\"http://hanson.gmu.edu/uploads.html\">great</a>&nbsp;<a href=\"http://www.youtube.com/watch?v=s2GIirg43sU\">job</a> of describing the future world and economy, under the assumption that easily copied \"uploads\" (<a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\">whole brain emulations</a>), and the standard laws of economics continue to apply. To oversimplify the conclusion:</p>\n<ul>\n<li>There will be great and rapidly increasing wealth. On the other hand, the uploads will be in Darwinian-like competition with each other and with copies, which will drive their wages down to subsistence levels: whatever is required to run their hardware and keep them working, and nothing more.</li>\n</ul>\n<p>The competition will not so much be driven by variation, but by selection: uploads with the required characteristics can be copied again and again, undercutting and&nbsp;literally&nbsp;crowding out any uploads wanting higher wages.</p>\n<p>&nbsp;</p>\n<h2>Megadeaths</h2>\n<p>Some have focused on the possibly troubling aspects voluntary or semi-voluntary death: some uploads would be willing to make copies of themselves for specific tasks, which would then be deleted or killed at the end of the process. This can pose problems, especially if the copy changes its mind about deletion. But much more troubling is the mass death among uploads that always wanted to live.</p>\n<p>What the selection process will favour is agents that want to live (if they didn't, they'd die out) and willing to work for an expectation of subsistence level wages. But now add a little risk to the process: not all jobs pay exactly the expected amount, sometimes they pay slightly higher, sometimes they pay slightly lower. That means that half of all jobs will result in a life-loving upload dying (charging extra to pay for insurance will squeeze that upload out of the market).&nbsp;Iterating&nbsp;the process means that the vast majority of the uploads will end up being killed - if not initially, then at some point later. The picture changes somewhat if you consider \"<a href=\"http://intelligence.org/files/WBE-Superorgs.pdf\">super-organisms</a>\" of uploads and their copies, but then the issue simply shifts to wage competition between the&nbsp;super-organisms.</p>\n<p>The only way this can be considered acceptable is if the killing of a (potentially unique) agent that doesn't want to die, is exactly compensated by the copying of another already existent agent. I <a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">don't find myself</a> in the camp arguing that that would be a morally neutral or positive action.</p>\n<p>&nbsp;</p>\n<h2>Pain and unhappiness<a id=\"more\"></a><br /></h2>\n<p>The&nbsp;preceding&nbsp;would be mitigated to some extent if the uploads were happy. It's quite easy to come up with <a href=\"http://www.overcomingbias.com/2012/04/jiro-lives-worth-living.html\">mental pictures</a> of potential uploads living happy and worthwhile lives. But evolution/selection is the true determiner of the personality traits of uploads.&nbsp;Successful&nbsp;uploads would have precisely the best amount of pain and happiness in their lives to motivate them to work at their maximum possible efficiency.</p>\n<p>Can we estimate what this pain/happiness balance would be? It's really tricky; we don't know exactly what work the uploads would be doing (\"office work\" is a good guess, but that can be extraordinarily broad). Since we are in extreme evolutionary dis-equilibrium ourselves, we don't have a clear picture of the best pain/happiness wiring for doing our current jobs today - or whether other motivational methods could be used.</p>\n<p>But if we take the outside view, and note that this is an&nbsp;evolutionary processes&nbsp;operating&nbsp;on agents at the edge of starvation, we can compare this with standard Darwinian evolution. And there&nbsp;the picture is clear: the&nbsp;disequilibrium&nbsp;between happiness and pain in the lives of evolved beings is tremendous, and all in the <a href=\"/lw/xi/serious_stories/\">direction of pain</a>. It's far too easy to cause pain to&nbsp;mammals, far too hard to cause happiness. If upload selection follows broadly similar processes, their lives will be filled with pain far more than they will be filled with happiness.</p>\n<p>&nbsp;</p>\n<p>All of which doesn't strike me as a <em>good</em> outcome, in total.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ynyemLY8YWX8rQ84f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 15, "extendedScore": null, "score": 9.345367863884964e-07, "legacy": true, "legacyId": "17136", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Robin Hanson has done a <a href=\"http://hanson.gmu.edu/uploads.html\">great</a>&nbsp;<a href=\"http://www.youtube.com/watch?v=s2GIirg43sU\">job</a> of describing the future world and economy, under the assumption that easily copied \"uploads\" (<a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\">whole brain emulations</a>), and the standard laws of economics continue to apply. To oversimplify the conclusion:</p>\n<ul>\n<li>There will be great and rapidly increasing wealth. On the other hand, the uploads will be in Darwinian-like competition with each other and with copies, which will drive their wages down to subsistence levels: whatever is required to run their hardware and keep them working, and nothing more.</li>\n</ul>\n<p>The competition will not so much be driven by variation, but by selection: uploads with the required characteristics can be copied again and again, undercutting and&nbsp;literally&nbsp;crowding out any uploads wanting higher wages.</p>\n<p>&nbsp;</p>\n<h2 id=\"Megadeaths\">Megadeaths</h2>\n<p>Some have focused on the possibly troubling aspects voluntary or semi-voluntary death: some uploads would be willing to make copies of themselves for specific tasks, which would then be deleted or killed at the end of the process. This can pose problems, especially if the copy changes its mind about deletion. But much more troubling is the mass death among uploads that always wanted to live.</p>\n<p>What the selection process will favour is agents that want to live (if they didn't, they'd die out) and willing to work for an expectation of subsistence level wages. But now add a little risk to the process: not all jobs pay exactly the expected amount, sometimes they pay slightly higher, sometimes they pay slightly lower. That means that half of all jobs will result in a life-loving upload dying (charging extra to pay for insurance will squeeze that upload out of the market).&nbsp;Iterating&nbsp;the process means that the vast majority of the uploads will end up being killed - if not initially, then at some point later. The picture changes somewhat if you consider \"<a href=\"http://intelligence.org/files/WBE-Superorgs.pdf\">super-organisms</a>\" of uploads and their copies, but then the issue simply shifts to wage competition between the&nbsp;super-organisms.</p>\n<p>The only way this can be considered acceptable is if the killing of a (potentially unique) agent that doesn't want to die, is exactly compensated by the copying of another already existent agent. I <a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">don't find myself</a> in the camp arguing that that would be a morally neutral or positive action.</p>\n<p>&nbsp;</p>\n<h2 id=\"Pain_and_unhappiness\">Pain and unhappiness<a id=\"more\"></a><br></h2>\n<p>The&nbsp;preceding&nbsp;would be mitigated to some extent if the uploads were happy. It's quite easy to come up with <a href=\"http://www.overcomingbias.com/2012/04/jiro-lives-worth-living.html\">mental pictures</a> of potential uploads living happy and worthwhile lives. But evolution/selection is the true determiner of the personality traits of uploads.&nbsp;Successful&nbsp;uploads would have precisely the best amount of pain and happiness in their lives to motivate them to work at their maximum possible efficiency.</p>\n<p>Can we estimate what this pain/happiness balance would be? It's really tricky; we don't know exactly what work the uploads would be doing (\"office work\" is a good guess, but that can be extraordinarily broad). Since we are in extreme evolutionary dis-equilibrium ourselves, we don't have a clear picture of the best pain/happiness wiring for doing our current jobs today - or whether other motivational methods could be used.</p>\n<p>But if we take the outside view, and note that this is an&nbsp;evolutionary processes&nbsp;operating&nbsp;on agents at the edge of starvation, we can compare this with standard Darwinian evolution. And there&nbsp;the picture is clear: the&nbsp;disequilibrium&nbsp;between happiness and pain in the lives of evolved beings is tremendous, and all in the <a href=\"/lw/xi/serious_stories/\">direction of pain</a>. It's far too easy to cause pain to&nbsp;mammals, far too hard to cause happiness. If upload selection follows broadly similar processes, their lives will be filled with pain far more than they will be filled with happiness.</p>\n<p>&nbsp;</p>\n<p>All of which doesn't strike me as a <em>good</em> outcome, in total.</p>", "sections": [{"title": "Megadeaths", "anchor": "Megadeaths", "level": 1}, {"title": "Pain and unhappiness", "anchor": "Pain_and_unhappiness", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "82 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pyTuR4ZbLfqpS2oMh", "6qS9q5zHafFXsB6hf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-02T19:09:44.550Z", "modifiedAt": null, "url": null, "title": "Learn Power Searching with Google ", "slug": "learn-power-searching-with-google", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:02.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bsa325tntncXWX4uJ/learn-power-searching-with-google", "pageUrlRelative": "/posts/Bsa325tntncXWX4uJ/learn-power-searching-with-google", "linkUrl": "https://www.lesswrong.com/posts/Bsa325tntncXWX4uJ/learn-power-searching-with-google", "postedAtFormatted": "Monday, July 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learn%20Power%20Searching%20with%20Google%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearn%20Power%20Searching%20with%20Google%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBsa325tntncXWX4uJ%2Flearn-power-searching-with-google%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learn%20Power%20Searching%20with%20Google%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBsa325tntncXWX4uJ%2Flearn-power-searching-with-google", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBsa325tntncXWX4uJ%2Flearn-power-searching-with-google", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<blockquote>\n<p>Google Search makes it amazingly easy to find information. Come learn about the powerful advanced tools we provide to help you find just the right information when the stakes are high.</p>\n</blockquote>\n<p><span><span class=\" chatzilla-bold\">Daniel Russell is doing a free <a href=\"http://www.google.com/insidesearch/landing/powersearching.html\">Google class on how to search the web</a>. Besides six 50-minute classes it will include interactive activities to practice new skills. </span></span>Upon passing the post-course assessment<span><span class=\" chatzilla-bold\"> you get a Certificate of Completion. </span></span></p>\n<p><span><span class=\" chatzilla-bold\">Advanced search skills are not only a useful everyday skill but <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">vital to doing scholarship</a></span></span>. Searching the web is a superpower that would make thinkers of previous centuries green with envy. Learn to use it well. I recommend checking out <a href=\"http://www.google.com/insidesearch/\">Inside Search</a>, <a href=\"http://searchresearch1.blogspot.com/\">Russel's Blog</a> or perhaps reading the article \"<strong><a href=\"http://www.johntedesco.net/blog/2012/06/21/how-to-solve-impossible-problems-daniel-russells-awesome-google-search-techniques/\">How to solve impossible problems</a></strong>\" to get a feeling about what you can expect to gain from it.</p>\n<p>I think for most the value of information is high enough to be worth the investment. Also I suspect it will be plain fun. I am doing the class and strongly recommend it to fellow LessWrong users. Anyone else who has registered please say so publicly in the comments as well. :)<strong><span><span class=\" chatzilla-bold\"> </span></span></strong></p>\n<p><a href=\"http://www.google.com/insidesearch/landing/powersearching.html\"><span><span class=\" chatzilla-bold\">Registration</span></span></a> is open from <em>June 26, 2012 to July 16, 2012</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bsa325tntncXWX4uJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 23, "extendedScore": null, "score": 9.346079956864235e-07, "legacy": true, "legacyId": "17371", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["64FdKLwmea8MCLWkE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-03T03:25:25.052Z", "modifiedAt": "2020-08-21T23:26:33.730Z", "url": null, "title": "Real World Solutions to Prisoners' Dilemmas", "slug": "real-world-solutions-to-prisoners-dilemmas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:32.343Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Scott Alexander", "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BroeiXGh9PrKZEkJ5/real-world-solutions-to-prisoners-dilemmas", "pageUrlRelative": "/posts/BroeiXGh9PrKZEkJ5/real-world-solutions-to-prisoners-dilemmas", "linkUrl": "https://www.lesswrong.com/posts/BroeiXGh9PrKZEkJ5/real-world-solutions-to-prisoners-dilemmas", "postedAtFormatted": "Tuesday, July 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Real%20World%20Solutions%20to%20Prisoners'%20Dilemmas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReal%20World%20Solutions%20to%20Prisoners'%20Dilemmas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBroeiXGh9PrKZEkJ5%2Freal-world-solutions-to-prisoners-dilemmas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Real%20World%20Solutions%20to%20Prisoners'%20Dilemmas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBroeiXGh9PrKZEkJ5%2Freal-world-solutions-to-prisoners-dilemmas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBroeiXGh9PrKZEkJ5%2Freal-world-solutions-to-prisoners-dilemmas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2046, "htmlBody": "<p>Why should there be real world solutions to Prisoners' Dilemmas? Because such dilemmas are a real-world problem.</p><p>If I am assigned to work on a school project with a group, I can either cooperate (work hard on the project) or defect (slack off while reaping the rewards of everyone else's hard work). If everyone defects, the project doesn't get done and we all fail - a bad outcome for everyone. If I defect but you cooperate, then I get to spend all day on the beach and still get a good grade - the best outcome for me, the worst for you. And if we all cooperate, then it's long hours in the library but at least we pass the class - a \u201cgood enough\u201d outcome, though not quite as good as me defecting against everyone else's cooperation. This exactly mirrors the Prisoner's Dilemma.</p><p>Diplomacy - both the concept and the board game - involves Prisoners' Dilemmas. Suppose Ribbentrop of Germany and Molotov of Russia agree to a peace treaty that demilitarizes their mutual border. If both cooperate, they can move their forces to other theaters, and have moderate success there - a good enough outcome. If Russia cooperates but Germany defects, it can launch a surprise attack on an undefended Russian border and enjoy spectacular success there (for a while, at least!) - the best outcome for Germany and the worst for Russia. But if both defect, then neither has any advantage at the German-Russian border, and they lose the use of those troops in other theaters as well - a bad outcome for both. Again, the Prisoner's Dilemma.</p><p>Civilization - again, both the concept and the game - involves Prisoners' Dilemmas. If everyone follows the rules and creates a stable society (cooperates), we all do pretty well. If everyone else works hard and I turn barbarian and pillage you (defect), then I get all of your stuff without having to work for it and you get nothing - the best solution for me, the worst for you. If everyone becomes a barbarian, there's nothing to steal and we all lose out. Prisoner's Dilemma.</p><p>If everyone who worries about global warming cooperates in cutting emissions, climate change is averted and everyone is moderately happy. If everyone else cooperates in cutting emissions, but one country defects, climate change is still mostly averted, and the defector is at a significant economic advantage. If everyone defects and keeps polluting, the climate changes and everyone loses out. Again a Prisoner's Dilemma,</p><p>Prisoners' Dilemmas even come up in nature. In baboon tribes, when a female is in \u201cheat\u201d, males often compete for the chance to woo her. The most successful males are those who can get a friend to help fight off the other monkeys, and who then helps that friend find his own monkey loving. But these monkeys are tempted to take their friend's female as well. Two males who cooperate each seduce one female. If one cooperates and the other defects, he has a good chance at both females. But if the two can't cooperate at all, then they will be beaten off by other monkey alliances and won't get to have sex with anyone. Still a Prisoner's Dilemma!</p><p>So one might expect the real world to have produced some practical solutions to Prisoners' Dilemmas.</p><p>One of the best known such systems is called \u201csociety\u201d. You may have heard of it. It boasts a series of norms, laws, and authority figures who will punish you when those norms and laws are broken.</p><p>Imagine that the two criminals in the original example were part of a criminal society - let's say the Mafia. The Godfather makes Alice and Bob an offer they can't refuse: turn against one another, and they will end up \u201csleeping with the fishes\u201d (this concludes my knowledge of the Mafia). Now the incentives are changed: defecting against a cooperator doesn't mean walking free, it means getting murdered.</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40f2af6ac205aadb2c5b2e2b529ba85e0ae5fdd87ff11fb0.png/w_997 997w\"></figure><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/06483e6d8ece37f022a6cfefcca2f1667debb8bf72857f18.png/w_997 997w\"></figure><p>Both prisoners cooperate, and amazingly the threat of murder ends up making them both better off (this is also the gist of some of the strongest arguments against libertarianism: in Prisoner's Dilemmas, threatening force against rational agents can increase the utility of all of them!)</p><p>Even when there is no godfather, society binds people by concern about their \u201creputation\u201d. If Bob got a reputation as a snitch, he might never be able to work as a criminal again. If a student gets a reputation for slacking off on projects, she might get ostracized on the playground. If a country gets a reputation for backstabbing, others might refuse to make treaties with them. If a person gets a reputation as a bandit, she might incur the hostility of those around her. If a country gets a reputation for not doing enough to fight global warming, it might...well, no one ever said it was a perfect system.</p><p>Aside from humans in society, evolution is also strongly motivated to develop a solution to the Prisoner's Dilemma. The Dilemma troubles not only lovestruck baboons, but <a href=\"http://brembs.net/ipd/ants.html#HD_NM_17\">ants</a>, <a href=\"http://brembs.net/ipd/inspect.html#HD_NM_18\">minnows</a>, <a href=\"http://brembs.net/ipd/vampire.html#HD_NM_20\">bats</a>, and even <a href=\"http://www.nature.com/nature/journal/v398/n6726/abs/398441a0.html\">viruses</a>. Here the payoff is denominated not in years of jail time, nor in dollars, but in reproductive fitness and number of potential offspring - so evolution will certainly take note.</p><p>Most people, when they hear the rational arguments in favor of defecting every single time on the iterated 100-crime Prisoner's Dilemma, will feel some kind of emotional resistance. Thoughts like \u201cWell, maybe I'll try cooperating anyway a few times, see if it works\u201d, or \u201cIf I promised to cooperate with my opponent, then it would be dishonorable for me to defect on the last turn, even if it helps me out., or even \u201cBob is my friend! Think of all the good times we've had together, robbing banks and running straight into waiting police cordons. I could never betray him!\u201d</p><p>And if two people with these sorts of emotional hangups play the Prisoner's Dilemma together, they'll end up cooperating on all hundred crimes, getting out of jail in a mere century and leaving rational utility maximizers to sit back and wonder how they did it.</p><p>Here's how: imagine you are a supervillain designing a robotic criminal (who's that go-to supervillain Kaj always uses for situations like this? Dr. Zany? Okay, let's say you're him). You expect to build several copies of this robot to work as a team, and expect they might end up playing the Prisoner's Dilemma against each other. You want them out of jail as fast as possible so they can get back to furthering your nefarious plots. So rather than have them bumble through the whole rational utility maximizing thing, you just insert an extra line of code: \u201cin a Prisoner's Dilemma, always cooperate with other robots\u201d. Problem solved.</p><p>Evolution followed the same strategy (no it didn't; this is a massive oversimplification). The emotions we feel around friendship, trust, altruism, and betrayal are partly a built-in hack to succeed in cooperating on Prisoner's Dilemmas where a rational utility-maximizer would defect a hundred times and fail miserably. The evolutionarily dominant strategy is commonly called \u201c<a href=\"http://en.wikipedia.org/wiki/Tit_for_tat\">Tit-for-tat</a>\u201d - basically, cooperate if and only if your opponent did so last time.</p><p>This so-called \"superrationality\u201d appears even more clearly in the Ultimatum Game. Two players are given $100 to distribute among themselves in the following way: the first player proposes a distribution (for example, \u201cFifty for me, fifty for you\u201d) and then the second player either accepts or rejects the distribution. If the second player accepts, the players get the money in that particular ratio. If the second player refuses, no one gets any money at all.</p><p>The first player's reasoning goes like this: \u201cIf I propose $99 for myself and $1 for my opponent, that means I get a lot of money and my opponent still has to accept. After all, she prefers $1 to $0, which is what she'll get if she refuses.</p><p>In the Prisoner's Dilemma, when players were able to communicate beforehand they could settle upon a winning strategy of precommiting to reciprocate: to take an action beneficial to their opponent if and only if their opponent took an action beneficial to them. Here, the second player should consider the same strategy: precommit to an ultimatum (hence the name) that unless Player 1 distributes the money 50-50, she will reject the offer.</p><p>But as in the Prisoner's Dilemma, this fails when you have no reason to expect your opponent to follow through on her precommitment. Imagine you're Player 2, playing a single Ultimatum Game against an opponent you never expect to meet again. You dutifully promise Player 1 that you will reject any offer less than 50-50. Player 1 offers 80-20 anyway. You reason \u201cWell, my ultimatum failed. If I stick to it anyway, I walk away with nothing. I might as well admit it was a good try, give in, and take the $20. After all, rejecting the offer won't magically bring my chance at $50 back, and there aren't any other dealings with this Player 1 guy for it to influence.\u201d</p><p>This is seemingly a rational way to think, but if Player 1 knows you're going to think that way, she offers 99-1, same as before, no matter how sincere your ultimatum sounds.</p><p>Notice all the similarities to the Prisoner's Dilemma: playing as a \"rational economic agent\" gets you a bad result, it looks like you can escape that bad result by making precommitments, but since the other player can't trust your precommitments, you're right back where you started</p><p>If evolutionary solutions to the Prisoners' Dilemma look like trust or friendship or altruism, solutions to the Ultimatum Game involve different emotions entirely. The Sultan presumably does not want you to elope with his daughter. He makes an ultimatum: \u201cTouch my daughter, and I will kill you.\u201d You elope with her anyway, and when his guards drag you back to his palace, you argue: \u201cKilling me isn't going to reverse what happened. Your ultimatum has failed. All you can do now by beheading me is get blood all over your beautiful palace carpet, which hurts you as well as me - the equivalent of pointlessly passing up the last dollar in an Ultimatum Game where you've just been offered a 99-1 split.\u201d</p><p>The Sultan might counter with an argument from social institutions: \u201cIf I let you go, I will look dishonorable. I will gain a reputation as someone people can mess with without any consequences. My choice isn't between bloody carpet and clean carpet, it's between bloody carpet and people respecting my orders, or clean carpet and people continuing to defy me.\u201d</p><p>But he's much more likely to just shout an incoherent stream of dreadful Arabic curse words. Because just as friendship is the evolutionary solution to a Prisoner's Dilemma, so anger is the evolutionary solution to an Ultimatum Game. As various gurus and psychologists have observed, anger makes us irrational. But this is the good kind of irrationality; it's the kind of irrationality that makes us pass up a 99-1 split even though the decision costs us a dollar.</p><p>And if we know that humans are the kind of life-form that tends to experience anger, then if we're playing an Ultimatum Game against a human, and that human precommits to rejecting any offer less than 50-50, we're much more likely to believe her than if we were playing against a rational utility-maximizing agent - and so much more likely to give the human a fair offer.</p><p>It is distasteful and a little bit contradictory to the spirit of rationality to believe it should lose out so badly to simple emotion, and the problem might be correctable. Here we risk crossing the poorly charted border between game theory and decision theory and reaching ideas like <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">timeless decision theory</a>: that one should act as if one's choices determined the output of the algorithm one instantiates (or more simply, you should assume everyone like you will make the same choice you do, and take that into account when choosing.)</p><p>More practically, however, most real-world solutions to Prisoner's Dilemmas and Ultimatum Games still hinge on one of three things: threats of reciprocation when the length of the game is unknown, social institutions and reputation systems that make defection less attractive, and emotions ranging from cooperation to anger that are hard-wired into us by evolution. In the next post, we'll look at how these play out in practice.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 2, "be2Mh2bddQ6ZaBcti": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BroeiXGh9PrKZEkJ5", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 70, "baseScore": 64, "extendedScore": null, "score": 0.000134, "legacy": true, "legacyId": "17373", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 64, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-03T03:31:35.608Z", "modifiedAt": null, "url": null, "title": "Boston MA: Optimal Philanthropy Meetup (July 6th)", "slug": "boston-ma-optimal-philanthropy-meetup-july-6th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:35.439Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KTQ7dTPTLQfRjTuos/boston-ma-optimal-philanthropy-meetup-july-6th", "pageUrlRelative": "/posts/KTQ7dTPTLQfRjTuos/boston-ma-optimal-philanthropy-meetup-july-6th", "linkUrl": "https://www.lesswrong.com/posts/KTQ7dTPTLQfRjTuos/boston-ma-optimal-philanthropy-meetup-july-6th", "postedAtFormatted": "Tuesday, July 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boston%20MA%3A%20Optimal%20Philanthropy%20Meetup%20(July%206th)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoston%20MA%3A%20Optimal%20Philanthropy%20Meetup%20(July%206th)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKTQ7dTPTLQfRjTuos%2Fboston-ma-optimal-philanthropy-meetup-july-6th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boston%20MA%3A%20Optimal%20Philanthropy%20Meetup%20(July%206th)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKTQ7dTPTLQfRjTuos%2Fboston-ma-optimal-philanthropy-meetup-july-6th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKTQ7dTPTLQfRjTuos%2Fboston-ma-optimal-philanthropy-meetup-july-6th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 128, "htmlBody": "<p>Julia and I have done a lot of thinking and writing about how to do the most good in the world [1] [2] but we don't have things all figured out. &nbsp;We'd love to talk to other people interested in <a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">optimal philanthropy</a>, so if you're in the Boston area on Friday July 6th we'd love to have you over for dinner and discussion.</p>\n<p>Details:</p>\n<p>&nbsp;</p>\n<ul>\n<li>RSVP for directions (West Medford)</li>\n<li>6pm to about 9pm</li>\n<li>94 bus from the red line, 95 bus from the orange line.</li>\n<li>We can give rides back into the city at the end of the evening.</li>\n<li>RSVPs would be nice; let us know if you have dietary restrictions.&nbsp;</li>\n</ul>\n<p>&nbsp;</p>\n<p>[1] Julia's blog: <a href=\"http://givinggladly.com\">givinggladly.com</a></p>\n<div>[2] Jeff's posts on giving: <a href=\"http://www.jefftk.com/news/giving\">jefftk.com/giving</a><span style=\"white-space:pre\"> </span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KTQ7dTPTLQfRjTuos", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 9.348419414116144e-07, "legacy": true, "legacyId": "17374", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hEqsWLm5zQtsPevd3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-03T06:10:25.144Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Lawrence Watt-Evans's Fiction", "slug": "seq-rerun-lawrence-watt-evans-s-fiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:31.591Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gwKRnqWnRbgK7LAFe/seq-rerun-lawrence-watt-evans-s-fiction", "pageUrlRelative": "/posts/gwKRnqWnRbgK7LAFe/seq-rerun-lawrence-watt-evans-s-fiction", "linkUrl": "https://www.lesswrong.com/posts/gwKRnqWnRbgK7LAFe/seq-rerun-lawrence-watt-evans-s-fiction", "postedAtFormatted": "Tuesday, July 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Lawrence%20Watt-Evans's%20Fiction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Lawrence%20Watt-Evans's%20Fiction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgwKRnqWnRbgK7LAFe%2Fseq-rerun-lawrence-watt-evans-s-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Lawrence%20Watt-Evans's%20Fiction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgwKRnqWnRbgK7LAFe%2Fseq-rerun-lawrence-watt-evans-s-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgwKRnqWnRbgK7LAFe%2Fseq-rerun-lawrence-watt-evans-s-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<p>Today's post, <a href=\"/lw/s7/lawrence_wattevanss_fiction/\">Lawrence Watt-Evans's Fiction</a> was originally published on 15 July 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Lawrence_Watt-Evans.27s_Fiction\">LW wiki</a>):</p>\n<blockquote>A review of Lawrence Watt-Evans's fiction.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/de2/seq_rerun_probability_is_subjectively_objective/\">Probability is Subjectively Objective</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gwKRnqWnRbgK7LAFe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.349160019078276e-07, "legacy": true, "legacyId": "17377", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Wr9DYtgEAkPGK2fer", "wmG5APgjzf7GzQuhH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-03T06:31:18.608Z", "modifiedAt": null, "url": null, "title": "Where to Intervene in a Human?", "slug": "where-to-intervene-in-a-human", "viewCount": null, "lastCommentedAt": "2022-01-02T04:27:35.975Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WkGBBiF6C9zqtR82R/where-to-intervene-in-a-human", "pageUrlRelative": "/posts/WkGBBiF6C9zqtR82R/where-to-intervene-in-a-human", "linkUrl": "https://www.lesswrong.com/posts/WkGBBiF6C9zqtR82R/where-to-intervene-in-a-human", "postedAtFormatted": "Tuesday, July 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Where%20to%20Intervene%20in%20a%20Human%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhere%20to%20Intervene%20in%20a%20Human%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWkGBBiF6C9zqtR82R%2Fwhere-to-intervene-in-a-human%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Where%20to%20Intervene%20in%20a%20Human%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWkGBBiF6C9zqtR82R%2Fwhere-to-intervene-in-a-human", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWkGBBiF6C9zqtR82R%2Fwhere-to-intervene-in-a-human", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 645, "htmlBody": "<p>The \"<a href=\"http://www.appliedrationality.org/rationality/\">What is Rationality?</a>\" page on <a href=\"http://www.appliedrationality.org/\">the new CFAR website</a> contains an illuminating story about Intel:</p>\n<blockquote>\n<p>\n<p>Semiconductor giant Intel was originally a memory chip manufacturer. But by 1985, memory chips had been losing them money for years. Co-founders Andy Grove (CEO) and Gordon Moore met to discuss the problem. The mood was grim. At one point, Andy turned to Gordon and asked, &ldquo;If we get kicked out and the board brings in a new CEO, what do you think he would do?&rdquo;</p>\n<p>Gordon replied without hesitation. &ldquo;He would get us out of the memory business.&rdquo;</p>\n<p>&ldquo;Okay,&rdquo; said Andy. &ldquo;Then why shouldn&rsquo;t you and I walk out the door, come back, and do it ourselves?&rdquo;</p>\n<p>That year, Andy and Gordon shifted the focus of the company to microprocessors, and created one of the greatest success stories in American business.</p>\n</p>\n</blockquote>\n<p>I presume Andy and Gordon had considered intervening at many different <a href=\"/lw/58g/levels_of_action/\">levels of action</a>: in middle management, in projects, in products, in details, etc. They had probably implemented some of these plans, too. But the problem with Intel &mdash; it was in the wrong market! &mdash; was so deep that the place to intervene was at a very low level, the foundations of the entire company. It's possible that in this situation, <em>no</em>&nbsp;change they could have made at higher levels of action would have made that big of a difference compared to <em>changing the company's market and mission</em>.</p>\n<p>In 1997, system analyst Donella Meadows wrote <a href=\"http://www.developerdotstar.com/mag/articles/places_intervene_system.html\">Places to Intervene in a System</a>, in which she outlined <a href=\"http://en.wikipedia.org/wiki/Twelve_leverage_points\">twelve leverage points</a> at which one could intervene in a system. Different levels of action, she claimed, would have effects of different magnitudes.</p>\n<p>This got me thinking about levels of action and <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">self-improvement</a>. \"I want to improve myself: where should I intervene in my own system <em>next</em>?\"</p>\n<p>My bet is that if the next greatest leverage point you can push on is something like <a href=\"http://en.wikipedia.org/wiki/Neurofeedback\">neurofeedback</a>, then you're pretty damn self-optimized already.</p>\n<p>In fact, I suspect <em>almost nobody</em>&nbsp;is that self-optimized. We do things like neurofeedback because (1) we don't think enough about choosing the highest-leverage self-interventions, (2) in any case, we don't know how to figure out which interventions would be higher leverage for ourselves, (3) even if there are higher-leverage interventions to be had, we might not successfully carry them through, but neurofeedback or whatever happens to be fun and engaging for us, and (3) sometimes, you gotta stop analyzing your situation and <em>just do some stuff that looks like it might help</em>.</p>\n<p>Anyway, how can one figure out what the next highest-leverage self-interventions are for oneself? Maybe I just haven't yet found the right keywords, but I don't think there's been much research on this topic.</p>\n<p>Intuitively, it seems like <a href=\"/lw/9wr/my_algorithm_for_beating_procrastination/\">hacking one's motivational system</a> is among the highest leverage interventions one can make, because high motivation allows on to carry through with lots of other interventions, and without sufficient motivation one <em>can't</em>&nbsp;follow through with many interventions.</p>\n<p>But if you've got a crippling emotional or physical condition, I suppose you've got to take care of that first &mdash; at least well enough to embark on the project of hacking your motivation system.</p>\n<p>Or, if you're in a crippling <em>environment</em>&nbsp;like North Korea or Nigeria or Detroit, then perhaps the highest level intervention for you is to get up and <em>move someplace better</em>. Only <em>then</em>&nbsp;will you be able to fix your emotions or hack your motivational system or whatever.</p>\n<p>Maybe there's something of a system to this that hasn't been discovered, or maybe there's no system at all because humans are too complex. I'm still in brainstorm mode on this topic.</p>\n<p>\n<ul>\n<li>What do <em>you</em>&nbsp;think are some generally highest-level self-improvement interventions that more people should be tackling before things like neurofeedback?</li>\n<li>What algorithm could be used for discovering the next best intervention one can make to improve oneself?</li>\n<li>Has there been any research on this issue?</li>\n</ul>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Y3oHd7CQpy8aQFWD9": 2, "Ng8Gice9KNkncxqcj": 2, "KJbjewaok72AS6fwn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WkGBBiF6C9zqtR82R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 53, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "17378", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["guDcrPqLsnhEjrPZj", "33KewgYhNSxFpbpXg", "Ty2tjPwv8uyPK9vrz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-03T15:01:32.174Z", "modifiedAt": null, "url": null, "title": "CFAR website launched", "slug": "cfar-website-launched", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:39.351Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EnK3kYQxhQPjnctE5/cfar-website-launched", "pageUrlRelative": "/posts/EnK3kYQxhQPjnctE5/cfar-website-launched", "linkUrl": "https://www.lesswrong.com/posts/EnK3kYQxhQPjnctE5/cfar-website-launched", "postedAtFormatted": "Tuesday, July 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CFAR%20website%20launched&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACFAR%20website%20launched%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEnK3kYQxhQPjnctE5%2Fcfar-website-launched%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CFAR%20website%20launched%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEnK3kYQxhQPjnctE5%2Fcfar-website-launched", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEnK3kYQxhQPjnctE5%2Fcfar-website-launched", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>The new <a href=\"http://www.appliedrationality.org/\">Center for Applied Rationality</a> website has launched! We'll be adding content as time goes by. Let us know if you find broken links, etc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"X7v7Fyp9cgBYaMe2e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EnK3kYQxhQPjnctE5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 45, "extendedScore": null, "score": 9.351637379324375e-07, "legacy": true, "legacyId": "17394", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-03T16:16:55.670Z", "modifiedAt": null, "url": null, "title": "More Irrationality Game", "slug": "more-irrationality-game", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:33.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Fill_Cluesome", "createdAt": "2012-06-19T00:14:56.914Z", "isAdmin": false, "displayName": "Fill_Cluesome"}, "userId": "LiHSB5WHc7gf34Ttb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/va44xMNtMdBikkdEX/more-irrationality-game", "pageUrlRelative": "/posts/va44xMNtMdBikkdEX/more-irrationality-game", "linkUrl": "https://www.lesswrong.com/posts/va44xMNtMdBikkdEX/more-irrationality-game", "postedAtFormatted": "Tuesday, July 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20More%20Irrationality%20Game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMore%20Irrationality%20Game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fva44xMNtMdBikkdEX%2Fmore-irrationality-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=More%20Irrationality%20Game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fva44xMNtMdBikkdEX%2Fmore-irrationality-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fva44xMNtMdBikkdEX%2Fmore-irrationality-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 998, "htmlBody": "<p>I thought it would be good to play the irrationality game again. Let's do it!</p>\n<div>Entire text of \"Will Newsome's\"&nbsp;<a href=\"/lw/2sl/the_irrationality_game/\">original post</a>:<br /><br /></div>\n<div>\n<blockquote>\n<p><big><strong>Please read the post before voting on the comments, as this is a game where voting works differently.</strong></big></p>\n<p><em>Warning: the comments section of this post will look odd. The most reasonable comments will have lots of negative karma. Do not be alarmed, it's all part of the plan.</em><em>&nbsp;In order to participate in this game you should disable any viewing threshold for negatively voted comments.</em></p>\n<p>Here's an irrationalist game meant to quickly collect a pool of controversial ideas for people to debate and assess. It kinda relies on people being honest and not being nitpickers, but it might be fun.</p>\n<p>Write a comment reply to this post describing a belief you think has a reasonable chance of being true&nbsp;<em>relative to the the beliefs of other Less Wrong folk.</em>&nbsp;Jot down a proposition and a rough probability estimate or qualitative description, like 'fairly confident'.</p>\n<p>Example (not my true belief): \"The U.S. government was directly responsible for financing the September 11th terrorist attacks. Very confident. (~95%).\"</p>\n<p>If you post a belief, you have to vote on the beliefs of all other comments. Voting works like this:&nbsp;if you basically agree with the comment,&nbsp;<em>vote the comment down.&nbsp;</em>If you basically&nbsp;<em>disagree</em>&nbsp;with the comment,&nbsp;<em>vote the comment up.&nbsp;</em>What 'basically' means here is intuitive; instead of using a precise mathy scoring system, just make a guess. In my view, if their stated probability is 99.9% and your degree of belief is 90%, that merits an upvote: it's a pretty big difference of opinion. If they're at 99.9% and you're at 99.5%, it could go either way. If you're genuinely unsure whether or not you basically agree with them, you can pass on voting (but try not to). Vote up if you think they are either overconfident or underconfident in their belief: any disagreement is valid disagreement.</p>\n<p>That's the spirit of the game, but some more qualifications and rules follow.<a id=\"more\"></a></p>\n<p>If the proposition in a comment isn't incredibly precise, use your best interpretation. If you&nbsp;<em>really</em>&nbsp;have to pick nits for whatever reason, say so in a comment reply.</p>\n<p><em>The more upvotes you get, the more irrational Less Wrong perceives your belief to be.</em><strong>&nbsp;</strong>Which means that if you have a large amount of Less Wrong karma and can still get lots of upvotes on your crazy beliefs then you will get lots of smart people to take your weird ideas a little more seriously.</p>\n<p>Some poor soul is going to come along and post \"I believe in God\". Don't pick nits and say \"Well in a a Tegmark multiverse there is definitely a universe exactly like ours where some sort of god rules over us...\" and downvote it. That's cheating. You better upvote the guy. For just this post, get over your desire to upvote rationality.&nbsp;<em>For this game, we reward perceived irrationality.</em></p>\n<p>Try to be precise in your propositions. Saying \"I believe in God. 99% sure.\" isn't informative because we don't quite know which God you're talking about. A deist god? The Christian God? Jewish?</p>\n<p>Y'all know this already, but just a reminder: preferences ain't beliefs. Downvote preferences disguised as beliefs. Beliefs that include the word \"should\" are are almost always imprecise: avoid them.</p>\n<div>That means our local theists are probably gonna get a lot of upvotes. Can you beat them with your confident but perceived-by-LW-as-irrational beliefs? It's a challenge!</div>\n<p>Additional rules:</p>\n<ul>\n<li>Generally, no repeating an altered version of a proposition already in the comments unless it's different in an interesting and important way.<em>&nbsp;</em>Use your judgement.</li>\n<li>If you have comments about the game, please reply to my comment below about meta discussion, not to the post itself. Only propositions to be judged for the game should be direct comments to this post.&nbsp;</li>\n<li>Don't post propositions as comment replies to other comments. That'll make it disorganized.</li>\n<li><strong>You have to actually think your degree of belief is rational. &nbsp;</strong>You should already have taken the fact that most people would disagree with you into account and updated on that information. That means that &nbsp;<strong>any proposition you make is a proposition that you think you are personally more rational about than the Less Wrong average. &nbsp;</strong>This could be good or bad. Lots of upvotes means lots of people disagree with you. That's generally bad. Lots of downvotes means you're probably right. That's good, but this is a game where perceived irrationality wins you karma.&nbsp;The game is only fun if you're trying to be completely honest in your stated beliefs. Don't post something crazy and expect to get karma. Don't exaggerate your beliefs. Play fair.</li>\n<li><strong>Debate and discussion is great, but keep it civil. &nbsp;</strong>Linking to the Sequences is barely civil -- summarize arguments from specific LW posts and maybe link, but don't tell someone to go read something. If someone says they believe in God with 100% probability and you don't want to take the time to give a brief but substantive counterargument, don't comment at all. We're inviting people to share beliefs we think are irrational; don't be mean about their responses.</li>\n<li>No propositions that people are unlikely to have an opinion about, like \"Yesterday I wore black socks. ~80%\" or \"Antipope Christopher would have been a good leader in his latter days had he not been dethroned by Pope Sergius III. ~30%.\" The goal is to be controversial and&nbsp;<em>interesting.</em></li>\n<li>Multiple propositions are fine, so long as they're moderately interesting.</li>\n<li>You are encouraged to reply to comments with your own probability estimates, but &nbsp;<strong>comment voting works normally for comment replies to other comments. &nbsp;</strong>That is, upvote for good discussion, not agreement or disagreement.</li>\n<li>In general, just keep within the spirit of the game: we're celebrating LW-contrarian beliefs for a change!</li>\n</ul>\n</blockquote>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "va44xMNtMdBikkdEX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -12, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "17395", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wDJaQG4QSKDYxzmor"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-03T18:50:20.055Z", "modifiedAt": null, "url": null, "title": "Irrationality Game II", "slug": "irrationality-game-ii", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.694Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "YTcLqiv4mD6QeH3Ap", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fufh5NEjKeMRgAkcd/irrationality-game-ii", "pageUrlRelative": "/posts/fufh5NEjKeMRgAkcd/irrationality-game-ii", "linkUrl": "https://www.lesswrong.com/posts/fufh5NEjKeMRgAkcd/irrationality-game-ii", "postedAtFormatted": "Tuesday, July 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Irrationality%20Game%20II&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIrrationality%20Game%20II%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffufh5NEjKeMRgAkcd%2Firrationality-game-ii%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Irrationality%20Game%20II%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffufh5NEjKeMRgAkcd%2Firrationality-game-ii", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffufh5NEjKeMRgAkcd%2Firrationality-game-ii", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1051, "htmlBody": "<p>I was very interested in the discussions and opinions that grew out of the last time <a href=\"/lw/2sl/the_irrationality_game/\">this</a> was played, but find digging through 800+ comments for a new game to start on the same thread annoying. I <a href=\"/r/discussion/lw/df7/more_irrationality_game/6y6c\">also</a> don't want this game ruined by a potential sock puppet (<a href=\"/user/Thrill_Shoesome/\">whom ever</a> <a href=\"/user/Fill_Cluesome/\">it may</a> be). So here's a non-sockpuppetiered Irrationality Game, if there's still interest. If there isn't, downvote to oblivion!<br /><br />The original rules:</p>\n<div style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><br /></div>\n<div style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<blockquote>\n<p style=\"margin: 0px 0px 1em;\"><big><strong>Please read the post before voting on the comments, as this is a game where voting works differently.</strong></big></p>\n<p style=\"margin: 0px 0px 1em;\"><em>Warning: the comments section of this post will look odd. The most reasonable comments will have lots of negative karma. Do not be alarmed, it's all part of the plan.</em><em>&nbsp;In order to participate in this game you should disable any viewing threshold for negatively voted comments.</em></p>\n<p style=\"margin: 0px 0px 1em;\">Here's an irrationalist game meant to quickly collect a pool of controversial ideas for people to debate and assess. It kinda relies on people being honest and not being nitpickers, but it might be fun.</p>\n<p style=\"margin: 0px 0px 1em;\">Write a comment reply to this post describing a belief you think has a reasonable chance of being true&nbsp;<em>relative to the the beliefs of other Less Wrong folk.</em>&nbsp;Jot down a proposition and a rough probability estimate or qualitative description, like 'fairly confident'.</p>\n<p style=\"margin: 0px 0px 1em;\">Example (not my true belief): \"The U.S. government was directly responsible for financing the September 11th terrorist attacks. Very confident. (~95%).\"</p>\n<p style=\"margin: 0px 0px 1em;\">If you post a belief, you have to vote on the beliefs of all other comments. Voting works like this:&nbsp;if you basically agree with the comment,&nbsp;<em>vote the comment down.&nbsp;</em>If you basically&nbsp;<em>disagree</em>&nbsp;with the comment,&nbsp;<em>vote the comment up.&nbsp;</em>What 'basically' means here is intuitive; instead of using a precise mathy scoring system, just make a guess. In my view, if their stated probability is 99.9% and your degree of belief is 90%, that merits an upvote: it's a pretty big difference of opinion. If they're at 99.9% and you're at 99.5%, it could go either way. If you're genuinely unsure whether or not you basically agree with them, you can pass on voting (but try not to). Vote up if you think they are either overconfident or underconfident in their belief: any disagreement is valid disagreement.</p>\n<p style=\"margin: 0px 0px 1em;\">That's the spirit of the game, but some more qualifications and rules follow.<a id=\"more\" style=\"color: #6a8a6b; text-decoration: underline;\"></a></p>\n<p style=\"margin: 0px 0px 1em;\">If the proposition in a comment isn't incredibly precise, use your best interpretation. If you&nbsp;<em>really</em>&nbsp;have to pick nits for whatever reason, say so in a comment reply.</p>\n<p style=\"margin: 0px 0px 1em;\"><em>The more upvotes you get, the more irrational Less Wrong perceives your belief to be.</em><strong>&nbsp;</strong>Which means that if you have a large amount of Less Wrong karma and can still get lots of upvotes on your crazy beliefs then you will get lots of smart people to take your weird ideas a little more seriously.</p>\n<p style=\"margin: 0px 0px 1em;\">Some poor soul is going to come along and post \"I believe in God\". Don't pick nits and say \"Well in a a Tegmark multiverse there is definitely a universe exactly like ours where some sort of god rules over us...\" and downvote it. That's cheating. You better upvote the guy. For just this post, get over your desire to upvote rationality.&nbsp;<em>For this game, we reward perceived irrationality.</em></p>\n<p style=\"margin: 0px 0px 1em;\">Try to be precise in your propositions. Saying \"I believe in God. 99% sure.\" isn't informative because we don't quite know which God you're talking about. A deist god? The Christian God? Jewish?</p>\n<p style=\"margin: 0px 0px 1em;\">Y'all know this already, but just a reminder: preferences ain't beliefs. Downvote preferences disguised as beliefs. Beliefs that include the word \"should\" are are almost always imprecise: avoid them.</p>\n<div>That means our local theists are probably gonna get a lot of upvotes. Can you beat them with your confident but perceived-by-LW-as-irrational beliefs? It's a challenge!</div>\n<p style=\"margin: 0px 0px 1em;\">Additional rules:</p>\n<ul style=\"padding: 0px;\">\n<li>Generally, no repeating an altered version of a proposition already in the comments unless it's different in an interesting and important way.<em>&nbsp;</em>Use your judgement.</li>\n<li>If you have comments about the game, please reply to my comment below about meta discussion, not to the post itself. Only propositions to be judged for the game should be direct comments to this post.&nbsp;</li>\n<li>Don't post propositions as comment replies to other comments. That'll make it disorganized.</li>\n<li><strong>You have to actually think your degree of belief is rational. &nbsp;</strong>You should already have taken the fact that most people would disagree with you into account and updated on that information. That means that &nbsp;<strong>any proposition you make is a proposition that you think you are personally more rational about than the Less Wrong average. &nbsp;</strong>This could be good or bad. Lots of upvotes means lots of people disagree with you. That's generally bad. Lots of downvotes means you're probably right. That's good, but this is a game where perceived irrationality wins you karma.&nbsp;The game is only fun if you're trying to be completely honest in your stated beliefs. Don't post something crazy and expect to get karma. Don't exaggerate your beliefs. Play fair.</li>\n<li><strong>Debate and discussion is great, but keep it civil. &nbsp;</strong>Linking to the Sequences is barely civil -- summarize arguments from specific LW posts and maybe link, but don't tell someone to go read something. If someone says they believe in God with 100% probability and you don't want to take the time to give a brief but substantive counterargument, don't comment at all. We're inviting people to share beliefs we think are irrational; don't be mean about their responses.</li>\n<li>No propositions that people are unlikely to have an opinion about, like \"Yesterday I wore black socks. ~80%\" or \"Antipope Christopher would have been a good leader in his latter days had he not been dethroned by Pope Sergius III. ~30%.\" The goal is to be controversial and&nbsp;<em>interesting.</em></li>\n<li>Multiple propositions are fine, so long as they're moderately interesting.</li>\n<li>You are encouraged to reply to comments with your own probability estimates, but &nbsp;<strong>comment voting works normally for comment replies to other comments. &nbsp;</strong>That is, upvote for good discussion, not agreement or disagreement.</li>\n<li>In general, just keep within the spirit of the game: we're celebrating LW-contrarian beliefs for a change!</li>\n</ul>\n</blockquote>\n</div>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Enjoy!</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fufh5NEjKeMRgAkcd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 17, "extendedScore": null, "score": 9.352701498611075e-07, "legacy": true, "legacyId": "17396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 383, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wDJaQG4QSKDYxzmor"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-04T00:29:52.306Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes July 2012", "slug": "rationality-quotes-july-2012", "viewCount": null, "lastCommentedAt": "2012-08-20T18:53:16.134Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5YzTwuzLKZc2CwoRX/rationality-quotes-july-2012", "pageUrlRelative": "/posts/5YzTwuzLKZc2CwoRX/rationality-quotes-july-2012", "linkUrl": "https://www.lesswrong.com/posts/5YzTwuzLKZc2CwoRX/rationality-quotes-july-2012", "postedAtFormatted": "Wednesday, July 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20July%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20July%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5YzTwuzLKZc2CwoRX%2Frationality-quotes-july-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20July%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5YzTwuzLKZc2CwoRX%2Frationality-quotes-july-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5YzTwuzLKZc2CwoRX%2Frationality-quotes-july-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p><span style=\"font-size: 12px; line-height: 11px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\">Here's the new thread for posting quotes, with the usual rules:</span></p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li><span style=\"font-size: 12px; line-height: 11px;\">Please post all quotes separately, so that they can be voted up/down separately. &nbsp;(If they are strongly related, reply to your own comments. &nbsp;If strongly ordered, then go ahead and post them together.)</span></li>\n<li><span style=\"font-size: 12px; line-height: 11px;\">Do not quote yourself</span></li>\n<li><span style=\"font-size: 12px; line-height: 11px;\">Do not quote comments/posts on LW/OB</span></li>\n<li><span style=\"font-size: 12px; line-height: 11px;\">No more than 5 quotes per person per monthly thread, please.</span></li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5YzTwuzLKZc2CwoRX", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.354289630964054e-07, "legacy": true, "legacyId": "17370", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 473, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-07-04T00:29:52.306Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-04T06:33:24.946Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Whither Moral Progress?", "slug": "seq-rerun-whither-moral-progress", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:32.618Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wj67EJvT5q8Fx2HeH/seq-rerun-whither-moral-progress", "pageUrlRelative": "/posts/Wj67EJvT5q8Fx2HeH/seq-rerun-whither-moral-progress", "linkUrl": "https://www.lesswrong.com/posts/Wj67EJvT5q8Fx2HeH/seq-rerun-whither-moral-progress", "postedAtFormatted": "Wednesday, July 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Whither%20Moral%20Progress%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Whither%20Moral%20Progress%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWj67EJvT5q8Fx2HeH%2Fseq-rerun-whither-moral-progress%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Whither%20Moral%20Progress%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWj67EJvT5q8Fx2HeH%2Fseq-rerun-whither-moral-progress", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWj67EJvT5q8Fx2HeH%2Fseq-rerun-whither-moral-progress", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"/lw/s9/whither_moral_progress/\">Whither Moral Progress?</a> was originally published on 16 July 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Whither_Moral_Progress.3F\">LW wiki</a>):</p>\n<blockquote>Does moral progress actually happen? And if it does so, how?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/dep/seq_rerun_lawrence_wattevanss_fiction/\">Lawrence Watt-Evans's Fiction</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wj67EJvT5q8Fx2HeH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.35598687865941e-07, "legacy": true, "legacyId": "17407", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["szAkYJDtXkcSAiHYE", "gwKRnqWnRbgK7LAFe", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-04T10:29:39.656Z", "modifiedAt": null, "url": null, "title": "[Link] Why the kids don\u2019t know no algebra", "slug": "link-why-the-kids-don-t-know-no-algebra", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.256Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rgi9mHZ4Q4WNEpeSb/link-why-the-kids-don-t-know-no-algebra", "pageUrlRelative": "/posts/rgi9mHZ4Q4WNEpeSb/link-why-the-kids-don-t-know-no-algebra", "linkUrl": "https://www.lesswrong.com/posts/rgi9mHZ4Q4WNEpeSb/link-why-the-kids-don-t-know-no-algebra", "postedAtFormatted": "Wednesday, July 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Why%20the%20kids%20don%E2%80%99t%20know%20no%20algebra&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Why%20the%20kids%20don%E2%80%99t%20know%20no%20algebra%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frgi9mHZ4Q4WNEpeSb%2Flink-why-the-kids-don-t-know-no-algebra%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Why%20the%20kids%20don%E2%80%99t%20know%20no%20algebra%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frgi9mHZ4Q4WNEpeSb%2Flink-why-the-kids-don-t-know-no-algebra", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frgi9mHZ4Q4WNEpeSb%2Flink-why-the-kids-don-t-know-no-algebra", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1194, "htmlBody": "<p><a href=\"http://blogs.discovermagazine.com/gnxp/2012/07/why-the-kids-dont-know-no-algebra/\">Post</a> by fellow LW reader Razib Khan, who many here probably know from&nbsp; the <a href=\"http://www.gnxp.com/\">gnxp site</a> or perhaps from his <a href=\"/lw/1qu/bhtv_eliezer_yudkowsky_razib_khan/\">debate with Eliezer</a>.</p>\n<blockquote>\n<p>A few days ago I stumbled upon a really interesting post. And I&rsquo;m wondering if my readers are at all familiar with the phenomenon outlined here (it was a total surprise to me), <a href=\"http://educationrealist.wordpress.com/2012/07/01/the-myth-of-they-werent-ever-taught/\">The myth of &ldquo;they weren&rsquo;t ever taught&hellip;.&rdquo;</a>:</p>\n<p>With all this I am not saying conditions which are non-hereditary are irrelevant. What I am saying is that we can&rsquo;t ignore the shape of the pre-existent landscape before we attempt to reshape it to our own image. Excoriating teachers for having pupils who can&rsquo;t master mid-level secondary school mathematics is in some cases like excoriating someone for the fact that their irrigation canals from the plains into the mountains are failures. You need to level the mountains before your canals can work (or, barring that design and implement a mechanical system which will move water against the grade). Easier said than done. E. O. Wilson said of Communism, &ldquo;Great Idea, Wrong Species.&rdquo; The reaction of Communist regimes to this reality was brutal and shocking. Obviously the modern rejection of unpalatable aspects of human nature are not so grotesque. But they have a human toll nonetheless. I&rsquo;m skeptical that this generation will pass before we have to acknowledge these realities and calibrate our policies accordingly.</p>\n<p style=\"padding-left: 60px;\">Stage One: I will describe this stage for algebra I teachers, but plug in reading, geometry, writing, science, any subject you choose, with the relevant details.<strong> This stage begins when teachers realize that easily half the class adds the numerators and denominators when adding fractions, doesn&rsquo;t see the difference between 3-5 and 5-3, counts on fingers to add 8 and 6, and looks blank when asked what 7 times 3 is.</strong></p>\n<p style=\"padding-left: 60px;\">Ah, they think. The kids weren&rsquo;t ever taught fractions and basic math facts! What the hell are these other teachers doing, then, taking a salary for showing the kids movies and playing Math Bingo? Insanity on the public penny. But hey, helping these kids, teaching them properly, is the reason they became teachers in the first place. So they push their schedule back, what, two weeks? Three? And go through fraction operations, reciprocals, negative numbers, the meaning of subtraction, a few properties of equality, and just wallow in the glories of basic arithmetic. Some use manipulatives, others use drills and games to increase engagement, but whatever the method, they&rsquo;re basking in the glow of knowledge that they are Closing the Gap, that their kids are finally getting the attention that privileged suburban students get by virtue of their summer enrichment and more expensive teachers.</p>\n<p style=\"padding-left: 60px;\">At first, it seems to work. The kids beam and say, &ldquo;You explain it so much better than my last teacher did!&rdquo; and the quizzes seem to show real progress. Phew! Now it&rsquo;s possible to get on to teaching algebra, rather than the material the kids just hadn&rsquo;t been taught.</p>\n<p style=\"padding-left: 60px;\"><strong>But then, a few weeks later, the kids go back to ignoring the difference between 3-5 and 5-3.</strong> Furthermore, despite hours of explanation and practice, half the class seems to do no better than toss a coin to make the call on positive or negative slopes. Many students who demonstrated mastery of distributing multiplication over addition are now making a complete hash of the process in multi-step equations. And many students are still counting on their fingers.</p>\n<p>The author is involved in education personally, so is posting their own reflections as well as what others report to them. In personal correspondence they explain that this phenomenon is common among children of <em>average</em> intelligence. The lowest quartile presumably would never have been able to master many of these rules in the first place. Some of the information resembles the stuff that a friend of mine experienced when he went in to do tutoring for disadvantaged students in Boston when he was getting his doctorate at MIT. At first my friend was totally taken aback at the level of ignorance (e.g., the inability to see the relationship between 1/10 and 10/100). Today he works at a major technology firm as a scientist, but continues to be involved in mentoring &ldquo;at risk&rdquo; kids. At some point you have to muddle on. He does his best, and does not indulge in the luxury of shock and disappointment. That helps no one.</p>\n<p><strong>This matters because American society is notionally obsessed with education</strong>. All this isn&rsquo;t too clear or important to be frank when you aren&rsquo;t a parent. It&rsquo;s somewhat in the realm of the abstract. That changes when you become a parent. Suddenly you become immersed in the data of your local schools, and begin to weight various options to optimize your child&rsquo;s schooling experience. Of course the real differences in school metrics have not only parental relevance, they matter in terms of national policy and attention. Both the political Left and the Right have their own pet solutions. More money, reform teachers&rsquo; unions, charter schools, vouchers, etc.</p>\n<p><strong>But the biggest problem at the heart of the matter is the fundamental populist drive to ignore human difference</strong>. American schools were designed to produce the citizen, and the citizen has the same rights and responsibilities from individual to individual. In some ways the public school system as it emerged in the 19th century was a project by the Protestant establishment to assimilate white ethnics, in particular Catholics (who of course created their own alternative educational system to maintain cultural separation and distinctiveness). In the 21st century the drive to produce <em>H. Americanus </em>seems quaint, rather, we want to citizens of the world with skills and abilities to navigate an information economy.</p>\n<p>What American society on a deep philosophical level, no matter the political outlook, detests acknowledging is that a simple and elegant public policy solution <strong>can not abolish human difference</strong>. Some children are more athletic than others, and some children are more intelligent than others. Starting among conservatives, but now <a href=\"http://en.wikipedia.org/wiki/Waiting_for_%22Superman%22\">spreading to some liberals</a>, is a rejection of this premise via blaming teachers. The premise is bewitching because it presents tractable problems with solutions on hand. Here is <a href=\"http://en.wikipedia.org/wiki/John_B._Watson#.22Twelve_infants.22_quotation\">John B. Watson, the father of behaviorism</a>:</p>\n<p style=\"padding-left: 60px;\">Give me a dozen healthy infants, well-formed, and my own specified world to bring them up in and I&rsquo;ll guarantee to take any one at random and train him to become any type of specialist I might select &ndash; doctor, lawyer, artist, merchant-chief and, yes, even beggar-man and thief, regardless of his talents, penchants, tendencies, abilities, vocations, and race of his ancestors. I am going beyond my facts and I admit it, but so have the advocates of the contrary and they have been doing it for many thousands of years</p>\n<p>I think if Watson were alive today he&rsquo;d have to admit he was wrong. Your ancestors are not destiny, <strong>but they are probability.</strong> If your father plays in the N.B.A., the probability that you will play in the N.B.A. is not high. But the probability is orders of magnitude higher than if you are a random person off the street.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 1, "4cKQgA4S7xfNeeWXg": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rgi9mHZ4Q4WNEpeSb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 30, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "17408", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 165, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4gP3692vXuauKGjMR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-05T01:36:42.888Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Gift We Give To Tomorrow", "slug": "seq-rerun-the-gift-we-give-to-tomorrow", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:34.285Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gy7CHgBzjrEtaxCZW/seq-rerun-the-gift-we-give-to-tomorrow", "pageUrlRelative": "/posts/Gy7CHgBzjrEtaxCZW/seq-rerun-the-gift-we-give-to-tomorrow", "linkUrl": "https://www.lesswrong.com/posts/Gy7CHgBzjrEtaxCZW/seq-rerun-the-gift-we-give-to-tomorrow", "postedAtFormatted": "Thursday, July 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Gift%20We%20Give%20To%20Tomorrow&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Gift%20We%20Give%20To%20Tomorrow%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGy7CHgBzjrEtaxCZW%2Fseq-rerun-the-gift-we-give-to-tomorrow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Gift%20We%20Give%20To%20Tomorrow%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGy7CHgBzjrEtaxCZW%2Fseq-rerun-the-gift-we-give-to-tomorrow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGy7CHgBzjrEtaxCZW%2Fseq-rerun-the-gift-we-give-to-tomorrow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>Today's post, <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">The Gift We Give To Tomorrow</a> was originally published on 17 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Gift_We_Give_To_Tomorrow\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>How did love ever come into the universe? How did that happen, and how special was it, really?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/dfj/seq_rerun_whither_moral_progress/\">Whither Moral Progress?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gy7CHgBzjrEtaxCZW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.361328061017269e-07, "legacy": true, "legacyId": "17411", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pGvyqAQw6yqTjpKf4", "Wj67EJvT5q8Fx2HeH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-05T03:01:43.322Z", "modifiedAt": null, "url": null, "title": "[Link] \"First Is Best\" - The serial position effect / primacy effect", "slug": "link-first-is-best-the-serial-position-effect-primacy-effect", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:32.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aelephant", "createdAt": "2012-02-19T03:46:36.462Z", "isAdmin": false, "displayName": "aelephant"}, "userId": "QNbh3Wzv4B3KqWv6N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ybSujYtMeSEE2sTWK/link-first-is-best-the-serial-position-effect-primacy-effect", "pageUrlRelative": "/posts/ybSujYtMeSEE2sTWK/link-first-is-best-the-serial-position-effect-primacy-effect", "linkUrl": "https://www.lesswrong.com/posts/ybSujYtMeSEE2sTWK/link-first-is-best-the-serial-position-effect-primacy-effect", "postedAtFormatted": "Thursday, July 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20%22First%20Is%20Best%22%20-%20The%20serial%20position%20effect%20%2F%20primacy%20effect&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20%22First%20Is%20Best%22%20-%20The%20serial%20position%20effect%20%2F%20primacy%20effect%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FybSujYtMeSEE2sTWK%2Flink-first-is-best-the-serial-position-effect-primacy-effect%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20%22First%20Is%20Best%22%20-%20The%20serial%20position%20effect%20%2F%20primacy%20effect%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FybSujYtMeSEE2sTWK%2Flink-first-is-best-the-serial-position-effect-primacy-effect", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FybSujYtMeSEE2sTWK%2Flink-first-is-best-the-serial-position-effect-primacy-effect", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 285, "htmlBody": "<p><a title=\"&quot;First Is Best&quot;\" href=\"http://www.plosone.org/article/fetchObjectAttachment.action;jsessionid=B6745ED09DA8A8C6C93DC951CC67D745?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0035088&amp;representation=PDF\">\"First is Best\"</a></p>\n<blockquote>\n<p><strong>Abstract</strong><br />We experience the world serially rather than simultaneously. A century of research on human and nonhuman animals has suggested that the first experience in a series of two or more is cognitively privileged. We report three experiments designed to test the effect of first position on implicit preference and choice using targets that range from individual humans and social groups to consumer goods.</p>\n</blockquote>\n<p>While this effect has been known about for many years, these researchers added an interesting component, an \"Implicit Association Test (IAT)\":</p>\n<blockquote>\n<p>Each option within a pair was presented sequentially for 30-seconds and participants were forced to maximally consider both options. Immediately after each choice-pair was presented, participants completed a measure which assessed automatic preference for each option (an Implicit Association Test, or IAT) [22].</p>\n</blockquote>\n<p>and</p>\n<blockquote>\n<p>Regardless of the actual option, the one presented first compared to the one presented next was significantly more strongly associated with the concept &lsquo;&lsquo;better&rsquo;&rsquo; rather than &lsquo;&lsquo;worse&rsquo;&rsquo;, F(1, 121) =20.20, p,.001; effect size r =.38 (Figure 1). There was no difference in self-reported preference for firsts versus seconds, F(1, 121) =.08, p= .78.</p>\n</blockquote>\n<p>I was surprised to find there is no reference to \"recency\", \"primacy\" or \"serial position\" on the LessWrong Wiki. A search on LessWrong.com for \"recency effect\" turns up 8 posts that mention it but don't give it a thorough discussion as far as I can tell; \"primacy effect\" turns up 1 post about Rationality &amp; Criminal Law; and \"serial position\" turns up nothing. Is there another name for this effect that I'm missing?</p>\n<p>Wikipedia has some discussion of the serial position effect <a title=\"here\" href=\"http://en.wikipedia.org/wiki/Serial_position_effect\">here</a>, although from a quick skim it doesn't appear that they talk about preference at all.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ybSujYtMeSEE2sTWK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 9.361725403530038e-07, "legacy": true, "legacyId": "17415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-05T08:51:30.456Z", "modifiedAt": null, "url": null, "title": "We prosecute CEOs for failing to do due diligence. But with people, we call it 'faith'", "slug": "we-prosecute-ceos-for-failing-to-do-due-diligence-but-with", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:51.282Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "avichapman", "createdAt": "2012-04-30T05:38:19.163Z", "isAdmin": false, "displayName": "avichapman"}, "userId": "AkCycCQcKgvoCiDgu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/skqYPuD9Gzb3aFc6f/we-prosecute-ceos-for-failing-to-do-due-diligence-but-with", "pageUrlRelative": "/posts/skqYPuD9Gzb3aFc6f/we-prosecute-ceos-for-failing-to-do-due-diligence-but-with", "linkUrl": "https://www.lesswrong.com/posts/skqYPuD9Gzb3aFc6f/we-prosecute-ceos-for-failing-to-do-due-diligence-but-with", "postedAtFormatted": "Thursday, July 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20We%20prosecute%20CEOs%20for%20failing%20to%20do%20due%20diligence.%20But%20with%20people%2C%20we%20call%20it%20'faith'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWe%20prosecute%20CEOs%20for%20failing%20to%20do%20due%20diligence.%20But%20with%20people%2C%20we%20call%20it%20'faith'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FskqYPuD9Gzb3aFc6f%2Fwe-prosecute-ceos-for-failing-to-do-due-diligence-but-with%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=We%20prosecute%20CEOs%20for%20failing%20to%20do%20due%20diligence.%20But%20with%20people%2C%20we%20call%20it%20'faith'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FskqYPuD9Gzb3aFc6f%2Fwe-prosecute-ceos-for-failing-to-do-due-diligence-but-with", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FskqYPuD9Gzb3aFc6f%2Fwe-prosecute-ceos-for-failing-to-do-due-diligence-but-with", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 971, "htmlBody": "<p>I wrote the following on my blog last night. I thought that I'd run it past an&nbsp;intelligent&nbsp;audience. Note that what I have referred to as an idea is what we here at lesswrong would call a 'belief'. I changed the name to remove any strange foggy baggage that might appear in the heads of potential readers who are not familier with belief vs belief-in-belief and other concepts like that.</p>\n<p>What are your thoughts?</p>\n<blockquote>\n<p><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\">I recently got into a discussion on Facebook that started with an assertion that&nbsp;free-thought/atheism/humanism/etc was no different than the certainties of&nbsp;fundamentalism. But that discussion moved into many topics, one of which is why it should not be controversial to assert that one idea can be more 'right' than another.</span></p>\n<p><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\">I asserted that the view that the universe was created 13.72 billion years ago was more 'right' than the view that the universe was sneezed out of a giant space cow. My interlocutor felt that the giant space cow could be 'right' for one person, even if it is not 'right' for others.</span><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\">It is at this point that we ran into a problem, as it became apparent that her view of the meaning of 'right' and my own were different. As best I can tell, she felt that 'right' meant that it feels right or brings comfort. I, of course, use the word 'right' to mean that an idea contains explanatory and predictive power. The idea that the universe started in a big bang explains a lot about what we see in the cosmos and predicts what we will see as we keep looking - with a high degree of accuracy. That makes is 'right'. And it's more 'right' now than it was two decades ago because we have found places where it is 'wrong' (the increase in the rate of expansion of the universe) and revised the idea to explain it (dark energy), making it more 'right'.</span><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\">So we had two versions of 'right'. It wasn't a given that she would accept my version, so I had to come up with a good reason why my version of 'right' was, well, 'right'.</span><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\">What I came up with was to point out an ethical&nbsp;imperative&nbsp;to be as 'right' as possible - using my definition. Consider this: If there are two ideas, one of which is 'right' enough to predict certain unintended consequences of an action that the other idea fails to predict. If you consciously choose the less 'right' of the two ideas (perhaps because it is 'more right for you'), you have consciously chosen to risk harming others in ways that could have been prevented by&nbsp;choosing&nbsp;the more 'right' of the two options.</span><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\">Perhaps it will be clearer with an example:&nbsp;</span><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px; background-color: white;\">&nbsp;</span><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px; background-color: white;\">Sally is worried about vaccinating herself before travelling to another country. She knows that the doctor says that it is necessary and safer than not being vaccinated. But she's also heard some bad stories about the side-effects of&nbsp;</span><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px; background-color: white;\">vaccinations</span><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px; background-color: white;\">. She decides that not vaccinating is 'right for her'. After all, if she's wrong, what's the harm? She might get sick, but that's a fate she brought on herself. What she fails to realise is that the more 'right' idea (that vaccines are safer than not having them) also predicts that if she fails to vaccinate, she can bring those diseases back to Australia and infect others.</span><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\">But this isn't just a problem on the left wing: Consider the case of&nbsp;</span><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px; background-color: white;\">Josephine, who concedes that there is little evidence for the existence of an afterlife. But she chooses to believe anyway because it is 'right for her'. Why not? If she's wrong, she'll never know it because she'll have ceased to have existed. But here comes those unintended consequences again. This time, they come in the form of predictions that the less 'right' of the two ideas makes - that death is not the end of all existence, but a transition to a greater existence. As it happens, Josephine is an Australian senator and is about to vote on an&nbsp;authorisation&nbsp;for the ADF to bomb some village &nbsp;in&nbsp;Afghanistan. She&nbsp;briefly&nbsp;worries about the fate of any innocent bystanders but is comforted by the fact if the ADF's aim is off, any innocents will go to heaven. But she's so busy that she fails to remember that her assumption about heaven was an&nbsp;arbitrary&nbsp;one for her own comfort and shouldn't be used outside the confines of her own skull.</span><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\">Now consider this case: The CEO of a company sees credible evidence that the government is about to change, leading to a major change in policies directly affecting the company's business environment. She should prob</span><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px; background-color: white;\">ably hedge her bets to prepare for the likely change. But what if she really liked the current government? What if the prospective change to the opposition caused her distress? Might she choose to believe that the government will almost certainly win the next election because that idea feels 'right for her'? I would suggest that the stock holders would feel that her due&nbsp;</span><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px; background-color: white;\">diligence</span><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px; background-color: white;\">&nbsp;required her to hedge the company's bets, whatever her feelings.</span><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><br style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\" /><span style=\"color: #333333; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 15px; line-height: 20px;\">But change this CEO to a mother making a choice on matters of vaccines or faith healing, and now she hasn't made any kind of ethical lapse - she's has just exercised her faith. We owe it ourselves and to those who are affected by our actions (which is everyone really) to try to be as 'right' as possible as often as possible. Never chose an idea because it is 'right for you'. And always be on the lookout for ideas that are even more 'right' than the ones you already cling to.</span></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "skqYPuD9Gzb3aFc6f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 16, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "17416", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-05T09:52:54.317Z", "modifiedAt": null, "url": null, "title": "What have you recently tried, and failed at?", "slug": "what-have-you-recently-tried-and-failed-at", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:37.939Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4jpAkMNhk2tytbNyX/what-have-you-recently-tried-and-failed-at", "pageUrlRelative": "/posts/4jpAkMNhk2tytbNyX/what-have-you-recently-tried-and-failed-at", "linkUrl": "https://www.lesswrong.com/posts/4jpAkMNhk2tytbNyX/what-have-you-recently-tried-and-failed-at", "postedAtFormatted": "Thursday, July 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20have%20you%20recently%20tried%2C%20and%20failed%20at%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20have%20you%20recently%20tried%2C%20and%20failed%20at%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4jpAkMNhk2tytbNyX%2Fwhat-have-you-recently-tried-and-failed-at%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20have%20you%20recently%20tried%2C%20and%20failed%20at%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4jpAkMNhk2tytbNyX%2Fwhat-have-you-recently-tried-and-failed-at", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4jpAkMNhk2tytbNyX%2Fwhat-have-you-recently-tried-and-failed-at", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>Kaj Sotala <a href=\"/lw/cu2/the_power_of_reinforcement/6vj5\">said</a>:</p>\n<blockquote>\n<p>[I]f you punish yourself for trying and failing, you stop wanting to try in the first place, as it becomes associated with the negative emotions. Also, accepting and being okay with the occasional failure makes you treat it as a genuine <em>choice</em> where you have agency, not something that you're forced to do against your will.</p>\n</blockquote>\n<p>So maybe we should celebrate failed attempts more often ... I for one can't think of anything I've failed at recently, which is probably a sign that I'm not trying enough new things.</p>\n<p>So, what specific things have you failed at recently?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1, "7thPfS2WbD2JKizr7": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4jpAkMNhk2tytbNyX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 32, "extendedScore": null, "score": 9.363647800961099e-07, "legacy": true, "legacyId": "17431", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 131, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-05T12:36:41.277Z", "modifiedAt": null, "url": null, "title": "Should you try to do good work on LW?", "slug": "should-you-try-to-do-good-work-on-lw", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:37.796Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nFwHtnfLZ9QWia7Zu/should-you-try-to-do-good-work-on-lw", "pageUrlRelative": "/posts/nFwHtnfLZ9QWia7Zu/should-you-try-to-do-good-work-on-lw", "linkUrl": "https://www.lesswrong.com/posts/nFwHtnfLZ9QWia7Zu/should-you-try-to-do-good-work-on-lw", "postedAtFormatted": "Thursday, July 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20you%20try%20to%20do%20good%20work%20on%20LW%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20you%20try%20to%20do%20good%20work%20on%20LW%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnFwHtnfLZ9QWia7Zu%2Fshould-you-try-to-do-good-work-on-lw%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20you%20try%20to%20do%20good%20work%20on%20LW%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnFwHtnfLZ9QWia7Zu%2Fshould-you-try-to-do-good-work-on-lw", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnFwHtnfLZ9QWia7Zu%2Fshould-you-try-to-do-good-work-on-lw", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 300, "htmlBody": "<p>I used to advocate trying to do good work on LW. Now I'm not sure, let me explain why.</p>\n<p>It's certainly true that good work stays valuable no matter where you're doing it. Unfortunately, the standards of \"good work\" are largely defined by where you're doing it. If you're in academia, your work is good or bad by scientific standards. If you're on LW, your work is good or bad compared to other LW posts. Internalizing that standard may harm you if you're capable of more.</p>\n<p>When you come to a place like <a href=\"http://projecteuler.net/\">Project Euler</a> and solve some problems, or come to <a href=\"http://www.openstreetmap.org/\">OpenStreetMap</a> and upload some GPS tracks, or come to academia and publish a paper, that makes you a participant and you know exactly where you stand, relative to others. But LW is not a task-focused community and is unlikely to ever become one. LW evolved from the basic activity \"let's comment on something Eliezer wrote\". We inherited our standard of quality from that.&nbsp;As a result, when someone posts their work here, that doesn't necessarily help them improve.</p>\n<p>For example, Yvain is a great contributor to LW and has the potential to be a star writer, but it seems to me that writing on LW doesn't test his limits, compared to trying new audiences. Likewise, my own work on decision theory math would've been held to a higher standard if the primary audience were mathematicians (though I hope to remedy that). Of course there have been many examples of seemingly good work posted to LW. Homestuck fandom also has a lot of nice-looking art, but it doesn't get fandoms of its own.</p>\n<p>In conclusion, if you want to do important work, cross-post it if you must, but don't do it for LW exclusively. Big fish in a small pond always looks kinda sad.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nFwHtnfLZ9QWia7Zu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 49, "extendedScore": null, "score": 0.000550405687542528, "legacy": true, "legacyId": "17432", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-05T16:11:16.545Z", "modifiedAt": null, "url": null, "title": "Meetup : First New Jersey Meetup", "slug": "meetup-first-new-jersey-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.828Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "C3Sc9XJXd5AsAyksf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iPnKA4eR9YTRSRFZr/meetup-first-new-jersey-meetup", "pageUrlRelative": "/posts/iPnKA4eR9YTRSRFZr/meetup-first-new-jersey-meetup", "linkUrl": "https://www.lesswrong.com/posts/iPnKA4eR9YTRSRFZr/meetup-first-new-jersey-meetup", "postedAtFormatted": "Thursday, July 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20New%20Jersey%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20New%20Jersey%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiPnKA4eR9YTRSRFZr%2Fmeetup-first-new-jersey-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20New%20Jersey%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiPnKA4eR9YTRSRFZr%2Fmeetup-first-new-jersey-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiPnKA4eR9YTRSRFZr%2Fmeetup-first-new-jersey-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/bn\">First New Jersey Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">12 July 2012 01:30:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Raritan Valley Community College</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>It's time for the first New Jersey meetup!</p>\n<p>My name is Chris. I'm a college student and I've been involved with LessWrong for 1-2 years. I'd like to meet fellow LWers, as I want to know more people with similar values and interests.</p>\n<p>My plan for the afternoon is minimalistic. Just some good \"get to know you\" conversation, possibly over coffee and a board game. If we all have a good time, we can also dedicate some time toward the end for making future plans.</p>\n<p>If this interests you, I encourage you to come by Raritan Valley Community College from 1:30 - 3:30 pm on Thursday, July 12. I chose RVCC because it is a public place and easily accessible from major highways.</p>\n<p>Depending on how many people are coming, the meetup may be held at the campus coffee shop or in a classroom close by. Either way, I will have a sign posted on the table or at the door. I'll likely be working on my laptop.</p>\n<p>If you're interested in attending this event or a future NJ meetup, leave a comment or PM me. Be sure to include your email address so I can contact you with any updates.</p>\n<p>Please let me know if you have any questions or concerns. I look forward to meeting you!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/bn\">First New Jersey Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iPnKA4eR9YTRSRFZr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "17433", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_New_Jersey_Meetup\">Discussion article for the meetup : <a href=\"/meetups/bn\">First New Jersey Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">12 July 2012 01:30:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Raritan Valley Community College</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>It's time for the first New Jersey meetup!</p>\n<p>My name is Chris. I'm a college student and I've been involved with LessWrong for 1-2 years. I'd like to meet fellow LWers, as I want to know more people with similar values and interests.</p>\n<p>My plan for the afternoon is minimalistic. Just some good \"get to know you\" conversation, possibly over coffee and a board game. If we all have a good time, we can also dedicate some time toward the end for making future plans.</p>\n<p>If this interests you, I encourage you to come by Raritan Valley Community College from 1:30 - 3:30 pm on Thursday, July 12. I chose RVCC because it is a public place and easily accessible from major highways.</p>\n<p>Depending on how many people are coming, the meetup may be held at the campus coffee shop or in a classroom close by. Either way, I will have a sign posted on the table or at the door. I'll likely be working on my laptop.</p>\n<p>If you're interested in attending this event or a future NJ meetup, leave a comment or PM me. Be sure to include your email address so I can contact you with any updates.</p>\n<p>Please let me know if you have any questions or concerns. I look forward to meeting you!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___First_New_Jersey_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/bn\">First New Jersey Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : First New Jersey Meetup", "anchor": "Discussion_article_for_the_meetup___First_New_Jersey_Meetup", "level": 1}, {"title": "Discussion article for the meetup : First New Jersey Meetup", "anchor": "Discussion_article_for_the_meetup___First_New_Jersey_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-05T17:04:21.187Z", "modifiedAt": null, "url": null, "title": "Less Wrong views on morality?", "slug": "less-wrong-views-on-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:53.225Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qH7GxzgfFhywNBPKS/less-wrong-views-on-morality", "pageUrlRelative": "/posts/qH7GxzgfFhywNBPKS/less-wrong-views-on-morality", "linkUrl": "https://www.lesswrong.com/posts/qH7GxzgfFhywNBPKS/less-wrong-views-on-morality", "postedAtFormatted": "Thursday, July 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20views%20on%20morality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20views%20on%20morality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqH7GxzgfFhywNBPKS%2Fless-wrong-views-on-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20views%20on%20morality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqH7GxzgfFhywNBPKS%2Fless-wrong-views-on-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqH7GxzgfFhywNBPKS%2Fless-wrong-views-on-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Do you believe in an objective morality capable of being scientifically investigated (a la Sam Harris *or others*), or are you a moral nihilist/relativist? There seems to be some division on this point. I would have thought Less Wrong to be well in the former camp.</p>\n<p>&nbsp;</p>\n<p><strong>Edit</strong>: There seems to be some confusion - when I say \"an objective morality capable of being scientifically investigated (a la Sam Harris *or others*)\" -&nbsp;I do NOT mean something like a \"one true, universal, metaphysical morality for all mind-designs\" like the Socratic/Platonic Form of Good or any such nonsense. I just mean something in reality that's mind-independent - in the sense that it is hard-wired, e.g. <a href=\"/lw/l3/thou_art_godshatter/\">by evolution</a>, and thus independent/prior to any later knowledge or cognitive content - and thus can be investigated scientifically. It is a definite \"is\" from which we can make true \"ought\" statements relative to that \"is\". See <a href=\"/r/discussion/lw/dga/less_wrong_views_on_morality/6z3s\">drethelin's comment</a>&nbsp;and my <a href=\"/r/discussion/lw/dga/less_wrong_views_on_morality/6z02\">analysis of Clippy</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qH7GxzgfFhywNBPKS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -1, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "17434", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 146, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cSXZpvqpa9vbGGLtG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-05T19:12:57.774Z", "modifiedAt": null, "url": null, "title": "On the Care and Feeding of Young Rationalists -- Revisited[Draft] [Request for Feedback]", "slug": "on-the-care-and-feeding-of-young-rationalists-revisited", "viewCount": null, "lastCommentedAt": "2012-10-31T12:51:26.529Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5QLBA96ZQfR3tHrFq/on-the-care-and-feeding-of-young-rationalists-revisited", "pageUrlRelative": "/posts/5QLBA96ZQfR3tHrFq/on-the-care-and-feeding-of-young-rationalists-revisited", "linkUrl": "https://www.lesswrong.com/posts/5QLBA96ZQfR3tHrFq/on-the-care-and-feeding-of-young-rationalists-revisited", "postedAtFormatted": "Thursday, July 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20the%20Care%20and%20Feeding%20of%20Young%20Rationalists%20--%20Revisited%5BDraft%5D%20%5BRequest%20for%20Feedback%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20the%20Care%20and%20Feeding%20of%20Young%20Rationalists%20--%20Revisited%5BDraft%5D%20%5BRequest%20for%20Feedback%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5QLBA96ZQfR3tHrFq%2Fon-the-care-and-feeding-of-young-rationalists-revisited%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20the%20Care%20and%20Feeding%20of%20Young%20Rationalists%20--%20Revisited%5BDraft%5D%20%5BRequest%20for%20Feedback%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5QLBA96ZQfR3tHrFq%2Fon-the-care-and-feeding-of-young-rationalists-revisited", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5QLBA96ZQfR3tHrFq%2Fon-the-care-and-feeding-of-young-rationalists-revisited", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 761, "htmlBody": "<p><strong>Planned top-level post -- any feedback very much welcome.</strong></p>\n<p>&nbsp;</p>\n<p>Obviously a followup to: <a href=\"/lw/25/on_the_care_and_feeding_of_young_rationalists/\">On the Care and Feeding of Young Rationalists</a><br /><br />My very first top-level post on LW was a solicitation for advice/feedback/discussion on the topic of rationalist parenting. I'd like to revisit the topic now.</p>\n<p>&nbsp;</p>\n<h2 dir=\"ltr\">Goals</h2>\n<p>First of all, let's talk about goals. I can think of four.</p>\n<ol>\n<li>Produce thriving, intelligent, rational, happy, good-hearted children who become thriving, intelligent, rational, happy, good-hearted adults. </li>\n<li>Have your children enjoy their childhoods</li>\n<li>Enjoy raising your children.</li>\n<li>Closely tied to 2 and 3 -- actually have a good relationship with your children. Like them and have them like you.<a id=\"more\"></a></li>\n</ol>\n<p>&nbsp;</p>\n<h2 dir=\"ltr\">What We Know</h2>\n<p>To speak to goal 1 first, <a href=\"http://www.amazon.com/Selfish-Reasons-Have-More-ebook/dp/B004OA64Q6/ref=kinw_dp_ke?ie=UTF8&amp;m=AG56TWVU5XWC2\">Bryan Caplan claims</a> flat outcomes for goal #1 under commonly tried parenting interventions, which seems counter-intuitive. More explanation of what exactly the studies in question proved would be welcome. &nbsp;<br /><a href=\"/lw/cu2/the_power_of_reinforcement/\">As Luke helpfully taught us</a>, negative reinforcement doesn't seem to work as well as positive. Spanking, in particular, is right out. This is in large part because reinforcement reinforces <em>everything</em> about what the subject's doing at the time it occurs. This means, in particular, that you're reinforcing both the target behavior and <em>being caught at it</em>. In the case of positive behavior/reinforcement, there's nothing particularly problematic about this, but for the negative case, you're also punishing being caught/noticed/seen, which can be problematic. &nbsp;</p>\n<p><br />Nutrition in early childhood <em>does</em> seem to influence life outcomes, mostly on the low end: <a href=\"http://ajp.psychiatryonline.org/article.aspx?Volume=161&amp;page=2005&amp;journalID=13\">serious malnutrition depresses IQ</a> -- try to avoid it.</p>\n<p><br />Praise seems to be important, first of all because it is often a powerful positive reinforcer in children. <a href=\"http://www.stanford.com/dept/psychology/cgi-bin/drupalm/system/files/Intelligence%2520Praise%2520Can%2520Undermine%2520Motivation%2520and%2520Performance.pdf\">Research has shown</a> that the target of the praise is important. Praising a child for having worked hard to understand a concept seems to lead to more future efforts of the same kind than praising their intelligence.</p>\n<p><br />Simply talking to children, well before they've developed language skills, seems to be important for goals 1 and 4. I plan to <a href=\"http://c2.com/cgi/wiki?RubberDucking\">use mine as rubber ducks</a>=).</p>\n<p>&nbsp;</p>\n<p>\n<h2 dir=\"ltr\">Defaults to Notice and Perhaps Reject</h2>\nThere are some idiosyncrasies of the default in modern American childrearing that don't seem to do anyone any favors.<br />\n<h3 dir=\"ltr\">Segregation by age:</h3>\nOutside of their siblings, American six-year-olds are socialized almost entirely with other six-year-olds. Historically, it has been possible for six-year-olds to be friends with 15-year olds -- to, I suspect, the benefit of both. The older children provide near(er)-term models for the younger children of their future growth, and the older children are thus <em>encouraged to be role-models</em>. &nbsp;<br />Extending this a bit further, there seems to be a taboo against children having adult friends who aren't their close relatives, or close friends of their parents. This taboo seems to be self-reinforcing -- any adult who indicates an interest in befriending a child <em>knowing that the taboo exists</em> signals that they may have suspicious intentions. I suspect that this is yet another way of infantilizing children. The solution is probably just to have lots of local close friends who can be friends to your children.<br />\n<h3 dir=\"ltr\">General Over-protectiveness:</h3>\nWe've all heard of the rise of \"helicopter parenting\". <a href=\"http://www.cbsnews.com/8301-505125_162-57455011/why-my-child-will-be-your-childs-boss/\">This article sums things up nicely</a>.<br />\n<h2 dir=\"ltr\"><br /></h2>\n<h2 dir=\"ltr\">What I think I know:</h2>\n</p>\n<p>In reference to Goals 2 and 3, most of the struggles I see seem to be about autonomy. Alicorn and I have both spontaneously remarked on how much we <em>love being adults</em>, and this is mostly a function of the extent to which we get to choose our activities from moment to moment, choose where we want to live, choose what/when we want to eat, go where we want to go, stay as long as we want to stay, etc. etc. Similarly, from watching childhood friends with their children, they seem to get in trouble -- the interaction seems to become massively Less Fun -- when they decide that, e.g. the child really must eat food X at time Y, when, as a spectator, I really can't understand why the child can't be left alone to find some food when ey want some.<br />This suggests increasing autonomy as an important subgoal. In particular, it points to criteria for \"optimizing the build\" of your child -- what skills do ey need before ey can stay home a few hours without you? Before ey can walk around the corner/bike across town to run an errand/meet a friend/etc.? Before you can safely encourage em to get a driver's license/car? Grind those skill points!</p>\n<p>&nbsp;</p>\n<p>That's all I've got -- what else do we, as a community, know?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5QLBA96ZQfR3tHrFq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 30, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "17230", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Planned_top_level_post____any_feedback_very_much_welcome_\">Planned top-level post -- any feedback very much welcome.</strong></p>\n<p>&nbsp;</p>\n<p>Obviously a followup to: <a href=\"/lw/25/on_the_care_and_feeding_of_young_rationalists/\">On the Care and Feeding of Young Rationalists</a><br><br>My very first top-level post on LW was a solicitation for advice/feedback/discussion on the topic of rationalist parenting. I'd like to revisit the topic now.</p>\n<p>&nbsp;</p>\n<h2 dir=\"ltr\" id=\"Goals\">Goals</h2>\n<p>First of all, let's talk about goals. I can think of four.</p>\n<ol>\n<li>Produce thriving, intelligent, rational, happy, good-hearted children who become thriving, intelligent, rational, happy, good-hearted adults. </li>\n<li>Have your children enjoy their childhoods</li>\n<li>Enjoy raising your children.</li>\n<li>Closely tied to 2 and 3 -- actually have a good relationship with your children. Like them and have them like you.<a id=\"more\"></a></li>\n</ol>\n<p>&nbsp;</p>\n<h2 dir=\"ltr\" id=\"What_We_Know\">What We Know</h2>\n<p>To speak to goal 1 first, <a href=\"http://www.amazon.com/Selfish-Reasons-Have-More-ebook/dp/B004OA64Q6/ref=kinw_dp_ke?ie=UTF8&amp;m=AG56TWVU5XWC2\">Bryan Caplan claims</a> flat outcomes for goal #1 under commonly tried parenting interventions, which seems counter-intuitive. More explanation of what exactly the studies in question proved would be welcome. &nbsp;<br><a href=\"/lw/cu2/the_power_of_reinforcement/\">As Luke helpfully taught us</a>, negative reinforcement doesn't seem to work as well as positive. Spanking, in particular, is right out. This is in large part because reinforcement reinforces <em>everything</em> about what the subject's doing at the time it occurs. This means, in particular, that you're reinforcing both the target behavior and <em>being caught at it</em>. In the case of positive behavior/reinforcement, there's nothing particularly problematic about this, but for the negative case, you're also punishing being caught/noticed/seen, which can be problematic. &nbsp;</p>\n<p><br>Nutrition in early childhood <em>does</em> seem to influence life outcomes, mostly on the low end: <a href=\"http://ajp.psychiatryonline.org/article.aspx?Volume=161&amp;page=2005&amp;journalID=13\">serious malnutrition depresses IQ</a> -- try to avoid it.</p>\n<p><br>Praise seems to be important, first of all because it is often a powerful positive reinforcer in children. <a href=\"http://www.stanford.com/dept/psychology/cgi-bin/drupalm/system/files/Intelligence%2520Praise%2520Can%2520Undermine%2520Motivation%2520and%2520Performance.pdf\">Research has shown</a> that the target of the praise is important. Praising a child for having worked hard to understand a concept seems to lead to more future efforts of the same kind than praising their intelligence.</p>\n<p><br>Simply talking to children, well before they've developed language skills, seems to be important for goals 1 and 4. I plan to <a href=\"http://c2.com/cgi/wiki?RubberDucking\">use mine as rubber ducks</a>=).</p>\n<p>&nbsp;</p>\n<p>\n</p><h2 dir=\"ltr\" id=\"Defaults_to_Notice_and_Perhaps_Reject\">Defaults to Notice and Perhaps Reject</h2>\nThere are some idiosyncrasies of the default in modern American childrearing that don't seem to do anyone any favors.<br>\n<h3 dir=\"ltr\" id=\"Segregation_by_age_\">Segregation by age:</h3>\nOutside of their siblings, American six-year-olds are socialized almost entirely with other six-year-olds. Historically, it has been possible for six-year-olds to be friends with 15-year olds -- to, I suspect, the benefit of both. The older children provide near(er)-term models for the younger children of their future growth, and the older children are thus <em>encouraged to be role-models</em>. &nbsp;<br>Extending this a bit further, there seems to be a taboo against children having adult friends who aren't their close relatives, or close friends of their parents. This taboo seems to be self-reinforcing -- any adult who indicates an interest in befriending a child <em>knowing that the taboo exists</em> signals that they may have suspicious intentions. I suspect that this is yet another way of infantilizing children. The solution is probably just to have lots of local close friends who can be friends to your children.<br>\n<h3 dir=\"ltr\" id=\"General_Over_protectiveness_\">General Over-protectiveness:</h3>\nWe've all heard of the rise of \"helicopter parenting\". <a href=\"http://www.cbsnews.com/8301-505125_162-57455011/why-my-child-will-be-your-childs-boss/\">This article sums things up nicely</a>.<br>\n<h2 dir=\"ltr\"><br></h2>\n<h2 dir=\"ltr\" id=\"What_I_think_I_know_\">What I think I know:</h2>\n<p></p>\n<p>In reference to Goals 2 and 3, most of the struggles I see seem to be about autonomy. Alicorn and I have both spontaneously remarked on how much we <em>love being adults</em>, and this is mostly a function of the extent to which we get to choose our activities from moment to moment, choose where we want to live, choose what/when we want to eat, go where we want to go, stay as long as we want to stay, etc. etc. Similarly, from watching childhood friends with their children, they seem to get in trouble -- the interaction seems to become massively Less Fun -- when they decide that, e.g. the child really must eat food X at time Y, when, as a spectator, I really can't understand why the child can't be left alone to find some food when ey want some.<br>This suggests increasing autonomy as an important subgoal. In particular, it points to criteria for \"optimizing the build\" of your child -- what skills do ey need before ey can stay home a few hours without you? Before ey can walk around the corner/bike across town to run an errand/meet a friend/etc.? Before you can safely encourage em to get a driver's license/car? Grind those skill points!</p>\n<p>&nbsp;</p>\n<p>That's all I've got -- what else do we, as a community, know?</p>\n<p>&nbsp;</p>", "sections": [{"title": "Planned top-level post -- any feedback very much welcome.", "anchor": "Planned_top_level_post____any_feedback_very_much_welcome_", "level": 3}, {"title": "Goals", "anchor": "Goals", "level": 1}, {"title": "What We Know", "anchor": "What_We_Know", "level": 1}, {"title": "Defaults to Notice and Perhaps Reject", "anchor": "Defaults_to_Notice_and_Perhaps_Reject", "level": 1}, {"title": "Segregation by age:", "anchor": "Segregation_by_age_", "level": 2}, {"title": "General Over-protectiveness:", "anchor": "General_Over_protectiveness_", "level": 2}, {"title": "What I think I know:", "anchor": "What_I_think_I_know_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "89 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SWraogEDJ6gocpvwa", "GGn8MBiY8Xz6NdNdH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-05T22:29:25.980Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Could Anything Be Right?", "slug": "seq-rerun-could-anything-be-right", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:33.839Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DE2xNLjRfRjR4WDpZ/seq-rerun-could-anything-be-right", "pageUrlRelative": "/posts/DE2xNLjRfRjR4WDpZ/seq-rerun-could-anything-be-right", "linkUrl": "https://www.lesswrong.com/posts/DE2xNLjRfRjR4WDpZ/seq-rerun-could-anything-be-right", "postedAtFormatted": "Thursday, July 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Could%20Anything%20Be%20Right%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Could%20Anything%20Be%20Right%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDE2xNLjRfRjR4WDpZ%2Fseq-rerun-could-anything-be-right%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Could%20Anything%20Be%20Right%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDE2xNLjRfRjR4WDpZ%2Fseq-rerun-could-anything-be-right", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDE2xNLjRfRjR4WDpZ%2Fseq-rerun-could-anything-be-right", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>Today's post, <a href=\"/lw/sb/could_anything_be_right/\">Could Anything Be Right?</a> was originally published on 18 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You do know quite a bit about morality. It's not perfect information, surely, or absolutely reliable, but you have someplace to start. If you didn't, you'd have a much harder time thinking about morality than you do.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dfn/seq_rerun_the_gift_we_give_to_tomorrow/\">The Gift We Give To Tomorrow</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DE2xNLjRfRjR4WDpZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.367186609883091e-07, "legacy": true, "legacyId": "17435", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vy9nnPdwTjSmt5qdb", "Gy7CHgBzjrEtaxCZW", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-06T01:34:25.968Z", "modifiedAt": null, "url": null, "title": "Negative and Positive Selection", "slug": "negative-and-positive-selection", "viewCount": null, "lastCommentedAt": "2021-10-31T19:05:50.362Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LsXtcLyzyfGg3gT5R/negative-and-positive-selection", "pageUrlRelative": "/posts/LsXtcLyzyfGg3gT5R/negative-and-positive-selection", "linkUrl": "https://www.lesswrong.com/posts/LsXtcLyzyfGg3gT5R/negative-and-positive-selection", "postedAtFormatted": "Friday, July 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Negative%20and%20Positive%20Selection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANegative%20and%20Positive%20Selection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLsXtcLyzyfGg3gT5R%2Fnegative-and-positive-selection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Negative%20and%20Positive%20Selection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLsXtcLyzyfGg3gT5R%2Fnegative-and-positive-selection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLsXtcLyzyfGg3gT5R%2Fnegative-and-positive-selection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 900, "htmlBody": "<p><em>(Originally <a href=\"http://rationalconspiracy.com/2012/06/19/negative-and-positive-selection/\">posted</a> to my blog, <a href=\"http://www.rationalconspiracy.com\">The Rationalist Conspiracy</a>; cross-posted here on request of Lukeprog.)</em></p>\n<p>You&rsquo;re the captain of a team, and you want to select really good players. How do you do it?</p>\n<p>One way is through what I call <em>positive selection</em>. You devise a test &ndash; say, who can run the fastest &ndash; and pick the people who do best. If you want to be really strict, like if you&rsquo;re selecting for the Olympics, you only pick the top fraction of a percent. If you&rsquo;re a player, and you want to get selected, you have to train to do better on the test.</p>\n<p>The opposite method is <em>negative selection. </em>Instead of one test to pick out winners, you design many tests to pick out losers. You test, say, who can&rsquo;t run very well when it&rsquo;s hot out, and get rid of them. Then you test who can&rsquo;t run very well when it&rsquo;s cold out, and get rid of them. Then you test running in the rain, and get rid of the losers there. And so on and so forth. When you&rsquo;re strict with negative selection, you have lots and lots of tests, so that it&rsquo;s very hard for any one person to pass through all the filters.</p>\n<p>I think a big part of where American society&rsquo;s gone wrong over the last hundred years is the ubiquitous use of <em>negative selection </em>over <em>positiv</em>e<em> selection</em>. (Athletics is one of the only exceptions. It&rsquo;s apparently so important that people really care about performance &ndash; as opposed to, say, in medicine, where we exclude brilliant doctors if they don&rsquo;t have the stamina to work <a href=\"http://en.wikipedia.org/wiki/Medical_resident_work_hours\">ninety hours a week</a>.) A single test can always be flawed; for example, IQ tests and SATs have many flaws. However, with negative selection, how badly you do is determined by the failure rate of every test <em>combined. </em>If you have twenty tests, and even one of them is so flawed it excludes good players, then your team will suck.</p>\n<p>Elite <a href=\"http://www.paulgraham.com/colleges.html\">college admissions</a> is an example of a negative selection test. There&rsquo;s no one way you can do really, really well, and thereby be admitted to Harvard. Instead, you have to pass a bunch of different selection filters: Are your SATs good enough? Are your grades good enough? Is your essay good enough? Are your extracurriculars good enough? Are your recommendations good enough? Failure on any one step usually means not getting admitted. And as competition has intensified, colleges have added more and more filters, like the supplemental applications top schools now require (in addition to the Common Application). It wasn&rsquo;t always this way &ndash; Harvard used to admit primarily based on an entrance exam &ndash; until they discovered this let too many Jews in (no, <a href=\"http://www.gladwell.com/2005/2005_10_10_a_admissions.html\">seriously</a>). More recently, the negative selection has been intensified by eliminating the SAT&rsquo;s high ceiling.</p>\n<p>Academia is another example of negative selection. To get tenure, first you have to get into a top PhD program. Then you have to graduate. Then you have to get a good recommendation from your advisor. Then you have to get a good postdoc. Then you have to get another good postdoc. Then you have to get a good assistant professorship. Then you have to get approved by the tenure committee. For the most part, if even one of those steps goes wrong &ndash; if you went to a second-tier PhD program, say &ndash; there&rsquo;s no way to recover. Once you&rsquo;re off the &ldquo;track&rdquo;, you&rsquo;re off, and there&rsquo;s no getting back on. It&rsquo;s fail once, fail forever.</p>\n<p>Grades are another example &ndash; A is a good grade, but there&rsquo;s no <em>excellent</em> grade. There&rsquo;s no grade that you only get if you&rsquo;re in the top 0.1%. Hence, getting a really good GPA doesn&rsquo;t mean excelling, so much as it means never failing. If you&rsquo;re in high school and are taking six classes, if you fail one, your GPA is now 3.3 or less, regardless of how good you are otherwise.</p>\n<p>In any field, at the top end, you tend to get a lot of variance. (Insert tales of the mad artist and mad mathematician.) Negative selection suppresses variance, by eliminating many of the dimensions on which people vary. Students at Yale are, for the most part, all strikingly similar &ndash; same socioeconomic class, same interests, same pursuits, same life goals, even the same style of dress. A lot of people tend to assume performance follows a bell curve, but in some cases, it&rsquo;s more like a Pareto distribution: the <a href=\"http://paulgraham.com/wealth.html\">top people</a> do hundreds or thousands of times better than average. Hence, if you eliminate the small fraction of people at the very top, your performance is <a href=\"http://www.paulgraham.com/yahoo.html\">hosed</a>. Fortunately for VC funds, the startup world is still <a href=\"http://grasshopper.com/blog/2011/11/the-early-failures-of-famous-entrepreneurs-and-what-they-learned-3/\">positive selection</a>.</p>\n<p>Less obviously, a world with lots of negative selection might be a nasty one to live in. If you think of yourself as trying to <em>eliminate bad</em>, rather than <em>encourage good</em>, you start operating on the <a href=\"http://www.nationalreview.com/corner/149137/wired-morality/jonah-goldberg\">purity vs. contamination</a> moral axis. Any tiny amount of bad, anywhere, must be gotten rid of, and that can lead to all sorts of <a href=\"/lw/lz/guardians_of_the_truth/\">nastiness</a>. &ldquo;When you are a Guardian of the Truth, all you can do is try to stave off the inevitable slide into entropy by zapping anything that departs from the Truth.&nbsp; If there&rsquo;s some way to pump against entropy, generate new true beliefs along with a little waste heat, that same pump can keep the truth alive without secret police.&rdquo;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MAp6Ft8b3s7kJdrQ9": 4, "zv7v2ziqexSn5iS9v": 2, "xexCWMyds6QLWognu": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LsXtcLyzyfGg3gT5R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 97, "baseScore": 134, "extendedScore": null, "score": 0.000281, "legacy": true, "legacyId": "17410", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 134, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 263, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["etBrzxdfNop3DqJvA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-06T03:59:00.000Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison: Probability Calibration", "slug": "meetup-madison-probability-calibration", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:34.169Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dmBpRCGf28NXADa8W/meetup-madison-probability-calibration", "pageUrlRelative": "/posts/dmBpRCGf28NXADa8W/meetup-madison-probability-calibration", "linkUrl": "https://www.lesswrong.com/posts/dmBpRCGf28NXADa8W/meetup-madison-probability-calibration", "postedAtFormatted": "Friday, July 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison%3A%20Probability%20Calibration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%3A%20Probability%20Calibration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdmBpRCGf28NXADa8W%2Fmeetup-madison-probability-calibration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%3A%20Probability%20Calibration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdmBpRCGf28NXADa8W%2Fmeetup-madison-probability-calibration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdmBpRCGf28NXADa8W%2Fmeetup-madison-probability-calibration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bo'>Madison: Probability Calibration</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 July 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Michelangelo's: 114 State St., Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'll run a probability calibration exercise. How confident are you? How confident should you be?</p>\n\n<p>I'll bring my Zendo pieces too, if folks would like to play, or maybe just build stuff with pyramids.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bo'>Madison: Probability Calibration</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dmBpRCGf28NXADa8W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.368728963672114e-07, "legacy": true, "legacyId": "17442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Madison__Probability_Calibration\">Discussion article for the meetup : <a href=\"/meetups/bo\">Madison: Probability Calibration</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 July 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Michelangelo's: 114 State St., Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'll run a probability calibration exercise. How confident are you? How confident should you be?</p>\n\n<p>I'll bring my Zendo pieces too, if folks would like to play, or maybe just build stuff with pyramids.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Madison__Probability_Calibration1\">Discussion article for the meetup : <a href=\"/meetups/bo\">Madison: Probability Calibration</a></h2>", "sections": [{"title": "Discussion article for the meetup : Madison: Probability Calibration", "anchor": "Discussion_article_for_the_meetup___Madison__Probability_Calibration", "level": 1}, {"title": "Discussion article for the meetup : Madison: Probability Calibration", "anchor": "Discussion_article_for_the_meetup___Madison__Probability_Calibration1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-06T08:02:11.372Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meta-meetup", "slug": "meetup-berkeley-meta-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hnmud3C48DJBXW3QH/meetup-berkeley-meta-meetup", "pageUrlRelative": "/posts/hnmud3C48DJBXW3QH/meetup-berkeley-meta-meetup", "linkUrl": "https://www.lesswrong.com/posts/hnmud3C48DJBXW3QH/meetup-berkeley-meta-meetup", "postedAtFormatted": "Friday, July 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meta-meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meta-meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhnmud3C48DJBXW3QH%2Fmeetup-berkeley-meta-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meta-meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhnmud3C48DJBXW3QH%2Fmeetup-berkeley-meta-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhnmud3C48DJBXW3QH%2Fmeetup-berkeley-meta-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 262, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/bp\">Berkeley meta-meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">11 July 2012 07:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Berkeley, CA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The next Berkeley meetup will be a Mountain View-style meta-meetup. The idea is that there are lots of rewarding things we can do if we know that we want to do them and if we have a plan for them. So on Wednesday, July 11, we'll gather at Zendo and discuss what topics and activities we want for future meetups, and plan out the next several weeks of meetups.</p>\n<p>For directions to Zendo, see the <a href=\"http://groups.google.com/group/bayarealesswrong\">mailing list</a>, or call me at <img style=\"vertical-align: middle;\" src=\"http://i.imgur.com/Vcafy.png\" alt=\"\" width=\"83\" height=\"18\" />.</p>\n<p><strong>EDIT:</strong>&nbsp;Michael Smith of the Center for Applied Rationality has volunteered to run interested parties through an experimental rationality training unit at the meetup. Here's his description:</p>\n<blockquote>\n<p>The topic is <em>fudoshin</em> &mdash; literally \"imperturbable mind.\" It's related to Julia's combat reflexes material, but it's more directly targeted at the feeling of againstness that can arise when feeling defensive or when wanting to push for a particular result. I've worked out some mental hacks that, when paired with some physiological tricks, seem to work really well for practicing this state of mind. I originally developed these tools as a method of teaching people to be effective at mentally challenging parts of aikido but I've found it gives me a very strong impression of being more open to listening to criticism and taking ideas seriously. It's also just a pleasant state to be in. :-)</p>\n</blockquote>\n<p>The agenda for Wednesday is as follows:</p>\n<p>7:00pm: Doors open.<br />7:15pm: Meta-meetup: Plan the next several meetups.<br />8:15pm: Michael's fudoshin session. (This should take 1-2 hours.)</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hnmud3C48DJBXW3QH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.369867363654503e-07, "legacy": true, "legacyId": "17452", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-06T14:39:16.209Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Cali, Columbia; Melbourne; Pittsburgh; St. Petersburg", "slug": "weekly-lw-meetups-cali-columbia-melbourne-pittsburgh-st", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nJbwJbanpCZYDo4dw/weekly-lw-meetups-cali-columbia-melbourne-pittsburgh-st", "pageUrlRelative": "/posts/nJbwJbanpCZYDo4dw/weekly-lw-meetups-cali-columbia-melbourne-pittsburgh-st", "linkUrl": "https://www.lesswrong.com/posts/nJbwJbanpCZYDo4dw/weekly-lw-meetups-cali-columbia-melbourne-pittsburgh-st", "postedAtFormatted": "Friday, July 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Cali%2C%20Columbia%3B%20Melbourne%3B%20Pittsburgh%3B%20St.%20Petersburg&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Cali%2C%20Columbia%3B%20Melbourne%3B%20Pittsburgh%3B%20St.%20Petersburg%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJbwJbanpCZYDo4dw%2Fweekly-lw-meetups-cali-columbia-melbourne-pittsburgh-st%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Cali%2C%20Columbia%3B%20Melbourne%3B%20Pittsburgh%3B%20St.%20Petersburg%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJbwJbanpCZYDo4dw%2Fweekly-lw-meetups-cali-columbia-melbourne-pittsburgh-st", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJbwJbanpCZYDo4dw%2Fweekly-lw-meetups-cali-columbia-melbourne-pittsburgh-st", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 425, "htmlBody": "<p><strong>Note: This weekly summary was posted to LW Main on the 29th of June. It has been moved to discussion, and the most recent summary post is <a href=\"/lw/dgt/weekly_lw_meetups_berkeley_hyderabad_madison_new/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/bd\">Pittsburgh: Belief in Belief:&nbsp;<span class=\"date\">30 June 2012 11:30AM</span></a></li>\n<li><a href=\"/meetups/aq\">First Cali, Colombia meetup:&nbsp;<span class=\"date\">02 July 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/bg\">St. Peterburg meetup:&nbsp;<span class=\"date\">05 July 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/b9\">Hyderabad Meetup:&nbsp;<span class=\"date\">08 July 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/bf\">Brussels meetup:&nbsp;<span class=\"date\">14 July 2012 12:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bb\">Melbourne, practical rationality:&nbsp;<span class=\"date\">06 July 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/bh\">(NYC) A Game of Nomic:&nbsp;<span class=\"date\">07 July 2012 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nJbwJbanpCZYDo4dw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.37172667586876e-07, "legacy": true, "legacyId": "17317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ANyffFJ6CtjnpgupG", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-06T18:01:51.400Z", "modifiedAt": null, "url": null, "title": "Transparency in Insurance (Edit: Solution found)", "slug": "transparency-in-insurance-edit-solution-found", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "RhSHWLPgsKFfNQX6e", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KsgPjKhpARhATzSDr/transparency-in-insurance-edit-solution-found", "pageUrlRelative": "/posts/KsgPjKhpARhATzSDr/transparency-in-insurance-edit-solution-found", "linkUrl": "https://www.lesswrong.com/posts/KsgPjKhpARhATzSDr/transparency-in-insurance-edit-solution-found", "postedAtFormatted": "Friday, July 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Transparency%20in%20Insurance%20(Edit%3A%20Solution%20found)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATransparency%20in%20Insurance%20(Edit%3A%20Solution%20found)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKsgPjKhpARhATzSDr%2Ftransparency-in-insurance-edit-solution-found%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Transparency%20in%20Insurance%20(Edit%3A%20Solution%20found)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKsgPjKhpARhATzSDr%2Ftransparency-in-insurance-edit-solution-found", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKsgPjKhpARhATzSDr%2Ftransparency-in-insurance-edit-solution-found", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 332, "htmlBody": "<p>&nbsp;</p>\n<h3><strong>EDIT: <a href=\"/lw/dgu/transparency_in_insurance/6zeo\">shminux </a>has found <a href=\"http://www.pkainsurance.com/user/insurancequote.jsp\">a tool that instantly delivers term life insurance quotes</a>. Most everything I've written below is now&nbsp;irrelevant&nbsp;and can now be ignored.</strong></h3>\n<p><br />Here is what seems to be the standard for acquiring insurance (of most kinds, though here I'll be focusing on life insurance):</p>\n<p>&nbsp;</p>\n<p>1. You contact a salesperson (agent) who is incentivized to sell you insurance policies which earn them more money.</p>\n<p>2. You provide a basic set of data regarding your health and general medical status.</p>\n<p>3. The agent takes this information and &lt;MYSTERIOUS BLACK BOX HERE&gt;, and then sends you the quote.</p>\n<p>You don't know how this quote was generated exactly. Presumably&nbsp;actuarial&nbsp;tables were involved at some point. Maybe it was marked up or down based on how confident you sounded to the agent. Or the agent divined the quote based on tea leaves.</p>\n<p>And if you want to comparison shop - you'll have to go to other agents, fill out more forms containing the same information over and over. Even getting quotes on different plans _from the same company_ often requires specifically requesting each one through an agent.</p>\n<p>This is insane.</p>\n<p>If there exists a way to easily get many life insurance quotes at once, please tell me about it. If the general algorithms to generate these plans are well known, please link to a&nbsp;thorough&nbsp;description or implementation so at least people have a means of determining whether or not they're being ripped off.</p>\n<p>But if those things truly do not exist - I think we should fix this system.</p>\n<p>Crowdsourcing the acquisition and publication of life insurance rates could go a long way towards bringing some transparency to what seems to be a very, very broken marketplace.</p>\n<p>So this is what I propose. If we systematically divide up the work of getting quotes, I think we can amass a considerable amount of data fairly quickly.</p>\n<p>(This should obviously be of particular interest to would-be test-subject/cryonics-enthusiasts.)</p>\n<p>Does this sound feasible? Does this information already exist in a form which renders this data raid unnecessary? Can it be improved?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KsgPjKhpARhATzSDr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "17454", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-06T20:12:51.125Z", "modifiedAt": null, "url": null, "title": "Interlude for Behavioral Economics", "slug": "interlude-for-behavioral-economics", "viewCount": null, "lastCommentedAt": "2021-03-05T17:54:02.372Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Yy7mgec8tsbTAuTqb/interlude-for-behavioral-economics", "pageUrlRelative": "/posts/Yy7mgec8tsbTAuTqb/interlude-for-behavioral-economics", "linkUrl": "https://www.lesswrong.com/posts/Yy7mgec8tsbTAuTqb/interlude-for-behavioral-economics", "postedAtFormatted": "Friday, July 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interlude%20for%20Behavioral%20Economics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInterlude%20for%20Behavioral%20Economics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYy7mgec8tsbTAuTqb%2Finterlude-for-behavioral-economics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interlude%20for%20Behavioral%20Economics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYy7mgec8tsbTAuTqb%2Finterlude-for-behavioral-economics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYy7mgec8tsbTAuTqb%2Finterlude-for-behavioral-economics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1710, "htmlBody": "<p>The so-called &ldquo;rational&rdquo; solutions to the Prisoners' Dilemma and Ultimatum Game are suboptimal to say the least. Humans have various kludges added by both nature or nurture to do better, but they're not perfect and they're certainly not simple. They leave entirely open the question of what real people will actually do in these situations, a question which can only be addressed by hard data.</p>\n<p><a id=\"more\"></a></p>\n<p>As in so many other areas, our most important information comes from reality television. <a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393062430\"><em>The Art of Strategy</em></a> discusses a US game show &ldquo;Friend or Foe&rdquo; where a team of two contestants earned money by answering trivia questions. At the end of the show, the team used a sort-of Prisoner's Dilemma to split their winnings: each team member chose &ldquo;Friend&rdquo; (cooperate) or &ldquo;Foe&rdquo; (defect). If one player cooperated and the other defected, the defector kept 100% of the pot. If both cooperated, each kept 50%. And if both defected, neither kept anything (this is a significant difference from the standard dilemma, where a player is a little better off defecting than cooperating if her opponent defects). <br /><br />Players chose &ldquo;Friend&rdquo; about 45% of the time. Significantly, this number remained constant despite the size of the pot: they were no more likely to cooperate when splitting small amounts of money than large. <br /><br />Players seemed to want to play &ldquo;Friend&rdquo; if and only if they expected their opponents to do so. This is not rational, but it accords with the &ldquo;Tit-for-Tat&rdquo; strategy hypothesized to be the evolutionary solution to Prisoner's Dilemma. This played out on the show in a surprising way: players' choices started off random, but as the show went on and contestants began participating who had seen previous episodes, they began to base their decision on observable characteristics about their opponents. For example, in the first season women cooperated more often than men, so by the second season a player was cooperating more often if their opponent was a woman - whether or not that player was a man or woman themselves. <br /><br />Among the superficial characteristics used, the only one to reach statistical significance <a href=\"https://docs.google.com/viewer?a=v&amp;q=cache:JRe4dpT-siQJ:bpp.wharton.upenn.edu/mawhite/Papers/FriendorFoe.pdf+&amp;hl=en&amp;gl=us&amp;pid=bl&amp;srcid=ADGEESjeRJCRI-h9qVGveKoOdAJHIP7U6eI86_1xhqdmuTsWrywbCF3IbpD3K6_i7nhqM9bSFouAM4vuNKbDrxjnZjUAfth3j8ztDy5g-QQ_IUVYPX6l0QDPoFDzlliMzL4jG5x8Z2Yd&amp;sig=AHIEtbSl0cC2MvFPFFnjXC6-O-K8Z8FOqw\">according to the study</a> was age: players below the median age of 27 played &ldquo;Foe&rdquo; more often than those over it (65% vs. 39%, p &lt; .001). Other nonsignificant tendencies were for men to defect more than women (53% vs. 46%, p=.34) and for black people to defect more than white people (58% vs. 48%, p=.33). These nonsignificant tendencies became important because the players themselves attributed significance to them: for example, by the second season women were playing &ldquo;Foe&rdquo; 60% of the time against men but only 45% of the time against women (p&lt;.01) presumably because women were perceived to be more likely to play &ldquo;Friend&rdquo; back; also during the second season, white people would play &ldquo;Foe&rdquo; 75% against black people, but only 54% of the time against other white people.<br /><br />(This risks self-fulfilling prophecies. If I am a black man playing a white woman, I expect she will expect me to play &ldquo;Foe&rdquo; against her, and she will &ldquo;reciprocate&rdquo; by playing &ldquo;Foe&rdquo; herself. Therefore, I may choose to &ldquo;reciprocate&rdquo; against her by playing &ldquo;Foe&rdquo; myself, even if I wasn't originally intending to do so, and other white women might observe this, thus creating a vicious cycle.)<br /><br />In any case, these attempts at coordinated play worked, but only imperfectly. By the second season, 57% of pairs chose the same option - either (C, C) or (D, D). <br /><br />Art of Strategy included another great Prisoner's Dilemma experiment. In this one, the experimenters spoiled the game: they told both players that they would be deciding simultaneously, but in fact, they let Player 1 decide first, and then secretly approached Player 2 and told her Player 1's decision, letting Player 2 consider this information when making her own choice. <br /><br />Why should this be interesting? From the previous data, we know that humans play &ldquo;tit-for-expected-tat&rdquo;: they will generally cooperate if they believe their opponent will cooperate too. We can come up with two hypotheses to explain this behavior. First, this could be a folk version of Timeless Decision Theory or Hofstadter's superrationality; a belief that their own decision literally determines their opponent's decision. Second, it could be based on a belief in fairness: if I think my opponent cooperated, it's only decent that I do the same.<br /><br />The &ldquo;researchers spoil the setup&rdquo; experiment can distinguish between these two hypotheses. If people believe their choice determines that of their opponent, then once they know their opponent's choice they no longer have to worry and can freely defect to maximize their own winnings. But if people want to cooperate to reward their opponent, then learning that their opponent cooperated for sure should only increase their willingness to reciprocate.<br /><br />The results: If you tell the second player that the first player defected, 3% still cooperate (apparently 3% of people are Jesus). If you tell the second player that the first player cooperated.........only 16%&nbsp; cooperate. When the same researchers in the same lab didn't tell the second player anything, 37% cooperated.<br /><br />This is a pretty resounding victory for the &ldquo;folk version of superrationality&rdquo; hypothesis. 21% of people wouldn't cooperate if they heard their opponent defected, wouldn't cooperate if they heard their opponent cooperated, but will cooperate if they don't know which of those two their opponent played.<br /><br />Moving on to the Ultimatum Game: very broadly, the first player usually offers between 30 and 50 percent, and the second player tends to accept. If the first player offers less than about 20 percent, the second player tends to reject it. <br /><br />Like the Prisoner's Dilemma, the amount of money at stake doesn't seem to matter. This is really surprising! Imagine you played an Ultimatum Game for a billion dollars. The first player proposes $990 million for herself, $10 million for you. On the one hand, this is a 99-1 split, just as unfair as $99 versus $1. On the other hand, ten million dollars! <br /><br />Although tycoons have yet to donate a billion dollars to use for Ultimatum Game experiments, researchers have done the next best thing and flown out to Third World countries where even $100 can be an impressive amount of money. In games in Indonesia played for a pot containing a sixth of Indonesians' average yearly income, Indonesians still rejected unfair offers. In fact, at these levels the first player tended to propose fairer deals than at lower stakes - maybe because it would be a disaster if her offer get rejected.<br /><br />It was originally believed that results in the Ultimatum Game were mostly independent of culture.&nbsp; Groups in the US, Israel, Japan, Eastern Europe, and Indonesia all got more or less the same results. But this elegant simplicity was, like so many other things, ruined by the Machiguenga Indians of eastern Peru. They tend to make offers around 25%, and will accept pretty much anything. <br /><br />One more interesting finding: people who accept low offers in the Ultimatum Game <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950304/\">have lower testosterone</a> than those who reject them.<br /><br />There is a certain degenerate form of the Ultimatum Game called the Dictator Game. In the Dictator Game, the second player doesn't have the option of vetoing the first player's distribution. In fact, the second player doesn't do anything at all; the first player distributes the money, both players receive the amount of money the first player decided upon, and the game ends. A perfectly selfish first player would take 100% of the money in the Dictator Game, leaving the second player with nothing.<br /><br />In <a href=\"http://www.coll.mpg.de/pdf_dat/2010_07online.pdf\">a metaanalysis of 129 papers consisting of over 41,000 individual games</a>, the average amount the first player gave the second player was 28.35%. 36% of first players take everything, 17% divide the pot equally, and 5% give everything to the second player, nearly doubling our previous estimate of what percent of people are Jesus.<br /><br />The meta-analysis checks many different results, most of which are insignificant, but a few stand out. Subjects playing the dictator game &ldquo;against&rdquo; a charity are much more generous; up to a quarter give everything. When the experimenter promises to &ldquo;match&rdquo; each dollar given away (eg the dictator gets $100, but if she gives it to the second player the second player gets $200), the dictator gives much more (somewhat surprising, as this might be an excuse to keep $66 for yourself and get away with it by claiming that both players still got equal money). On the other hand, if the experimenters give the second player a free $100, so that they start off richer than the dictator, the dictator compensates by not giving them nearly as much money. <br /><br />Old people give more than young people, and non-students give more than students. People from &ldquo;primitive&rdquo; societies give more than people from more developed societies, and the more primitive the society, the stronger the effect.&nbsp; The most important factor, though? As always, sex. Women both give more and get more in dictator games.<br /><br />It is somewhat inspiring that so many people give so much in this game, but before we become too excited about the fundamental goodness of humanity, Art of Strategy mentions <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=494422\">a great experiment by Dana, Cain, and Dawes</a>. The subjects were offered a choice: either play the Dictator Game with a second player for $10, or get $9 and the second subject is sent home and never even knows what the experiment is about. A third of participants took the second option.<br /><br />So generosity in the Dictator Game isn't always about wanting to help other people. It seems to be about knowing, deep down, that some anonymous person who probably doesn't even know your name and who will never see you again is disappointed in you. Remove the little problem of the other person knowing what you did, and they will not only keep the money, but even be willing to pay the experiment a dollar to keep them quiet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Yy7mgec8tsbTAuTqb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 72, "baseScore": 85, "extendedScore": null, "score": 0.000178, "legacy": true, "legacyId": "17436", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 85, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-07T05:00:59.138Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Existential Angst Factory", "slug": "seq-rerun-existential-angst-factory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:36.194Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aHyqBCdfdc5TzsqRz/seq-rerun-existential-angst-factory", "pageUrlRelative": "/posts/aHyqBCdfdc5TzsqRz/seq-rerun-existential-angst-factory", "linkUrl": "https://www.lesswrong.com/posts/aHyqBCdfdc5TzsqRz/seq-rerun-existential-angst-factory", "postedAtFormatted": "Saturday, July 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Existential%20Angst%20Factory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Existential%20Angst%20Factory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaHyqBCdfdc5TzsqRz%2Fseq-rerun-existential-angst-factory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Existential%20Angst%20Factory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaHyqBCdfdc5TzsqRz%2Fseq-rerun-existential-angst-factory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaHyqBCdfdc5TzsqRz%2Fseq-rerun-existential-angst-factory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>Today's post, <a href=\"/lw/sc/existential_angst_factory/\">Existential Angst Factory</a> was originally published on 19 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>As a general rule, if you find yourself suffering from existential angst, check and see if you're not just feeling unhappy because of something else going on in your life. An awful lot of existential angst comes from people trying to solve the wrong problem.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dgb/seq_rerun_could_anything_be_right/\">Could Anything Be Right?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aHyqBCdfdc5TzsqRz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 12, "extendedScore": null, "score": 9.37576386535573e-07, "legacy": true, "legacyId": "17461", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8rdoea3g6QGhWQtmx", "DE2xNLjRfRjR4WDpZ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-07T17:16:15.592Z", "modifiedAt": null, "url": null, "title": "Stupid Questions Open Thread Round 3", "slug": "stupid-questions-open-thread-round-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:37.357Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DqMDZ6zHJwPgw44BY/stupid-questions-open-thread-round-3", "pageUrlRelative": "/posts/DqMDZ6zHJwPgw44BY/stupid-questions-open-thread-round-3", "linkUrl": "https://www.lesswrong.com/posts/DqMDZ6zHJwPgw44BY/stupid-questions-open-thread-round-3", "postedAtFormatted": "Saturday, July 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stupid%20Questions%20Open%20Thread%20Round%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStupid%20Questions%20Open%20Thread%20Round%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqMDZ6zHJwPgw44BY%2Fstupid-questions-open-thread-round-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stupid%20Questions%20Open%20Thread%20Round%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqMDZ6zHJwPgw44BY%2Fstupid-questions-open-thread-round-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqMDZ6zHJwPgw44BY%2Fstupid-questions-open-thread-round-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 258, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">From the </span><a style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \" href=\"/lw/bws/stupid_questions_open_thread_round_2/\">last thread</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">:</span></p>\n<blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">From Costanza's original&nbsp;<a style=\"color: #8a8a8b; \" href=\"/r/discussion/lw/932/stupid_questions_open_thread/\">thread</a>&nbsp;(entire text):</p>\n<p style=\"margin: 0px 0px 1em; \">\"This is for anyone in the LessWrong community who has made at least some effort to read the sequences and follow along, but is still confused on some point, and is perhaps feeling a bit embarrassed. Here, newbies and not-so-newbies are free to ask very basic but still relevant questions with the understanding that the answers are probably somewhere in the sequences. Similarly, LessWrong tends to presume a rather high threshold for understanding science and technology. Relevant questions in those areas are welcome as well.&nbsp; Anyone who chooses to respond should respectfully guide the questioner to a helpful resource, and questioners should be appropriately grateful. Good faith should be presumed on both sides, unless and until it is shown to be absent.&nbsp; If a questioner is not sure whether a question is relevant, ask it, and also ask if it's relevant.\"</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \"><span style=\"text-align: -webkit-auto; \"><strong>Meta</strong>:</span></p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\n<li>How often should these be made? I think one every three months is the correct frequency.</li>\n<li>Costanza made the original thread, but I am OpenThreadGuy. I am therefore not only entitled but&nbsp;<em>required</em>&nbsp;to post this in his stead. But I got his permission anyway.</li>\n</ul>\n</blockquote>\n<p><strong>Meta</strong>:</p>\n<p>&nbsp;</p>\n<ul>\n<li>I still haven't figured out a satisfactory answer to the previous meta question, how often these should be made. It was requested that I make a new one, so I did.</li>\n<li>I promise I won't quote the entire previous threads from now on. Blockquoting in articles only goes one level deep, anyway.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DqMDZ6zHJwPgw44BY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 9.379211102416351e-07, "legacy": true, "legacyId": "17473", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 209, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["s9exm8K8CHkEGtiPC", "8dijcs9BjxaXE2A9G"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-07T23:50:35.665Z", "modifiedAt": null, "url": null, "title": "To Learn Critical Thinking, Study Critical Thinking", "slug": "to-learn-critical-thinking-study-critical-thinking", "viewCount": null, "lastCommentedAt": "2012-07-09T00:50:41.598Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MGtKNd5GBXN5oNj7p/to-learn-critical-thinking-study-critical-thinking", "pageUrlRelative": "/posts/MGtKNd5GBXN5oNj7p/to-learn-critical-thinking-study-critical-thinking", "linkUrl": "https://www.lesswrong.com/posts/MGtKNd5GBXN5oNj7p/to-learn-critical-thinking-study-critical-thinking", "postedAtFormatted": "Saturday, July 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20To%20Learn%20Critical%20Thinking%2C%20Study%20Critical%20Thinking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATo%20Learn%20Critical%20Thinking%2C%20Study%20Critical%20Thinking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMGtKNd5GBXN5oNj7p%2Fto-learn-critical-thinking-study-critical-thinking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=To%20Learn%20Critical%20Thinking%2C%20Study%20Critical%20Thinking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMGtKNd5GBXN5oNj7p%2Fto-learn-critical-thinking-study-critical-thinking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMGtKNd5GBXN5oNj7p%2Fto-learn-critical-thinking-study-critical-thinking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3296, "htmlBody": "<blockquote>\n<p><a href=\"http://en.wikipedia.org/wiki/Critical_thinking\">Critical thinking</a> courses may increase students&rsquo; rationality, especially if they do <a href=\"http://en.wikipedia.org/wiki/Argument_map\">argument mapping</a>.</p>\n</blockquote>\n<p>The following excerpts are from <a href=\"http://images.austhink.com/pdf/Claudia-Alvarez-thesis.pdf\">&ldquo;Does philosophy improve critical thinking skills?&rdquo;</a>, Ortiz 2007.</p>\n<h1 id=\"excerpts\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Excerpts</a></h1>\n<blockquote>\n<p>This thesis makes a first attempt to subject the assumption that studying [Anglo-American analytic] philosophy improves critical thinking skills to rigorous investigation.</p>\n<p>&hellip;Thus the second task, in Chapter 3, is to articulate and critically examine the standard arguments that are raised in support of the assumption (or rather, would be raised if philosophers were in the habit of providing support for the assumption). These arguments are found to be too weak to establish the truth of the assumption. The failure of the standard arguments leaves open the question of whether the assumption is in fact true. The thesis argues at this point that, since the assumption is making an empirical assertion, it should be investigated using standard empirical techniques as developed in the social sciences. In Chapter 4, I conduct an informal review of the empirical literature. The review finds that evidence from the existing empirical literature is inconclusive. Chapter 5 presents the empirical core of the thesis. I use the technique of meta-analysis to integrate data from a large number of empirical studies. This meta-analysis gives us the best yet fix on the extent to which critical thinking skills improve over a semester of studying philosophy, general university study, and studying critical thinking. The meta-analysis results indicate that students do improve while studying philosophy, and apparently more so than general university students, though we cannot be very confident that this difference is not just the result of random variation. More importantly, studying philosophy is less effective than studying critical thinking, regardless of whether one is being taught in a philosophy department or in some other department. Finally, studying philosophy is much less effective than studying critical thinking using techniques known to be particularly effective such as LAMP.</p>\n<p><a id=\"more\"></a>&hellip;To establish whether and how improvement is in fact occurring in students&rsquo; CTS, we need ways of measuring such improvement. In other words, we have to be able to operationalize CTS . Several attempts to devise such measures have been made. They include common written tests (essays), direct classroom observations, individual interviews, and student and teacher journals. Although techniques for gathering information on students&rsquo; CTS can come in a variety of forms, the most objective, standardized measures, however, are multiple-choice tests (e.g.&nbsp;Watson-Glaser Critical Thinking Appraisal, Cornell Critical Thinking Test, California Critical Thinking Skills Test, and the College Assessment of Academic Proficiency tests). The objectivity of these tests is based on the fact they have received, more than other CT measuring techniques, constant evaluation with regard to two major indicators of quality information in educational evaluation: validity and reliability . As regards to this, two experts on CT evaluation have said that &ldquo;these tests have been carefully developed and are likely to serve more effectively than hastily constructed instruments&rdquo; (Norris &amp; Ennis, 1990, p.55). &hellip;Therefore, we have focused on research studies that have measured CT gain by pre and post-testing of skills, using the standard CTS tests mentioned. This means, studies that have measured students&rsquo; CTS at the beginning of a course or academic period and then again at the end of it. In this sense, CTS improvement means to get better on CTS tests within a given, standard time interval.</p>\n<p><a id=\"more\"></a></p>\n</blockquote>\n<blockquote>\n<p>&hellip;The crucial link, however, is that, when well taught in philosophy, students are required to extensively practice the core skills of reasoning and making or analysing arguments &ndash; the central CT skills. Such practice is highly likely to lead to improvement in reasoning and argument skills, and thus to gains in CT abilities. This is a vital consideration, as we shall see; since it is the nature of such practice, its quality and quantity, rather than the study of philosophy per se, which turns out to be the indispensable condition for the improvement of CTS. There is also an additional element in support for the claim that philosophy tends to help improve CTS. It is the fact that philosophy students receive the guidance of experts in reasoning &ndash; that is what professors of philosophy are. This element requires special attention.</p>\n<p>&hellip;We shall turn, shortly, to a careful review of the existing evidence, but because the findings of this review are s central to the present thesis, let us summarize them briefly first. With the exception of the Philosophy for Children program (P4C), there has been strikingly little interest in empirical evaluation by philosophers of the extent to which their discipline actually enhances critical thinking. At present, the volume of published work in connection with P4C pedagogy shows a positive impact on children&rsquo;s reasoning skills (Garcia-Moriyon, Rebollo, &amp; Colom, 2005). Two recent meta-analyses support this claim&hellip;Two meta-analyses have also been conducted to evaluate the program (Garcia-Moriyon et al., 2005; Trickey &amp; Topping, 2004). In the Trickey &amp; Topping meta-analysis, similar &ldquo;positive outcomes&ldquo; were found. The review included ten studies (three of which are included in the Montclair list) measuring outcomes in reading, reasoning, cognitive ability, and other curriculum-related abilities. The study showed a effect size of 0.43 , with low variance, indicating a consistent moderate positive effect for P4C on a wide range of outcomes (p.365). In Garcia-Moriyon&rsquo;s meta-analysis, 18 studies fit the following criteria: (1) studies testing the effectiveness of P4C in improving reasoning skills or mental abilities and (2) studies including enough statistical information to calculate the magnitude of effects sizes. All effect sizes except one are positive and the estimation of the mean effect size yielded a value of 0.58 (p&lt; .01; CI .53-.64), indicating that P4C has a positive effect on reasoning skills14. (p.21)..of the 116 studies selected for the meta-analysis by Garcia-Moriyon et al., he and his colleagues had to exclude the vast majority for not meeting the minimum criteria related to research design, data analysis, and reporting. Only 18 of the 116 fitted the criteria. This deficiency in so much in the P4C research has led Reznitskaya, 2005, to draw particular attention to the lack of appropriate statistical procedures in the great majority of these studies.</p>\n<p>&hellip;With regard to undergraduate and graduate students, only a few studies of the impact of philosophy on critical thinking were found. This is despite the fact that philosophy departments in Western universities commonly claim that philosophy has this effect. There is also a striking contrast between the abundant research regarding the impact of social sciences, especially psychology and education, or nursing, for example, on critical thinking and the paucity of research done on philosophy (and, for that matter, the hard sciences) in this regard.</p>\n<p>&hellip;I have been able to discover only a small number of studies (five) which have sought to directly measure the relationship between the study of philosophy by undergraduates and the development of CTS. Even within this group, there were distinct variations in research design. What sets them apart, however, is that, albeit in different ways, they were all concerned to measure the link between studying philosophy at college and improving CTS. In all (Annis &amp; Annis, 1979; Harrell, 2004; Reiter, 1994; Ross &amp; Semb, 1981) but one (Facione, 1990) of these cases, the philosophy students belonged to the experimental group.</p>\n<p>&hellip;The specific study by Ross &amp; Semb 1981 raised a suggestion that is surely worthy of closer and more serious analysis. These authors indicated that the gains in CTS due to studies in philosophy appeared to depend on the method of instruction. The implication here is that traditional philosophy (lectures, discussions) does not generate the high degree of effectiveness in improving CT abilities that are achievable with more innovative methods of teaching. This fourth point needs amplification. Ross and Semb set up a second experiment, in which the experimental group (following the Keller Plan) were encouraged to concentrate on informal argumentation and the comprehension of written texts with personalized instruction (PSI), while the control group were taught by traditional means of lectures and discussion. They found that the experimental group made greater gains in CTS than did the control group. Harrell, in 2004, established a similar correlation using argument mapping in the experimental group. Her conclusion was: &ldquo;While, on average, all of the students in each of the sections improved their abilities on these tasks over the course of the semester, the most dramatic improvements were made by the students who learned how to construct arguments.&rdquo; (p.&nbsp;14) &hellip;This fourth point is further underscored by the fact that consistently positive results in CT gain have been provided by critical thinking courses offered by Western Philosophy Departments (Butchard, 2006; Donohue et al., 2002; Hitchcock, 2003; Rainbolt &amp; Rieber, 1997; Spurret, 2005; Twardy, 2004). These courses are exceptional, in that they are not standard philosophy courses. They are focused on teaching critical thinking skills in their own right. Their positive results reinforce the idea that innovative methods of teaching within philosophy departments might produce better results in the teaching of critical thinking skills. However, we cannot infer from that that philosophy per se improves critical thinking skills &hellip;While Panowitsch and Balkcum were interested in the implications of these results concerning the development of moral judgment and the utility of the DIT, their findings also showed something else, which interests us: that a course in Formal Logic yields greater CT gains than a course in Ethics. In other words, the kind of philosophy course one studies makes a difference to the gain in CTS&hellip;There are, in addition, some studies which have sought to explore the relationship between Logic and the development of reasoning skills. In so far as reasoning and CT skills are not wholly co-terminous, these studies could not resolve the debate about the relationship between Logic and CTS, even were they definitive in themselves. As it happens, their findings, like those of a number of the studies already considered, are not conclusive even within their prescribed domain. Some of these studies seem to indicate that the study of Logic produces statistically significant gains in reasoning skills (Stenning, Cox, &amp; Oberlander, 1995; Van der Pal &amp; Eysink, 1999). Conversely, others indicate that the study of the abstract principles of logic produces no discernible improvement in reasoning (Cheng, Holyoak, Nisbett, &amp; Oliver, 1986).</p>\n<p>&hellip;There appear to have been only three studies that have tried to estimate the growth in reasoning and CTS in students who have completed, or are close to having completed, an undergraduate major in philosophy. Two of these studies (Hoekema, 1987; Nieswiadomy, 1998) were centred on determining the level of competence in reasoning abilities of such students seeking to enter various graduate schools at university. Although these studies suggest that philosophy majors generally do better on graduate tests (GMAT, LSAT, GRE), they do not disentangle the contribution of philosophy from the students&rsquo; selection effect. People that choose to study philosophy may, in general, be good at reasoning anyway and the evidence these two studies provide does not indicate to what extent the study of philosophy in itself improves reasoning skills.</p>\n<p>&hellip;In summary, we classified the studies into seven groups. These groups will make it possible for us to measure the impact of the two major independent variables selected for the purposes of this inquiry: the amount of philosophy and CT instruction the students have received. These groups are as follows:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Courses offered by philosophy departments consisting of formal instruction in Anglo-American analytic philosophy, or what I shall call &lsquo;pure philosophy&rsquo; (Pure Phil).</li>\n<li>Critical thinking courses offered by philosophy departments with no instruction in argument mapping (Phil CT No AM).</li>\n<li>Critical thinking courses offered by philosophy departments with some instruction in argument mapping (Phil CT-AM).</li>\n<li>Critical thinking courses offered by philosophy departments with lots of argument mapping practice (Phil LAMP). These are courses fully dedicated to teaching CT with argument mapping.</li>\n<li>Courses offered by non-philosophy departments and wholly dedicated to explicit instruction in CT (No Phil, Ded-CT).</li>\n<li>Courses offered by non-philosophy departments with some form of conventional CT instruction embedded (No Phil, Some-CT).</li>\n<li>Courses offered by non-philosophy departments with no special attempts being made to cultivate CT skills (No Phil, No-CT).</li>\n</ol></blockquote>\n<p>Ortiz gives what is a fairly good overview of what a meta-analysis is and how you would do a basic one; I&rsquo;ll skip over this material. The graphical results are on pg82: All 7 categories give positive effect sizes; the smallest difference between the improvements exhibited by subjects in the controls (#7) and subjects in the the various philosophy approaches (#1-6) is: <em>d</em>=0.09 (#1,6); the largest, <em>d</em>=0.63 (#4).</p>\n<blockquote>\n<p>&hellip;The Keller Plan and Logic courses yielded results so much better than the other Introduction to Philosophy courses that they inflated the results for this pool of studies. In fact, if these two are separated out, the difference made to CT skills by the Introduction to Philosophy courses becomes appreciably smaller (ES = 0.19 SD). What is more important, however, is the intriguing implication that particular approaches to teaching made a more substantial difference than the subject matter in itself. As we have seen, this turns out to be true in regard to other approaches, also. (See section 4.3, &ldquo;Evidence from Undergraduate Students&ldquo;, in the Review of the Existing Evidence) &hellip;The results of the Michigan Meta-Analysis of Research on Instructional Technology in Higher Education (Wittrock, 1986) found a &ldquo;moderate&rdquo; (medium) effect size (.49) for students in Keller Plan courses. Although this effect size was described as &lsquo;moderate&rsquo;, it was found to be greater than those for other teaching strategies, such as Audio-Tutorials (.20), Computer based-teaching (.25), Program Instruction (.28) and Visual-Based Instruction (.15). What all this suggests is that, while studying philosophy will make some difference, a greater difference will be made if one does either of two things: concentrate on Logic itself or keep the broader content, but change the way you teach it.</p>\n<p>&hellip;The discussion about the net effect of the study of philosophy on the development of CTS also leads us to ask ourselves: To what extent would those philosophy students have made the gains they did had they not been studying Anglo-American analytic philosophy? Or to put it another way, is the gain due to philosophy, or would it have been made by students in the normal course of events, spending a semester in any serious discipline? Whence the question, Does (Anglo-American analytic) philosophy improve critical thinking skills over and above university education in general? In order to answer this question, we must first establish what difference a university education in general actually makes.</p>\n<p>We have data for this, but the data on its own is insufficient. What our data shows is that a university education, in general, produces a gain of 0.12 of an SD over any given semester. Superficially, this compares unfavourably with the gain of 0.26 SD in a semester for philosophy. We have noted several caveats with respect to the results for philosophy. We need to ask here what the 0.12 SD for university education in general actually means. As it happens, those not attending university at all appear to improve their CTS by 0.10 SD in the equivalent of the first semester after leaving school (Pascarella, 1989). This would suggest that attending university, at least initially, makes no appreciable difference, because CTS improve at that age anyway by much the same amount.</p>\n<p>We need, however, to be a little cautious in drawing conclusions here, since Pascarella&rsquo;s own studies suggested an improvement of 0.26 SD in the first semester of university &ndash; the improvement our own data from the meta-analysis suggest is achieved by students in philosophy. What does all this mean? In fact, statistically speaking, as explained briefly above (5.4 Results section) once you allow for confidence intervals, there is not much to choose between gains of 0.1, 0.12 and 0.26. In short, whether we use Pascarella&rsquo;s data alone or the more systematic data deriving from the meta-analysis, it appears that there is little to choose between not going to university, going to university in general and studying (pure) Philosophy at university, as regards improvements in CTS in a given semester period. This is both a counterintuitive and even a disconcerting conclusion.</p>\n</blockquote>\n<p>The results seem sensible to me. I often reflected while earning my own philosophy degree that of all the courses, the one I valued and drew on most was a course on &lsquo;contemporary philosophy&rsquo; where the sole activity was the teacher handing us a syllogism on, say, personal identity, asking whether it was logically valid and when it finally was, what premise we would reject, and why - which seems similar to the descriptions of the contents of critical thinking courses. After that, I valued the symbolic logic courses, and then a Pre-Socratics course because I liked them a lot; but I did not think I gained very much from the more &lsquo;historical&rsquo; courses on, say, Descartes. A good critical thinking course sounds like being taught on the core philosophical approach: taking little for granted, constructing arguments, and examining each piece critically. So it does not surprise me much that time spent on critical thinking and argument mapping will produce more gains than time spent on topics like ethics.</p>\n<p>This also suggests that the &lsquo;great books&rsquo; method is not suitable for learning critical thinking or rationality, at least for freshmen college students (the principal focus of the studies).</p>\n<h1 id=\"see-also\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> See Also</a></h1>\n<p>According to Google Scholar, Ortiz&rsquo;s thesis has been cited once, by inclusion into the bibliography of a thesis discussing religion education; a Google search turned up citation in another paper, on understanding psychology questions, which did not seem very useful reading. Googling, I did find an <a href=\"http://ethicalrealism.wordpress.com/2012/04/27/introduction-to-argument-mapping-critical-thinking-ebook/\">ebook on argument mapping</a> which may be of use.</p>\n<p>Previous argument mapping discussion:</p>\n<ul>\n<li>\n<p><a href=\"/lw/16u/argument_maps_improve_critical_thinking/\">&ldquo;Argument Maps Improve Critical Thinking&rdquo;</a>:</p>\n<blockquote>\n<p>Charles R. Twardy provides evidence that a course in argument mapping, using a particular software tool improves critical thinking. The improvement in critical thinking is measured by performance on a specific multiple choice test (California Critical Thinking Skills Test). This may not be the best way to measure rationality, but my point is that unlike almost everybody else, there was measurement and statistical improvement!&hellip;</p>\n<p>To summarize my (clumsy) understanding of the activity of argument mapping: One takes a real argument in natural language. (op-eds are a good source of short arguments, philosophy is a source of long arguments). Then elaborate it into a tree structure, with the main conclusion at the root of the tree. The tree has two kinds of nodes (it is a bipartite graph). The root conclusion is a &ldquo;claim&rdquo; node. Every claim node has approximately one sentence of English text associated. The children of a claim are &ldquo;reasons&rdquo;, which do NOT have English text associated. The children of a reason are claims. Unless I am mistaken, the intended meaning of the connection from a claim&rsquo;s child (a reason) to the parent is implication, and the meaning of a reason is the conjunction of its children.</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"/lw/2x5/how_are_critical_thinking_skills_acquired_five/\">&ldquo;How are critical thinking skills acquired? Five perspectives&rdquo;</a>:</p>\n<blockquote>\n<p>How are critical thinking skills acquired? Five perspectives: Tim van Gelder discusses acquisition of critical thinking skills, suggesting several theories of skill acquisition that don&rsquo;t work, and one with which he and hundreds of his students have had significant success. [LAMP/argument mapping]</p>\n</blockquote>\n</li>\n<li><a href=\"/lw/1qq/debate_tools_an_experience_report/\">&ldquo;Debate tools: an experience report&rdquo;</a></li>\n<li>\n<p><a href=\"/lw/51a/software_for_critical_thinking_prof_geoff_cumming/\">&ldquo;Software for Critical Thinking, Prof.&nbsp;Geoff Cumming&rdquo;</a></p>\n</li>\n</ul>\n<p>Relevant links:</p>\n<ul>\n<li><a href=\"/lw/76x/is_rationality_teachable/\">&ldquo;Is Rationality Teachable?&rdquo;</a></li>\n<li><a href=\"http://www.overcomingbias.com/2009/12/why-read-old-thinkers.html\">&ldquo;Why Read Old Thinkers?&rdquo;</a></li>\n<li><a href=\"/lw/1ul/for_progress_to_be_by_accumulation_and_not_by/\">&ldquo;For progress to be by accumulation and not by random walk, read great books&rdquo;</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksdiAMKfgSyEeKMo6": 1, "GLykb6NukBeBQtDvQ": 1, "fH8jPjHF2R27sRTTG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MGtKNd5GBXN5oNj7p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 40, "extendedScore": null, "score": 8.9e-05, "legacy": true, "legacyId": "17474", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p><a href=\"http://en.wikipedia.org/wiki/Critical_thinking\">Critical thinking</a> courses may increase students\u2019 rationality, especially if they do <a href=\"http://en.wikipedia.org/wiki/Argument_map\">argument mapping</a>.</p>\n</blockquote>\n<p>The following excerpts are from <a href=\"http://images.austhink.com/pdf/Claudia-Alvarez-thesis.pdf\">\u201cDoes philosophy improve critical thinking skills?\u201d</a>, Ortiz 2007.</p>\n<h1 id=\"1_Excerpts\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Excerpts</a></h1>\n<blockquote>\n<p>This thesis makes a first attempt to subject the assumption that studying [Anglo-American analytic] philosophy improves critical thinking skills to rigorous investigation.</p>\n<p>\u2026Thus the second task, in Chapter 3, is to articulate and critically examine the standard arguments that are raised in support of the assumption (or rather, would be raised if philosophers were in the habit of providing support for the assumption). These arguments are found to be too weak to establish the truth of the assumption. The failure of the standard arguments leaves open the question of whether the assumption is in fact true. The thesis argues at this point that, since the assumption is making an empirical assertion, it should be investigated using standard empirical techniques as developed in the social sciences. In Chapter 4, I conduct an informal review of the empirical literature. The review finds that evidence from the existing empirical literature is inconclusive. Chapter 5 presents the empirical core of the thesis. I use the technique of meta-analysis to integrate data from a large number of empirical studies. This meta-analysis gives us the best yet fix on the extent to which critical thinking skills improve over a semester of studying philosophy, general university study, and studying critical thinking. The meta-analysis results indicate that students do improve while studying philosophy, and apparently more so than general university students, though we cannot be very confident that this difference is not just the result of random variation. More importantly, studying philosophy is less effective than studying critical thinking, regardless of whether one is being taught in a philosophy department or in some other department. Finally, studying philosophy is much less effective than studying critical thinking using techniques known to be particularly effective such as LAMP.</p>\n<p><a id=\"more\"></a>\u2026To establish whether and how improvement is in fact occurring in students\u2019 CTS, we need ways of measuring such improvement. In other words, we have to be able to operationalize CTS . Several attempts to devise such measures have been made. They include common written tests (essays), direct classroom observations, individual interviews, and student and teacher journals. Although techniques for gathering information on students\u2019 CTS can come in a variety of forms, the most objective, standardized measures, however, are multiple-choice tests (e.g.&nbsp;Watson-Glaser Critical Thinking Appraisal, Cornell Critical Thinking Test, California Critical Thinking Skills Test, and the College Assessment of Academic Proficiency tests). The objectivity of these tests is based on the fact they have received, more than other CT measuring techniques, constant evaluation with regard to two major indicators of quality information in educational evaluation: validity and reliability . As regards to this, two experts on CT evaluation have said that \u201cthese tests have been carefully developed and are likely to serve more effectively than hastily constructed instruments\u201d (Norris &amp; Ennis, 1990, p.55). \u2026Therefore, we have focused on research studies that have measured CT gain by pre and post-testing of skills, using the standard CTS tests mentioned. This means, studies that have measured students\u2019 CTS at the beginning of a course or academic period and then again at the end of it. In this sense, CTS improvement means to get better on CTS tests within a given, standard time interval.</p>\n<p><a id=\"more\"></a></p>\n</blockquote>\n<blockquote>\n<p>\u2026The crucial link, however, is that, when well taught in philosophy, students are required to extensively practice the core skills of reasoning and making or analysing arguments \u2013 the central CT skills. Such practice is highly likely to lead to improvement in reasoning and argument skills, and thus to gains in CT abilities. This is a vital consideration, as we shall see; since it is the nature of such practice, its quality and quantity, rather than the study of philosophy per se, which turns out to be the indispensable condition for the improvement of CTS. There is also an additional element in support for the claim that philosophy tends to help improve CTS. It is the fact that philosophy students receive the guidance of experts in reasoning \u2013 that is what professors of philosophy are. This element requires special attention.</p>\n<p>\u2026We shall turn, shortly, to a careful review of the existing evidence, but because the findings of this review are s central to the present thesis, let us summarize them briefly first. With the exception of the Philosophy for Children program (P4C), there has been strikingly little interest in empirical evaluation by philosophers of the extent to which their discipline actually enhances critical thinking. At present, the volume of published work in connection with P4C pedagogy shows a positive impact on children\u2019s reasoning skills (Garcia-Moriyon, Rebollo, &amp; Colom, 2005). Two recent meta-analyses support this claim\u2026Two meta-analyses have also been conducted to evaluate the program (Garcia-Moriyon et al., 2005; Trickey &amp; Topping, 2004). In the Trickey &amp; Topping meta-analysis, similar \u201cpositive outcomes\u201c were found. The review included ten studies (three of which are included in the Montclair list) measuring outcomes in reading, reasoning, cognitive ability, and other curriculum-related abilities. The study showed a effect size of 0.43 , with low variance, indicating a consistent moderate positive effect for P4C on a wide range of outcomes (p.365). In Garcia-Moriyon\u2019s meta-analysis, 18 studies fit the following criteria: (1) studies testing the effectiveness of P4C in improving reasoning skills or mental abilities and (2) studies including enough statistical information to calculate the magnitude of effects sizes. All effect sizes except one are positive and the estimation of the mean effect size yielded a value of 0.58 (p&lt; .01; CI .53-.64), indicating that P4C has a positive effect on reasoning skills14. (p.21)..of the 116 studies selected for the meta-analysis by Garcia-Moriyon et al., he and his colleagues had to exclude the vast majority for not meeting the minimum criteria related to research design, data analysis, and reporting. Only 18 of the 116 fitted the criteria. This deficiency in so much in the P4C research has led Reznitskaya, 2005, to draw particular attention to the lack of appropriate statistical procedures in the great majority of these studies.</p>\n<p>\u2026With regard to undergraduate and graduate students, only a few studies of the impact of philosophy on critical thinking were found. This is despite the fact that philosophy departments in Western universities commonly claim that philosophy has this effect. There is also a striking contrast between the abundant research regarding the impact of social sciences, especially psychology and education, or nursing, for example, on critical thinking and the paucity of research done on philosophy (and, for that matter, the hard sciences) in this regard.</p>\n<p>\u2026I have been able to discover only a small number of studies (five) which have sought to directly measure the relationship between the study of philosophy by undergraduates and the development of CTS. Even within this group, there were distinct variations in research design. What sets them apart, however, is that, albeit in different ways, they were all concerned to measure the link between studying philosophy at college and improving CTS. In all (Annis &amp; Annis, 1979; Harrell, 2004; Reiter, 1994; Ross &amp; Semb, 1981) but one (Facione, 1990) of these cases, the philosophy students belonged to the experimental group.</p>\n<p>\u2026The specific study by Ross &amp; Semb 1981 raised a suggestion that is surely worthy of closer and more serious analysis. These authors indicated that the gains in CTS due to studies in philosophy appeared to depend on the method of instruction. The implication here is that traditional philosophy (lectures, discussions) does not generate the high degree of effectiveness in improving CT abilities that are achievable with more innovative methods of teaching. This fourth point needs amplification. Ross and Semb set up a second experiment, in which the experimental group (following the Keller Plan) were encouraged to concentrate on informal argumentation and the comprehension of written texts with personalized instruction (PSI), while the control group were taught by traditional means of lectures and discussion. They found that the experimental group made greater gains in CTS than did the control group. Harrell, in 2004, established a similar correlation using argument mapping in the experimental group. Her conclusion was: \u201cWhile, on average, all of the students in each of the sections improved their abilities on these tasks over the course of the semester, the most dramatic improvements were made by the students who learned how to construct arguments.\u201d (p.&nbsp;14) \u2026This fourth point is further underscored by the fact that consistently positive results in CT gain have been provided by critical thinking courses offered by Western Philosophy Departments (Butchard, 2006; Donohue et al., 2002; Hitchcock, 2003; Rainbolt &amp; Rieber, 1997; Spurret, 2005; Twardy, 2004). These courses are exceptional, in that they are not standard philosophy courses. They are focused on teaching critical thinking skills in their own right. Their positive results reinforce the idea that innovative methods of teaching within philosophy departments might produce better results in the teaching of critical thinking skills. However, we cannot infer from that that philosophy per se improves critical thinking skills \u2026While Panowitsch and Balkcum were interested in the implications of these results concerning the development of moral judgment and the utility of the DIT, their findings also showed something else, which interests us: that a course in Formal Logic yields greater CT gains than a course in Ethics. In other words, the kind of philosophy course one studies makes a difference to the gain in CTS\u2026There are, in addition, some studies which have sought to explore the relationship between Logic and the development of reasoning skills. In so far as reasoning and CT skills are not wholly co-terminous, these studies could not resolve the debate about the relationship between Logic and CTS, even were they definitive in themselves. As it happens, their findings, like those of a number of the studies already considered, are not conclusive even within their prescribed domain. Some of these studies seem to indicate that the study of Logic produces statistically significant gains in reasoning skills (Stenning, Cox, &amp; Oberlander, 1995; Van der Pal &amp; Eysink, 1999). Conversely, others indicate that the study of the abstract principles of logic produces no discernible improvement in reasoning (Cheng, Holyoak, Nisbett, &amp; Oliver, 1986).</p>\n<p>\u2026There appear to have been only three studies that have tried to estimate the growth in reasoning and CTS in students who have completed, or are close to having completed, an undergraduate major in philosophy. Two of these studies (Hoekema, 1987; Nieswiadomy, 1998) were centred on determining the level of competence in reasoning abilities of such students seeking to enter various graduate schools at university. Although these studies suggest that philosophy majors generally do better on graduate tests (GMAT, LSAT, GRE), they do not disentangle the contribution of philosophy from the students\u2019 selection effect. People that choose to study philosophy may, in general, be good at reasoning anyway and the evidence these two studies provide does not indicate to what extent the study of philosophy in itself improves reasoning skills.</p>\n<p>\u2026In summary, we classified the studies into seven groups. These groups will make it possible for us to measure the impact of the two major independent variables selected for the purposes of this inquiry: the amount of philosophy and CT instruction the students have received. These groups are as follows:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Courses offered by philosophy departments consisting of formal instruction in Anglo-American analytic philosophy, or what I shall call \u2018pure philosophy\u2019 (Pure Phil).</li>\n<li>Critical thinking courses offered by philosophy departments with no instruction in argument mapping (Phil CT No AM).</li>\n<li>Critical thinking courses offered by philosophy departments with some instruction in argument mapping (Phil CT-AM).</li>\n<li>Critical thinking courses offered by philosophy departments with lots of argument mapping practice (Phil LAMP). These are courses fully dedicated to teaching CT with argument mapping.</li>\n<li>Courses offered by non-philosophy departments and wholly dedicated to explicit instruction in CT (No Phil, Ded-CT).</li>\n<li>Courses offered by non-philosophy departments with some form of conventional CT instruction embedded (No Phil, Some-CT).</li>\n<li>Courses offered by non-philosophy departments with no special attempts being made to cultivate CT skills (No Phil, No-CT).</li>\n</ol></blockquote>\n<p>Ortiz gives what is a fairly good overview of what a meta-analysis is and how you would do a basic one; I\u2019ll skip over this material. The graphical results are on pg82: All 7 categories give positive effect sizes; the smallest difference between the improvements exhibited by subjects in the controls (#7) and subjects in the the various philosophy approaches (#1-6) is: <em>d</em>=0.09 (#1,6); the largest, <em>d</em>=0.63 (#4).</p>\n<blockquote>\n<p>\u2026The Keller Plan and Logic courses yielded results so much better than the other Introduction to Philosophy courses that they inflated the results for this pool of studies. In fact, if these two are separated out, the difference made to CT skills by the Introduction to Philosophy courses becomes appreciably smaller (ES = 0.19 SD). What is more important, however, is the intriguing implication that particular approaches to teaching made a more substantial difference than the subject matter in itself. As we have seen, this turns out to be true in regard to other approaches, also. (See section 4.3, \u201cEvidence from Undergraduate Students\u201c, in the Review of the Existing Evidence) \u2026The results of the Michigan Meta-Analysis of Research on Instructional Technology in Higher Education (Wittrock, 1986) found a \u201cmoderate\u201d (medium) effect size (.49) for students in Keller Plan courses. Although this effect size was described as \u2018moderate\u2019, it was found to be greater than those for other teaching strategies, such as Audio-Tutorials (.20), Computer based-teaching (.25), Program Instruction (.28) and Visual-Based Instruction (.15). What all this suggests is that, while studying philosophy will make some difference, a greater difference will be made if one does either of two things: concentrate on Logic itself or keep the broader content, but change the way you teach it.</p>\n<p>\u2026The discussion about the net effect of the study of philosophy on the development of CTS also leads us to ask ourselves: To what extent would those philosophy students have made the gains they did had they not been studying Anglo-American analytic philosophy? Or to put it another way, is the gain due to philosophy, or would it have been made by students in the normal course of events, spending a semester in any serious discipline? Whence the question, Does (Anglo-American analytic) philosophy improve critical thinking skills over and above university education in general? In order to answer this question, we must first establish what difference a university education in general actually makes.</p>\n<p>We have data for this, but the data on its own is insufficient. What our data shows is that a university education, in general, produces a gain of 0.12 of an SD over any given semester. Superficially, this compares unfavourably with the gain of 0.26 SD in a semester for philosophy. We have noted several caveats with respect to the results for philosophy. We need to ask here what the 0.12 SD for university education in general actually means. As it happens, those not attending university at all appear to improve their CTS by 0.10 SD in the equivalent of the first semester after leaving school (Pascarella, 1989). This would suggest that attending university, at least initially, makes no appreciable difference, because CTS improve at that age anyway by much the same amount.</p>\n<p>We need, however, to be a little cautious in drawing conclusions here, since Pascarella\u2019s own studies suggested an improvement of 0.26 SD in the first semester of university \u2013 the improvement our own data from the meta-analysis suggest is achieved by students in philosophy. What does all this mean? In fact, statistically speaking, as explained briefly above (5.4 Results section) once you allow for confidence intervals, there is not much to choose between gains of 0.1, 0.12 and 0.26. In short, whether we use Pascarella\u2019s data alone or the more systematic data deriving from the meta-analysis, it appears that there is little to choose between not going to university, going to university in general and studying (pure) Philosophy at university, as regards improvements in CTS in a given semester period. This is both a counterintuitive and even a disconcerting conclusion.</p>\n</blockquote>\n<p>The results seem sensible to me. I often reflected while earning my own philosophy degree that of all the courses, the one I valued and drew on most was a course on \u2018contemporary philosophy\u2019 where the sole activity was the teacher handing us a syllogism on, say, personal identity, asking whether it was logically valid and when it finally was, what premise we would reject, and why - which seems similar to the descriptions of the contents of critical thinking courses. After that, I valued the symbolic logic courses, and then a Pre-Socratics course because I liked them a lot; but I did not think I gained very much from the more \u2018historical\u2019 courses on, say, Descartes. A good critical thinking course sounds like being taught on the core philosophical approach: taking little for granted, constructing arguments, and examining each piece critically. So it does not surprise me much that time spent on critical thinking and argument mapping will produce more gains than time spent on topics like ethics.</p>\n<p>This also suggests that the \u2018great books\u2019 method is not suitable for learning critical thinking or rationality, at least for freshmen college students (the principal focus of the studies).</p>\n<h1 id=\"2_See_Also\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> See Also</a></h1>\n<p>According to Google Scholar, Ortiz\u2019s thesis has been cited once, by inclusion into the bibliography of a thesis discussing religion education; a Google search turned up citation in another paper, on understanding psychology questions, which did not seem very useful reading. Googling, I did find an <a href=\"http://ethicalrealism.wordpress.com/2012/04/27/introduction-to-argument-mapping-critical-thinking-ebook/\">ebook on argument mapping</a> which may be of use.</p>\n<p>Previous argument mapping discussion:</p>\n<ul>\n<li>\n<p><a href=\"/lw/16u/argument_maps_improve_critical_thinking/\">\u201cArgument Maps Improve Critical Thinking\u201d</a>:</p>\n<blockquote>\n<p>Charles R. Twardy provides evidence that a course in argument mapping, using a particular software tool improves critical thinking. The improvement in critical thinking is measured by performance on a specific multiple choice test (California Critical Thinking Skills Test). This may not be the best way to measure rationality, but my point is that unlike almost everybody else, there was measurement and statistical improvement!\u2026</p>\n<p>To summarize my (clumsy) understanding of the activity of argument mapping: One takes a real argument in natural language. (op-eds are a good source of short arguments, philosophy is a source of long arguments). Then elaborate it into a tree structure, with the main conclusion at the root of the tree. The tree has two kinds of nodes (it is a bipartite graph). The root conclusion is a \u201cclaim\u201d node. Every claim node has approximately one sentence of English text associated. The children of a claim are \u201creasons\u201d, which do NOT have English text associated. The children of a reason are claims. Unless I am mistaken, the intended meaning of the connection from a claim\u2019s child (a reason) to the parent is implication, and the meaning of a reason is the conjunction of its children.</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"/lw/2x5/how_are_critical_thinking_skills_acquired_five/\">\u201cHow are critical thinking skills acquired? Five perspectives\u201d</a>:</p>\n<blockquote>\n<p>How are critical thinking skills acquired? Five perspectives: Tim van Gelder discusses acquisition of critical thinking skills, suggesting several theories of skill acquisition that don\u2019t work, and one with which he and hundreds of his students have had significant success. [LAMP/argument mapping]</p>\n</blockquote>\n</li>\n<li><a href=\"/lw/1qq/debate_tools_an_experience_report/\">\u201cDebate tools: an experience report\u201d</a></li>\n<li>\n<p><a href=\"/lw/51a/software_for_critical_thinking_prof_geoff_cumming/\">\u201cSoftware for Critical Thinking, Prof.&nbsp;Geoff Cumming\u201d</a></p>\n</li>\n</ul>\n<p>Relevant links:</p>\n<ul>\n<li><a href=\"/lw/76x/is_rationality_teachable/\">\u201cIs Rationality Teachable?\u201d</a></li>\n<li><a href=\"http://www.overcomingbias.com/2009/12/why-read-old-thinkers.html\">\u201cWhy Read Old Thinkers?\u201d</a></li>\n<li><a href=\"/lw/1ul/for_progress_to_be_by_accumulation_and_not_by/\">\u201cFor progress to be by accumulation and not by random walk, read great books\u201d</a></li>\n</ul>", "sections": [{"title": "1 Excerpts", "anchor": "1_Excerpts", "level": 1}, {"title": "2 See Also", "anchor": "2_See_Also", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NJ8k2RTwy3ELmwYZT", "ig4QYxoQHEDcRy8S5", "dJJYgmaYYFmHoQM4L", "CguJ3j39MRAnCsW8K", "H2zKAfiSJR6WJQ8pn", "ufBYjpi9gK6uvtkh5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-08T05:39:12.159Z", "modifiedAt": null, "url": null, "title": "Links about naturalism", "slug": "links-about-naturalism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:35.338Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "lukeprog", "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yRmft6DuNRZrDySwL/links-about-naturalism", "pageUrlRelative": "/posts/yRmft6DuNRZrDySwL/links-about-naturalism", "linkUrl": "https://www.lesswrong.com/posts/yRmft6DuNRZrDySwL/links-about-naturalism", "postedAtFormatted": "Sunday, July 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Links%20about%20naturalism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALinks%20about%20naturalism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyRmft6DuNRZrDySwL%2Flinks-about-naturalism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Links%20about%20naturalism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyRmft6DuNRZrDySwL%2Flinks-about-naturalism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyRmft6DuNRZrDySwL%2Flinks-about-naturalism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 259, "htmlBody": "<p>I'm about to launch a new website (as a personal project; it's got nothing to do with <a href=\"http://intelligence.org/\">SI</a> or <a href=\"http://appliedrationality.org/\">CFAR</a>) about <a href=\"http://en.wikipedia.org/wiki/Naturalism_(philosophy)\">naturalistic</a> views of the world, and that website will contain links to engaging and accessible books and articles about naturalistic approaches to:</p>\n<ul>\n<li>seeking truth</li>\n<li>the self &amp; free will</li>\n<li>ethics &amp; society</li>\n<li>happiness &amp; self-help</li>\n<li>meaning &amp; spirituality</li>\n</ul>\n<p>What links do other LWers recommend on these topics? To show you what kinds of articles I have in mind, here's what I have so far:</p>\n<p><em>Seeking Truth</em>:</p>\n<ul>\n<li>Yudkowsky, <a href=\"http://wiki.lesswrong.com/wiki/Map_and_Territory_(sequence)\">Map and Territory</a></li>\n<li>Yudkowsky, <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">How to Actually Change Your Mind</a></li>\n<li>Yudkowsky, <a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a></li>\n<li>Baron, <em><a href=\"http://www.amazon.com/Thinking-Deciding-Jonathan-Baron/dp/B0085SIQSC\">Thinking and Deciding</a></em></li>\n<li>Clark, <a href=\"http://naturalism.org/epistemology.htm#rivals\">Reality and Its Rivals: Putting Epistemology First</a></li>\n</ul>\n<p><br /> <em>The Self &amp; Free Will</em>:</p>\n<ul>\n<li>Clark, <a href=\"http://www.naturalism.org/determinism.htm\">Fully Cause: Coming to Terms with Determinism</a></li>\n<li>Yudkowsky, <a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution)\">Free Will (Solution)</a></li>\n<li>Thagard, <em><a href=\"http://www.amazon.com/Brain-Meaning-Life-Paul-Thagard/dp/0691154406/\">The Brain and the Meaning of Life</a></em></li>\n<li>Harris, <em><a href=\"http://www.amazon.com/Free-Will-Sam-Harris/dp/1451683405/\">Free Will</a></em></li>\n</ul>\n<p><br /> <em>Ethics and Society</em>:</p>\n<ul>\n<li>Muehlhauser, <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">No-Nonsense Metaethics</a></li>\n<li>Rachels, <em><a href=\"http://www.amazon.com/Elements-Moral-Philosophy-James-Rachels/dp/0073386715/\">The Elements of Moral Philosophy</a></em></li>\n<li>Alexander, <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">Efficient Charity: Do Unto Others...</a></li>\n</ul>\n<p><br /> <em>Happiness and Self-Help</em>:</p>\n<ul>\n<li>Muehlhauser, <a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">The Science of Winning at Life</a></li>\n<li>Wiseman, <em><a href=\"http://www.amazon.com/59-Seconds-Change-Minute-Vintage/dp/0307474860/\">59 Seconds</a></em></li>\n<li>Steel, <em><a href=\"http://www.amazon.com/Procrastination-Equation-Putting-Things-Getting/dp/B005CDTQ1O/\">The Procrastination Equation</a></em></li>\n<li>Dixit &amp; Nalebuff, <em><a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393337170/\">The Art of Strategy</a></em></li>\n</ul>\n<p><br /> <em>Meaning and Spirituality</em>:</p>\n<ul>\n<li>Thagard, <em><a href=\"http://www.amazon.com/Brain-Meaning-Life-Paul-Thagard/dp/0691154406/\">The Brain and the Meaning of Life</a></em></li>\n<li>Clark, <a href=\"http://www.naturalism.org/spiritua1.htm\">Spirituality Without Faith</a></li>\n<li>Comte-Sponville, <em><a href=\"http://www.amazon.com/The-Little-Book-Atheist-Spirituality/dp/0143114433/\">The Little Book of Atheist Spirituality</a></em></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yRmft6DuNRZrDySwL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 9.38269657226752e-07, "legacy": true, "legacyId": "17475", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pC47ZTsPNAkjavkXs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-08T06:50:48.724Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Should We Ban Physics?", "slug": "seq-rerun-should-we-ban-physics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:34.815Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CrnuGGZyZKcu7uD3H/seq-rerun-should-we-ban-physics", "pageUrlRelative": "/posts/CrnuGGZyZKcu7uD3H/seq-rerun-should-we-ban-physics", "linkUrl": "https://www.lesswrong.com/posts/CrnuGGZyZKcu7uD3H/seq-rerun-should-we-ban-physics", "postedAtFormatted": "Sunday, July 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Should%20We%20Ban%20Physics%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Should%20We%20Ban%20Physics%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCrnuGGZyZKcu7uD3H%2Fseq-rerun-should-we-ban-physics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Should%20We%20Ban%20Physics%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCrnuGGZyZKcu7uD3H%2Fseq-rerun-should-we-ban-physics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCrnuGGZyZKcu7uD3H%2Fseq-rerun-should-we-ban-physics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"/lw/se/should_we_ban_physics/\">Should We Ban Physics?</a> was originally published on 21 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There is a chance, however remote, that novel physics experiments could destroy the earth. Is banning physics experiments a good idea?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dh1/seq_rerun_existential_angst_factory/\">Existential Angst Factory</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CrnuGGZyZKcu7uD3H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.383032644357281e-07, "legacy": true, "legacyId": "17477", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rnk9gmWSrcqNfg7p8", "aHyqBCdfdc5TzsqRz", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-08T14:30:08.864Z", "modifiedAt": null, "url": null, "title": "Morality open thread", "slug": "morality-open-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:01.509Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/riCjSasFn4QwDNzgv/morality-open-thread", "pageUrlRelative": "/posts/riCjSasFn4QwDNzgv/morality-open-thread", "linkUrl": "https://www.lesswrong.com/posts/riCjSasFn4QwDNzgv/morality-open-thread", "postedAtFormatted": "Sunday, July 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Morality%20open%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMorality%20open%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FriCjSasFn4QwDNzgv%2Fmorality-open-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Morality%20open%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FriCjSasFn4QwDNzgv%2Fmorality-open-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FriCjSasFn4QwDNzgv%2Fmorality-open-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"></span></p>\n<p>I figure morality as a topic is popular enough and important enough and related-to-rationality enough to deserve its own thread.</p>\n<p>Questions, comments, rants, links, whatever are all welcome. If you're like me you've probably been aching to share your ten paragraph take on meta-ethics or whatever for about three uncountable eons now. Here's your chance.</p>\n<p>I recommend reading <a href=\"http://en.wikipedia.org/wiki/Meta-ethics\">Wikipedia's article on meta-ethics</a> before jumping into the fray, if only to get familiar with the standard terminology. The standard terminology is often abused. This makes some people sad. Please don't make those people sad.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2, "ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "riCjSasFn4QwDNzgv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "17478", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-08T16:03:14.779Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Thursday 7pm", "slug": "meetup-fort-collins-colorado-meetup-thursday-7pm-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.667Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tQLHSpL6B7E4bTTEy/meetup-fort-collins-colorado-meetup-thursday-7pm-1", "pageUrlRelative": "/posts/tQLHSpL6B7E4bTTEy/meetup-fort-collins-colorado-meetup-thursday-7pm-1", "linkUrl": "https://www.lesswrong.com/posts/tQLHSpL6B7E4bTTEy/meetup-fort-collins-colorado-meetup-thursday-7pm-1", "postedAtFormatted": "Sunday, July 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQLHSpL6B7E4bTTEy%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQLHSpL6B7E4bTTEy%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQLHSpL6B7E4bTTEy%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bq'>Fort Collins, Colorado Meetup Thursday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 July 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Freshly back from Less Wrong Central, copt and EvelynM have new games and stories of wizardry in the West.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bq'>Fort Collins, Colorado Meetup Thursday 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tQLHSpL6B7E4bTTEy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.385626005503738e-07, "legacy": true, "legacyId": "17480", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm\">Discussion article for the meetup : <a href=\"/meetups/bq\">Fort Collins, Colorado Meetup Thursday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 July 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Freshly back from Less Wrong Central, copt and EvelynM have new games and stories of wizardry in the West.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm1\">Discussion article for the meetup : <a href=\"/meetups/bq\">Fort Collins, Colorado Meetup Thursday 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-09T03:04:44.339Z", "modifiedAt": null, "url": null, "title": "Rationality Games & Apps Brainstorming", "slug": "rationality-games-and-apps-brainstorming", "viewCount": null, "lastCommentedAt": "2017-07-04T16:44:58.616Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CNZQesEh6Ts5G2r7b/rationality-games-and-apps-brainstorming", "pageUrlRelative": "/posts/CNZQesEh6Ts5G2r7b/rationality-games-and-apps-brainstorming", "linkUrl": "https://www.lesswrong.com/posts/CNZQesEh6Ts5G2r7b/rationality-games-and-apps-brainstorming", "postedAtFormatted": "Monday, July 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Games%20%26%20Apps%20Brainstorming&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Games%20%26%20Apps%20Brainstorming%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNZQesEh6Ts5G2r7b%2Frationality-games-and-apps-brainstorming%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Games%20%26%20Apps%20Brainstorming%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNZQesEh6Ts5G2r7b%2Frationality-games-and-apps-brainstorming", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNZQesEh6Ts5G2r7b%2Frationality-games-and-apps-brainstorming", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 694, "htmlBody": "<p>Last month, mobile gaming superstar <a href=\"http://en.wikipedia.org/wiki/Angry_Birds\">Angry Birds</a>&nbsp;was <a href=\"http://www.wired.com/geekdad/2012/06/dragonbox/all/\">out-sold in some countries</a>&nbsp;by <a href=\"http://dragonboxapp.com/\">DragonBox</a>, a kids game in which players <em>solve alegbra equations</em>.</p>\n<p>How does the game work? Jonathan Liu <a href=\"http://www.wired.com/geekdad/2012/06/dragonbox/all/\">explains</a>:</p>\n<blockquote>\n<p>There are five &ldquo;worlds,&rdquo; each with twenty levels, and as you progress through the levels the &ldquo;dragons&rdquo; hatch and grow into their full-sized versions. While this in itself has nothing to do with algebra, I mention this because my kids love this. It&rsquo;s a very tiny incentive (along with earning stars) but they really want to beat the next level to watch the dragon grow into its next form. I was told that the dragons were all drawn by a fourteen-year-old girl, and they&rsquo;re a lot of fun. (They aren&rsquo;t all typical dragons &mdash; One starts off more like a fish, one looks like a squid, and so on.)</p>\n<p>You are presented with a big screen with two trays, each containing a number of &ldquo;cards&rdquo; with different images on it. Somewhere on the screen there will be a little box with a star on it, sparkling and glowing. The app gives very minimal instructions in a hand-written font with arrows pointing to relevant spots on the screen, but it tells you to get the box by itself. At first you do this simply by tapping the green spirally cards, which vanish when you tap them. Then, you&rsquo;ll start to get some &ldquo;night&rdquo; versions of cards &mdash; drag these onto the &ldquo;day&rdquo; versions and they become green swirls, which you already know how to handle.</p>\n<p>After you&rsquo;ve gotten past several levels of moving cards around and tapping on swirls, you&rsquo;ll get a few cards down at the bottom which you can drag onto the trays &mdash; but whenever you drag a card onto one side, you have to also drag a copy to the other side as well. (This, of course, simulates adding the same number to both sides.) And then, a few levels on, you learn that you can flip these extra cards from day to night (and vice versa) before dragging them onto the trays.</p>\n<p>As the game progresses, you&rsquo;ll start seeing cards that are above and below each other, with a bar in the middle &mdash; and you&rsquo;ll learn to cancel these out by dragging one onto the other, which then turns into a one-dot. And you&rsquo;ll learn that a one-dot vanishes when you drag it onto a card it&rsquo;s attached to (with a little grey dot between them). These, of course, are fractions &mdash; multiplication and division &mdash; but you don&rsquo;t need to know that to play the game, either.</p>\n</blockquote>\n<p><img src=\"http://www.wired.com/geekdad/wp-content/uploads/2012/06/Dragonbox-division-660x495.jpg\" alt=\"\" /></p>\n<p>The key to DragonBox's success is not that it's the best algebra tutorial available, but rather that <em>it's actually fun for its target audience to play</em>.</p>\n<p>Others have noticed the potential of \"computer-assisted education\" before. Aubrey Daniels <a href=\"http://books.google.com/books?id=ZoxwDIc6nkwC&amp;lpg=PA143&amp;ots=q69a-1Qcwo&amp;dq=%22when%20you%20analyze%20the%20games%2C%20you%20will%20see%20that%20the%20player%20is%20clear%20about%20what%20is%20expected%22&amp;pg=PA143#v=onepage&amp;q&amp;f=false\">writes</a>:</p>\n<blockquote>\n<p>When you analyze [video games], you will see that the player is clear about what is expected of him, that [the] player's behavior is continually measured, and [that] the player is provided with feedback so he knows what the measurements reveal about his performance. Finally, and most importantly, as he plays the game, the player receives high rates of reinforcement which motivate him to play the game over and over again. In fact, reinforcement occurs up to 100 times a minute.</p>\n</blockquote>\n<p>Remember <a href=\"/lw/cu2/the_power_of_reinforcement/\">what works in reinforcement</a>:&nbsp;Small reinforcements are fine, but the reinforcer should <em>immediately</em> follow the target behavior, and it should be conditional on the specific behavior you want to strengthen.</p>\n<p>Video games are perfect for that! Little hits of reinforcement can be given many times a minute, conditional on exactly the kind of behavior your want to reinforce, and conditional on exactly the behavior you want to reinforce.</p>\n<p>DragonBox is just a particularly successful implementation of this insight.</p>\n<p>One of the goals for the <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a> is to develop <a href=\"http://appliedrationality.org/apps/\">rationality games and apps</a>. But it's tricky to think of how to make addictive games that actually teach rationality skills. So I'd like to provide a place for people to <strong>brainstorm ideas about what would make an addictive and instructive rationality game</strong>.</p>\n<p><small>See also: <a href=\"/lw/7od/rationality_and_video_games/\">Rationality and Video Games</a>, <a href=\"/lw/569/gamification_and_rationality_training/\">Gamification and Rationality Training</a>, <a href=\"/lw/8hp/raytheon_to_develop_rationalitytraining_games/\">Raytheon to Develop Rationality-Training Games</a>.</small></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "EdDGrAxYcrXnKkDca": 1, "fH8jPjHF2R27sRTTG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CNZQesEh6Ts5G2r7b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 42, "extendedScore": null, "score": 9.388733001824544e-07, "legacy": true, "legacyId": "17483", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GGn8MBiY8Xz6NdNdH", "7rhfWs3Ttw2XM9YrL", "ZhjqpQxPCnuthYqNo", "H4PskshEWw5i56pLM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-09T03:37:18.421Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Fake Norms, or \"Truth\" vs. Truth", "slug": "seq-rerun-fake-norms-or-truth-vs-truth", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:35.113Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PaGHGSRMP6AJJDb4S/seq-rerun-fake-norms-or-truth-vs-truth", "pageUrlRelative": "/posts/PaGHGSRMP6AJJDb4S/seq-rerun-fake-norms-or-truth-vs-truth", "linkUrl": "https://www.lesswrong.com/posts/PaGHGSRMP6AJJDb4S/seq-rerun-fake-norms-or-truth-vs-truth", "postedAtFormatted": "Monday, July 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Fake%20Norms%2C%20or%20%22Truth%22%20vs.%20Truth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Fake%20Norms%2C%20or%20%22Truth%22%20vs.%20Truth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPaGHGSRMP6AJJDb4S%2Fseq-rerun-fake-norms-or-truth-vs-truth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Fake%20Norms%2C%20or%20%22Truth%22%20vs.%20Truth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPaGHGSRMP6AJJDb4S%2Fseq-rerun-fake-norms-or-truth-vs-truth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPaGHGSRMP6AJJDb4S%2Fseq-rerun-fake-norms-or-truth-vs-truth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<p>Today's post, <a href=\"/lw/sf/fake_norms_or_truth_vs_truth/\">Fake Norms, or \"Truth\" vs. Truth</a> was originally published on 22 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Our society has a moral norm for applauding \"truth\", but actual truths get much less applause (this is a bad thing).</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dhh/seq_rerun_should_we_ban_physics/\">Should We Ban Physics?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PaGHGSRMP6AJJDb4S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.388886018799524e-07, "legacy": true, "legacyId": "17485", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zGJw9PGhu9e8Z6BEX", "CrnuGGZyZKcu7uD3H", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-09T05:01:50.955Z", "modifiedAt": null, "url": null, "title": "Nick Bostrom's TED talk and setting priorities", "slug": "nick-bostrom-s-ted-talk-and-setting-priorities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.702Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FAv3ZLPDPRG78dytY/nick-bostrom-s-ted-talk-and-setting-priorities", "pageUrlRelative": "/posts/FAv3ZLPDPRG78dytY/nick-bostrom-s-ted-talk-and-setting-priorities", "linkUrl": "https://www.lesswrong.com/posts/FAv3ZLPDPRG78dytY/nick-bostrom-s-ted-talk-and-setting-priorities", "postedAtFormatted": "Monday, July 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nick%20Bostrom's%20TED%20talk%20and%20setting%20priorities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANick%20Bostrom's%20TED%20talk%20and%20setting%20priorities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFAv3ZLPDPRG78dytY%2Fnick-bostrom-s-ted-talk-and-setting-priorities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nick%20Bostrom's%20TED%20talk%20and%20setting%20priorities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFAv3ZLPDPRG78dytY%2Fnick-bostrom-s-ted-talk-and-setting-priorities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFAv3ZLPDPRG78dytY%2Fnick-bostrom-s-ted-talk-and-setting-priorities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 393, "htmlBody": "<p>I just watched Nick Bostrom's TED talk titled <a href=\"http://www.youtube.com/watch?v=Yd9cf_vLviI\">\"Humanity's biggest problems aren't what you think they are.\"</a> I was expecting a talk giving Bostrom's take on what he thought the three biggest existential (or at least catastrophic) risks are, but instead, \"existential risk\" was just one item on the list. The other two were \"death\" and \"life isn't usually as wonderful as it could be.\"</p>\n<p>Putting these other two in the same category as \"existential risk\" seems like a mistake. This seems especially obvious in the case of (the present, normal rate of) death and existential risk. Bostrom's talk gives an annual death rate of 56 million, whereas if you take future generations into account, a 1% reduction in existential risk could save 10^32 lives.</p>\n<p>More importantly, if we screw up solving \"death\" and \"life isn't usually as wonderful as it could be\" in the next century, there will be other centuries where we can solve them. On the other hand, if we screw up existential risk in the next century, it means that's it, humanity's run will be over. There are no second chances when it comes to averting existential risk.</p>\n<p>One possible counter argument is that the sooner we solve \"death\"&nbsp;and \"life isn't usually as wonderful as it could be,\" the sooner we can start spreading our utopia throughout the galaxy and even to other galaxies, and with exponential growth a century head start on that could lead to a manyfold increase in the number of utils in the history of the universe.</p>\n<p>However, given the difficulties of building probes that travel at even a significant fraction of the speed of light, and the fact that colonizing new star systems may be a slow process even with advanced nanotech, a century may not matter much when it comes to colonizing the galaxy. Furthermore, colonizing the galaxy (or universe) may not be the sort of thing that follows an exponential curve, it may follow a cubic curve as probes spread out in a sphere.</p>\n<p>So I lean towards thinking that averting existential risks should be a much higher priority than creating a death-free, always wonderful utopia. Or maybe not. Either way, the answer would seem to be very important for questions like how we should focus our resources, and also whether your should push the button to turn on a machine that will allegedly create a utopia.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FAv3ZLPDPRG78dytY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "17486", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-09T12:24:42.257Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Calibration Games meetup", "slug": "meetup-washington-dc-calibration-games-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8TuitSYHJJngiwBJc/meetup-washington-dc-calibration-games-meetup", "pageUrlRelative": "/posts/8TuitSYHJJngiwBJc/meetup-washington-dc-calibration-games-meetup", "linkUrl": "https://www.lesswrong.com/posts/8TuitSYHJJngiwBJc/meetup-washington-dc-calibration-games-meetup", "postedAtFormatted": "Monday, July 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Calibration%20Games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Calibration%20Games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8TuitSYHJJngiwBJc%2Fmeetup-washington-dc-calibration-games-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Calibration%20Games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8TuitSYHJJngiwBJc%2Fmeetup-washington-dc-calibration-games-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8TuitSYHJJngiwBJc%2Fmeetup-washington-dc-calibration-games-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/br'>Washington DC Calibration Games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 July 2012 03:00:00AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Washington DC, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next meetup will be oxt Sunday, 7/22, at 3 pm, at Ben's apartment. Please check the email list (or PM me) if you need the address. The topic will be calibration games; making guesses with confidence intervals, and seeing if you're over or underconfident.</p>\n\n<p>I will be there, as will Maia. Ben is obviously going to be there as well.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/br'>Washington DC Calibration Games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8TuitSYHJJngiwBJc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.3913645510077e-07, "legacy": true, "legacyId": "17493", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Calibration_Games_meetup\">Discussion article for the meetup : <a href=\"/meetups/br\">Washington DC Calibration Games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 July 2012 03:00:00AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Washington DC, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next meetup will be oxt Sunday, 7/22, at 3 pm, at Ben's apartment. Please check the email list (or PM me) if you need the address. The topic will be calibration games; making guesses with confidence intervals, and seeing if you're over or underconfident.</p>\n\n<p>I will be there, as will Maia. Ben is obviously going to be there as well.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Calibration_Games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/br\">Washington DC Calibration Games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Calibration Games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Calibration_Games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Calibration Games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Calibration_Games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-09T13:31:26.337Z", "modifiedAt": null, "url": null, "title": "Meetup : Edinburgh summer meetup", "slug": "meetup-edinburgh-summer-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:57.062Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "scaphandre", "createdAt": "2011-09-21T11:22:42.167Z", "isAdmin": false, "displayName": "scaphandre"}, "userId": "gNFMMR5ygQQi4wfAq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M9BYFbJ5FcfmXehxj/meetup-edinburgh-summer-meetup", "pageUrlRelative": "/posts/M9BYFbJ5FcfmXehxj/meetup-edinburgh-summer-meetup", "linkUrl": "https://www.lesswrong.com/posts/M9BYFbJ5FcfmXehxj/meetup-edinburgh-summer-meetup", "postedAtFormatted": "Monday, July 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Edinburgh%20summer%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Edinburgh%20summer%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM9BYFbJ5FcfmXehxj%2Fmeetup-edinburgh-summer-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Edinburgh%20summer%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM9BYFbJ5FcfmXehxj%2Fmeetup-edinburgh-summer-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM9BYFbJ5FcfmXehxj%2Fmeetup-edinburgh-summer-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bs'>Edinburgh summer meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 July 2012 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Upstairs at Biblos, 1 Chambers Street, Edinburgh EH1 1HU</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's the summer months, many of the students are away and we have not had a proper Edinburgh meetup in a while. We will probably get a big table in <strong>Biblos</strong> on South Bridge on Friday lunchtime and be talking loudly about success or failure of procrastination avoidance plans. All welcome - come say hello.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bs'>Edinburgh summer meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M9BYFbJ5FcfmXehxj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 9.391678256681481e-07, "legacy": true, "legacyId": "17494", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Edinburgh_summer_meetup\">Discussion article for the meetup : <a href=\"/meetups/bs\">Edinburgh summer meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 July 2012 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Upstairs at Biblos, 1 Chambers Street, Edinburgh EH1 1HU</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's the summer months, many of the students are away and we have not had a proper Edinburgh meetup in a while. We will probably get a big table in <strong>Biblos</strong> on South Bridge on Friday lunchtime and be talking loudly about success or failure of procrastination avoidance plans. All welcome - come say hello.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Edinburgh_summer_meetup1\">Discussion article for the meetup : <a href=\"/meetups/bs\">Edinburgh summer meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Edinburgh summer meetup", "anchor": "Discussion_article_for_the_meetup___Edinburgh_summer_meetup", "level": 1}, {"title": "Discussion article for the meetup : Edinburgh summer meetup", "anchor": "Discussion_article_for_the_meetup___Edinburgh_summer_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-09T16:05:32.498Z", "modifiedAt": null, "url": null, "title": "[link] Cargo Cult Debugging", "slug": "link-cargo-cult-debugging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:52.179Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MarkL", "createdAt": "2012-05-31T16:30:38.379Z", "isAdmin": false, "displayName": "MarkL"}, "userId": "QfuP3eXpC4iGfQWg5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yBnPa4sKDZhFuQDA6/link-cargo-cult-debugging", "pageUrlRelative": "/posts/yBnPa4sKDZhFuQDA6/link-cargo-cult-debugging", "linkUrl": "https://www.lesswrong.com/posts/yBnPa4sKDZhFuQDA6/link-cargo-cult-debugging", "postedAtFormatted": "Monday, July 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Cargo%20Cult%20Debugging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Cargo%20Cult%20Debugging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyBnPa4sKDZhFuQDA6%2Flink-cargo-cult-debugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Cargo%20Cult%20Debugging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyBnPa4sKDZhFuQDA6%2Flink-cargo-cult-debugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyBnPa4sKDZhFuQDA6%2Flink-cargo-cult-debugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<blockquote>\n<p><span style=\"font-family: Palatino, 'Palatino Linotype'; font-size: 17px; line-height: 25px; \">[...] Here is the right way to address this bug:</span></p>\n<ol style=\"border: 0px; margin: 0px 0px 20px; padding: 0px; font-size: 17px; list-style-position: initial; font-family: Palatino, 'Palatino Linotype'; line-height: 25px; \">\n<li style=\"border: 0px; margin: 0px 0px 0px 30px; padding: 0px; \">Learn more about manifests, so I know what a good one looks like.</li>\n<li style=\"border: 0px; margin: 0px 0px 0px 30px; padding: 0px; \">Take a look at the one we&rsquo;re generating for Kiln; see if anything obvious screams out.</li>\n<li style=\"border: 0px; margin: 0px 0px 0px 30px; padding: 0px; \">If so, dive into the build system&nbsp;<em style=\"border: 0px; margin: 0px; padding: 0px; \">[blech]</em>&nbsp;and have it fix up the manifest, or generate a better one, or whatever&rsquo;s involved here. This part&rsquo;s a second black box to me, since the Kiln Storage Service is just a&nbsp;<a style=\"border: 0px; margin: 0px; padding: 0px; color: #960b0b; text-decoration: none; font-weight: bold; \" href=\"http://www.py2exe.org/\">py2exe</a>&nbsp;executable, meaning that we might be hitting a bug in py2exe, not our build system.</li>\n<li style=\"border: 0px; margin: 0px 0px 0px 30px; padding: 0px; \">If not, burn a Microsoft support ticket so I can learn how to get some more debugging info out of the error message.</li>\n</ol>\n<p style=\"border: 0px; margin: 0px 0px 20px; padding: 0px; font-size: 17px; font-family: Palatino, 'Palatino Linotype'; line-height: 25px; \">Here&rsquo;s the first thing I actually did:</p>\n<ol style=\"border: 0px; margin: 0px 0px 20px; padding: 0px; font-size: 17px; list-style-position: initial; font-family: Palatino, 'Palatino Linotype'; line-height: 25px; \">\n<li style=\"border: 0px; margin: 0px 0px 0px 30px; padding: 0px; \">Look at the executable using a dependency checker to see what DLLs it was using, then make sure they were present on Windows 2003.</li>\n</ol>\n<p style=\"border: 0px; margin: 0px 0px 20px; padding: 0px; font-size: 17px; font-family: Palatino, 'Palatino Linotype'; line-height: 25px; \">This is not the behavior of a rational man. [...]</p>\n</blockquote>\n<p style=\"border: 0px; margin: 0px 0px 20px; padding: 0px; font-size: 17px; font-family: Palatino, 'Palatino Linotype'; line-height: 25px; \"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; line-height: normal; background-color: #ffffff;\"><a href=\"http://bitquabit.com/post/cargo-cult-debugging/\">http://bitquabit.com/post/cargo-cult-debugging/</a></span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yBnPa4sKDZhFuQDA6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -9, "extendedScore": null, "score": 9.392402731890508e-07, "legacy": true, "legacyId": "17496", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-09T17:33:43.323Z", "modifiedAt": null, "url": null, "title": "Value of a Computational Process?", "slug": "value-of-a-computational-process", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.722Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4XffsAd4GgvHS7zyD/value-of-a-computational-process", "pageUrlRelative": "/posts/4XffsAd4GgvHS7zyD/value-of-a-computational-process", "linkUrl": "https://www.lesswrong.com/posts/4XffsAd4GgvHS7zyD/value-of-a-computational-process", "postedAtFormatted": "Monday, July 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value%20of%20a%20Computational%20Process%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue%20of%20a%20Computational%20Process%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4XffsAd4GgvHS7zyD%2Fvalue-of-a-computational-process%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value%20of%20a%20Computational%20Process%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4XffsAd4GgvHS7zyD%2Fvalue-of-a-computational-process", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4XffsAd4GgvHS7zyD%2Fvalue-of-a-computational-process", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 262, "htmlBody": "<p style=\"font-family: Times; font-size: medium;\">Consider a future person living a happy and fulfilling life. They're unfortunate enough to suffer a severe accident, but there's time to&nbsp;<a href=\"/lw/b93/brain_preservation/\">preserve</a>&nbsp;and then scan their brain fully, after which they can be brought back up in&nbsp;<a href=\"http://en.wikipedia.org/wiki/Emulator\">an emulator</a>&nbsp;on a computer. [1] It doesn't matter that they're now running on digital hardware instead of in a biological brain; they're still a person and they still&nbsp;<a href=\"http://www.jefftk.com/news/2012-04-30.html\">count</a>.</p>\n<p style=\"font-family: Times; font-size: medium;\">Now imagine this person or \"em\" asks to be let alone, cuts off all communication from the rest of the world, and rejoices privately in finally being able to fully explore their introverted nature. This isn't what I imagine myself doing, but is a choice I can respect.</p>\n<p style=\"font-family: Times; font-size: medium;\">Someone comes along and suggests turning off this person's emulation on the grounds that no one will know the difference, and we can use the hardware for something else. This seems wrong. Which means this computational process is valuable entirely for its own sake, independent of its effect on the world.</p>\n<p style=\"font-family: Times; font-size: medium;\">Unlike biological brains, computational processes are very flexible. We could run many copies, or run them much faster or slower than usual. We could run a specific segment of their independent experience repeatedly, perhaps the happiest few moments. It also seems unlikely that a full emulation of a human is the only thing that's valuable. Perhaps there are simpler patterns we could emulate that would be much better in terms of value per dollar?</p>\n<p style=\"font-family: Times; font-size: medium;\">I'm trying to&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Reductionism_%28sequence%29\">reduce</a>&nbsp;my concept of value and getting lots of strange questions.</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/news/2012-07-09.html\">on my blog</a></em></small></p>\n<p style=\"font-family: Times; font-size: medium;\"><br />[1] I think this will be possible, but&nbsp;<a href=\"/r/discussion/lw/88g/whole_brain_emulation_looking_at_progress_on_c/\">not for a while</a>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4XffsAd4GgvHS7zyD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 4, "extendedScore": null, "score": 9.392817334523906e-07, "legacy": true, "legacyId": "17497", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nk9928vPqoeAMrTh6", "XhHetxjWxZ6b85HK9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-09T18:52:42.271Z", "modifiedAt": null, "url": null, "title": "Advice On Getting A Software Job", "slug": "advice-on-getting-a-software-job", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cKyMoRotH7yGhTPun/advice-on-getting-a-software-job", "pageUrlRelative": "/posts/cKyMoRotH7yGhTPun/advice-on-getting-a-software-job", "linkUrl": "https://www.lesswrong.com/posts/cKyMoRotH7yGhTPun/advice-on-getting-a-software-job", "postedAtFormatted": "Monday, July 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20On%20Getting%20A%20Software%20Job&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20On%20Getting%20A%20Software%20Job%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcKyMoRotH7yGhTPun%2Fadvice-on-getting-a-software-job%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20On%20Getting%20A%20Software%20Job%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcKyMoRotH7yGhTPun%2Fadvice-on-getting-a-software-job", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcKyMoRotH7yGhTPun%2Fadvice-on-getting-a-software-job", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1187, "htmlBody": "<p><em>(Note to LWers: This post was written for a general audience at <a href=\"http://rationalconspiracy.com/2012/07/09/getting-a-software-job/\">my blog</a>, but I think it's particularly applicable to Less Wrong, as many here are already interested in programming. Programming is also an important route into two of the main paths to get rich, entrepreneurship and angel investing. Many of the leading donors to the Singularity Institute are professional programmers.) </em></p>\n<p>You&rsquo;ve already graduated college and found a job, but aren&rsquo;t making the money you&rsquo;d like. Or you live in the middle of nowhere. (Or, your job just sucks.) You&rsquo;re pretty smart, and want to do something about this. What should you do?</p>\n<p>One option is working as a computer programmer. Programming has a lot going for it: people tend to enjoy it, software companies have great perks, the work is usually in a laid-back atmosphere, and of course there&rsquo;s no manual labor. Programming salaries generally range from high five figures (just out of college) to mid six figures (for senior people, and quants at Wall Street banks). This assumes you live in a major city, so be sure to factor that into cost-of-living math. (If you aren&rsquo;t in a major city, seriously consider moving &ndash; most of the best jobs are there.)</p>\n<p>Before you apply, you&rsquo;ll need to learn how to program. To get started, there are lots of books on introductory programming &ndash; just search for &ldquo;Introduction to C&rdquo;, &ldquo;Introduction to Python&rdquo;, &ldquo;Introduction to Haskell&rdquo; and stuff like that. It&rsquo;s good to know at least one language well, and also have experience with a few others, preferably ones that differ in important ways. Once you&rsquo;ve learned the basics, there are lots of problems online to practice on. If you&rsquo;re into math, <a href=\"http://projecteuler.net/\">Project Euler</a> has a big, well-curated collection of them. You&rsquo;ll also want to know your way around Linux, since it&rsquo;s the main operating system of web servers; try <a href=\"http://www.debian.org/\">installing it</a>, and using it as your main OS for a few months.</p>\n<p>To actually get a programming job, you&rsquo;ll mainly need to demonstrate a) programming experience, and b) knowledge of computer science. For the first one, the most important thing is to build lots of real software that people use, since this teaches some vital practical skills you won&rsquo;t learn in books or college classes. Working on open source projects is an excellent way to build experience &ndash; check out the list of <a href=\"http://packages.debian.org/stable/\">Debian packages</a> for stuff people are currently doing. You should also get an account on <a href=\"https://github.com/\">Github</a>, browse some of the projects, and pay attention to the issues; notice how programmers talk about bugs and ideas for fixing them. It&rsquo;s sort of like being an artist &ndash; you want to have a portfolio of past work to show employers.</p>\n<p>As you learn to program, you&rsquo;ll discover that a majority of programming is debugging &ndash; discovering errors, and figuring out how to fix them. When you get an error (and you will get errors <em>constantly</em>), Google, Google, Google. Especially when working with open source software, just about any concrete problem you encounter has been dealt with before. If there&rsquo;s anything you want to learn to do, or an error message you don&rsquo;t understand, Googling should be a reflex.</p>\n<p>For the second part, a lot of interview questions ask about computer science concepts like linked lists, sorting, standard search algorithms, and so on. To learn these, you can usually read a standard freshman comp sci textbook all the way through, and do a few of each chapter&rsquo;s problems. It might be helpful to write out code on paper, as practice for job interviews where you might not have a computer.</p>\n<p>For most smart people, learning programming won&rsquo;t be terribly difficult&hellip; but it does require sitting down at a desk every day, and putting in the hours. If you aren&rsquo;t disciplined enough to work without a boss or a formal structure, you&rsquo;ll have to figure out how to solve that problem first. Try working on problems you care about, ones that inspire you. It can be really helpful to work with a more experienced programmer, especially in person &ndash; they&rsquo;ll review your work, correct your mistakes, and (more importantly) it&rsquo;ll make you feel like you have to get things done on time.</p>\n<p>Practicing programming isn&rsquo;t like studying for a test &ndash; anything that requires flash cards or lots of memorization is likely a waste of time. &ldquo;The best way to learn to program is by doing it.&rdquo; Don&rsquo;t memorize; build. If you have any task you find boring, annoying, or repetitive, see if you can write a program to do it instead. (This may require using other people&rsquo;s software. Eg. if a website makes you type in the same number ten thousand times, <a href=\"http://seleniumhq.org/\">Selenium</a> is your best friend.)</p>\n<p>A college degree isn&rsquo;t necessary, but it can be very useful, since a lot of larger companies will only hire people with degrees (especially if you lack previous experience). If you don&rsquo;t have a degree, try applying to smaller companies and startups, which have more flexible hiring procedures. Majoring in computer science and having a high GPA may help, but it won&rsquo;t get you a job by itself, and a lot of places don&rsquo;t care much. A lot of job postings will say things like &ldquo;X years experience in languages A, B and C required&rdquo; &ndash; ignore these, apply anyway. (Famously, one HR department once posted &ldquo;ten years of experience required&rdquo; for a language invented eight years ago.)</p>\n<p>Some words of caution: Don&rsquo;t work for anyone who offers you a full-time job without a decent salary (generally, at least $50,000) because you&rsquo;ll be &ldquo;paid in equity&rdquo;. This is almost certainly a scam. If you want to get experience or learn skills, and someone is working on a cool project but doesn&rsquo;t have money to pay you, do it on a volunteer basis so you won&rsquo;t be tempted to see it as a job.</p>\n<p>Also, don&rsquo;t quit your current job to be a programmer, unless you either a) have lots of professional programming experience, or b) have some other way to pay rent. Generally, finding programming jobs in a major city is fast and easy (1-2 months), but a lot of people overestimate their skills, and you don&rsquo;t want to run out of cash while discovering you aren&rsquo;t as good as you hoped. It isn&rsquo;t an overnight project; getting basic competence will take months, and true skill takes years.</p>\n<p>Lastly, like most fields, don&rsquo;t be afraid to apply to a zillion different jobs, and network, network, network. If you&rsquo;ve been working with any other programmers for a while, ask (once, politely) if they know where you can get a job; they&rsquo;ll likely have some ideas. Also, you might want to check out the <a href=\"http://news.ycombinator.com/item?id=4184755\">Who&rsquo;s Hiring</a> thread on Hacker News. Go forth, and build the future!</p>\n<p>(Disclaimer: This is based on my and my friends&rsquo; personal experiences. It isn&rsquo;t applicable to everyone, and the reader must be responsible for the consequences of any decisions they make.)</p>\n<p><em>This post was co-authored with Mike Blume, who went from having little programming knowledge as a physics grad student to being a software engineer at Loggly.</em></p>\n<p>Further resources: <a href=\"http://catb.org/%7Eesr/faqs/hacker-howto.html\">How To Become A Hacker</a>, <a href=\"http://steve-yegge.blogspot.com/2008/03/get-that-job-at-google.html\">Get That Job At Google</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1, "HFou6RHqFagkyrKkW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cKyMoRotH7yGhTPun", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 31, "extendedScore": null, "score": 9.39318500905397e-07, "legacy": true, "legacyId": "17498", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-09T22:16:02.312Z", "modifiedAt": null, "url": null, "title": "How Bayes' theorem is consistent with Solomonoff induction", "slug": "how-bayes-theorem-is-consistent-with-solomonoff-induction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.076Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alex_Altair", "createdAt": "2010-07-21T18:30:40.806Z", "isAdmin": false, "displayName": "Alex_Altair"}, "userId": "5wu9jG4pm9q6xjZ9R", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5pgsbB5sqC2wLwr4d/how-bayes-theorem-is-consistent-with-solomonoff-induction", "pageUrlRelative": "/posts/5pgsbB5sqC2wLwr4d/how-bayes-theorem-is-consistent-with-solomonoff-induction", "linkUrl": "https://www.lesswrong.com/posts/5pgsbB5sqC2wLwr4d/how-bayes-theorem-is-consistent-with-solomonoff-induction", "postedAtFormatted": "Monday, July 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20Bayes'%20theorem%20is%20consistent%20with%20Solomonoff%20induction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20Bayes'%20theorem%20is%20consistent%20with%20Solomonoff%20induction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pgsbB5sqC2wLwr4d%2Fhow-bayes-theorem-is-consistent-with-solomonoff-induction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20Bayes'%20theorem%20is%20consistent%20with%20Solomonoff%20induction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pgsbB5sqC2wLwr4d%2Fhow-bayes-theorem-is-consistent-with-solomonoff-induction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pgsbB5sqC2wLwr4d%2Fhow-bayes-theorem-is-consistent-with-solomonoff-induction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 443, "htmlBody": "<p>You've&nbsp;read the <a href=\"http://yudkowsky.net/rational/bayes/\">introduction to Bayes' theorem</a>. You've read the <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">introduction to Solomonoff induction</a>. Both describe fundamental theories of epistemic rationality. But how do they fit together?</p>\n<p>It turns out that it&rsquo;s pretty simple. Let&rsquo;s take a look at Bayes&rsquo; theorem.</p>\n<p><img src=\"http://www.codecogs.com/png.latex?P(H|E)=\\frac{P(E|H)P(H)}{\\sum_{i}P(E|H_i)P(H_i)}\" alt=\"\" /></p>\n<p>For a review:</p>\n<ul>\n<li>H is the particular hypothesis in question.</li>\n<li>E is the evidence, or data that we have observed.</li>\n<li>P(H) is the &ldquo;prior&rdquo; probability of the hypothesis alone.</li>\n<li>P(E|H) is the probability of seeing the evidence given that the hypothesis is true.</li>\n<li>P(H|E) is the probability that the hypothesis is true, given that you&rsquo;ve seen the evidence.</li>\n<li>H<sub>i</sub> is an arbitrary element in the set of all hypotheses.&nbsp;</li>\n</ul>\n<p>In terms of Solomonoff induction:</p>\n<ul>\n<li>H is the set of all binary sequences which we input to the universal Turing machine.</li>\n<li>E is the binary sequence of data that we are given to match.</li>\n<li>P(H) is <img src=\"http://www.codecogs.com/png.latex?2^{-l(H)}\" alt=\"\" />&nbsp;where l(H) is the length of the binary sequence H. This is a basic premise of Solomonoff induction.</li>\n<li>P(E|H) is the probability that we will see data sequence E, given that we run program H on the universal Turing machine. Because this is deterministic it is either 1, if H outputs E, or 0, if H does not output E.</li>\n<li>P(H|E) is still what we&rsquo;re looking for. Of course, if H does not output E, then P(E|H) = 0, which means P(H|E) = 0. This makes sense; if a program does not output the data you have, it cannot be the true program which output the data you have.</li>\n<li>H<sub>i</sub>&nbsp;is an arbitrary binary sequence.</li>\n</ul>\n<p>The denominator is the same meaning as the numerator, except as a sum for every possible hypothesis. This essentially normalizes the probability in the numerators. Any hypotheses that do not match the data E exactly will cause P(E|H<sub>i</sub>) = 0, and therefore that term will contribute nothing to the sum. If the hypothesis does output E exactly, then P(E|H<sub>i</sub>) = 1, and the matching hypothesis contributes its weight to the renormalizing sum in the denominator.</p>\n<p>Let's see an example with these things substituted. Here, the set of H<sub>i</sub> is the set of hypotheses that match.</p>\n<p><img src=\"http://www.codecogs.com/png.latex?P(H|E)=\\frac{2^{-l(H)}}{\\sum_{i}2^{-l(H_i)}}\" alt=\"\" /></p>\n<p>In summary; Bayes&rsquo; theorem says that once we find all matching hypotheses, we can find their individual probability by dividing their individual weight of <img src=\"http://www.codecogs.com/png.latex?2^{-l(H)}\" alt=\"\" />&nbsp;by the weights of all the matching hypotheses.</p>\n<p>This is intuitive, and matches Bayes&rsquo; theorem both mathematically and philosophically. Updating will occur when you get more bits of evidence E. This will eliminate some of the hypotheses&nbsp;H<sub>i</sub>, which will cause the renormalization in the denominator to get smaller.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5pgsbB5sqC2wLwr4d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 18, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "17499", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Kyc5dFDzBg4WccrbK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-09T23:11:44.697Z", "modifiedAt": null, "url": null, "title": "Muehlhauser-Hibbard Dialogue on AGI", "slug": "muehlhauser-hibbard-dialogue-on-agi", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.932Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cFP6iBezD4KCfXkpW/muehlhauser-hibbard-dialogue-on-agi", "pageUrlRelative": "/posts/cFP6iBezD4KCfXkpW/muehlhauser-hibbard-dialogue-on-agi", "linkUrl": "https://www.lesswrong.com/posts/cFP6iBezD4KCfXkpW/muehlhauser-hibbard-dialogue-on-agi", "postedAtFormatted": "Monday, July 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Muehlhauser-Hibbard%20Dialogue%20on%20AGI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMuehlhauser-Hibbard%20Dialogue%20on%20AGI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcFP6iBezD4KCfXkpW%2Fmuehlhauser-hibbard-dialogue-on-agi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Muehlhauser-Hibbard%20Dialogue%20on%20AGI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcFP6iBezD4KCfXkpW%2Fmuehlhauser-hibbard-dialogue-on-agi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcFP6iBezD4KCfXkpW%2Fmuehlhauser-hibbard-dialogue-on-agi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5988, "htmlBody": "<p><small>Part of the <a href=\"http://wiki.lesswrong.com/wiki/Muehlhauser_interview_series_on_AGI\">Muehlhauser series on AGI</a>.</small></p>\n<p><small><a href=\"http://lukeprog.com/\">Luke Muehlhauser</a>&nbsp;is Executive Director of the <a href=\"http://intelligence.org/\">Singularity Institute</a>, a non-profit research institute studying AGI safety.</small></p>\n<p><small><a href=\"http://www.ssec.wisc.edu/~billh/homepage1.html\">Bill Hibbard</a>&nbsp;is an emeritus senior scientist at University of Wisconsin-Madison and the author of <em><a href=\"http://www.amazon.com/Super-Intelligent-Machines-International-Systems-Engineering/dp/0306473887/\">Super-Intelligent Machines</a></em>.</small></p>\n<h2><br /></h2>\n<h2>Luke Muehlhauser:</h2>\n<p>[Apr. 8, 2012]</p>\n<p>Bill, I'm glad you agreed to discuss artificial general intelligence (AGI) with me. I hope our dialogue will be informative to many readers, and to us!</p>\n<p>On what do we agree? In separate conversations, Ben Goertzel and Pei Wang agreed with me on the following statements (though I've clarified the wording for our conversation):</p>\n<ol>\n<li>Involuntary death is bad, and can be avoided with the right technology.</li>\n<li>Humans can be enhanced by merging with technology.</li>\n<li>Humans are on a risky course in general, because powerful technologies can destroy us, humans often behave not in their own best interests, and we are unlikely to voluntarily halt technological progress.</li>\n<li>AGI is likely this century.</li>\n<li>AGI will greatly transform the world. It poses existential and other serious risks, but could also be the best thing that ever happens to us if we do it right.</li>\n<li>Careful effort will be required to ensure that AGI results in good things rather than bad things for humanity.</li>\n</ol>\n<p>You stated in private communication that you agree with these statements, so we have substantial common ground.</p>\n<p>I'd be curious to learn what you think about AGI safety. If you agree that AGI is an existential risk that will arrive this century, and if you value humanity, one might expect you to think it's very important that we accelerate AI <em>safety&nbsp;</em>research and decelerate AI <em>capabilities&nbsp;</em>research so that we develop <em>safe&nbsp;</em>superhuman AGI before we develop arbitrary superhuman AGI. (This is what Anna Salamon and I recommend in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a>.) What are your thoughts on the matter?</p>\n<p>And, which questions would <em>you&nbsp;</em>like to raise?</p>\n<p><a id=\"more\"></a></p>\n<h2>Bill Hibbard:</h2>\n<p>[Apr. 11, 2012]</p>\n<p>Luke, thanks for the invitation to this dialog and thanks for making my minor edits to your six statements. I agree with them, with the usual reservation that they are predictions and hence uncertain.</p>\n<p>Before I answer your other questions it would be useful to set a context by reference to an issue you and Anna raised in your excellent paper, Intelligence Explosion: Evidence and Import. In Section 4.1 you wrote:</p>\n<blockquote>\n<p>Unfortunately, specifying what humans value may be extraordinarily difficult, given the complexity and fragility of human preferences (Yudkowsky 2011; Muehlhauser and Helm, this volume), and allowing an AI to learn desirable goals from reward and punishment may be no easier (Yudkowsky 2008a). If this is correct, then the creation of self-improving AI may be detrimental by default unless we first solve the problem of how to build an AI with a stable, desirable utility function - a \"Friendly AI\" (Yudkowsky 2001).</p>\n</blockquote>\n<p>I think this is overly pessimistic about machines learning human values. We humans are good at learning what other humans value, if we are close to them for a long period of time. It is reasonable to think that machines much more intelligent than humans will be able to learn what humans value.</p>\n<p>Solomonoff induction and AIXI are about algorithms learning accurate world models. Human values are part of the world so the AIXI world model includes a model of the values of humans. &nbsp;Yudkowsky's CEV assumed that a future machine (called a Really Powerful Optimization Process) could learn the values of humans, and could even predict the values of humans assuming evolution to some convergent limit.</p>\n<p>I am less worried about an AI exterminating all humans because it cannot learn our values, and more worried about AI that accurately knows the values of its wealthy and powerful human masters and dominates the rest of humanity for its masters' benefit. I think the political problems of AI safety are more difficult than the technical problems.</p>\n<p>My JAGI paper still under review, Model-based Utility Functions, is an effort to help with the technical problems of AI safety. So I do take those problems seriously. But I am more worried about the political problems than the technical problems.</p>\n<p>In your Conclusion, you and Anna wrote:</p>\n<blockquote>\n<p>... and achieving economic stability is ultimately a problem of being smart enough to figure out how to achieve it.</p>\n</blockquote>\n<p>To a significant extent we know how to achieve economic stability, by limiting debt/equity ratios for individuals and for financial institutions, and enforcing accounting standards that prohibit hiding debt and exaggerating equity. But there are serious political problems in imposing those limits and standards.</p>\n<p>Now to your question about the rates of research on AI safety versus AI capabilities. My only previous expression about this issue has been a desire to accelerate the public's understanding of the social issues around AI, and their exercise of democratic control over the technology to ensure that it benefits everyone. For example, AI will enable machines to produce all the goods and services everyone needs, so we should use AI to eliminate poverty rather than causing poverty by depriving people of any way to earn a living. This is a political issue.</p>\n<p>Even if you disagree with me about the relative importance of technical versus political problems with AI, I think you will still need politics to achieve your goals. To accelerate AI safety research and decelerate AI capabilities research on a global scale will require intense political effort. And once we think we know how to build safe AI, it will be a political problem to enforce this knowledge on AI designs.</p>\n<p>I do agree about the need to accelerate AI safety research and think that the AGI conferences are very useful for that goal. However, the question of decelerating AI capability research is more difficult. AI will be useful in the struggle against poverty, disease and aging if we get the politics right. That argues for accelerating AI capabilities to alleviate human suffering. On the other hand, AI before the technical and political problems are solved will be a disaster. I think the resolution of this dilemma is to recognize that it will be impossible to decelerate AI capabilities, so any effort expended on that goal could be better applied to the political and technical problems of AI safety.</p>\n<p>The struggle for wealth and power in human society, and the struggle to improve people's lives, are very strong forces accelerating AI capabilities. Organizations want to cut costs and improve performance by replacing human workers by machines. Businesses want to improve their services by sensing and analyzing huge amounts of consumer and other social data. Investors want to gain market advantage through all sorts of information processing. Politicians want to sense and analyze social data in order to gain political power. Militaries want to gain advantages by sensing and processing battlefield (which increasingly includes civil society) information. Medical researchers want to understand and cure diseases by sensing and analyzing enormously complex biological processes. There is a long list of strong social motives for accelerating AI capabilities, so I think it will be impossible to decelerate AI capabilities.</p>\n<p>The question I'd raise is: How can we educate the public about the opportunities and dangers of AI, and how can we create a political movement in favor of AI benefitting all humans? The Singularity Institute's annual summits are useful for educating the public.</p>\n<h2><br /></h2>\n<h2>Luke:</h2>\n<p>[Apr 13th, 2012]</p>\n<p>I'm not sure how much we disagree about machines learning human values. That is, as you note, how <a href=\"http://intelligence.org/upload/CEV.html\">CEV</a>&nbsp;might work: a seed AI learns our values, extrapolates them, and uses these extrapolated values to build a utility function for a successor superintelligence. I just don't think naive neural net approaches as in <a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=1667949&amp;contentType=Journals+%26+Magazines&amp;sortType%3Dasc_p_Sequence%26filter%3DAND%28p_IS_Number%3A34917%29\">Guarini (2006)</a>&nbsp;will work, for reasons explained in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>.</p>\n<p>So how do we program an AI to \"Figure out what the humans want and do that\"? That, I think, is a Very Hard Technical Problem that breaks down into <a href=\"http://lukeprog.com/SaveTheWorld.html\">lots of difficult sub-problems</a>. I'm glad you're one of the relatively few people working in this problem space, e.g. with <a href=\"http://arxiv.org/pdf/1111.3934v1.pdf\">Model-Based Utility Functions</a>.</p>\n<p>I share your concerns about the political problems related to powerful AGI. That, too, is a Very Hard Problem. I'm not sure whether the technical or political problems are more difficult.</p>\n<p>I agree that economic and political incentives will make it very difficult to decelerate AGI progress. But it's not impossible. Here are just two ideas:</p>\n<p><em>Persuade key AGI researchers of the importance of safety.</em> The most promising AGI work is done by a select few individuals. It may be possible to get them to <a href=\"/lw/bob/reframing_the_problem_of_ai_progress/\">grok</a>&nbsp;that AGI capabilities research merely brings forward the date by which we must&nbsp;solve these Very Hard Problems, and it's already unclear whether we'll be able to solve them in time. Researchers have changed their minds before about the moral value of their work. <a href=\"http://en.wikipedia.org/wiki/Joseph_Rotblat\">Joseph Rotblat</a>&nbsp;resigned from the Manhattan Project due to conscience. <a href=\"http://en.wikipedia.org/wiki/Le%C3%B3_Szil%C3%A1rd\">Leo Szilard</a>&nbsp;switched from physics to molecular biology when he realized the horror of atomic weapons. <a href=\"http://www.michaelagmichaud.com/\">Michael Michaud</a>&nbsp;resigned from SETI in part because of concerns about the danger of contacting advanced civilizations. And of course, Eliezer Yudkowsky <a href=\"http://extropians.weidai.com/extropians.1Q99/3561.html\">once argued</a>&nbsp;that we should bring about the Singularity as quickly as possible, and then <a href=\"http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age\">made a complete U-Turn</a>. If we can change the minds of a few key AGI scientists, it may be that key insights into AGI are delayed by years or decades. Somebody would have come up with General Relativity if Einstein hadn't, but we may have had to wait a few decades.</p>\n<p><em>Get machine superintelligence treated as a weapon of mass destruction.</em> This could make it more difficult to create machine superintelligence. On the other hand, if we got governments to take machine superintelligence this seriously, our actions might instead trigger an arms race and accelerate&nbsp;progress toward machine superintelligence.</p>\n<p>But yes: it will be easier to accelerate AGI safety research, which means doing things like (1) raising awareness, (2) raising funds for those doing AGI safety research, and (3) drawing more brainpower to the relevant research problems.</p>\n<p>Finally, you asked:</p>\n<blockquote>\n<p>How can we educate the public about the opportunities and dangers of AI, and how can we create a political movement in favor of AI benefitting all humans?</p>\n</blockquote>\n<p>I hope that deliverables like the <a href=\"http://singularitysummit.com/\">Singularity Summit</a>&nbsp;and<em> <a href=\"http://facingthesingularity.com/\">Facing the Singularity</a></em>&nbsp;go some distance toward educating the public. But I'm not sure how much this helps. If we educate the masses, are they capable of making wise decisions with that kind of complex information? History, I think, suggests the answer may be \"<a href=\"http://en.wikipedia.org/wiki/Extraordinary_Popular_Delusions_and_the_Madness_of_Crowds\">No</a>.\" Unfortunately, intelligent and well-educated experts may think <a href=\"http://www.scientificamerican.com/article.cfm?id=rational-and-irrational-thought\">just as poorly</a>&nbsp;as laymen <a href=\"http://intelligence.org/upload/cognitive-biases.pdf\">about these issues</a>.</p>\n<p>Do you have your own thoughts on how to reduce the severity of the political problem?</p>\n<p>Also: I think my biggest problem is there not being enough good researchers working on <a href=\"http://lukeprog.com/SaveTheWorld.html\">the sub-problems of AGI safety</a>. Do you have suggestions for how to attract more brainpower to the field?</p>\n<h2><br /></h2>\n<h2>Bill:</h2>\n<p>[16 April 2012]</p>\n<p>Luke, here are responses to a few questions from your message, starting with:</p>\n<blockquote>\n<p>I'm not sure how much we disagree about machines learning human values. That is, as you note, how CEV might work: a seed AI learns our values, extrapolates them, and uses these extrapolated values to build a utility function for a successor superintelligence. I just don't think naive neural net approaches as in Guarini (2006) will work, for reasons explained in The Singularity and Machine Ethics.</p>\n</blockquote>\n<p>I'm glad we don't have much to disagree about here and I certainly do not advocate a naive neural net approach. Currently we do not know how to build intelligent machines. When we do then we can apply those machines to learning human values. If a machine is sufficiently intelligent to pose an existential threat to humanity then it is sufficiently intelligent to learn human values.</p>\n<blockquote>\n<p>If we educate the masses, are they capable of making wise decisions with that kind of complex information? History, I think, suggests the answer may be \"No.\" Unfortunately, intelligent and well-educated experts may think just as poorly as laymen about these issues. &nbsp;</p>\n</blockquote>\n<p>Yes, the lack of wisdom by the public and by experts is frustrating. It is good that the Singularity Institute has done so much to describe cognitive biases that affect human decision making.</p>\n<p>On the other hand, there have been some successes in the environmental, consumer safety, civil rights and arms control movements. I'd like to see a similar political movement in favor of AI benefitting all humans. Perhaps this movement can build on current social issues linked to information technology, like privacy and the economic plight of people whose skills become obsolete in our increasingly automated and networked economy.</p>\n<blockquote>\n<p>Do you have your own thoughts on how to reduce the severity of the political problem?</p>\n</blockquote>\n<p>One thing I'd like to see is more AI experts speaking publicly about the political problem. When they get an opportunity to speak to a wide audience on TV or other mass venue, they often talk about the benefits without addressing the risks. I discussed this issue in an <a href=\"http://hplusmagazine.com/2011/03/03/when-future-watsons-play-politics/\">H+ Magazine article</a>. My article specifically noted the <a href=\"http://research.microsoft.com/en-us/um/people/horvitz/note_from_AAAI_panel_chairs.pdf\">2009 Interim Report</a>&nbsp;of the AAAI Presidential Panel on Long-Term AI Futures, Ray Kurzweil's book <a href=\"http://www.amazon.com/gp/product/0143037889?ie=UTF8&amp;tag=hma0b-20&amp;linkCode=as2&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0143037889\">The Singularity is Near</a>, and Jaron Lanier's 2010 <a href=\"http://www.nytimes.com/2010/08/09/opinion/09lanier.html?_r=2\">New York Times OP-ED</a>. In my opinion each of these seriously underestimated the risks of AI. Of course these experts are entitled to their own opinions. However, I believe they discount the risk because they only consider the risk of AI escaping all human control, what we are calling the technical risk, and which they do not take seriously. I think these experts do not consider the risk that AI will enable a small group of humans to dominate the rest of humanity, what I call the political risk. I have tried to raise the political risk in letters to the editor of the New York Times and in my other publications, but I have not been very effective in reaching the broader culture.</p>\n<p>I'd also like to see the Singularity Institute address the political as well as the technical risks. Making you the Executive Director is hopefully a step in the right direction.</p>\n<blockquote>\n<p>Also: I think my biggest problem is there not being enough good researchers working on the sub-problems of AGI safety. Do you have suggestions for how to attract more brainpower to the field?</p>\n</blockquote>\n<p>That is an excellent question. The mathematical theory of AGI seems to have taken off since Hutter's AIXI in 2005. Now the AGI conferences and JAGI are providing respectable publication venues specifically aimed at the mathematical theory of AGI. So hopefully smart young researchers will be inspired and see opportunities to work in this theory, including AGI safety.</p>\n<p>And if more AI experts will speak about the risks of AI in the broader culture, that would probably also inspire young researchers, and research funding agencies, to focus on AGI safety.</p>\n<h2><br /></h2>\n<h2>Luke:</h2>\n<p>[Apr. 16, 2012]</p>\n<p>It looks to me like we <em>do&nbsp;</em>disagree significantly about the difficulty of getting a machine to learn human values, so let me ask you more about that. You write:</p>\n<blockquote>\n<p>Currently we do not know how to build intelligent machines. When we do then we can apply those machines to learning human values. If a machine is sufficiently intelligent to pose an existential threat to humanity then it is sufficiently intelligent to learn human values.</p>\n</blockquote>\n<p>I generally agree that superintelligent machines capable of destroying humanity will be capable of learning human values and maximizing for them <em>if&nbsp;</em>\"learn human values and maximize for them\" is a coherent request at all. But capability does not imply motivation. The problem is that human extinction is a <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">convergent outcome</a>&nbsp;of billions of possible goal systems in superintelligent AI, whereas getting the first superintelligent AI to learn human values and maximize for them is like figuring out how to make the very first atomic bomb explode in the shape of an elephant.</p>\n<p>To illustrate, consider the <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">Golem Genie scenario</a>:</p>\n<blockquote>\n<p>Suppose an unstoppably powerful genie appears to you and announces that it will return in fifty years. Upon its return, you will be required to supply it with a [utility function] which it will then enforce with great precision throughout the universe. For example, if you supply the genie with hedonistic utilitarianism, it will maximize pleasure by harvesting all available resources and using them to tile the universe with identical copies of the smallest possible mind each copy of which will experience an endless loop of the most pleasurable experience possible.</p>\n<p>Let us call this precise, instruction-following genie a Golem Genie. (A golem is a creature from Jewish folklore that would in some stories do exactly as told... often with unintended consequences, for example polishing a dish until it is as thin as paper...)</p>\n<p>If by the appointed time you fail to supply your Golem Genie with a [utility function], then it will permanently model its goal system after the first logically coherent [utility function] that anyone articulates to it, and that&rsquo;s not a risk you want to take. Moreover, once you have supplied the Golem Genie with its [utility function], there will be no turning back. Until the end of time, the genie will enforce that one [utility function] without exception, not even to satisfy its own (previous) desires.</p>\n</blockquote>\n<p>So let's say you want to specify \"Learn human values and maximize that\" as a utility function. How do you do that? That seems <em>incredibly</em>&nbsp;hard to me, for reasons I can explore in detail if you wish.</p>\n<p>And that's not where <em>most&nbsp;</em>of the difficulty is, either. Most of the difficulty is in building an AI capable of executing that kind of utility function in the first place, as opposed to some kluge or reinforcement learner or something that isn't even capable of taking goals like that and therefore isn't even capable&nbsp;of doing what we want when given superintelligence.</p>\n<h2><br /></h2>\n<h2>Bill:</h2>\n<p>[Apr. 20, 2012]</p>\n<p>Your Golem Genie poses an interesting dilemma. But \"hedonistic utilitarianism\" is not an accurate summary of human values. Reasonable humans would not value a history that converges on the universe tiled with tiny minds enjoying constant maximum pleasure and neither would an AI implementing an accurate model of human values.</p>\n<p>But any summary of values that a human could write down would certainly be inaccurate. Consider an analogy with automatic language processing. Smart linguists have spent generations formulating language rules, yet attempts to automate language processing based on those rules have worked poorly. Recent efforts at language processing by automated statistical learning of rules from large amounts of actual human language use have been more successful (although still inaccurate because they are not yet truly intelligent). It seems that humans cannot write down rules that accurately model their behavior, including rules that model their values.</p>\n<p>I do agree with you that there are tricky problems in getting from human values to a utility function, although you probably think it is a bigger, more dangerous problem than I do. The issues include:</p>\n<ol>\n<li>A person's value preferences often violate the laws of probability, making it impossible to express their preferences as a utility function.</li>\n<li>A person's preferences vary over time depending on the person's experiences (e.g., propaganda works).</li>\n<li>Different people have different preferences, which need to be combined into a single utility function for an AI. Of course this only needs to be done if the AI serves more than one person. If the AI serves a single person or a small group that is what I call the political problem.</li>\n</ol>\n<p>While these are all tricky problems, every reasonable person assigns maximal negative utility value to any history that includes human extinction. It seems reasonable that a solution for deriving an AI utility function from human values should not cause human extinction.</p>\n<p>I think the most difficult issue is the strong human tendency toward group identities and treating people differently based on group membership. There are groups of people who wish the extinction or subjugation of other groups of people. Filtering such wishes out of human values, in forming AI utility functions, may require some sort of \"semantic surgery\" on values (just to be clear, this \"semantic surgery\" consists of changes to the values being processed into an AI utility function, not any changes to the people holding those values).</p>\n<p>I am familiar with the concern that an AI will have an instrumental sub-goal to gather resources in order to increase its ability to maximize its utility function, and that it may exterminate humans in order to reuse the atoms from their bodies as resources for itself. However, any reasonable human would assign maximum negative value to any history that includes exterminating humans for their atoms and so would a reasonable utility function model of human values. An AI motivated by such a utility function would gather atoms only to the extent that such behavior increases utility. Gathering atoms to the extent that it exterminates humans would have maximum negative utility and the agent wouldn't do it.</p>\n<p>In your recent message you wrote:</p>\n<blockquote>\n<p>... as opposed to some kluge or reinforcement learner or something that isn't even capable of taking goals like that and therefore isn't even capable of doing what we want when given superintelligence.</p>\n</blockquote>\n<p>I am curious about your attitude toward reinforcement learning. Here you disparage it, but in Section 2.4 of Intelligence Explosion: Evidence and Import you discuss AIXI as an important milestone towards AI and AIXI is a reinforcement learner.</p>\n<p>One problem with reinforcement learning is that it is commonly restricted to utility functions whose values are rewards sent to the agent from the environment. I don't think this is a reasonable model of the way that real-world agents compute their utility functions. (Informally I've used reinforcement learning to refer to any agent motivated to maximize sums of discounted future utility function values, but in my mathematical papers about AGI theory I try to conform to standard usage.) The AGI-11 papers of Dewey, and Ring and Orseau also demonstrate problems with reinforcement learning. These problems can be avoided using utility function definitions that are not restricted to rewards from the environment to the agent.</p>\n<h2><br /></h2>\n<h2>Luke:</h2>\n<p>[Apr. 20, 2012]</p>\n<p>We seem to agree that \"any summary of values that a human could write down would certainly be inaccurate\" (including, e.g., hedonistic utilitarianism). You have nicely summarized some of the difficulties we face in trying to write a utility function that captures human values, but if you explained why this problem is not&nbsp;going to be as big a problem as I&nbsp;am saying it is, then I missed it. You also seem to agree that convergent instrumental goals for (e.g.) resource acquisition pose an existential threat to humanity (as argued by <a href=\"http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/\">Omohundro 2008</a>&nbsp;and <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">Bostrom 2012</a>), and again if you have a solution in mind, I missed it. So I'm not sure why you're less worried about the technical problems than I am.</p>\n<p>As for reinforcement learners: I don't \"disparage\" them, I just say that they aren't capable of taking human-friendly goals. Reinforcement learning is indeed an important advance in AI, but we can't build a Friendly AI from a reinforcement learner (or from other existing AI architectures), which is why AI by default will be disastrous for humans. As <a href=\"http://intelligence.org/upload/learning-what-to-value.pdf\">Dewey (2011)</a>&nbsp;explains:</p>\n<blockquote>\n<p>Reinforcement learning can only be used in the real world to define agents whose goal is to maximize expected rewards, and since this goal does not match with human goals, AGIs based on reinforcement learning will often work at cross-purposes to us. To solve this problem, we define <em>value learners</em>, agents that can be designed to learn and maximize any initially unknown utility function so long as we provide them with an idea of what constitutes evidence about that utility function.</p>\n</blockquote>\n<p>(Above, I am using \"reinforcement learner\" in the traditional, narrower sense.)</p>\n<p>Back to the \"technical problem\" of defining a human-friendly utility function and building an AI architecture capable of maximizing it. Do you have a sense for why you're less worried about the difficulty of that problem than I am?</p>\n<h2><br /></h2>\n<h2>Bill:</h2>\n<p>[April 23, 2012]</p>\n<p>I think our assessments of the relative danger from the technical versus the political problem come from our different intuitions. I do not have a proof of a solution to the technical problem. If I did, I would publish it in a paper.</p>\n<p>In your message of 16 April 2012 you wrote:</p>\n<blockquote>\n<p>So let's say you want to specify \"Learn human values and maximize that\" as a utility function. How do you do that? That seems incredibly hard to me, for reasons I can explore in detail if you wish.</p>\n</blockquote>\n<p>Yes Please. I'd be interested if you'd like to explain your reasons.</p>\n<h2><br /></h2>\n<h2>Luke:</h2>\n<p>[May 1, 2012]</p>\n<p>I tried to summarize some aspects of the technical difficulty of Friendly AI in \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>.\"</p>\n<p>But let's zoom in on the proposal to specify P&nbsp;as a utility function in an AI, where P&nbsp;= \"Learn human values and maximize their satisfaction.\" There are two types of difficulty we might face when trying to implement this project:</p>\n<ol>\n<li>We know what P&nbsp;means but we aren't sure how to make it precise enough to encode it in a utility function.</li>\n<li>We are fundamentally, philosophically confused about what P&nbsp;means, and&nbsp;we aren't sure how to make it precise enough to encode it in a utility function.</li>\n</ol>\n<p>I think we are in situation #2, which is why the technical difficulty of Friendly AI is so hard. At the moment, it looks like we'd need to solve huge swaths of philosophy before being able to build Friendly AI, and that seems difficult because most philosophical questions aren't considered to be \"solved\" after 2.5 millennia of work.</p>\n<p>For example, consider the following questions:</p>\n<ol>\n<li>What counts as a \"human\"? Do those with brain injuries or neurological diseases count for these purposes? Do infants? Do anencephalic infants? If we use DNA to <a href=\"http://www.nytimes.com/2009/02/13/science/13neanderthal.html?_r=1\">resurrect</a>&nbsp;the strands of <em>Homo&nbsp;</em>very similar to the modern human, do those people count? Suppose we can radically augment the human mind with cognitive enhancements and brain-computer interfaces in the next few decades&mdash;do heavily augmented humans count? If we get mind uploading before AI, do uploaded humans count?</li>\n<li>What are human \"values\"? Are we talking about the <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">value encodings</a>&nbsp;in the dopaminergic reward system and perhaps elsewhere? (If so, we're going to <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">have trouble</a>&nbsp;translating that into anything like a coherent utility function.) Are we talking about some kind of <a href=\"http://intelligence.org/upload/CEV.html\">extrapolation</a>&nbsp;or <a href=\"/lw/c3t/stanovich_on_cev/\">rational integration</a>&nbsp;of our values? (If so, what's the algorithm for this extrapolation or rational integration?)</li>\n<li>\"Learn human values and maximize their satisfaction\" seems to assume the aggregation of the preferences of multiple humans. How do we do that?</li>\n</ol>\n<p>Since <em>I</em>&nbsp;don't know what I \"mean\" by these things, I sure as hell can't tell an AI what I mean. I couldn't even describe it in conversation, let alone encode it precisely in a utility function. Occasionally, creative solutions to the problem are proposed, like <a href=\"/lw/c0k/formalizing_value_extrapolation/\">Paul Christiano's</a>, but they only nascent proposals and suffer major difficulties of their own.</p>\n<h2><br /></h2>\n<h2>Bill:</h2>\n<p>[May 9, 2012]</p>\n<p>Yes, there are technical problems of expressing a person's values as a utility function, normalizing utility functions between different people, and combining the utility functions of different people to derive an overall AI utility function.</p>\n<p>However, every reasonable person assigns maximally negative utility to human extinction, so over a broad range of solutions to these technical problems, the overall AI utility function will assign maximally negative value to any future history that includes human extinction.</p>\n<p>Do you agree with this?</p>\n<p>If you do agree, how does this lead to a high probability of human extinction?</p>\n<h2><br /></h2>\n<h2>Luke:</h2>\n<p>[June 10, 2012]</p>\n<p>Bill, you acknowledge the technical difficulty of expressing a person's values as a utility function (see the literature on \"<a href=\"/lw/a73/a_brief_tutorial_on_preferences_in_ai/\">preference acquisition</a>\"), and of combining many utility functions to derive a desirable AI utility function (see the literatures on \"<a href=\"/lw/a73/a_brief_tutorial_on_preferences_in_ai/\">population ethics</a>\" and \"<a href=\"http://en.wikipedia.org/wiki/Social_choice_theory\">social choice</a>\"). But if I understand you correctly, you seem to think that:</p>\n<blockquote>\n<p>P: almost <em>any&nbsp;</em>AI utility function resulting from an attempt to combine the preferences of humans will assign maximally negative utility to any future history that includes human extinction, because humans are agreed on the badness of human extinction.</p>\n</blockquote>\n<p>I think proposition P&nbsp;is false, for several reasons:</p>\n<p>First: Most people think there are fates worth than death, so a future including human extinction wouldn't be assigned <em>maximally</em>&nbsp;negative utility.</p>\n<p>Second: Even ignoring this, many people think human extinction would be better than the status quo (at least, if you human extinction could occur without much suffering). See Benatar's <em><a href=\"http://www.amazon.com/Better-Never-Have-Been-Existence/dp/0199549265/\">Better Never to Have Been</a></em>. Many negative utilitarians agree with Benatar's conclusion, if not his reasoning. The surest way to minimize conscious suffering is to eliminate beings capable of conscious suffering.</p>\n<p>Third: There are many problems similar to problem #1 I gave above (\"What counts as a 'human'?\"). Suppose we manage to design an AI architecture that has preferences over states of affairs (or future histories) as opposed to preferences over a reward function, and we define a utility function for the AI that might be sub-optimal in many ways but \"clearly\" assigns utility 0 (or negative 1m, or whatever) to human extinction. We're still left with the problems of specification explained in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics.</a>&nbsp;A machine superintelligence would possess two features that make it dangerous to give it any utility function at all:</p>\n<ol>\n<li><em>Superpower</em>: A machine superintelligence would have unprecedented powers to reshape reality, and would therefore achieve its goals with highly efficient methods that confound human expectations (e.g. it could \"maximize pleasure\" by tiling the universe with trillions of digital minds running a loop of a single pleasurable experience).</li>\n<li><em>Literalness</em>: A machine superintelligence would recognize only precise specifications of rules and values (see <a href=\"http://intelligence.org/upload/complex-value-systems.pdf\">Yudkowsky 2011</a>), acting in ways that violate what feels like &ldquo;common sense&rdquo; to humans, and in ways that fail to respect the subtlety of human values.</li>\n</ol>\n<p>We can't predict <em>exactly&nbsp;</em>what an advanced AI would do, but sci-fi stories like <em><a href=\"http://en.wikipedia.org/wiki/With_Folded_Hands\">With Folded Hands</a></em>&nbsp;provide concrete <em>illustrations&nbsp;</em>of the sort of thing that could go wrong as we try to tell advanced AIs \"Don't let humans die\" in a way that captures the <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">hidden complexity</a>&nbsp;of what we mean by that.</p>\n<h2><br /></h2>\n<h2>Bill:</h2>\n<p>[June 24, 2012]</p>\n<p>A first point: your description of my proposition P included \"almost any AI utility function resulting from an attempt to combine the preferences of humans\" but what I wrote was \"over a broad range of solutions to these technical problems\" which is not quite the same. I think the inconsistency of individual human values and the ambiguity of combining values from multiple humans imply that there is no perfect solution. Rather there is a range of adequate solutions that fall within what we might call the \"convex hull\" of the inconsistency and ambiguity. I should remove the word \"broad\" from my proposition, because I didn't mean \"almost any.\"</p>\n<p>Now I address your three reasons for thinking P is false:</p>\n<ol>\n<li>I agree that people under duress sometimes prefer their own deaths. But at any given time most people are not under such duress and prefer life. Many who kill themselves do so because they can see no other solution to their problems. Some have diseases that cannot be cured, and would not kill themselves if a cure was available. Others are being abused or in poverty with no escape, but would not kill themselves if they could escape their abuse or poverty. A powerful AI accurately motivated by human values would provide solutions to suffering people other than suicide. Natural selection does not create beings that are generally motivated to not exist. However, I should reword \"will assign maximally negative value to any future history that includes human extinction\" to \"will assign nearly maximal negative value to any future history that includes human extinction\", to recognize that under duress humans may prefer death to some other circumstances.</li>\n<li>&nbsp;Benatar's Better Never to Have Been is not an accurate expression of human values. Natural selection has created ambitious human values: to survive, reproduce and understand and control our environment.</li>\n<li>The story With Folded Hands illustrates a problem other than human extinction. My proposition was about human extinction, as illustrated by the assertion in your paper Intelligence Explosion: Evidence and Import:</li>\n</ol>\n<blockquote>\n<p>Later we shall see why these convergent instrumental goals suggest that the default outcome from advanced AI is human extinction.</p>\n</blockquote>\n<p>Furthermore, the prime directive in the story, \"to serve and obey and guard men from harm,\" is not included in what I called \"a broad range of solutions to these technical problems.\" It is not an attempt to resolve the inconsistency of individual human values or the ambiguity of combining values from different people. The nine-word prime directive was designed to make the story work, rather than as a serious effort to create safe AI. Humans value liberty and the mechanicals in the story are not behaving in accord with those values. In fact, the prime directive suffers from ambiguities similar to those that affect Asimov's Laws of Robotics: forced lobotomy is not serving or obeying humans.</p>\n<p>More generally, I agree that the superpower and literalness of AI imply that poorly specified AI utility functions will lead to serious risks. The question is how narrow is the target for an AI utility function to avoid human extinction.</p>\n<p>My proposition was about human extinction but I am more concerned about other threats. Recall that I asked to reword \"It is a potential existential risk\" as \"It poses existential and other serious risks\" in our initial agreement at the start of our dialog. I am more concerned that AI utility functions will serve a narrow group of humans.</p>\n<h2><br /></h2>\n<h2>Luke:</h2>\n<p>[June 27, 2012]</p>\n<p>Replies below...</p>\n<blockquote>\n<p>I didn't mean \"almost any.\"</p>\n</blockquote>\n<p>Oh, okay. Sorry to have misinterpreted you.</p>\n<blockquote>\n<p>I should reword \"will assign maximally negative value to any future history that includes human extinction\" to \"will assign nearly maximal negative value to any future history that includes human extinction\",</p>\n</blockquote>\n<p>Okay.</p>\n<blockquote>\n<p>&nbsp;Benatar's Better Never to Have Been is not an accurate expression of human values. Natural selection has created ambitious human values: to survive, reproduce and understand and control our environment.</p>\n</blockquote>\n<p>I'm confused, so maybe we mean different things by \"human values.\" Clearly, many humans today <em>believe&nbsp;</em>and <em>claim&nbsp;</em>that, for example, they are negative utilitarians and would prefer to minimize suffering, even if this implies eliminating all beings capable of suffering (in a painless way). Are the things you're calling \"human values\" <em>not&nbsp;</em>ever dependent on abstract reasoning and philosophical reflection? Are you using the term \"human values\" to refer only to some set of basic drives written into humans by evolution?</p>\n<blockquote>\n<p>The story With Folded Hands illustrates a problem other than human extinction</p>\n</blockquote>\n<p>Right, I just meant to use <em>With Folded Hands</em>&nbsp;as an example of <em>unintended consequences</em>&nbsp;aka Why It Is Very Hard to Precisely Specify What We Want.</p>\n<blockquote>\n<p>&nbsp;I agree that the superpower and literalness of AI imply that poorly specified AI utility functions will lead to serious risks. The question is how narrow is the target for an AI utility function to avoid human extinction.</p>\n</blockquote>\n<p>Right. But of course there are lots of futures we hate besides&nbsp;extinction, too, such as the ones where we are all lobotomized. Or the ones where we are kept in zoos and AI-run science labs. Or the ones where the AIs maximize happiness by rejiggering our dopamine reward systems and then leave us to lie still on the ground attached to IVs. Or the ones where we get <em>everything&nbsp;</em>we want except for the one single value we call \"novelty,\" and thus the AI tiles the solar system with tiny digital minds replaying nothing but the single most amazing experience again and again until the Sun explodes.</p>\n<blockquote>\n<p>I am more concerned that AI utility functions will serve a narrow group of humans.</p>\n</blockquote>\n<p>So, I'm still unsure how much we disagree. I'm very worried about AI utility functions designed to serve only a narrow group of humans, but I'm just as worried about people who <em>try&nbsp;</em>to build an AGI with an altruistic utility function who simply don't take the problems of unintended consequences seriously enough.</p>\n<p>This has been a helpful and clarifying conversation. Do you think there's more for us to discuss at this time?</p>\n<h2><br /></h2>\n<h2>Bill:</h2>\n<p>[7 July 2012]</p>\n<blockquote>\n<p>This has been a helpful and clarifying conversation. Do you think there's more for us to discuss at this time?</p>\n</blockquote>\n<p>Yes, this has been a useful dialog and we have both expressed our views reasonably clearly. So I am happy to end it for now.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cFP6iBezD4KCfXkpW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 9.394406935816126e-07, "legacy": true, "legacyId": "17502", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Part of the <a href=\"http://wiki.lesswrong.com/wiki/Muehlhauser_interview_series_on_AGI\">Muehlhauser series on AGI</a>.</small></p>\n<p><small><a href=\"http://lukeprog.com/\">Luke Muehlhauser</a>&nbsp;is Executive Director of the <a href=\"http://intelligence.org/\">Singularity Institute</a>, a non-profit research institute studying AGI safety.</small></p>\n<p><small><a href=\"http://www.ssec.wisc.edu/~billh/homepage1.html\">Bill Hibbard</a>&nbsp;is an emeritus senior scientist at University of Wisconsin-Madison and the author of <em><a href=\"http://www.amazon.com/Super-Intelligent-Machines-International-Systems-Engineering/dp/0306473887/\">Super-Intelligent Machines</a></em>.</small></p>\n<h2><br></h2>\n<h2 id=\"Luke_Muehlhauser_\">Luke Muehlhauser:</h2>\n<p>[Apr. 8, 2012]</p>\n<p>Bill, I'm glad you agreed to discuss artificial general intelligence (AGI) with me. I hope our dialogue will be informative to many readers, and to us!</p>\n<p>On what do we agree? In separate conversations, Ben Goertzel and Pei Wang agreed with me on the following statements (though I've clarified the wording for our conversation):</p>\n<ol>\n<li>Involuntary death is bad, and can be avoided with the right technology.</li>\n<li>Humans can be enhanced by merging with technology.</li>\n<li>Humans are on a risky course in general, because powerful technologies can destroy us, humans often behave not in their own best interests, and we are unlikely to voluntarily halt technological progress.</li>\n<li>AGI is likely this century.</li>\n<li>AGI will greatly transform the world. It poses existential and other serious risks, but could also be the best thing that ever happens to us if we do it right.</li>\n<li>Careful effort will be required to ensure that AGI results in good things rather than bad things for humanity.</li>\n</ol>\n<p>You stated in private communication that you agree with these statements, so we have substantial common ground.</p>\n<p>I'd be curious to learn what you think about AGI safety. If you agree that AGI is an existential risk that will arrive this century, and if you value humanity, one might expect you to think it's very important that we accelerate AI <em>safety&nbsp;</em>research and decelerate AI <em>capabilities&nbsp;</em>research so that we develop <em>safe&nbsp;</em>superhuman AGI before we develop arbitrary superhuman AGI. (This is what Anna Salamon and I recommend in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a>.) What are your thoughts on the matter?</p>\n<p>And, which questions would <em>you&nbsp;</em>like to raise?</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Bill_Hibbard_\">Bill Hibbard:</h2>\n<p>[Apr. 11, 2012]</p>\n<p>Luke, thanks for the invitation to this dialog and thanks for making my minor edits to your six statements. I agree with them, with the usual reservation that they are predictions and hence uncertain.</p>\n<p>Before I answer your other questions it would be useful to set a context by reference to an issue you and Anna raised in your excellent paper, Intelligence Explosion: Evidence and Import. In Section 4.1 you wrote:</p>\n<blockquote>\n<p>Unfortunately, specifying what humans value may be extraordinarily difficult, given the complexity and fragility of human preferences (Yudkowsky 2011; Muehlhauser and Helm, this volume), and allowing an AI to learn desirable goals from reward and punishment may be no easier (Yudkowsky 2008a). If this is correct, then the creation of self-improving AI may be detrimental by default unless we first solve the problem of how to build an AI with a stable, desirable utility function - a \"Friendly AI\" (Yudkowsky 2001).</p>\n</blockquote>\n<p>I think this is overly pessimistic about machines learning human values. We humans are good at learning what other humans value, if we are close to them for a long period of time. It is reasonable to think that machines much more intelligent than humans will be able to learn what humans value.</p>\n<p>Solomonoff induction and AIXI are about algorithms learning accurate world models. Human values are part of the world so the AIXI world model includes a model of the values of humans. &nbsp;Yudkowsky's CEV assumed that a future machine (called a Really Powerful Optimization Process) could learn the values of humans, and could even predict the values of humans assuming evolution to some convergent limit.</p>\n<p>I am less worried about an AI exterminating all humans because it cannot learn our values, and more worried about AI that accurately knows the values of its wealthy and powerful human masters and dominates the rest of humanity for its masters' benefit. I think the political problems of AI safety are more difficult than the technical problems.</p>\n<p>My JAGI paper still under review, Model-based Utility Functions, is an effort to help with the technical problems of AI safety. So I do take those problems seriously. But I am more worried about the political problems than the technical problems.</p>\n<p>In your Conclusion, you and Anna wrote:</p>\n<blockquote>\n<p>... and achieving economic stability is ultimately a problem of being smart enough to figure out how to achieve it.</p>\n</blockquote>\n<p>To a significant extent we know how to achieve economic stability, by limiting debt/equity ratios for individuals and for financial institutions, and enforcing accounting standards that prohibit hiding debt and exaggerating equity. But there are serious political problems in imposing those limits and standards.</p>\n<p>Now to your question about the rates of research on AI safety versus AI capabilities. My only previous expression about this issue has been a desire to accelerate the public's understanding of the social issues around AI, and their exercise of democratic control over the technology to ensure that it benefits everyone. For example, AI will enable machines to produce all the goods and services everyone needs, so we should use AI to eliminate poverty rather than causing poverty by depriving people of any way to earn a living. This is a political issue.</p>\n<p>Even if you disagree with me about the relative importance of technical versus political problems with AI, I think you will still need politics to achieve your goals. To accelerate AI safety research and decelerate AI capabilities research on a global scale will require intense political effort. And once we think we know how to build safe AI, it will be a political problem to enforce this knowledge on AI designs.</p>\n<p>I do agree about the need to accelerate AI safety research and think that the AGI conferences are very useful for that goal. However, the question of decelerating AI capability research is more difficult. AI will be useful in the struggle against poverty, disease and aging if we get the politics right. That argues for accelerating AI capabilities to alleviate human suffering. On the other hand, AI before the technical and political problems are solved will be a disaster. I think the resolution of this dilemma is to recognize that it will be impossible to decelerate AI capabilities, so any effort expended on that goal could be better applied to the political and technical problems of AI safety.</p>\n<p>The struggle for wealth and power in human society, and the struggle to improve people's lives, are very strong forces accelerating AI capabilities. Organizations want to cut costs and improve performance by replacing human workers by machines. Businesses want to improve their services by sensing and analyzing huge amounts of consumer and other social data. Investors want to gain market advantage through all sorts of information processing. Politicians want to sense and analyze social data in order to gain political power. Militaries want to gain advantages by sensing and processing battlefield (which increasingly includes civil society) information. Medical researchers want to understand and cure diseases by sensing and analyzing enormously complex biological processes. There is a long list of strong social motives for accelerating AI capabilities, so I think it will be impossible to decelerate AI capabilities.</p>\n<p>The question I'd raise is: How can we educate the public about the opportunities and dangers of AI, and how can we create a political movement in favor of AI benefitting all humans? The Singularity Institute's annual summits are useful for educating the public.</p>\n<h2><br></h2>\n<h2 id=\"Luke_\">Luke:</h2>\n<p>[Apr 13th, 2012]</p>\n<p>I'm not sure how much we disagree about machines learning human values. That is, as you note, how <a href=\"http://intelligence.org/upload/CEV.html\">CEV</a>&nbsp;might work: a seed AI learns our values, extrapolates them, and uses these extrapolated values to build a utility function for a successor superintelligence. I just don't think naive neural net approaches as in <a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=1667949&amp;contentType=Journals+%26+Magazines&amp;sortType%3Dasc_p_Sequence%26filter%3DAND%28p_IS_Number%3A34917%29\">Guarini (2006)</a>&nbsp;will work, for reasons explained in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>.</p>\n<p>So how do we program an AI to \"Figure out what the humans want and do that\"? That, I think, is a Very Hard Technical Problem that breaks down into <a href=\"http://lukeprog.com/SaveTheWorld.html\">lots of difficult sub-problems</a>. I'm glad you're one of the relatively few people working in this problem space, e.g. with <a href=\"http://arxiv.org/pdf/1111.3934v1.pdf\">Model-Based Utility Functions</a>.</p>\n<p>I share your concerns about the political problems related to powerful AGI. That, too, is a Very Hard Problem. I'm not sure whether the technical or political problems are more difficult.</p>\n<p>I agree that economic and political incentives will make it very difficult to decelerate AGI progress. But it's not impossible. Here are just two ideas:</p>\n<p><em>Persuade key AGI researchers of the importance of safety.</em> The most promising AGI work is done by a select few individuals. It may be possible to get them to <a href=\"/lw/bob/reframing_the_problem_of_ai_progress/\">grok</a>&nbsp;that AGI capabilities research merely brings forward the date by which we must&nbsp;solve these Very Hard Problems, and it's already unclear whether we'll be able to solve them in time. Researchers have changed their minds before about the moral value of their work. <a href=\"http://en.wikipedia.org/wiki/Joseph_Rotblat\">Joseph Rotblat</a>&nbsp;resigned from the Manhattan Project due to conscience. <a href=\"http://en.wikipedia.org/wiki/Le%C3%B3_Szil%C3%A1rd\">Leo Szilard</a>&nbsp;switched from physics to molecular biology when he realized the horror of atomic weapons. <a href=\"http://www.michaelagmichaud.com/\">Michael Michaud</a>&nbsp;resigned from SETI in part because of concerns about the danger of contacting advanced civilizations. And of course, Eliezer Yudkowsky <a href=\"http://extropians.weidai.com/extropians.1Q99/3561.html\">once argued</a>&nbsp;that we should bring about the Singularity as quickly as possible, and then <a href=\"http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age\">made a complete U-Turn</a>. If we can change the minds of a few key AGI scientists, it may be that key insights into AGI are delayed by years or decades. Somebody would have come up with General Relativity if Einstein hadn't, but we may have had to wait a few decades.</p>\n<p><em>Get machine superintelligence treated as a weapon of mass destruction.</em> This could make it more difficult to create machine superintelligence. On the other hand, if we got governments to take machine superintelligence this seriously, our actions might instead trigger an arms race and accelerate&nbsp;progress toward machine superintelligence.</p>\n<p>But yes: it will be easier to accelerate AGI safety research, which means doing things like (1) raising awareness, (2) raising funds for those doing AGI safety research, and (3) drawing more brainpower to the relevant research problems.</p>\n<p>Finally, you asked:</p>\n<blockquote>\n<p>How can we educate the public about the opportunities and dangers of AI, and how can we create a political movement in favor of AI benefitting all humans?</p>\n</blockquote>\n<p>I hope that deliverables like the <a href=\"http://singularitysummit.com/\">Singularity Summit</a>&nbsp;and<em> <a href=\"http://facingthesingularity.com/\">Facing the Singularity</a></em>&nbsp;go some distance toward educating the public. But I'm not sure how much this helps. If we educate the masses, are they capable of making wise decisions with that kind of complex information? History, I think, suggests the answer may be \"<a href=\"http://en.wikipedia.org/wiki/Extraordinary_Popular_Delusions_and_the_Madness_of_Crowds\">No</a>.\" Unfortunately, intelligent and well-educated experts may think <a href=\"http://www.scientificamerican.com/article.cfm?id=rational-and-irrational-thought\">just as poorly</a>&nbsp;as laymen <a href=\"http://intelligence.org/upload/cognitive-biases.pdf\">about these issues</a>.</p>\n<p>Do you have your own thoughts on how to reduce the severity of the political problem?</p>\n<p>Also: I think my biggest problem is there not being enough good researchers working on <a href=\"http://lukeprog.com/SaveTheWorld.html\">the sub-problems of AGI safety</a>. Do you have suggestions for how to attract more brainpower to the field?</p>\n<h2><br></h2>\n<h2 id=\"Bill_\">Bill:</h2>\n<p>[16 April 2012]</p>\n<p>Luke, here are responses to a few questions from your message, starting with:</p>\n<blockquote>\n<p>I'm not sure how much we disagree about machines learning human values. That is, as you note, how CEV might work: a seed AI learns our values, extrapolates them, and uses these extrapolated values to build a utility function for a successor superintelligence. I just don't think naive neural net approaches as in Guarini (2006) will work, for reasons explained in The Singularity and Machine Ethics.</p>\n</blockquote>\n<p>I'm glad we don't have much to disagree about here and I certainly do not advocate a naive neural net approach. Currently we do not know how to build intelligent machines. When we do then we can apply those machines to learning human values. If a machine is sufficiently intelligent to pose an existential threat to humanity then it is sufficiently intelligent to learn human values.</p>\n<blockquote>\n<p>If we educate the masses, are they capable of making wise decisions with that kind of complex information? History, I think, suggests the answer may be \"No.\" Unfortunately, intelligent and well-educated experts may think just as poorly as laymen about these issues. &nbsp;</p>\n</blockquote>\n<p>Yes, the lack of wisdom by the public and by experts is frustrating. It is good that the Singularity Institute has done so much to describe cognitive biases that affect human decision making.</p>\n<p>On the other hand, there have been some successes in the environmental, consumer safety, civil rights and arms control movements. I'd like to see a similar political movement in favor of AI benefitting all humans. Perhaps this movement can build on current social issues linked to information technology, like privacy and the economic plight of people whose skills become obsolete in our increasingly automated and networked economy.</p>\n<blockquote>\n<p>Do you have your own thoughts on how to reduce the severity of the political problem?</p>\n</blockquote>\n<p>One thing I'd like to see is more AI experts speaking publicly about the political problem. When they get an opportunity to speak to a wide audience on TV or other mass venue, they often talk about the benefits without addressing the risks. I discussed this issue in an <a href=\"http://hplusmagazine.com/2011/03/03/when-future-watsons-play-politics/\">H+ Magazine article</a>. My article specifically noted the <a href=\"http://research.microsoft.com/en-us/um/people/horvitz/note_from_AAAI_panel_chairs.pdf\">2009 Interim Report</a>&nbsp;of the AAAI Presidential Panel on Long-Term AI Futures, Ray Kurzweil's book <a href=\"http://www.amazon.com/gp/product/0143037889?ie=UTF8&amp;tag=hma0b-20&amp;linkCode=as2&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0143037889\">The Singularity is Near</a>, and Jaron Lanier's 2010 <a href=\"http://www.nytimes.com/2010/08/09/opinion/09lanier.html?_r=2\">New York Times OP-ED</a>. In my opinion each of these seriously underestimated the risks of AI. Of course these experts are entitled to their own opinions. However, I believe they discount the risk because they only consider the risk of AI escaping all human control, what we are calling the technical risk, and which they do not take seriously. I think these experts do not consider the risk that AI will enable a small group of humans to dominate the rest of humanity, what I call the political risk. I have tried to raise the political risk in letters to the editor of the New York Times and in my other publications, but I have not been very effective in reaching the broader culture.</p>\n<p>I'd also like to see the Singularity Institute address the political as well as the technical risks. Making you the Executive Director is hopefully a step in the right direction.</p>\n<blockquote>\n<p>Also: I think my biggest problem is there not being enough good researchers working on the sub-problems of AGI safety. Do you have suggestions for how to attract more brainpower to the field?</p>\n</blockquote>\n<p>That is an excellent question. The mathematical theory of AGI seems to have taken off since Hutter's AIXI in 2005. Now the AGI conferences and JAGI are providing respectable publication venues specifically aimed at the mathematical theory of AGI. So hopefully smart young researchers will be inspired and see opportunities to work in this theory, including AGI safety.</p>\n<p>And if more AI experts will speak about the risks of AI in the broader culture, that would probably also inspire young researchers, and research funding agencies, to focus on AGI safety.</p>\n<h2><br></h2>\n<h2 id=\"Luke_1\">Luke:</h2>\n<p>[Apr. 16, 2012]</p>\n<p>It looks to me like we <em>do&nbsp;</em>disagree significantly about the difficulty of getting a machine to learn human values, so let me ask you more about that. You write:</p>\n<blockquote>\n<p>Currently we do not know how to build intelligent machines. When we do then we can apply those machines to learning human values. If a machine is sufficiently intelligent to pose an existential threat to humanity then it is sufficiently intelligent to learn human values.</p>\n</blockquote>\n<p>I generally agree that superintelligent machines capable of destroying humanity will be capable of learning human values and maximizing for them <em>if&nbsp;</em>\"learn human values and maximize for them\" is a coherent request at all. But capability does not imply motivation. The problem is that human extinction is a <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">convergent outcome</a>&nbsp;of billions of possible goal systems in superintelligent AI, whereas getting the first superintelligent AI to learn human values and maximize for them is like figuring out how to make the very first atomic bomb explode in the shape of an elephant.</p>\n<p>To illustrate, consider the <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">Golem Genie scenario</a>:</p>\n<blockquote>\n<p>Suppose an unstoppably powerful genie appears to you and announces that it will return in fifty years. Upon its return, you will be required to supply it with a [utility function] which it will then enforce with great precision throughout the universe. For example, if you supply the genie with hedonistic utilitarianism, it will maximize pleasure by harvesting all available resources and using them to tile the universe with identical copies of the smallest possible mind each copy of which will experience an endless loop of the most pleasurable experience possible.</p>\n<p>Let us call this precise, instruction-following genie a Golem Genie. (A golem is a creature from Jewish folklore that would in some stories do exactly as told... often with unintended consequences, for example polishing a dish until it is as thin as paper...)</p>\n<p>If by the appointed time you fail to supply your Golem Genie with a [utility function], then it will permanently model its goal system after the first logically coherent [utility function] that anyone articulates to it, and that\u2019s not a risk you want to take. Moreover, once you have supplied the Golem Genie with its [utility function], there will be no turning back. Until the end of time, the genie will enforce that one [utility function] without exception, not even to satisfy its own (previous) desires.</p>\n</blockquote>\n<p>So let's say you want to specify \"Learn human values and maximize that\" as a utility function. How do you do that? That seems <em>incredibly</em>&nbsp;hard to me, for reasons I can explore in detail if you wish.</p>\n<p>And that's not where <em>most&nbsp;</em>of the difficulty is, either. Most of the difficulty is in building an AI capable of executing that kind of utility function in the first place, as opposed to some kluge or reinforcement learner or something that isn't even capable of taking goals like that and therefore isn't even capable&nbsp;of doing what we want when given superintelligence.</p>\n<h2><br></h2>\n<h2 id=\"Bill_1\">Bill:</h2>\n<p>[Apr. 20, 2012]</p>\n<p>Your Golem Genie poses an interesting dilemma. But \"hedonistic utilitarianism\" is not an accurate summary of human values. Reasonable humans would not value a history that converges on the universe tiled with tiny minds enjoying constant maximum pleasure and neither would an AI implementing an accurate model of human values.</p>\n<p>But any summary of values that a human could write down would certainly be inaccurate. Consider an analogy with automatic language processing. Smart linguists have spent generations formulating language rules, yet attempts to automate language processing based on those rules have worked poorly. Recent efforts at language processing by automated statistical learning of rules from large amounts of actual human language use have been more successful (although still inaccurate because they are not yet truly intelligent). It seems that humans cannot write down rules that accurately model their behavior, including rules that model their values.</p>\n<p>I do agree with you that there are tricky problems in getting from human values to a utility function, although you probably think it is a bigger, more dangerous problem than I do. The issues include:</p>\n<ol>\n<li>A person's value preferences often violate the laws of probability, making it impossible to express their preferences as a utility function.</li>\n<li>A person's preferences vary over time depending on the person's experiences (e.g., propaganda works).</li>\n<li>Different people have different preferences, which need to be combined into a single utility function for an AI. Of course this only needs to be done if the AI serves more than one person. If the AI serves a single person or a small group that is what I call the political problem.</li>\n</ol>\n<p>While these are all tricky problems, every reasonable person assigns maximal negative utility value to any history that includes human extinction. It seems reasonable that a solution for deriving an AI utility function from human values should not cause human extinction.</p>\n<p>I think the most difficult issue is the strong human tendency toward group identities and treating people differently based on group membership. There are groups of people who wish the extinction or subjugation of other groups of people. Filtering such wishes out of human values, in forming AI utility functions, may require some sort of \"semantic surgery\" on values (just to be clear, this \"semantic surgery\" consists of changes to the values being processed into an AI utility function, not any changes to the people holding those values).</p>\n<p>I am familiar with the concern that an AI will have an instrumental sub-goal to gather resources in order to increase its ability to maximize its utility function, and that it may exterminate humans in order to reuse the atoms from their bodies as resources for itself. However, any reasonable human would assign maximum negative value to any history that includes exterminating humans for their atoms and so would a reasonable utility function model of human values. An AI motivated by such a utility function would gather atoms only to the extent that such behavior increases utility. Gathering atoms to the extent that it exterminates humans would have maximum negative utility and the agent wouldn't do it.</p>\n<p>In your recent message you wrote:</p>\n<blockquote>\n<p>... as opposed to some kluge or reinforcement learner or something that isn't even capable of taking goals like that and therefore isn't even capable of doing what we want when given superintelligence.</p>\n</blockquote>\n<p>I am curious about your attitude toward reinforcement learning. Here you disparage it, but in Section 2.4 of Intelligence Explosion: Evidence and Import you discuss AIXI as an important milestone towards AI and AIXI is a reinforcement learner.</p>\n<p>One problem with reinforcement learning is that it is commonly restricted to utility functions whose values are rewards sent to the agent from the environment. I don't think this is a reasonable model of the way that real-world agents compute their utility functions. (Informally I've used reinforcement learning to refer to any agent motivated to maximize sums of discounted future utility function values, but in my mathematical papers about AGI theory I try to conform to standard usage.) The AGI-11 papers of Dewey, and Ring and Orseau also demonstrate problems with reinforcement learning. These problems can be avoided using utility function definitions that are not restricted to rewards from the environment to the agent.</p>\n<h2><br></h2>\n<h2 id=\"Luke_2\">Luke:</h2>\n<p>[Apr. 20, 2012]</p>\n<p>We seem to agree that \"any summary of values that a human could write down would certainly be inaccurate\" (including, e.g., hedonistic utilitarianism). You have nicely summarized some of the difficulties we face in trying to write a utility function that captures human values, but if you explained why this problem is not&nbsp;going to be as big a problem as I&nbsp;am saying it is, then I missed it. You also seem to agree that convergent instrumental goals for (e.g.) resource acquisition pose an existential threat to humanity (as argued by <a href=\"http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/\">Omohundro 2008</a>&nbsp;and <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">Bostrom 2012</a>), and again if you have a solution in mind, I missed it. So I'm not sure why you're less worried about the technical problems than I am.</p>\n<p>As for reinforcement learners: I don't \"disparage\" them, I just say that they aren't capable of taking human-friendly goals. Reinforcement learning is indeed an important advance in AI, but we can't build a Friendly AI from a reinforcement learner (or from other existing AI architectures), which is why AI by default will be disastrous for humans. As <a href=\"http://intelligence.org/upload/learning-what-to-value.pdf\">Dewey (2011)</a>&nbsp;explains:</p>\n<blockquote>\n<p>Reinforcement learning can only be used in the real world to define agents whose goal is to maximize expected rewards, and since this goal does not match with human goals, AGIs based on reinforcement learning will often work at cross-purposes to us. To solve this problem, we define <em>value learners</em>, agents that can be designed to learn and maximize any initially unknown utility function so long as we provide them with an idea of what constitutes evidence about that utility function.</p>\n</blockquote>\n<p>(Above, I am using \"reinforcement learner\" in the traditional, narrower sense.)</p>\n<p>Back to the \"technical problem\" of defining a human-friendly utility function and building an AI architecture capable of maximizing it. Do you have a sense for why you're less worried about the difficulty of that problem than I am?</p>\n<h2><br></h2>\n<h2 id=\"Bill_2\">Bill:</h2>\n<p>[April 23, 2012]</p>\n<p>I think our assessments of the relative danger from the technical versus the political problem come from our different intuitions. I do not have a proof of a solution to the technical problem. If I did, I would publish it in a paper.</p>\n<p>In your message of 16 April 2012 you wrote:</p>\n<blockquote>\n<p>So let's say you want to specify \"Learn human values and maximize that\" as a utility function. How do you do that? That seems incredibly hard to me, for reasons I can explore in detail if you wish.</p>\n</blockquote>\n<p>Yes Please. I'd be interested if you'd like to explain your reasons.</p>\n<h2><br></h2>\n<h2 id=\"Luke_3\">Luke:</h2>\n<p>[May 1, 2012]</p>\n<p>I tried to summarize some aspects of the technical difficulty of Friendly AI in \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>.\"</p>\n<p>But let's zoom in on the proposal to specify P&nbsp;as a utility function in an AI, where P&nbsp;= \"Learn human values and maximize their satisfaction.\" There are two types of difficulty we might face when trying to implement this project:</p>\n<ol>\n<li>We know what P&nbsp;means but we aren't sure how to make it precise enough to encode it in a utility function.</li>\n<li>We are fundamentally, philosophically confused about what P&nbsp;means, and&nbsp;we aren't sure how to make it precise enough to encode it in a utility function.</li>\n</ol>\n<p>I think we are in situation #2, which is why the technical difficulty of Friendly AI is so hard. At the moment, it looks like we'd need to solve huge swaths of philosophy before being able to build Friendly AI, and that seems difficult because most philosophical questions aren't considered to be \"solved\" after 2.5 millennia of work.</p>\n<p>For example, consider the following questions:</p>\n<ol>\n<li>What counts as a \"human\"? Do those with brain injuries or neurological diseases count for these purposes? Do infants? Do anencephalic infants? If we use DNA to <a href=\"http://www.nytimes.com/2009/02/13/science/13neanderthal.html?_r=1\">resurrect</a>&nbsp;the strands of <em>Homo&nbsp;</em>very similar to the modern human, do those people count? Suppose we can radically augment the human mind with cognitive enhancements and brain-computer interfaces in the next few decades\u2014do heavily augmented humans count? If we get mind uploading before AI, do uploaded humans count?</li>\n<li>What are human \"values\"? Are we talking about the <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">value encodings</a>&nbsp;in the dopaminergic reward system and perhaps elsewhere? (If so, we're going to <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">have trouble</a>&nbsp;translating that into anything like a coherent utility function.) Are we talking about some kind of <a href=\"http://intelligence.org/upload/CEV.html\">extrapolation</a>&nbsp;or <a href=\"/lw/c3t/stanovich_on_cev/\">rational integration</a>&nbsp;of our values? (If so, what's the algorithm for this extrapolation or rational integration?)</li>\n<li>\"Learn human values and maximize their satisfaction\" seems to assume the aggregation of the preferences of multiple humans. How do we do that?</li>\n</ol>\n<p>Since <em>I</em>&nbsp;don't know what I \"mean\" by these things, I sure as hell can't tell an AI what I mean. I couldn't even describe it in conversation, let alone encode it precisely in a utility function. Occasionally, creative solutions to the problem are proposed, like <a href=\"/lw/c0k/formalizing_value_extrapolation/\">Paul Christiano's</a>, but they only nascent proposals and suffer major difficulties of their own.</p>\n<h2><br></h2>\n<h2 id=\"Bill_3\">Bill:</h2>\n<p>[May 9, 2012]</p>\n<p>Yes, there are technical problems of expressing a person's values as a utility function, normalizing utility functions between different people, and combining the utility functions of different people to derive an overall AI utility function.</p>\n<p>However, every reasonable person assigns maximally negative utility to human extinction, so over a broad range of solutions to these technical problems, the overall AI utility function will assign maximally negative value to any future history that includes human extinction.</p>\n<p>Do you agree with this?</p>\n<p>If you do agree, how does this lead to a high probability of human extinction?</p>\n<h2><br></h2>\n<h2 id=\"Luke_4\">Luke:</h2>\n<p>[June 10, 2012]</p>\n<p>Bill, you acknowledge the technical difficulty of expressing a person's values as a utility function (see the literature on \"<a href=\"/lw/a73/a_brief_tutorial_on_preferences_in_ai/\">preference acquisition</a>\"), and of combining many utility functions to derive a desirable AI utility function (see the literatures on \"<a href=\"/lw/a73/a_brief_tutorial_on_preferences_in_ai/\">population ethics</a>\" and \"<a href=\"http://en.wikipedia.org/wiki/Social_choice_theory\">social choice</a>\"). But if I understand you correctly, you seem to think that:</p>\n<blockquote>\n<p>P: almost <em>any&nbsp;</em>AI utility function resulting from an attempt to combine the preferences of humans will assign maximally negative utility to any future history that includes human extinction, because humans are agreed on the badness of human extinction.</p>\n</blockquote>\n<p>I think proposition P&nbsp;is false, for several reasons:</p>\n<p>First: Most people think there are fates worth than death, so a future including human extinction wouldn't be assigned <em>maximally</em>&nbsp;negative utility.</p>\n<p>Second: Even ignoring this, many people think human extinction would be better than the status quo (at least, if you human extinction could occur without much suffering). See Benatar's <em><a href=\"http://www.amazon.com/Better-Never-Have-Been-Existence/dp/0199549265/\">Better Never to Have Been</a></em>. Many negative utilitarians agree with Benatar's conclusion, if not his reasoning. The surest way to minimize conscious suffering is to eliminate beings capable of conscious suffering.</p>\n<p>Third: There are many problems similar to problem #1 I gave above (\"What counts as a 'human'?\"). Suppose we manage to design an AI architecture that has preferences over states of affairs (or future histories) as opposed to preferences over a reward function, and we define a utility function for the AI that might be sub-optimal in many ways but \"clearly\" assigns utility 0 (or negative 1m, or whatever) to human extinction. We're still left with the problems of specification explained in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics.</a>&nbsp;A machine superintelligence would possess two features that make it dangerous to give it any utility function at all:</p>\n<ol>\n<li><em>Superpower</em>: A machine superintelligence would have unprecedented powers to reshape reality, and would therefore achieve its goals with highly efficient methods that confound human expectations (e.g. it could \"maximize pleasure\" by tiling the universe with trillions of digital minds running a loop of a single pleasurable experience).</li>\n<li><em>Literalness</em>: A machine superintelligence would recognize only precise specifications of rules and values (see <a href=\"http://intelligence.org/upload/complex-value-systems.pdf\">Yudkowsky 2011</a>), acting in ways that violate what feels like \u201ccommon sense\u201d to humans, and in ways that fail to respect the subtlety of human values.</li>\n</ol>\n<p>We can't predict <em>exactly&nbsp;</em>what an advanced AI would do, but sci-fi stories like <em><a href=\"http://en.wikipedia.org/wiki/With_Folded_Hands\">With Folded Hands</a></em>&nbsp;provide concrete <em>illustrations&nbsp;</em>of the sort of thing that could go wrong as we try to tell advanced AIs \"Don't let humans die\" in a way that captures the <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">hidden complexity</a>&nbsp;of what we mean by that.</p>\n<h2><br></h2>\n<h2 id=\"Bill_4\">Bill:</h2>\n<p>[June 24, 2012]</p>\n<p>A first point: your description of my proposition P included \"almost any AI utility function resulting from an attempt to combine the preferences of humans\" but what I wrote was \"over a broad range of solutions to these technical problems\" which is not quite the same. I think the inconsistency of individual human values and the ambiguity of combining values from multiple humans imply that there is no perfect solution. Rather there is a range of adequate solutions that fall within what we might call the \"convex hull\" of the inconsistency and ambiguity. I should remove the word \"broad\" from my proposition, because I didn't mean \"almost any.\"</p>\n<p>Now I address your three reasons for thinking P is false:</p>\n<ol>\n<li>I agree that people under duress sometimes prefer their own deaths. But at any given time most people are not under such duress and prefer life. Many who kill themselves do so because they can see no other solution to their problems. Some have diseases that cannot be cured, and would not kill themselves if a cure was available. Others are being abused or in poverty with no escape, but would not kill themselves if they could escape their abuse or poverty. A powerful AI accurately motivated by human values would provide solutions to suffering people other than suicide. Natural selection does not create beings that are generally motivated to not exist. However, I should reword \"will assign maximally negative value to any future history that includes human extinction\" to \"will assign nearly maximal negative value to any future history that includes human extinction\", to recognize that under duress humans may prefer death to some other circumstances.</li>\n<li>&nbsp;Benatar's Better Never to Have Been is not an accurate expression of human values. Natural selection has created ambitious human values: to survive, reproduce and understand and control our environment.</li>\n<li>The story With Folded Hands illustrates a problem other than human extinction. My proposition was about human extinction, as illustrated by the assertion in your paper Intelligence Explosion: Evidence and Import:</li>\n</ol>\n<blockquote>\n<p>Later we shall see why these convergent instrumental goals suggest that the default outcome from advanced AI is human extinction.</p>\n</blockquote>\n<p>Furthermore, the prime directive in the story, \"to serve and obey and guard men from harm,\" is not included in what I called \"a broad range of solutions to these technical problems.\" It is not an attempt to resolve the inconsistency of individual human values or the ambiguity of combining values from different people. The nine-word prime directive was designed to make the story work, rather than as a serious effort to create safe AI. Humans value liberty and the mechanicals in the story are not behaving in accord with those values. In fact, the prime directive suffers from ambiguities similar to those that affect Asimov's Laws of Robotics: forced lobotomy is not serving or obeying humans.</p>\n<p>More generally, I agree that the superpower and literalness of AI imply that poorly specified AI utility functions will lead to serious risks. The question is how narrow is the target for an AI utility function to avoid human extinction.</p>\n<p>My proposition was about human extinction but I am more concerned about other threats. Recall that I asked to reword \"It is a potential existential risk\" as \"It poses existential and other serious risks\" in our initial agreement at the start of our dialog. I am more concerned that AI utility functions will serve a narrow group of humans.</p>\n<h2><br></h2>\n<h2 id=\"Luke_5\">Luke:</h2>\n<p>[June 27, 2012]</p>\n<p>Replies below...</p>\n<blockquote>\n<p>I didn't mean \"almost any.\"</p>\n</blockquote>\n<p>Oh, okay. Sorry to have misinterpreted you.</p>\n<blockquote>\n<p>I should reword \"will assign maximally negative value to any future history that includes human extinction\" to \"will assign nearly maximal negative value to any future history that includes human extinction\",</p>\n</blockquote>\n<p>Okay.</p>\n<blockquote>\n<p>&nbsp;Benatar's Better Never to Have Been is not an accurate expression of human values. Natural selection has created ambitious human values: to survive, reproduce and understand and control our environment.</p>\n</blockquote>\n<p>I'm confused, so maybe we mean different things by \"human values.\" Clearly, many humans today <em>believe&nbsp;</em>and <em>claim&nbsp;</em>that, for example, they are negative utilitarians and would prefer to minimize suffering, even if this implies eliminating all beings capable of suffering (in a painless way). Are the things you're calling \"human values\" <em>not&nbsp;</em>ever dependent on abstract reasoning and philosophical reflection? Are you using the term \"human values\" to refer only to some set of basic drives written into humans by evolution?</p>\n<blockquote>\n<p>The story With Folded Hands illustrates a problem other than human extinction</p>\n</blockquote>\n<p>Right, I just meant to use <em>With Folded Hands</em>&nbsp;as an example of <em>unintended consequences</em>&nbsp;aka Why It Is Very Hard to Precisely Specify What We Want.</p>\n<blockquote>\n<p>&nbsp;I agree that the superpower and literalness of AI imply that poorly specified AI utility functions will lead to serious risks. The question is how narrow is the target for an AI utility function to avoid human extinction.</p>\n</blockquote>\n<p>Right. But of course there are lots of futures we hate besides&nbsp;extinction, too, such as the ones where we are all lobotomized. Or the ones where we are kept in zoos and AI-run science labs. Or the ones where the AIs maximize happiness by rejiggering our dopamine reward systems and then leave us to lie still on the ground attached to IVs. Or the ones where we get <em>everything&nbsp;</em>we want except for the one single value we call \"novelty,\" and thus the AI tiles the solar system with tiny digital minds replaying nothing but the single most amazing experience again and again until the Sun explodes.</p>\n<blockquote>\n<p>I am more concerned that AI utility functions will serve a narrow group of humans.</p>\n</blockquote>\n<p>So, I'm still unsure how much we disagree. I'm very worried about AI utility functions designed to serve only a narrow group of humans, but I'm just as worried about people who <em>try&nbsp;</em>to build an AGI with an altruistic utility function who simply don't take the problems of unintended consequences seriously enough.</p>\n<p>This has been a helpful and clarifying conversation. Do you think there's more for us to discuss at this time?</p>\n<h2><br></h2>\n<h2 id=\"Bill_5\">Bill:</h2>\n<p>[7 July 2012]</p>\n<blockquote>\n<p>This has been a helpful and clarifying conversation. Do you think there's more for us to discuss at this time?</p>\n</blockquote>\n<p>Yes, this has been a useful dialog and we have both expressed our views reasonably clearly. So I am happy to end it for now.</p>", "sections": [{"title": "Luke Muehlhauser:", "anchor": "Luke_Muehlhauser_", "level": 1}, {"title": "Bill Hibbard:", "anchor": "Bill_Hibbard_", "level": 1}, {"title": "Luke:", "anchor": "Luke_", "level": 1}, {"title": "Bill:", "anchor": "Bill_", "level": 1}, {"title": "Luke:", "anchor": "Luke_1", "level": 1}, {"title": "Bill:", "anchor": "Bill_1", "level": 1}, {"title": "Luke:", "anchor": "Luke_2", "level": 1}, {"title": "Bill:", "anchor": "Bill_2", "level": 1}, {"title": "Luke:", "anchor": "Luke_3", "level": 1}, {"title": "Bill:", "anchor": "Bill_3", "level": 1}, {"title": "Luke:", "anchor": "Luke_4", "level": 1}, {"title": "Bill:", "anchor": "Bill_4", "level": 1}, {"title": "Luke:", "anchor": "Luke_5", "level": 1}, {"title": "Bill:", "anchor": "Bill_5", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 16}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nGP4soWSbFmzemM4i", "hN2aRnu798yas5b2k", "fa5o2tg9EfJE77jEQ", "GzYu2acxWL6pZyzyc", "9DWcNS2rkvd2J8mHH", "Q9nhAp3q27xS3AgtB", "4ARaTpNX62uaL86j6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-10T01:42:39.259Z", "modifiedAt": null, "url": null, "title": "Should LW have a separate AI section?", "slug": "should-lw-have-a-separate-ai-section", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:54.702Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alex_Altair", "createdAt": "2010-07-21T18:30:40.806Z", "isAdmin": false, "displayName": "Alex_Altair"}, "userId": "5wu9jG4pm9q6xjZ9R", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RaPDPmwHMSHXYDYzD/should-lw-have-a-separate-ai-section", "pageUrlRelative": "/posts/RaPDPmwHMSHXYDYzD/should-lw-have-a-separate-ai-section", "linkUrl": "https://www.lesswrong.com/posts/RaPDPmwHMSHXYDYzD/should-lw-have-a-separate-ai-section", "postedAtFormatted": "Tuesday, July 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20LW%20have%20a%20separate%20AI%20section%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20LW%20have%20a%20separate%20AI%20section%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRaPDPmwHMSHXYDYzD%2Fshould-lw-have-a-separate-ai-section%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20LW%20have%20a%20separate%20AI%20section%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRaPDPmwHMSHXYDYzD%2Fshould-lw-have-a-separate-ai-section", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRaPDPmwHMSHXYDYzD%2Fshould-lw-have-a-separate-ai-section", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>LessWrong seems to have two main topics for discussion; rationality and AI. This, of course, is caused by Eliezer Yudkowsky's sequences (and interests), which are mostly about rationality, but also include a lot of writing about AI. LessWrong currently has two sections, Main and Discussion. This is meant to separate the purpose and quality of the post. I think&nbsp;usability&nbsp;of the site has improved greatly since adding the Discussion section. Should split the discussion section, and have one for rationality and one for AI?</p>\n<p><strong>Advantages</strong></p>\n<p>\n<ul>\n<li>It would make LW more pleasant for&nbsp;rationality enthusiasts who don't want to sort through lots of AI discussions.</li>\n<li>It would make LW more pleasant for AI enthusiasts who don't want to sort through lots of rationality discussions.</li>\n<li>It would not split up the community as much as a new AI community site would.</li>\n<li>It would increase LW's capacity for discussion.</li>\n<li>Newcomers could learn about rationality, while people who want higher quality of discussion on AI topics could have it.</li>\n</ul>\n</p>\n<p><strong>Disadvantages</strong></p>\n<p>\n<ul>\n<li>Many posts about both subjects are highly relevant about both subjects</li>\n<li>It would make LW less pleasant for enthusiasts of both subjects.</li>\n<li>It would split up the community more than doing nothing.</li>\n<li>Everybody has their ideas for separate sections, and we can't do them all.</li>\n</ul>\n<div>What do you guys think?</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RaPDPmwHMSHXYDYzD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 9.395116762410029e-07, "legacy": true, "legacyId": "17503", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>LessWrong seems to have two main topics for discussion; rationality and AI. This, of course, is caused by Eliezer Yudkowsky's sequences (and interests), which are mostly about rationality, but also include a lot of writing about AI. LessWrong currently has two sections, Main and Discussion. This is meant to separate the purpose and quality of the post. I think&nbsp;usability&nbsp;of the site has improved greatly since adding the Discussion section. Should split the discussion section, and have one for rationality and one for AI?</p>\n<p><strong id=\"Advantages\">Advantages</strong></p>\n<p>\n</p><ul>\n<li>It would make LW more pleasant for&nbsp;rationality enthusiasts who don't want to sort through lots of AI discussions.</li>\n<li>It would make LW more pleasant for AI enthusiasts who don't want to sort through lots of rationality discussions.</li>\n<li>It would not split up the community as much as a new AI community site would.</li>\n<li>It would increase LW's capacity for discussion.</li>\n<li>Newcomers could learn about rationality, while people who want higher quality of discussion on AI topics could have it.</li>\n</ul>\n<p></p>\n<p><strong id=\"Disadvantages\">Disadvantages</strong></p>\n<p>\n</p><ul>\n<li>Many posts about both subjects are highly relevant about both subjects</li>\n<li>It would make LW less pleasant for enthusiasts of both subjects.</li>\n<li>It would split up the community more than doing nothing.</li>\n<li>Everybody has their ideas for separate sections, and we can't do them all.</li>\n</ul>\n<div>What do you guys think?</div>\n<p></p>", "sections": [{"title": "Advantages", "anchor": "Advantages", "level": 1}, {"title": "Disadvantages", "anchor": "Disadvantages", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "24 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-10T02:32:58.362Z", "modifiedAt": null, "url": null, "title": "Low hanging fruit: Websites that significantly improve your life?", "slug": "low-hanging-fruit-websites-that-significantly-improve-your", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.123Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v9Jha3MR74Tb5nbxe/low-hanging-fruit-websites-that-significantly-improve-your", "pageUrlRelative": "/posts/v9Jha3MR74Tb5nbxe/low-hanging-fruit-websites-that-significantly-improve-your", "linkUrl": "https://www.lesswrong.com/posts/v9Jha3MR74Tb5nbxe/low-hanging-fruit-websites-that-significantly-improve-your", "postedAtFormatted": "Tuesday, July 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Low%20hanging%20fruit%3A%20Websites%20that%20significantly%20improve%20your%20life%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALow%20hanging%20fruit%3A%20Websites%20that%20significantly%20improve%20your%20life%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9Jha3MR74Tb5nbxe%2Flow-hanging-fruit-websites-that-significantly-improve-your%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Low%20hanging%20fruit%3A%20Websites%20that%20significantly%20improve%20your%20life%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9Jha3MR74Tb5nbxe%2Flow-hanging-fruit-websites-that-significantly-improve-your", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9Jha3MR74Tb5nbxe%2Flow-hanging-fruit-websites-that-significantly-improve-your", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p>It suddenly occurred to me that not everyone uses a website I love. &nbsp;Of all the websites I browse if you asked me which one improves my standard of living in the largest, most concrete way, it would be slickdeals.</p>\n<p>What it is: a community driven \"hot deals\" website with voting. &nbsp;By itself this is already useful, but the best feature is deal alerts. &nbsp;you never have to actually browse slickdeals. &nbsp;Just create an account, set up deal alerts for whatever strings interest you (I for example have a deal alert for \"whey\" to get alerted to deals on whey protein powder).</p>\n<p>More than 50% of my belongings come from sales I've been alerted to via slickdeals. &nbsp;Any big ticket items I need, but don't need immediately I just set alerts and wait. &nbsp;My laptop, desktop, tablet, smartphone, clothes, toiletries, books, games, even some food are all slightly better for the price I paid than they otherwise would have been.</p>\n<p>So what other sites am I missing out on that would make my life a lot better?</p>\n<p>A website I just discovered for habit building chains.cc seems cool, but I haven't had time to evaluate its usefulness yet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v9Jha3MR74Tb5nbxe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 16, "extendedScore": null, "score": 9.395353463656181e-07, "legacy": true, "legacyId": "17506", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-10T03:36:39.203Z", "modifiedAt": null, "url": null, "title": "Dual N-Back browser-based \"game\" in public alpha-testing state.", "slug": "dual-n-back-browser-based-game-in-public-alpha-testing-state", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.052Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Logos01", "createdAt": "2011-07-21T18:59:16.270Z", "isAdmin": false, "displayName": "Logos01"}, "userId": "WZxoXCWQviJp9dNc5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iChEzNBM6Y55Fr8kG/dual-n-back-browser-based-game-in-public-alpha-testing-state", "pageUrlRelative": "/posts/iChEzNBM6Y55Fr8kG/dual-n-back-browser-based-game-in-public-alpha-testing-state", "linkUrl": "https://www.lesswrong.com/posts/iChEzNBM6Y55Fr8kG/dual-n-back-browser-based-game-in-public-alpha-testing-state", "postedAtFormatted": "Tuesday, July 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dual%20N-Back%20browser-based%20%22game%22%20in%20public%20alpha-testing%20state.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADual%20N-Back%20browser-based%20%22game%22%20in%20public%20alpha-testing%20state.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiChEzNBM6Y55Fr8kG%2Fdual-n-back-browser-based-game-in-public-alpha-testing-state%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dual%20N-Back%20browser-based%20%22game%22%20in%20public%20alpha-testing%20state.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiChEzNBM6Y55Fr8kG%2Fdual-n-back-browser-based-game-in-public-alpha-testing-state", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiChEzNBM6Y55Fr8kG%2Fdual-n-back-browser-based-game-in-public-alpha-testing-state", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p><a href=\"http://namuol.github.com/banal-duck/\">Link.</a></p>\n<p>&nbsp;</p>\n<p>Found here: http://www.reddit.com/r/cogsci/comments/wb44q/my_dual_nback_browsergame_is_ready_for/</p>\n<p>&nbsp;</p>\n<p>From the author:</p>\n<blockquote>\n<p>&nbsp;Quick notes:</p>\n<div class=\"usertext-body\">\n<div class=\"md\">\n<ul>\n<li>If you experience any technical problems running the game, please let me know what browser and OS you're using.</li>\n<li>If you're unfamiliar with what Dual N-Back is, <span class=\" keyNavAnnotation\">[1] </span><a class=\" imgScanned hasListener\" rel=\"nofollow\" href=\"http://www.gwern.net/DNB%20FAQ\">this is a good place to start reading</a>.</li>\n</ul>\n<p>&nbsp;</p>\n</div>\n</div>\n<p>&nbsp;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iChEzNBM6Y55Fr8kG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 9.395653037616644e-07, "legacy": true, "legacyId": "17510", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-10T05:11:43.065Z", "modifiedAt": null, "url": null, "title": "Steam Greenlight", "slug": "steam-greenlight", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrandonReinhart", "createdAt": "2009-03-06T04:00:54.689Z", "isAdmin": false, "displayName": "BrandonReinhart"}, "userId": "ugRLNpaFuDrXntoeK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YtwwLfXKn4AeDrew8/steam-greenlight", "pageUrlRelative": "/posts/YtwwLfXKn4AeDrew8/steam-greenlight", "linkUrl": "https://www.lesswrong.com/posts/YtwwLfXKn4AeDrew8/steam-greenlight", "postedAtFormatted": "Tuesday, July 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Steam%20Greenlight&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASteam%20Greenlight%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYtwwLfXKn4AeDrew8%2Fsteam-greenlight%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Steam%20Greenlight%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYtwwLfXKn4AeDrew8%2Fsteam-greenlight", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYtwwLfXKn4AeDrew8%2Fsteam-greenlight", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<p>I know there is interest among this community in building rationality oriented games as teaching tools. Today we announced Steam Greenlight. We're essentially turning the game approval process over to the community. It may be possible for quality rationality games produced by the Less Wrong community to create enough gamer-community interest to get placed on Steam for distribution.&nbsp;</p>\n<p><a href=\"http://steamcommunity.com/greenlight\">http://steamcommunity.com/greenlight</a></p>\n<p>I feel that this creates a better opportunity for rationality games as teaching tools to find broad distribution than if it had to go through the Steam product review team. Ultimately, it shifts the responsibility onto the games' creators and their community to create and drive interest for the product and it removes our limited decision making from the system.</p>\n<p>I'm posting this here for awareness of this possible avenue toward reaching a broader audience.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YtwwLfXKn4AeDrew8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 24, "extendedScore": null, "score": 9.396100281972144e-07, "legacy": true, "legacyId": "17512", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-10T05:33:02.802Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] When (Not) To Use Probabilities", "slug": "seq-rerun-when-not-to-use-probabilities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.824Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CDc6NQ2e7cmBDXjfk/seq-rerun-when-not-to-use-probabilities", "pageUrlRelative": "/posts/CDc6NQ2e7cmBDXjfk/seq-rerun-when-not-to-use-probabilities", "linkUrl": "https://www.lesswrong.com/posts/CDc6NQ2e7cmBDXjfk/seq-rerun-when-not-to-use-probabilities", "postedAtFormatted": "Tuesday, July 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20When%20(Not)%20To%20Use%20Probabilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20When%20(Not)%20To%20Use%20Probabilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCDc6NQ2e7cmBDXjfk%2Fseq-rerun-when-not-to-use-probabilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20When%20(Not)%20To%20Use%20Probabilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCDc6NQ2e7cmBDXjfk%2Fseq-rerun-when-not-to-use-probabilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCDc6NQ2e7cmBDXjfk%2Fseq-rerun-when-not-to-use-probabilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>Today's post, <a href=\"/lw/sg/when_not_to_use_probabilities/\">When (Not) To Use Probabilities</a> was originally published on 23 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When you don't have a numerical procedure to generate probabilities, you're probably better off using your own evolved abilities to reason in the presence of uncertainty.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dhp/seq_rerun_fake_norms_or_truth_vs_truth/\">Fake Norms, or \"Truth\" vs. Truth</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CDc6NQ2e7cmBDXjfk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.396200632315874e-07, "legacy": true, "legacyId": "17513", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AJ9dX59QXokZb35fk", "PaGHGSRMP6AJJDb4S", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-10T08:18:25.478Z", "modifiedAt": null, "url": null, "title": "Cultural norms in choice of mate", "slug": "cultural-norms-in-choice-of-mate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.645Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TraderJoe", "createdAt": "2012-03-02T17:26:49.458Z", "isAdmin": false, "displayName": "TraderJoe"}, "userId": "PvoRSceD7dHzEway5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7TdbFETsdrcfBXZHJ/cultural-norms-in-choice-of-mate", "pageUrlRelative": "/posts/7TdbFETsdrcfBXZHJ/cultural-norms-in-choice-of-mate", "linkUrl": "https://www.lesswrong.com/posts/7TdbFETsdrcfBXZHJ/cultural-norms-in-choice-of-mate", "postedAtFormatted": "Tuesday, July 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cultural%20norms%20in%20choice%20of%20mate&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACultural%20norms%20in%20choice%20of%20mate%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TdbFETsdrcfBXZHJ%2Fcultural-norms-in-choice-of-mate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cultural%20norms%20in%20choice%20of%20mate%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TdbFETsdrcfBXZHJ%2Fcultural-norms-in-choice-of-mate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TdbFETsdrcfBXZHJ%2Fcultural-norms-in-choice-of-mate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"   DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"   LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-priority:99;\n\tmso-style-qformat:yes;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin:0in;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:10.0pt;\n\tfont-family:\"Times New Roman\",\"serif\";\n\tmso-fareast-font-family:\"Times New Roman\";}\n</style>\n<![endif]--></p>\n<p class=\"MsoNormal\">Why don&rsquo;t men go for younger women? That&rsquo;s not quite accurate, because they do, but we seem to have a cultural norm against girls who have recently exited puberty, even though they are highly fertile. Some time in the last few hundred years, we reached a cultural norm that as a man gets older, it&rsquo;s increasingly less appropriate for him to be with a sixteen year old. That&rsquo;s not long enough to override thousands of years of evolution, so what could have contributed to it?<br /> <br /> The best solution I&rsquo;ve heard started by looking at who benefits from this norm [older women] and wondering whether they could have contributed to it. After all, the strength of this norm has been increased in the last sixty years or so, which coincides with the period in which women&rsquo;s power has increased.<br /> <br /> One alternative I heard recently is that it doesn&rsquo;t make sense biologically to go for women who are recently post-pubescent, as they don&rsquo;t make the best mothers. Instead, a slightly older woman [mid-twenties] makes sense to be the best mother. This is perfectly plausible. <br /> <br /> Another, less plausible, suggestion I&rsquo;ve heard is that it&rsquo;s to do with mental capacity. I find this unconvincing because we have few objections to a high-status man dating a beautiful but low-intellect woman. <br /> <br /> Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7TdbFETsdrcfBXZHJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": -27, "extendedScore": null, "score": -5.1e-05, "legacy": true, "legacyId": "17529", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-10T08:35:27.873Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 7/9/12", "slug": "group-rationality-diary-7-9-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:29.607Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/as8784LJq7cMfxqoY/group-rationality-diary-7-9-12", "pageUrlRelative": "/posts/as8784LJq7cMfxqoY/group-rationality-diary-7-9-12", "linkUrl": "https://www.lesswrong.com/posts/as8784LJq7cMfxqoY/group-rationality-diary-7-9-12", "postedAtFormatted": "Tuesday, July 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%207%2F9%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%207%2F9%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fas8784LJq7cMfxqoY%2Fgroup-rationality-diary-7-9-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%207%2F9%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fas8784LJq7cMfxqoY%2Fgroup-rationality-diary-7-9-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fas8784LJq7cMfxqoY%2Fgroup-rationality-diary-7-9-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the public group instrumental rationality diary for the week of July 9th. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Thanks to everyone who contributes!</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Academian put up a <a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">wiki page with links to the prior May and June threads</a> for reference. &nbsp;Good idea, thanks!</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "as8784LJq7cMfxqoY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 9.39705896392434e-07, "legacy": true, "legacyId": "17530", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-10T22:57:03.573Z", "modifiedAt": null, "url": null, "title": "SI visiting fellow Diego Caleiro gives a TED talk on Friendly AI", "slug": "si-visiting-fellow-diego-caleiro-gives-a-ted-talk-on", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.350Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/327q6So9Y79AooBkQ/si-visiting-fellow-diego-caleiro-gives-a-ted-talk-on", "pageUrlRelative": "/posts/327q6So9Y79AooBkQ/si-visiting-fellow-diego-caleiro-gives-a-ted-talk-on", "linkUrl": "https://www.lesswrong.com/posts/327q6So9Y79AooBkQ/si-visiting-fellow-diego-caleiro-gives-a-ted-talk-on", "postedAtFormatted": "Tuesday, July 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SI%20visiting%20fellow%20Diego%20Caleiro%20gives%20a%20TED%20talk%20on%20Friendly%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASI%20visiting%20fellow%20Diego%20Caleiro%20gives%20a%20TED%20talk%20on%20Friendly%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F327q6So9Y79AooBkQ%2Fsi-visiting-fellow-diego-caleiro-gives-a-ted-talk-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SI%20visiting%20fellow%20Diego%20Caleiro%20gives%20a%20TED%20talk%20on%20Friendly%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F327q6So9Y79AooBkQ%2Fsi-visiting-fellow-diego-caleiro-gives-a-ted-talk-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F327q6So9Y79AooBkQ%2Fsi-visiting-fellow-diego-caleiro-gives-a-ted-talk-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<p>&nbsp;</p>\n<p>Diego Caleiro, a past SI <a href=\"http://intelligence.org/visiting-fellows/\">visiting fellow</a>&nbsp;and current&nbsp;leader of a transhumanist institution in Brazil, recently delivered <a href=\"http://talentsearch.ted.com/video/Diego-Caleiro-What-if-I-had-don;TEDSao-Paulo\">this TED@SaoPaulo talk</a> on Friendly AI and Effective Altruism. If it goes viral, it has a chance to be selected for TED Global. (The importance thing is to have lots of positive comments on the TED page, methinks.) TED Global's talks reach on average 40,000 viewers within the first 24 hours, and 500,000 within half a year.</p>\n<p>Check it out, share, and especially: comment!</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "327q6So9Y79AooBkQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "17532", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-10T23:20:18.690Z", "modifiedAt": null, "url": null, "title": "Reply to Holden on The Singularity Institute", "slug": "reply-to-holden-on-the-singularity-institute", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:01.796Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5GskScdvYXBpL78wL/reply-to-holden-on-the-singularity-institute", "pageUrlRelative": "/posts/5GskScdvYXBpL78wL/reply-to-holden-on-the-singularity-institute", "linkUrl": "https://www.lesswrong.com/posts/5GskScdvYXBpL78wL/reply-to-holden-on-the-singularity-institute", "postedAtFormatted": "Tuesday, July 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reply%20to%20Holden%20on%20The%20Singularity%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReply%20to%20Holden%20on%20The%20Singularity%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GskScdvYXBpL78wL%2Freply-to-holden-on-the-singularity-institute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reply%20to%20Holden%20on%20The%20Singularity%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GskScdvYXBpL78wL%2Freply-to-holden-on-the-singularity-institute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GskScdvYXBpL78wL%2Freply-to-holden-on-the-singularity-institute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7735, "htmlBody": "<p>Holden Karnofsky of <a href=\"http://www.givewell.org/\">GiveWell</a> has <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">objected</a> to the Singularity Institute (SI) as a target for <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">optimal</a> <a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">philanthropy</a>. As someone who thinks that existential risk reduction is really important and also that the Singularity Institute is an important target of optimal philanthropy, I would like to explain why I disagree with Holden on these subjects. (I am also SI's Executive Director.)</p>\n<p>Mostly, I'd like to explain my views to a broad audience. But I'd also like to explain my views to Holden himself. I value Holden's work, I enjoy interacting with him, and I think he is both intelligent <em>and</em> capable of changing his mind about Big Things like this. Hopefully Holden and I can continue to work through the arguments together, though of course we are both busy with many other things.</p>\n<p>I appreciate the clarity and substance of <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">Holden's objections</a>, and I hope to reply in kind. I begin with an overview of some basic points that may be familiar to most <em>Less Wrong</em> veterans, and then I reply point-by-point to Holden's post. In the final section, I summarize my reply to Holden.</p>\n<p>Holden raised <em>many</em> different issues, so unfortunately this post needed to be <em>long</em>. My apologies to Holden if I have misinterpreted him at any point.</p>\n<h2 id=\"contents\"><br /></h2>\n<h2>Contents</h2>\n<ul>\n<li>Existential risk reduction is a critical concern for many people, given their values and given many plausible models of the future. <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#x-risk\">Details here</a>. </li>\n<li>Among existential risks, AI risk is probably the most important. <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#ai-risk\">Details here</a>. </li>\n<li>SI can purchase many kinds of AI risk reduction more efficiently than other groups can. <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#si\">Details here</a>. </li>\n<li>These points and many others weigh against many of Holden's claims and conclusions. <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#Holden\">Details here</a>.</li>\n<li><a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#summary\">Summary of my reply to Holden</a></li>\n</ul>\n<h2 id=\"comments\"><a id=\"more\"></a><br /></h2>\n<h2>Comments</h2>\n<p>I must be brief, so while reading this post I am sure many objections will leap to your mind. To encourage <em>constructive</em> discussion on this post, <strong>each question (posted as a comment on this page) that follows the template described below will receive a reply from myself or another SI representative.</strong></p>\n<p>Please word your question as clearly and succinctly as possible, and don't assume your readers will have read this post before reading your question (because: the conversations here may be used as source material for a comprehensive FAQ).</p>\n<p>Here's an example of how you could word the first paragraph of your question: \"You claimed that [insert direct quote here], and also that [insert another direct quote here]. That seems to imply that [something something]. But that doesn't seem to take into account that [blah blah blah]. What do you think of that?\"</p>\n<p>If your question needs more explaining, leave the details to <em>subsequent</em> paragraphs in your comment. Please post multiple questions as multiple comments, so they can be voted upon and replied to individually. If you don't follow these rules, I can't guarantee SI will have time to give you a reply. (We probably <em>won't</em>.)</p>\n<h2 id=\"x-risk\"><br /></h2>\n<h2>Why many people care greatly about existential risk reduction</h2>\n<p>Why do many people consider existential risk reduction to be humanity's most important task? I can't say it much better than <a href=\"http://www.existential-risk.org/concept.pdf\">Nick Bostrom does</a>, so I'll just quote him:</p>\n<blockquote>\n<p>An existential risk is one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development. Although it is often difficult to assess the probability of existential risks, there are many reasons to suppose that the total such risk confronting humanity over the next few centuries is significant...</p>\n<p>Humanity has survived what we might call <em>natural existential risks</em> [asteroid impacts, gamma ray bursts, etc.] for hundreds of thousands of years; thus it is prima facie unlikely that any of them will do us in within the next hundred...</p>\n<p>In contrast, our species is introducing entirely new kinds of existential risk&mdash;threats we have no track record of surviving... In particular, most of the biggest existential risks seem to be linked to potential future technological breakthroughs that may radically expand our ability to manipulate the external world or our own biology. As our powers expand, so will the scale of their potential consequences&mdash;intended and unintended, positive and negative. For example, there appear to be significant existential risks in some of the advanced forms of biotechnology, molecular nanotechnology, and machine intelligence that might be developed in the decades ahead.</p>\n<p>What makes existential catastrophes especially bad is not that they would [cause] a precipitous drop in world population or average quality of life. Instead, their significance lies primarily in the fact that they would destroy the future... To calculate the loss associated with an existential catastrophe, we must consider how much value would come to exist in its absence. It turns out that the ultimate potential for Earth-originating intelligent life is literally astronomical.</p>\n<p>One gets a large number even if one confines one&rsquo;s consideration to the potential for biological human beings living on Earth. If we suppose... that our planet will remain habitable for at least another billion years, and we assume that at least one billion people could live on it sustainably, then the potential exist for at least 10<sup>18</sup> human lives. [The numbers get <em>way</em> bigger if you consider the expansion of posthuman civilization to the rest of the galaxy or the prospect of <a href=\"http://en.wikipedia.org/wiki/Whole_brain_emulation\">mind uploading</a>.]</p>\n<p>Even if we use the most conservative of these estimates, which entirely ignores the possibility of space colonization and software minds, we find that the expected loss of an existential catastrophe is greater than the value of 10<sup>16</sup> human lives...</p>\n<p>These considerations suggest that the loss in expected value resulting from an existential catastrophe is so enormous that the objective of reducing existential risks should be a dominant consideration whenever we act out of an impersonal concern for humankind as a whole.</p>\n</blockquote>\n<p>I refer the reader to <a href=\"http://www.existential-risk.org/concept.pdf\">Bostrom's paper</a> for further details and additional arguments, but neither his paper nor this post can answer every objection one might think of.</p>\n<p>Nor can I summarize all the arguments and evidence related to estimating the severity and time horizon of every proposed existential risk. Even the 500+ pages of Oxford University Press' <em><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">Global Catastrophic Risks</a></em> can barely scratch the surface of this enormous topic. As explained in <a href=\"http://intelligence.org/files/IE-EI.pdf\">Intelligence Explosion: Evidence and Import</a>, predicting long-term technological progress is <em>hard</em>. Thus, we must</p>\n<blockquote>\n<p>examine convergent outcomes that&mdash;like the evolution of eyes or the emergence of markets&mdash;can come about through any of several different paths and can gather momentum once they begin.</p>\n</blockquote>\n<p>I'll say more about convergent outcomes later, but for now I'd just like to suggest that:</p>\n<ol>\n<li>\n<p>Many humans living today value both current and future people enough that <em>if</em> existential catastrophe is plausible this century, then upon reflection (e.g. after counteracting their unconscious, default <a href=\"/lw/hw/scope_insensitivity/\">scope insensitivity</a>) they would conclude that reducing the risk of existential catastrophe is the most valuable thing they can do &mdash; whether through direct work or by <a href=\"http://intelligence.org/donate/\">donating</a> to support direct work. It is to <em>these</em> people I appeal. (I also have much to say to people who e.g. <em>don't</em> care about future people, but it is too much to say here and now.)</p>\n</li>\n<li>\n<p>As it turns out, we <em>do</em> have good reason to believe that existential catastrophe is plausible this century.</p>\n</li>\n</ol>\n<p>I don't have the space here to discuss the likelihood of different kinds of existential catastrophe that could plausibly occur this century (see <em><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">GCR</a></em> for more details), so instead I'll talk about just one of them: an AI catastrophe.</p>\n<h2 id=\"ai-risk\"><br /></h2>\n<h2>AI risk: the most important existential risk</h2>\n<p>There are two primary reasons I think AI is the most important existential risk:</p>\n<p>Reason 1: <strong>Mitigating AI risk could mitigate all other existential risks, but not vice-versa.</strong> There is an asymmetry between AI risk and other existential risks. If we mitigate the risks from (say) synthetic biology and nanotechnology (without building Friendly AI), this only means we have bought a few years or decades for ourselves before we must face yet another existential risk from powerful new technologies. But if we manage <em>AI risk</em> well enough (i.e. if we build a <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">Friendly AI</a> or \"FAI\"), we may be able to \"permanently\" (for several billion years) secure a desirable future. Machine superintelligence working in the service of humane goals could use its intelligence and resources to prevent all other existential catastrophes. (<a href=\"http://intelligence.org/files/CEV.pdf\">Eliezer</a>: \"I distinguish 'human', that which we <em>are</em>, from 'humane'&mdash;that which, being human, we <em>wish</em> we were.\")</p>\n<p>Reason 2: <strong>AI is probably the first existential risk we must face</strong> (given <em>my</em> evidence, only the tiniest fraction of which I can share in a blog post).</p>\n<p>One reason AI may be the most urgent existential risk is that it's more likely for AI (compared to other sources of catastrophic risk) to be a full-blown <em>existential</em> catastrophe (as opposed to a merely <em>billions dead</em> catastrophe). Humans are smart and adaptable; we are <a href=\"http://reflectivedisequilibrium.blogspot.com/2012/05/what-to-eat-during-impact-winter.html\">already set up</a> for a species-preserving number of humans to survive (e.g. in underground bunkers with stockpiled food, water, and medicine) major catastrophes from nuclear war, superviruses, supervolcano eruption, and many cases of asteroid impact or nanotechnological <a href=\"http://en.wikipedia.org/wiki/Ecophagy\">ecophagy</a>.</p>\n<p>Machine superintelligences, however, could intelligently seek out and neutralize humans which they (correctly) recognize as threats to the maximal realization of their goals. Humans are surprisingly easy to kill if an intelligent process is <em>trying</em> to do so. Cut off John's access to air for a few minutes, or cut off his water supply for a few days, or poke him with a <a href=\"http://en.wikipedia.org/wiki/Sword\">sharp stick</a>, and he dies. <em>Forever</em>. (Post-humans might shudder at this absurdity like we shudder at the idea that people used to die from their <em>teeth</em>.)</p>\n<p>Why think AI is coming anytime soon? This is too complicated a topic to breach here. See <a href=\"http://intelligence.org/files/IE-EI.pdf\">Intelligence Explosion: Evidence and Import</a> for a brief analysis of AI timelines. Or try <a href=\"http://www.theuncertainfuture.com/\">The Uncertain Future</a>, which outputs an estimated timeline for human-level AI based on <em>your</em> predictions of various technological developments. (SI is currently collaborating with the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> to write another paper on this subject.)</p>\n<p>It's also important to mention that the case for caring about AI risk is less conjunctive that many seem to think, which I discuss in more detail <a href=\"#disjunctive\">here</a>.</p>\n<h2 id=\"si\"><br /></h2>\n<h2>SI can purchase several kinds of AI risk reduction more efficiently than others can</h2>\n<p>The two organizations working most directly to reduce AI risk are the <a href=\"http://intelligence.org/\">Singularity Institute</a> and the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> (FHI). Luckily, these organizations complement each other well, as I <a href=\"/lw/7sc/siai_vs_fhi_achievements_20082010/4w6v\">pointed out</a> back before I was running SI:</p>\n<ul>\n<li>\n<p>FHI is part of Oxford, and thus can bring credibility to existential risk reduction. Resulting output: lots of peer-reviewed papers, books from OUP like <em>Global Catastrophic Risks</em>, conferences, media appearances, etc.</p>\n</li>\n<li>\n<p>SI is independent and is less constrained by conservatism or the university system. Resulting output: Very novel (and, to the mainstream, \"weird\") research on Friendly AI, and the ability to do unusual things that are nevertheless quite effective at finding/creating lots of new people interested in rationality and existential risk reduction: (1) <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a>, the best tool I know for creating aspiring rationalists, (2) <em><a href=\"http://hpmor.com/\">Harry Potter and the Methods of Rationality</a></em>, a surprisingly successful tool for grabbing the attention of mathematicians and computer scientists around the world, and (3) the <a href=\"http://singularitysummit.com/\">Singularity Summit</a>, a mainstream-aimed conference that brings in people who end up making significant contributions to the movement &mdash; e.g. Tomer Kagan (an SI donor and board member) and David Chalmers (author of&nbsp;<a href=\"http://www.consc.net/papers/singularityjcs.pdf\">The Singularity: A Philosophical Analysis</a>&nbsp;and <a href=\"http://consc.net/papers/singreply.pdf\">The Singularity: A Reply</a>).</p>\n</li>\n</ul>\n<p>A few weeks later, Nick Bostrom (Director of FHI) <a href=\"/lw/8fw/new_qa_by_nick_bostrom/597e\">said the same things</a> (as far as I know, <em>without</em> having read <em>my</em> comment):</p>\n<blockquote>\n<p>I think there is a sense that both organizations are synergistic. If one were about to go under... that would probably be the one [to donate to]. If both were doing well... different people will have different opinions. We work quite closely with the folks from [the Singularity Institute]...</p>\n<p>There is an advantage to having one academic platform and one outside academia. There are different things these types of organizations give us. If you wanna get academics to pay more attention to this, to get postdocs to work on this, that's much easier to do within academia; also to get the ear of policy-makers and media... On the other hand, for [SI] there might be things that are easier for them to do. More flexibility, they're not embedded in a big bureaucracy. So they can more easily hire people with non-standard backgrounds... and also more grass-roots stuff like <em>Less Wrong</em>...</p>\n</blockquote>\n<p>FHI is, despite its small size, a highly productive philosophy department. More importantly, FHI has focused its research work on AI risk issues for the past 9 months, and plans to continue on that path for at least another 12 months. This is important work that should be supported. (Note that FHI recently hired SI research associate <a href=\"http://www.danieldewey.net/\">Daniel Dewey</a>.)</p>\n<p>SI lacks FHI's publishing productivity and its university credibility, but as an organization SI is <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">improving quickly</a>, and it can seize <a href=\"/lw/cs6/how_to_purchase_ai_risk_reduction/\">many opportunities for AI risk reduction</a> that FHI is not well-positioned to seize. (<em>New</em> organizations will <em>also</em> tend to be less capable of seizing these opportunities than SI, due to the financial and human capital already concentrated at SI and FHI.)</p>\n<p>Here are some examples of projects that SI is probably better able to carry out than FHI, given its greater flexibility (and assuming sufficient funding):</p>\n<ul>\n<li>A <a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">scholarly AI risk wiki</a> written and maintained by dozens of part-time researchers from around the world.</li>\n<li><a href=\"/r/discussion/lw/cs7/reaching_young_mathcompsci_talent/\">Reaching young math/compsci talent</a> in unusual ways, e.g. <em><a href=\"http://hpmor.com/\">HPMoR</a></em>.</li>\n<li>Writing <em><a href=\"/lw/cr7/proposal_for_open_problems_in_friendly_ai/\">Open Problems in Friendly AI</a></em> (Eliezer has spent far more time working on the mathy sub-problems of FAI than anyone else).</li>\n</ul>\n<h2 id=\"Holden\"><br /></h2>\n<h2>My replies to Holden, point by point</h2>\n<p>Holden's post makes so <em>many</em> claims that I'll just have to work through his post from beginning to end, and then summarize where I think we stand at the end.</p>\n<h4 id=\"labs\"><br /></h4>\n<h4>GiveWell Labs</h4>\n<p>Holden opened \"<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">Thoughts on the Singularity Institute</a>\" by noting that SI was previously outside Givewell's scope, since GiveWell was focused on specific domains like poverty reduction. With the launch of <a href=\"http://www.givewell.org/about/labs\">GiveWell Labs</a>, GiveWell is now open to evaluating <em>any</em> giving opportunity, including SI.</p>\n<p>I admire this move. I'm sure people have been bugging GiveWell to do this for a long time, but almost none of those people appreciate how hard it is to launch broad new initiatives like this with the limited budget of an organization like Givewell or the Singularity Institute. Most of them <em>also</em> do not understand how much work is required to write something like \"<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">Thoughts on the Singularity Institute</a>\", \"<a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Reply to Holden on Tool AI</a>\", or <em>this</em> post.</p>\n<h4 id=\"outcomes\"><br /></h4>\n<h4>Three possible outcomes</h4>\n<p>Next, Holden wrote:</p>\n<blockquote>\n<p>[I hope] that one of these three things (or some combination) will happen:</p>\n<ol>\n<li>\n<p>New arguments are raised that cause me to change my mind and recognize SI as an outstanding giving opportunity. If this happens I will likely attempt to raise more money for SI (most likely by discussing it with other GiveWell staff and collectively considering a GiveWell Labs recommendation).</p>\n</li>\n<li>\n<p>SI concedes that my objections are valid and increases its determination to address them. A few years from now, SI is a better organization and more effective in its mission.</p>\n</li>\n<li>\n<p>SI can't or won't make changes, and SI's supporters feel my objections are valid, so SI loses some support, freeing up resources for other approaches to doing good.</p>\n</li>\n</ol></blockquote>\n<p>As explained at the top of Holden's post, I had already conceded that many of Holden's objections (especially concerning past organizational competence) are valid, and had been working to address them, even <em>before</em> Holden's post was published. So outcome #2 is already true in part.</p>\n<p>I hope for outcome #1, too, but I don't expect Holden to change his opinion overnight. There are too many possible objections to which Holden has not yet heard a good response. But hopefully this post and its comment threads will successfully address <em>some</em> of Holden's (and others') objections.</p>\n<p>Outcome #3 is unlikely since SI is <em>already</em> making changes, though of course it's possible we will be unable to raise sufficient funding for SI <em>despite</em> making these changes, or even <em>because of</em> our efforts to make these changes. (Improving general organizational effectiveness is important but it costs money and is not exciting to donors.)</p>\n<h4 id=\"mission\"><br /></h4>\n<h4>SI's mission is more important than SI as an organization</h4>\n<p>Holden said:</p>\n<blockquote>\n<p>whatever happens as a result of my post will be positive for SI's mission, whether or not it is positive for SI as an organization. I believe that most of SI's supporters and advocates care more about the former than about the latter, and that this attitude is far too rare in the nonprofit world.</p>\n</blockquote>\n<p>Clearly, SI's mission is more important than SI as an organization. If somebody launches an organization more effective (at AI risk reduction) than SI but just as flexible, then SI should probably fold itself and try to move its donor base, support community, and the best of its human capital to that new organization.</p>\n<p>That said, it's probably easier to reform SI into a more effective organization than it is to launch a new one, since SI has successfully concentrated lots of attention, donor support, and human capital. Also, SI has learned many lessons about how to run a very tricky kind of organization. AI risk reduction is a mission that (1) is beyond most people's time horizons for caring, (2) is hard to understand and visualize, (3) pattern-matches to science fiction and apocalyptic religion, (4) suffers under complicated and <em>necessarily</em> uncertain <a href=\"/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">strategic considerations</a> (compare to the simplicity of <a href=\"http://www.givewell.org/international/top-charities/AMF\">bed nets</a>), (5) has a very small pool of people from which to recruit researchers, etc. SI has lots of experience with these issues; experience that probably takes a long time and lots of money to acquire.</p>\n<p>(On the other hand, SI has also concentrated some bad reputation which a new organization could launch without. But I still think the weight of the arguments is in favor of reforming SI.)</p>\n<h4 id=\"clarity\"><br /></h4>\n<h4>SI's arguments need to be clearer</h4>\n<p>Holden:</p>\n<blockquote>\n<p>I do not believe that [my objections to SI's apparent views] constitute a sharp/tight case for the idea that SI's work has low/negative value; I believe, instead, that SI's own arguments are too vague for such a rebuttal to be possible. There are many possible responses to my objections, but SI's public arguments (and the private arguments) do not make clear which possible response (if any) SI would choose to take up and defend. Hopefully the dialogue following this post will clarify what SI believes and why.</p>\n</blockquote>\n<p>I agree that SI's arguments are often vague. For example, Chris Hallquist <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6k30\">reported</a>:</p>\n<blockquote>\n<p>I've been trying to write something about Eliezer's debate with Robin Hanson, but the problem I keep running up against is that Eliezer's points are not clearly articulated at all. Even making my best educated guesses about what's supposed to go in the gaps in his arguments, I still ended up with very little.</p>\n</blockquote>\n<p>I know the feeling! That's why I've tried to write as many clarifying documents as I can, including the <a href=\"http://intelligence.org/singularity-faq/\">Singularity FAQ</a>, <a href=\"http://intelligence.org/files/IE-EI.pdf\">Intelligence Explosion: Evidence and Import</a>, <a href=\"http://intelligence.org/files/SaME.pdf\">The Singularity and Machine Ethics</a>, <em><a href=\"http://facingthesingularity.com/\">Facing the Singularity</a></em>, <a href=\"http://lukeprog.com/SaveTheWorld.html\">So You Want to Save the World</a>, and <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a>.</p>\n<p>Unfortunately, it takes lots of <a href=\"http://intelligence.org/donate/\">resources</a> to write up hundreds of arguments and responses to objections in clear and precise language, and we're <a href=\"http://intelligence.org/research/\">working on it</a>. (For comparison, Nick Bostrom's forthcoming book on machine superintelligence will barely scratch the surface of the things SI and FHI researchers have worked out in conversation, and it will probably take him 2+ years to write in total, and Bostrom is <em>already</em> an unusually prolific writer.) Hopefully SI's responses to Holden's post have helped to clarify our positions already.</p>\n<h4 id=\"first_objection\"><br /></h4>\n<h4>Holden's objection #1 punts to objection #2</h4>\n<p>The first objection on Holden's numbered list was:</p>\n<blockquote>\n<p>it seems to me that any AGI that was set to maximize a \"Friendly\" utility function would be extraordinarily dangerous.</p>\n</blockquote>\n<p>I'm glad Holden agrees with us that successful Friendly AI is <em>very hard</em>. SI has spent much of its effort trying to show people that the first 20 solutions they come up with all fail. See: <a href=\"http://intelligence.org/files/AIRisk.pdf\">AI as a Positive and Negative Factor in Global Risk</a>, <a href=\"http://intelligence.org/files/SaME.pdf\">The Singularity and Machine Ethics</a>, <a href=\"http://intelligence.org/files/ComplexValues.pdf\">Complex Value Systems are Required to Realize Valuable Futures</a>, etc. Holden mentions the standard SI worry about the <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">hidden complexity of wishes</a>, and the one about a friendly utility function still causing havoc because the AI's priors are wrong (problem 3.6 from my <a href=\"http://lukeprog.com/SaveTheWorld.html\">list of open problems in AI risk research</a>).</p>\n<p>There are reasons to think FAI is harder <em>still</em>. What if we get the utility function right and we get the priors right but the AI's values <em>change</em> for the worse when it <a href=\"http://intelligence.org/files/OntologicalCrises.pdf\">updates its ontology</a>? What if the smartest, most careful, most insanely safety-conscious AI researchers humanity can produce <em>just aren't smart enough</em> to solve the problem? What if <em>no</em> humans are altruistic enough to choose to build FAI over an AI that will make them king of the universe? What if the idea of FAI is incoherent? (The human brain is an existence proof for the possibility of general intelligence, but we have <em>no</em> existence proof for the possibility of a decision theoretic agent which stably optimizes the world according to a set of preferences over states of affairs.)</p>\n<p>So, yeah. Friendly AI is <em>hard</em>. But as I said <a href=\"/lw/cck/holden_karnofskys_singularity_institute_objection/\">elsewhere</a>:</p>\n<blockquote>\n<p>The point is that <em>not</em> trying as hard as you can to build Friendly AI is even <em>worse</em>, because then you <em>almost certainly get uFAI</em>. At least by <em>trying</em> to build FAI, we've got some chance of winning.</p>\n</blockquote>\n<p>So Holden's objection #1 objection really just punts to objection #2, about tool-AGI, as the last paragraph in this section of Holden's post seems to indicate:</p>\n<blockquote>\n<p>So far, all I have argued is that the development of \"Friendliness\" theory can achieve at best only a limited reduction in the probability of an unfavorable outcome. However, as I argue in the next section, I believe there is at least one concept - the \"tool-agent\" distinction - that has more potential to reduce risks, and that SI appears to ignore this concept entirely.</p>\n</blockquote>\n<p>So if Holden's objection #2 doesn't work, then objection #1 ends up reducing to \"the development of Friendliness theory can achieve at best a reduction in AI risk,\" which is what SI has been saying all along.</p>\n<h4 id=\"toolAI\"><br /></h4>\n<h4>Tool AI</h4>\n<p>Holden's second numbered objection was:</p>\n<blockquote>\n<p>SI appears to neglect the potentially important distinction between \"tool\" and \"agent\" AI.</p>\n</blockquote>\n<p>Eliezer wrote a whole post about this <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">here</a>. To sum up:</p>\n<p>(1) <strong>Whether you're working with Tool AI or Agent AI, you need the \"Friendly AI\" domain experts that SI is trying to recruit</strong>:</p>\n<blockquote>\n<p>A \"Friendly AI programmer\" is somebody who specializes in seeing the correspondence of mathematical structures to What Happens in the Real World. It's somebody who looks at Hutter's specification of AIXI and reads the actual equations - actually stares at the Greek symbols and not just the accompanying English text - and sees, \"Oh, this AI will try to gain control of its reward channel,\" as well as numerous subtler issues like, \"This AI presumes a Cartesian boundary separating itself from the environment; it may drop an anvil on its own head.\" Similarly, working on TDT means e.g. looking at a mathematical specification of decision theory, and seeing \"Oh, this is vulnerable to blackmail\" and coming up with a mathematical counter-specification of an AI that isn't so vulnerable to blackmail.</p>\n<p>Holden's post seems to imply that if you're building a non-self-modifying planning Oracle (aka 'tool AI') rather than an acting-in-the-world agent, you don't need a Friendly AI programmer because FAI programmers only work on agents. But this isn't how the engineering skills are split up. Inside the AI, whether an agent AI or a planning Oracle, there would be similar AGI-challenges like \"build a predictive model of the world\", and similar FAI-conjugates of those challenges like finding the 'user' inside an AI-created model of the universe. The insides would look a lot more similar than the outsides. An analogy would be supposing that a machine learning professional who does sales optimization for an orange company couldn't possibly do sales optimization for a banana company, because their skills must be about oranges rather than bananas.</p>\n</blockquote>\n<p>(2) <strong>Tool AI isn't that much safer than Agent AI, because Tool AIs have lots of hidden \"gotchas\" that cause havoc, too</strong>. (See <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Eliezer's post</a> for examples.)</p>\n<p>These points illustrate something else Eliezer wrote:</p>\n<blockquote>\n<p>What the human species needs from an x-risk perspective is experts on This Whole Damn Problem [of AI risk], who will acquire whatever skills are needed to that end. The Singularity Institute exists to host such people and enable their research&mdash;once we have enough funding to find and recruit them.</p>\n</blockquote>\n<p>Indeed. We need places for experts who specialize in seeing the consequences of mathematical objects for things humans value (e.g. the Singularity Institute) just like we need places for experts on efficient charity (e.g. <a href=\"http://www.givewell.org/\">Givewell</a>).</p>\n<p>Anyway, it's worth pointing out that Holden did <em>not</em> make the common (and mistaken) argument that \"We should just build Tool AIs instead of Agent AIs and then we'll be fine.\" This is wrong for many reasons, but one obvious point is that there are incentives to build Agent AIs (because they're powerful), so even if the first 6 teams are careful enough to build only Tool AIs, the 7th team could still build Agent AI and destroy the world.</p>\n<p>Instead, Holden pointed out that <em>you could use Tool AI to increase your chances of successfully building agenty FAI</em>:</p>\n<blockquote>\n<p>if developing \"Friendly AI\" is what we seek, a tool-AGI could likely be helpful enough in thinking through this problem as to render any previous work on \"Friendliness theory\" moot. Among other things, a tool-AGI would allow transparent views into the AGI's reasoning and predictions without any reason to fear being purposefully misled, and would facilitate safe experimental testing of any utility function that one wished to eventually plug into an \"agent.\"</p>\n</blockquote>\n<p>After reading <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Eliezer's reply</a>, however, you can probably guess my replies to this paragraph:</p>\n<ol>\n<li>Tool AI isn't as safe as Holden thinks.</li>\n<li>But yeah, a Friendly AI team may very well use \"Tool AI\" to aid Friendliness research if it can figure out a safe way to do that. This doesn't obviate the need for Friendly AI researchers; it's <em>part</em> of their research toolbox.</li>\n</ol>\n<p>So Holden's Objection #2 doesn't work, which (as explained earlier) means that his Objection #1 (as stated) doesn't work either.</p>\n<h4 id=\"disjunctive\"><br /></h4>\n<h4>SI's mission assumes a scenario that is far <em>less</em> conjunctive than it initially appears.</h4>\n<p>Holden's objection #3 is:</p>\n<blockquote>\n<p>SI's envisioned scenario is far more specific and conjunctive than it appears at first glance, and I believe this scenario to be highly unlikely.</p>\n</blockquote>\n<p>His main concern here seemed to be that technological developments and other factors would render earlier FAI work irrelevant. But Eliezer's <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">clarifications</a> about what we mean by \"FAI team\" render this objection moot, at least as it is currently stated. The <em>purpose</em> of an FAI team is not to blindly develop one particular approach to Friendly AI without checking to see whether this work will be obsoleted by future developments. Instead, the purpose of an FAI team is to develop highly specialized expertise on, among other things, which kinds of research are more and less likely to be relevant given future developments.</p>\n<p>Holden's confusion about what SI means by \"FAI team\" is common and understandable, and it is one reason that SI's mission assumes a scenario that is far <em>less</em> conjunctive than it appears to many. We aren't saying we need an FAI team because we know lots of specific things about how AGI will be built 30 years from now. We're saying you need experts on \"the consequences of mathematical objects for things humans value\" (an FAI team) because AGIs are mathematical objects and will have big consequences. That's pretty disjunctive.</p>\n<p>Similarly, many people think SI's mission is predicated on <a href=\"http://wiki.lesswrong.com/wiki/Hard_takeoff\">hard takeoff</a>. After all, we call ourselves the \"Singularity Institute,\" Eliezer has spent a lot of time arguing for hard takeoff, and our <a href=\"http://intelligence.org/summary/\">current research summary</a> frames AI risk in terms of <a href=\"http://wiki.lesswrong.com/wiki/Recursive_self-improvement\">recursive self-improvement</a>.</p>\n<p>But the case for AI as a global risk, and thus the need for dedicated experts on AI risk and \"the consequences of mathematical objects for things humans value\", isn't predicated on hard takeoff. Instead, it looks something like this:</p>\n<p>(1) <strong>Eventually, most tasks are performed by machine intelligences.</strong></p>\n<p>The improved flexibility, copyability, and modifiability of machine intelligences make them economically dominant even without other advantages (<a href=\"http://www.amazon.com/Race-Against-The-Machine-ebook/dp/B005WTR4ZI/\">Brynjolfsson &amp; McAfee 2011</a>; <a href=\"http://maven.smith.edu/~thiebaut/research/singularity/ieee_spectrum__economics_of_the_singularity.pdf\">Hanson 2008</a>). In addition, there is <a href=\"http://facingthesingularity.com/2011/plenty-of-room-above-us/\">plenty of room</a> \"above\" the human brain in terms of hardware and software for general intelligence (<a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon 2012</a>; <a href=\"http://www.xuenay.net/Papers/DigitalAdvantages.pdf\">Sotala 2012</a>; <a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\">Kurzweil 2005</a>).</p>\n<p>(2) <strong>Machine intelligences don't necessarily do things we like.</strong></p>\n<p>We don't necessarily control AIs, since advanced intelligences may be inherently goal-oriented (<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\">Omohundro 2007</a>), and even if we build advanced \"Tool AIs,\" these aren't necessarily safe either (<a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Yudkowsky 2012</a>) and there will be significant economic incentives to transform them into autonomous agents (<a href=\"http://www.amazon.com/Race-Against-The-Machine-ebook/dp/B005WTR4ZI/\">Brynjolfsson &amp; McAfee 2011</a>). We don't value most possible futures, but it's <em>very</em> hard to get an autonomous AI to do exactly what you want (<a href=\"http://intelligence.org/files/AIRisk.pdf\">Yudkowsky 2008</a>, <a href=\"http://intelligence.org/files/ComplexValues.pdf\">2011</a>; <a href=\"http://intelligence.org/files/SaME.pdf\">Muehlhauser &amp; Helm 2012</a>; <a href=\"http://www.amazon.com/Governing-Lethal-Behavior-Autonomous-Robots/dp/1420085948/\">Arkin 2009</a>).</p>\n<p>(3) <strong>There are things we can do to increase the probability that machine intelligences do things we like.</strong></p>\n<p>Further research can clarify (1) the nature and severity of the risk, (2) how to engineer goal-oriented systems safely, (3) how to increase safety with <a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>, (4) how to limit and control machine intelligences (<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. 2012</a>; <a href=\"http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf\">Yampolskiy 2012</a>), (5) solutions to AI development coordination problems, and more.</p>\n<p>(4) <strong>We should do those things now.</strong></p>\n<p>People aren't doing much about these issues now. We could wait until we understand better (e.g.) what kind of AI is likely, but: (1) it might take a long time to resolve the core issues, including difficult technical subproblems that require time-consuming mathematical breakthroughs, (2) incentives <a href=\"http://intelligence.org/files/ArmsControl.pdf\">may be badly aligned</a> (e.g. there seem to be strong economic incentives to build AI, but not to take into account social and global risks for AI), (3) AI may not be that far away (<a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon 2012</a>), and (4) the transition to machine dominance may be surprisingly rapid due to (e.g.) intelligence explosion (<a href=\"http://consc.net/papers/singularityjcs.pdf\">Chalmers 2010</a>, <a href=\"http://consc.net/papers/singreply.pdf\">2012</a>; <a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon 2012</a>) or <a href=\"http://wiki.lesswrong.com/wiki/Computing_overhang\">computing overhang</a>.</p>\n<p>What do I mean by \"computing overhang\"? We may get the hardware needed for AI long before we get the software, such that once software for general intelligence is figured out, there is tons of computing hardware sitting around for running AIs (a \"computing overhang\"). Thus we could switch from a world with one autonomous AI to a world with 10 billion autonomous AIs at the speed of copying software, and thereby transition rapidly from human dominance to AI dominance even without an intelligence explosion. (This is one of the many, many things we haven't yet written up in detail up due to lack of resources.)</p>\n<p><small>(This broad argument is greatly compressed from a paper outline developed by <a href=\"http://ordinaryideas.wordpress.com/\">Paul Christiano</a>, <a href=\"/lw/7ob/timeline_of_carl_shulman_publications/\">Carl Shulman</a>, <a href=\"https://sites.google.com/site/nbeckstead/\">Nick Beckstead</a>, and myself. We'd love to write the paper at some point, but haven't had the resources to do so. The fuller version of this argument is of course more detailed.)</small></p>\n<h4 id=\"argumentation\"><br /></h4>\n<h4>SI's public argumentation</h4>\n<p>Next, Holden turned to the topic of SI's organizational effectiveness:</p>\n<blockquote>\n<p>when evaluating a group such as SI, I can't avoid placing a heavy weight on (my read on) the general competence, capability and \"intangibles\" of the people and organization, because SI's mission is not about repeating activities that have worked in the past...</p>\n<p>There are several reasons that I currently have a negative impression of SI's general competence, capability and \"intangibles.\"</p>\n</blockquote>\n<p>The first reason Holden gave for his negative impression of SI is:</p>\n<blockquote>\n<p>SI has produced enormous quantities of public argumentation... Yet I have never seen a clear response to any of the three basic objections I listed in the previous section. One of SI's major goals is to raise awareness of AI-related risks; given this, the fact that it has not advanced clear/concise/compelling arguments speaks, in my view, to its general competence.</p>\n</blockquote>\n<p>I agree <em>in part</em>. Here's what I think:</p>\n<ul>\n<li>SI <em>hasn't</em> made its arguments as clear, concise, and compelling as I would like. We're <a href=\"http://intelligence.org/research/\">working on that</a>. It takes time, money, and people who are (1) smart and capable enough to do AI risk research work and yet somehow (2) willing to work for non-profit salaries and (3) willing to <em>not</em> advance their careers like they would if they chose instead to work at a university.</li>\n<li>There are a <em>huge</em> number of possible objections to SI's arguments, and we haven't had the resources to write up clear and compelling replies to <em>all</em> of them. (See <a href=\"http://consc.net/papers/singreply.pdf\">Chalmers 2012</a> for quick rebuttals to many objections to intelligence explosion, but what he covers in that paper barely scratches the surface.) As <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Eliezer wrote</a>, Holden's complaint that SI hasn't addressed <em>his</em> particular objections \"seems to lack perspective on how <em>many</em> different things various people see as the <em>one obvious solution</em> to Friendly AI. Tool AI wasn't the obvious solution to John McCarthy, I.J. Good, or Marvin Minsky. Today's leading AI textbook, <em>Artificial Intelligence: A Modern Approach</em>... discusses Friendly AI and AI risk for 3.5 pages but doesn't mention tool AI as an obvious solution. For Ray Kurzweil, the obvious solution is merging humans and AIs. For Jurgen Schmidhuber, the obvious solution is AIs that value a certain complicated definition of complexity in their sensory inputs. Ben Goertzel, J. Storrs Hall, and Bill Hibbard, among others, have all written about how silly Singinst is to pursue Friendly AI when the solution is obviously X, for various different X. Among current leading people working on serious AGI programs labeled as such, neither Demis Hassabis (VC-funded to the tune of several million dollars) nor Moshe Looks (head of AGI research at Google) nor Henry Markram (Blue Brain at IBM) think that the obvious answer is Tool AI. Vernor Vinge, Isaac Asimov, and any number of other SF writers with technical backgrounds who spent serious time thinking about these issues didn't converge on that solution.\"</li>\n<li>SI <em>has</em> done a decent job of raising awareness of AI risk, I think. Writing <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a> and <em><a href=\"http://hpmor.com/\">HPMoR</a></em> have (indirectly) raised more awareness for AI risk that one can normally expect from, say, writing a bunch of clear and precise academic papers about a subject. (At least, it seems that way to me.)</li>\n</ul>\n<h4 id=\"endorsements\"><br /></h4>\n<h4>SI's endorsements</h4>\n<p>The second reason Holden gave for his negative impression of SI is \"a lack of impressive endorsements.\" This one is generally true, despite the three \"celebrity endorsements\" on <a href=\"http://intelligence.org/donate/\">our new donate page</a>. More impressive than these is the fact that, as Eliezer mentioned, the latest edition of the leading AI textbook spend <a href=\"/lw/4rx/singularity_and_friendly_ai_in_the_dominant_ai/\">several pages</a> talking about AI risk and Friendly AI, and discusses the work of SI-associated researchers like <a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> and <a href=\"http://steveomohundro.com/\">Steve Omohundro</a> while <em>completely ignoring</em> the existence of the older, more prestigious, and vastly larger mainstream academic field of \"<a href=\"http://en.wikipedia.org/wiki/Machine_ethics\">machine ethics</a>.\"</p>\n<p>Why don't we have impressive endorsements? To my knowledge, SI hasn't tried very hard to get them. That's <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">another thing</a> we're in the process of changing.</p>\n<h4 id=\"feedback\"><br /></h4>\n<h4>SI and feedback loops</h4>\n<p>The third reason Holden gave for his negative impression of SI is:</p>\n<blockquote>\n<p>SI seems to have passed up opportunities to test itself and its own rationality by e.g. aiming for objectively impressive accomplishments... Pursuing more impressive endorsements and developing benign but objectively recognizable innovations (particularly commercially viable ones) are two possible ways to impose more demanding feedback loops.</p>\n</blockquote>\n<p>We have thought many times about commercially viable innovations we could develop, but these would generally be large distractions from the work of our core mission. (The <a href=\"http://appliedrationality.org\">Center for Applied Rationality</a>, in contrast, has <em>many</em> opportunities to develop commercially viable innovations in line with its core mission.)</p>\n<p>Still, I <em>do</em> think it's important for the Singularity Institute to test itself with tight feedback loops wherever feasible. This is particularly difficult to do for a research organization doing a <a href=\"http://www.nickbostrom.com/old/predict.html\">philosophy of long-term forecasting</a> (30 years is not a \"tight\" feedback loop in the slightest), but that's what <a href=\"http://www.fhi.ox.ac.uk/\">FHI</a> does and they have more \"objectively impressive\" (that is, \"externally proclaimed\") accomplishments: lots of peer-reviewed publications, some major awards for its top researcher Nick Bostrom, etc.</p>\n<h4 id=\"rationality\"><br /></h4>\n<h4>SI and rationality</h4>\n<p>Holden's fourth concern about SI is that it is overconfident about the level of its own rationality, and that this seems to show itself in (e.g.) \"insufficient self-skepticism\" and \"being too selective (in terms of looking for people who share its preconceptions) when determining whom to hire and whose feedback to take seriously.\"</p>\n<p>What would provide good evidence of rationality? Holden explains:</p>\n<blockquote>\n<p>I endorse <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Eliezer Yudkowsky's statement</a>, \"Be careful &hellip; any time you find yourself defining the [rationalist] as someone other than the agent who is currently smiling from on top of a giant heap of utility.\" To me, the best evidence of superior general rationality (or of insight into it) would be objectively impressive achievements (successful commercial ventures, highly prestigious awards, clear innovations, etc.) and/or accumulation of wealth and power. As mentioned above, SI staff/supporters/advocates do not seem particularly impressive on these fronts...</p>\n</blockquote>\n<p>Unfortunately, this seems to misunderstand the term \"rationality\" as it is meant in cognitive science. As I explained <a href=\"/lw/c7g/rationality_and_winning/\">elsewhere</a>:</p>\n<blockquote>\n<p>Like intelligence and money, rationality is only a ceteris paribus predictor of success.</p>\n<p>So while it's empirically true (<a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/\">Stanovich 2010</a>) that rationality is a predictor of life success, it's a weak one. (At least, it's a weak predictor of success at the levels of human rationality we are capable of <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">training</a> <em>today</em>.) If you want to more reliably achieve life success, I recommend inheriting a billion dollars or, failing that, being born+raised to have an excellent work ethic and low akrasia.</p>\n<p>The reason you should \"be careful&hellip; any time you find yourself defining the [rationalist] as someone other than the agent who is currently smiling from on top of a giant heap of utility\" is because you should \"<a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">never end up envying someone else's mere <em>choices</em></a>.\" You are still allowed to envy their resources, intelligence, work ethic, mastery over akrasia, and other predictors of success.</p>\n</blockquote>\n<p>But I don't mean to dodge the key issue. I think SIers <em>are</em> generally more rational than most people (and so are LWers, <a href=\"http://commonsenseatheism.com/?p=16199\">it seems</a>), but I think SIers <em>have</em> often overestimated their own rationality, myself included. Certainly, I think SI's leaders have been <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6l4h\">pretty irrational</a> about <em>organizational development</em> at many times in the past. In internal communications about why SI should help launch <a href=\"http://appliedrationality.org\">CFAR</a>, one reason on my list has been: \"We need to improve our own rationality, and figure out how to create better rationalists than exist today.\"</p>\n<h4 id=\"activities\"><br /></h4>\n<h4>SI's goals and activities</h4>\n<p>Holden's fifth concern about SI is the apparent disconnect between SI's goals and its activities:</p>\n<blockquote>\n<p>SI seeks to build FAI and/or to develop and promote \"Friendliness theory\" that can be useful to others in building FAI. Yet it seems that most of its time goes to activities other than developing AI or theory.</p>\n</blockquote>\n<p>This one is pretty easy to answer. We've focused mostly on movement-building rather than direct research because, until very recently, there wasn't enough community interest or funding to seriously begin to form an FAI team. To do that you need (1) at least a few million dollars a year, and (2) enough smart, altruistic people to care about AI risk that there exist some potential <a href=\"/r/discussion/lw/cv9/building_toward_a_friendly_ai_team/\">superhero mathematicians</a> for the FAI team. And to get those two things, you've got to do <em>mostly</em> movement-building, e.g. <em><a href=\"/\">Less Wrong</a></em>, <em><a href=\"http://hpmor.com/\">HPMoR</a></em>, the <a href=\"http://singularitysummit.com/\">Singularity Summit</a>, etc.</p>\n<h4 id=\"theft\"><br /></h4>\n<h4>Theft</h4>\n<p>And of course, Holden is (rightly) concerned about the <a href=\"/lw/5il/siai_an_examination/\">2009 theft of $118,000</a> from SI, and the lack of public statements from SI on the matter.</p>\n<p>Briefly:</p>\n<ul>\n<li>Two former employees stole $118,000 from SI. Earlier this year we finally won stipulated judgments against both individuals, forcing them to pay back the full amounts they stole. We have already recovered several thousand dollars of this.</li>\n<li>We do have much better financial controls now. We consolidated our accounts so there are fewer accounts to watch, and at least three staff members check them regularly, as does our treasurer, who is <em>not</em> an SI staff member or board member.</li>\n</ul>\n<h4 id=\"mugging\"><br /></h4>\n<h4>Pascal's Mugging</h4>\n<p>In another section, Holden wrote:</p>\n<blockquote>\n<p>A common argument that SI supporters raise with me is along the lines of, \"Even if SI's arguments are weak and its staff isn't as capable as one would like to see, their goal is so important that they would be a good investment even at a tiny probability of success.\"</p>\n<p>I believe this argument to be a form of <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a> and I have outlined the reasons I believe it to be invalid...</p>\n</blockquote>\n<p>Some problems with Holden's <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">two</a> <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">posts</a>&nbsp;on this subject will be explained in a forthcoming post by Steven Kaas. But as Holden notes, some SI principals like <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4nzy\">Eliezer</a> don't use \"small probability of large impact\" arguments, anyway. We in fact argue that the probability of a large impact is <em>not</em> tiny.</p>\n<h2 id=\"summary\"><br /></h2>\n<h2>Summary of my reply to Holden</h2>\n<p>Now that I have addressed so many details, let us return to the big picture. My summarized reply to Holden goes like this:</p>\n<p>Holden's first two objections can be summarized as arguing that developing the Friendly AI approach is more dangerous than developing non-agent \"Tool\" AI. <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Eliezer's post</a> points out that \"Friendly AI\" domain experts are what you need whether you're working with Tool AI or Agent AI, because (1) both of these approaches require FAI experts (experts in seeing the consequences of mathematical objects for what humans value), and because (2) Tool AI isn't necessarily much safer than Agent AI, because Tool AIs have lots of hidden gotchas, too. Thus, \"What the human species needs from an x-risk perspective is experts on This Whole Damn Problem [of AI risk], who will acquire whatever skills are needed to that end. The Singularity Institute exists to host such people and enable their research &mdash; once we have enough funding to find and recruit them.\"</p>\n<p>Holden's third objection was that the argument behind SI's mission is more conjunctive than it seems. I <a href=\"#disjunctive\">replied</a> that the argument behind SI's mission is actually <em>less</em> conjunctive than it often seems, because an \"FAI team\" works on a broader set of problems than Holden had realized, and because the case for AI risk is more disjunctive than many people realize. These confusions are understandable, however, and they probably are a result of insufficient clear argumentative writing from SI on these matters &mdash; a problem we am trying to fix with several recent and forthcoming <a href=\"http://intelligence.org/research/\">papers</a> and other communications (like this one).</p>\n<p>Holden's next objection concerned SI as an organization: \"SI has, or has had, multiple properties that I associate with ineffective organizations.\" I acknowledged these problems before Holden published his post, and have since outlined the <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">many improvements</a> we've made to organizational effectiveness since I was made Executive Director. I addressed several of Holden's specific worries <a href=\"#argumentation\">here</a>.</p>\n<p>Finally, Holden recommended giving to a donor-advised fund rather than to SI:</p>\n<blockquote>\n<p>I don't think that \"Cause X is the one I care about and Organization Y is the only one working on it\" to be a good reason to support Organization Y. For donors determined to donate within this cause, I encourage you to consider donating to a donor-advised fund while making it clear that you intend to grant out the funds to existential-risk-reduction-related organizations in the future....</p>\n<p>For one who accepts my arguments about SI, I believe withholding funds in this way is likely to be better for SI's mission than donating to SI</p>\n</blockquote>\n<p>By now I've called into question most of Holden's arguments about SI, but I will still address the issue of donating to SI vs. donating to a <a href=\"http://en.wikipedia.org/wiki/Donor_advised_fund\">donor-advised fund</a>.</p>\n<p>First: Which public charity would administer the donor-advised fund? Remember also that in the U.S., the administering charity need not spend from the donor-advised fund as the donor wishes, though they often do.</p>\n<p>Second: As I said <a href=\"#mission\">earlier</a>,</p>\n<blockquote>\n<p>it's probably easier to reform SI into a more effective organization than it is to launch a new one, since SI has successfully concentrated lots of attention, donor support, and human capital. Also, SI has learned many lessons about how to run a very tricky kind of organization. AI risk reduction is a mission that (1) is beyond most people's time horizons for caring, (2) is hard to understand and visualize, (3) pattern-matches to science fiction and apocalyptic religion, (4) suffers under complicated and <em>necessarily</em> uncertain <a href=\"/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">strategic considerations</a> (compare to the simplicity of <a href=\"http://www.givewell.org/international/top-charities/AMF\">bed nets</a>), (5) has a very small pool of people from which to recruit researchers, etc. SI has lots of experience with these issues; experience that probably takes a long time and lots of money to acquire.</p>\n</blockquote>\n<p>The case for funding improvements and growth at SI (as opposed to starving SI as Holden suggests) is bolstered by the fact that <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">SI's productivity and effectiveness have been improving rapidly</a> of late, and many other improvements (and <a href=\"/lw/cs6/how_to_purchase_ai_risk_reduction/\">exciting projects</a>) are on our \"to-do\" list if we can raise sufficient funding to implement them.</p>\n<p>Holden even seems to share some of this optimism:</p>\n<blockquote>\n<p>Luke's... recognition of the problems I raise... increases my estimate of the likelihood that SI will work to address them...</p>\n<p>I'm aware that SI has relatively new leadership that is attempting to address the issues behind some of my complaints. I have a generally positive impression of the new leadership; I believe the Executive Director and Development Director, in particular, to represent a step forward in terms of being interested in transparency and in testing their own general rationality. So I will not be surprised if there is some improvement in the coming years...</p>\n</blockquote>\n<h4 id=\"conclusion\"><br /></h4>\n<h4>Conclusion</h4>\n<p>For brevity's sake I have skipped many important details. I may also have misinterpreted Holden somewhere. And surely, Holden and other readers have follow-up questions and objections. This is not the end of the conversation; it is closer to the beginning. I invite you to leave your comments, preferably in accordance with <a href=\"#comments\">these guidelines</a> (for improved discussion clarity).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 4, "LXk7bxNkYSjgatdAt": 1, "sYm3HiWcfZvrGu3ui": 1, "xEZwTHPd5AWpgQx9w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5GskScdvYXBpL78wL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 69, "extendedScore": null, "score": 0.00017677694839090508, "legacy": true, "legacyId": "17500", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Holden Karnofsky of <a href=\"http://www.givewell.org/\">GiveWell</a> has <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">objected</a> to the Singularity Institute (SI) as a target for <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">optimal</a> <a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">philanthropy</a>. As someone who thinks that existential risk reduction is really important and also that the Singularity Institute is an important target of optimal philanthropy, I would like to explain why I disagree with Holden on these subjects. (I am also SI's Executive Director.)</p>\n<p>Mostly, I'd like to explain my views to a broad audience. But I'd also like to explain my views to Holden himself. I value Holden's work, I enjoy interacting with him, and I think he is both intelligent <em>and</em> capable of changing his mind about Big Things like this. Hopefully Holden and I can continue to work through the arguments together, though of course we are both busy with many other things.</p>\n<p>I appreciate the clarity and substance of <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">Holden's objections</a>, and I hope to reply in kind. I begin with an overview of some basic points that may be familiar to most <em>Less Wrong</em> veterans, and then I reply point-by-point to Holden's post. In the final section, I summarize my reply to Holden.</p>\n<p>Holden raised <em>many</em> different issues, so unfortunately this post needed to be <em>long</em>. My apologies to Holden if I have misinterpreted him at any point.</p>\n<h2 id=\"contents\"><br></h2>\n<h2 id=\"Contents\">Contents</h2>\n<ul>\n<li>Existential risk reduction is a critical concern for many people, given their values and given many plausible models of the future. <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#x-risk\">Details here</a>. </li>\n<li>Among existential risks, AI risk is probably the most important. <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#ai-risk\">Details here</a>. </li>\n<li>SI can purchase many kinds of AI risk reduction more efficiently than other groups can. <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#si\">Details here</a>. </li>\n<li>These points and many others weigh against many of Holden's claims and conclusions. <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#Holden\">Details here</a>.</li>\n<li><a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#summary\">Summary of my reply to Holden</a></li>\n</ul>\n<h2 id=\"comments\"><a id=\"more\"></a><br></h2>\n<h2 id=\"Comments\">Comments</h2>\n<p>I must be brief, so while reading this post I am sure many objections will leap to your mind. To encourage <em>constructive</em> discussion on this post, <strong>each question (posted as a comment on this page) that follows the template described below will receive a reply from myself or another SI representative.</strong></p>\n<p>Please word your question as clearly and succinctly as possible, and don't assume your readers will have read this post before reading your question (because: the conversations here may be used as source material for a comprehensive FAQ).</p>\n<p>Here's an example of how you could word the first paragraph of your question: \"You claimed that [insert direct quote here], and also that [insert another direct quote here]. That seems to imply that [something something]. But that doesn't seem to take into account that [blah blah blah]. What do you think of that?\"</p>\n<p>If your question needs more explaining, leave the details to <em>subsequent</em> paragraphs in your comment. Please post multiple questions as multiple comments, so they can be voted upon and replied to individually. If you don't follow these rules, I can't guarantee SI will have time to give you a reply. (We probably <em>won't</em>.)</p>\n<h2 id=\"x-risk\"><br></h2>\n<h2 id=\"Why_many_people_care_greatly_about_existential_risk_reduction\">Why many people care greatly about existential risk reduction</h2>\n<p>Why do many people consider existential risk reduction to be humanity's most important task? I can't say it much better than <a href=\"http://www.existential-risk.org/concept.pdf\">Nick Bostrom does</a>, so I'll just quote him:</p>\n<blockquote>\n<p>An existential risk is one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development. Although it is often difficult to assess the probability of existential risks, there are many reasons to suppose that the total such risk confronting humanity over the next few centuries is significant...</p>\n<p>Humanity has survived what we might call <em>natural existential risks</em> [asteroid impacts, gamma ray bursts, etc.] for hundreds of thousands of years; thus it is prima facie unlikely that any of them will do us in within the next hundred...</p>\n<p>In contrast, our species is introducing entirely new kinds of existential risk\u2014threats we have no track record of surviving... In particular, most of the biggest existential risks seem to be linked to potential future technological breakthroughs that may radically expand our ability to manipulate the external world or our own biology. As our powers expand, so will the scale of their potential consequences\u2014intended and unintended, positive and negative. For example, there appear to be significant existential risks in some of the advanced forms of biotechnology, molecular nanotechnology, and machine intelligence that might be developed in the decades ahead.</p>\n<p>What makes existential catastrophes especially bad is not that they would [cause] a precipitous drop in world population or average quality of life. Instead, their significance lies primarily in the fact that they would destroy the future... To calculate the loss associated with an existential catastrophe, we must consider how much value would come to exist in its absence. It turns out that the ultimate potential for Earth-originating intelligent life is literally astronomical.</p>\n<p>One gets a large number even if one confines one\u2019s consideration to the potential for biological human beings living on Earth. If we suppose... that our planet will remain habitable for at least another billion years, and we assume that at least one billion people could live on it sustainably, then the potential exist for at least 10<sup>18</sup> human lives. [The numbers get <em>way</em> bigger if you consider the expansion of posthuman civilization to the rest of the galaxy or the prospect of <a href=\"http://en.wikipedia.org/wiki/Whole_brain_emulation\">mind uploading</a>.]</p>\n<p>Even if we use the most conservative of these estimates, which entirely ignores the possibility of space colonization and software minds, we find that the expected loss of an existential catastrophe is greater than the value of 10<sup>16</sup> human lives...</p>\n<p>These considerations suggest that the loss in expected value resulting from an existential catastrophe is so enormous that the objective of reducing existential risks should be a dominant consideration whenever we act out of an impersonal concern for humankind as a whole.</p>\n</blockquote>\n<p>I refer the reader to <a href=\"http://www.existential-risk.org/concept.pdf\">Bostrom's paper</a> for further details and additional arguments, but neither his paper nor this post can answer every objection one might think of.</p>\n<p>Nor can I summarize all the arguments and evidence related to estimating the severity and time horizon of every proposed existential risk. Even the 500+ pages of Oxford University Press' <em><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">Global Catastrophic Risks</a></em> can barely scratch the surface of this enormous topic. As explained in <a href=\"http://intelligence.org/files/IE-EI.pdf\">Intelligence Explosion: Evidence and Import</a>, predicting long-term technological progress is <em>hard</em>. Thus, we must</p>\n<blockquote>\n<p>examine convergent outcomes that\u2014like the evolution of eyes or the emergence of markets\u2014can come about through any of several different paths and can gather momentum once they begin.</p>\n</blockquote>\n<p>I'll say more about convergent outcomes later, but for now I'd just like to suggest that:</p>\n<ol>\n<li>\n<p>Many humans living today value both current and future people enough that <em>if</em> existential catastrophe is plausible this century, then upon reflection (e.g. after counteracting their unconscious, default <a href=\"/lw/hw/scope_insensitivity/\">scope insensitivity</a>) they would conclude that reducing the risk of existential catastrophe is the most valuable thing they can do \u2014 whether through direct work or by <a href=\"http://intelligence.org/donate/\">donating</a> to support direct work. It is to <em>these</em> people I appeal. (I also have much to say to people who e.g. <em>don't</em> care about future people, but it is too much to say here and now.)</p>\n</li>\n<li>\n<p>As it turns out, we <em>do</em> have good reason to believe that existential catastrophe is plausible this century.</p>\n</li>\n</ol>\n<p>I don't have the space here to discuss the likelihood of different kinds of existential catastrophe that could plausibly occur this century (see <em><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">GCR</a></em> for more details), so instead I'll talk about just one of them: an AI catastrophe.</p>\n<h2 id=\"ai-risk\"><br></h2>\n<h2 id=\"AI_risk__the_most_important_existential_risk\">AI risk: the most important existential risk</h2>\n<p>There are two primary reasons I think AI is the most important existential risk:</p>\n<p>Reason 1: <strong>Mitigating AI risk could mitigate all other existential risks, but not vice-versa.</strong> There is an asymmetry between AI risk and other existential risks. If we mitigate the risks from (say) synthetic biology and nanotechnology (without building Friendly AI), this only means we have bought a few years or decades for ourselves before we must face yet another existential risk from powerful new technologies. But if we manage <em>AI risk</em> well enough (i.e. if we build a <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">Friendly AI</a> or \"FAI\"), we may be able to \"permanently\" (for several billion years) secure a desirable future. Machine superintelligence working in the service of humane goals could use its intelligence and resources to prevent all other existential catastrophes. (<a href=\"http://intelligence.org/files/CEV.pdf\">Eliezer</a>: \"I distinguish 'human', that which we <em>are</em>, from 'humane'\u2014that which, being human, we <em>wish</em> we were.\")</p>\n<p>Reason 2: <strong>AI is probably the first existential risk we must face</strong> (given <em>my</em> evidence, only the tiniest fraction of which I can share in a blog post).</p>\n<p>One reason AI may be the most urgent existential risk is that it's more likely for AI (compared to other sources of catastrophic risk) to be a full-blown <em>existential</em> catastrophe (as opposed to a merely <em>billions dead</em> catastrophe). Humans are smart and adaptable; we are <a href=\"http://reflectivedisequilibrium.blogspot.com/2012/05/what-to-eat-during-impact-winter.html\">already set up</a> for a species-preserving number of humans to survive (e.g. in underground bunkers with stockpiled food, water, and medicine) major catastrophes from nuclear war, superviruses, supervolcano eruption, and many cases of asteroid impact or nanotechnological <a href=\"http://en.wikipedia.org/wiki/Ecophagy\">ecophagy</a>.</p>\n<p>Machine superintelligences, however, could intelligently seek out and neutralize humans which they (correctly) recognize as threats to the maximal realization of their goals. Humans are surprisingly easy to kill if an intelligent process is <em>trying</em> to do so. Cut off John's access to air for a few minutes, or cut off his water supply for a few days, or poke him with a <a href=\"http://en.wikipedia.org/wiki/Sword\">sharp stick</a>, and he dies. <em>Forever</em>. (Post-humans might shudder at this absurdity like we shudder at the idea that people used to die from their <em>teeth</em>.)</p>\n<p>Why think AI is coming anytime soon? This is too complicated a topic to breach here. See <a href=\"http://intelligence.org/files/IE-EI.pdf\">Intelligence Explosion: Evidence and Import</a> for a brief analysis of AI timelines. Or try <a href=\"http://www.theuncertainfuture.com/\">The Uncertain Future</a>, which outputs an estimated timeline for human-level AI based on <em>your</em> predictions of various technological developments. (SI is currently collaborating with the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> to write another paper on this subject.)</p>\n<p>It's also important to mention that the case for caring about AI risk is less conjunctive that many seem to think, which I discuss in more detail <a href=\"#disjunctive\">here</a>.</p>\n<h2 id=\"si\"><br></h2>\n<h2 id=\"SI_can_purchase_several_kinds_of_AI_risk_reduction_more_efficiently_than_others_can\">SI can purchase several kinds of AI risk reduction more efficiently than others can</h2>\n<p>The two organizations working most directly to reduce AI risk are the <a href=\"http://intelligence.org/\">Singularity Institute</a> and the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> (FHI). Luckily, these organizations complement each other well, as I <a href=\"/lw/7sc/siai_vs_fhi_achievements_20082010/4w6v\">pointed out</a> back before I was running SI:</p>\n<ul>\n<li>\n<p>FHI is part of Oxford, and thus can bring credibility to existential risk reduction. Resulting output: lots of peer-reviewed papers, books from OUP like <em>Global Catastrophic Risks</em>, conferences, media appearances, etc.</p>\n</li>\n<li>\n<p>SI is independent and is less constrained by conservatism or the university system. Resulting output: Very novel (and, to the mainstream, \"weird\") research on Friendly AI, and the ability to do unusual things that are nevertheless quite effective at finding/creating lots of new people interested in rationality and existential risk reduction: (1) <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a>, the best tool I know for creating aspiring rationalists, (2) <em><a href=\"http://hpmor.com/\">Harry Potter and the Methods of Rationality</a></em>, a surprisingly successful tool for grabbing the attention of mathematicians and computer scientists around the world, and (3) the <a href=\"http://singularitysummit.com/\">Singularity Summit</a>, a mainstream-aimed conference that brings in people who end up making significant contributions to the movement \u2014 e.g. Tomer Kagan (an SI donor and board member) and David Chalmers (author of&nbsp;<a href=\"http://www.consc.net/papers/singularityjcs.pdf\">The Singularity: A Philosophical Analysis</a>&nbsp;and <a href=\"http://consc.net/papers/singreply.pdf\">The Singularity: A Reply</a>).</p>\n</li>\n</ul>\n<p>A few weeks later, Nick Bostrom (Director of FHI) <a href=\"/lw/8fw/new_qa_by_nick_bostrom/597e\">said the same things</a> (as far as I know, <em>without</em> having read <em>my</em> comment):</p>\n<blockquote>\n<p>I think there is a sense that both organizations are synergistic. If one were about to go under... that would probably be the one [to donate to]. If both were doing well... different people will have different opinions. We work quite closely with the folks from [the Singularity Institute]...</p>\n<p>There is an advantage to having one academic platform and one outside academia. There are different things these types of organizations give us. If you wanna get academics to pay more attention to this, to get postdocs to work on this, that's much easier to do within academia; also to get the ear of policy-makers and media... On the other hand, for [SI] there might be things that are easier for them to do. More flexibility, they're not embedded in a big bureaucracy. So they can more easily hire people with non-standard backgrounds... and also more grass-roots stuff like <em>Less Wrong</em>...</p>\n</blockquote>\n<p>FHI is, despite its small size, a highly productive philosophy department. More importantly, FHI has focused its research work on AI risk issues for the past 9 months, and plans to continue on that path for at least another 12 months. This is important work that should be supported. (Note that FHI recently hired SI research associate <a href=\"http://www.danieldewey.net/\">Daniel Dewey</a>.)</p>\n<p>SI lacks FHI's publishing productivity and its university credibility, but as an organization SI is <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">improving quickly</a>, and it can seize <a href=\"/lw/cs6/how_to_purchase_ai_risk_reduction/\">many opportunities for AI risk reduction</a> that FHI is not well-positioned to seize. (<em>New</em> organizations will <em>also</em> tend to be less capable of seizing these opportunities than SI, due to the financial and human capital already concentrated at SI and FHI.)</p>\n<p>Here are some examples of projects that SI is probably better able to carry out than FHI, given its greater flexibility (and assuming sufficient funding):</p>\n<ul>\n<li>A <a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">scholarly AI risk wiki</a> written and maintained by dozens of part-time researchers from around the world.</li>\n<li><a href=\"/r/discussion/lw/cs7/reaching_young_mathcompsci_talent/\">Reaching young math/compsci talent</a> in unusual ways, e.g. <em><a href=\"http://hpmor.com/\">HPMoR</a></em>.</li>\n<li>Writing <em><a href=\"/lw/cr7/proposal_for_open_problems_in_friendly_ai/\">Open Problems in Friendly AI</a></em> (Eliezer has spent far more time working on the mathy sub-problems of FAI than anyone else).</li>\n</ul>\n<h2 id=\"Holden\"><br></h2>\n<h2 id=\"My_replies_to_Holden__point_by_point\">My replies to Holden, point by point</h2>\n<p>Holden's post makes so <em>many</em> claims that I'll just have to work through his post from beginning to end, and then summarize where I think we stand at the end.</p>\n<h4 id=\"labs\"><br></h4>\n<h4 id=\"GiveWell_Labs\">GiveWell Labs</h4>\n<p>Holden opened \"<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">Thoughts on the Singularity Institute</a>\" by noting that SI was previously outside Givewell's scope, since GiveWell was focused on specific domains like poverty reduction. With the launch of <a href=\"http://www.givewell.org/about/labs\">GiveWell Labs</a>, GiveWell is now open to evaluating <em>any</em> giving opportunity, including SI.</p>\n<p>I admire this move. I'm sure people have been bugging GiveWell to do this for a long time, but almost none of those people appreciate how hard it is to launch broad new initiatives like this with the limited budget of an organization like Givewell or the Singularity Institute. Most of them <em>also</em> do not understand how much work is required to write something like \"<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">Thoughts on the Singularity Institute</a>\", \"<a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Reply to Holden on Tool AI</a>\", or <em>this</em> post.</p>\n<h4 id=\"outcomes\"><br></h4>\n<h4 id=\"Three_possible_outcomes\">Three possible outcomes</h4>\n<p>Next, Holden wrote:</p>\n<blockquote>\n<p>[I hope] that one of these three things (or some combination) will happen:</p>\n<ol>\n<li>\n<p>New arguments are raised that cause me to change my mind and recognize SI as an outstanding giving opportunity. If this happens I will likely attempt to raise more money for SI (most likely by discussing it with other GiveWell staff and collectively considering a GiveWell Labs recommendation).</p>\n</li>\n<li>\n<p>SI concedes that my objections are valid and increases its determination to address them. A few years from now, SI is a better organization and more effective in its mission.</p>\n</li>\n<li>\n<p>SI can't or won't make changes, and SI's supporters feel my objections are valid, so SI loses some support, freeing up resources for other approaches to doing good.</p>\n</li>\n</ol></blockquote>\n<p>As explained at the top of Holden's post, I had already conceded that many of Holden's objections (especially concerning past organizational competence) are valid, and had been working to address them, even <em>before</em> Holden's post was published. So outcome #2 is already true in part.</p>\n<p>I hope for outcome #1, too, but I don't expect Holden to change his opinion overnight. There are too many possible objections to which Holden has not yet heard a good response. But hopefully this post and its comment threads will successfully address <em>some</em> of Holden's (and others') objections.</p>\n<p>Outcome #3 is unlikely since SI is <em>already</em> making changes, though of course it's possible we will be unable to raise sufficient funding for SI <em>despite</em> making these changes, or even <em>because of</em> our efforts to make these changes. (Improving general organizational effectiveness is important but it costs money and is not exciting to donors.)</p>\n<h4 id=\"mission\"><br></h4>\n<h4 id=\"SI_s_mission_is_more_important_than_SI_as_an_organization\">SI's mission is more important than SI as an organization</h4>\n<p>Holden said:</p>\n<blockquote>\n<p>whatever happens as a result of my post will be positive for SI's mission, whether or not it is positive for SI as an organization. I believe that most of SI's supporters and advocates care more about the former than about the latter, and that this attitude is far too rare in the nonprofit world.</p>\n</blockquote>\n<p>Clearly, SI's mission is more important than SI as an organization. If somebody launches an organization more effective (at AI risk reduction) than SI but just as flexible, then SI should probably fold itself and try to move its donor base, support community, and the best of its human capital to that new organization.</p>\n<p>That said, it's probably easier to reform SI into a more effective organization than it is to launch a new one, since SI has successfully concentrated lots of attention, donor support, and human capital. Also, SI has learned many lessons about how to run a very tricky kind of organization. AI risk reduction is a mission that (1) is beyond most people's time horizons for caring, (2) is hard to understand and visualize, (3) pattern-matches to science fiction and apocalyptic religion, (4) suffers under complicated and <em>necessarily</em> uncertain <a href=\"/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">strategic considerations</a> (compare to the simplicity of <a href=\"http://www.givewell.org/international/top-charities/AMF\">bed nets</a>), (5) has a very small pool of people from which to recruit researchers, etc. SI has lots of experience with these issues; experience that probably takes a long time and lots of money to acquire.</p>\n<p>(On the other hand, SI has also concentrated some bad reputation which a new organization could launch without. But I still think the weight of the arguments is in favor of reforming SI.)</p>\n<h4 id=\"clarity\"><br></h4>\n<h4 id=\"SI_s_arguments_need_to_be_clearer\">SI's arguments need to be clearer</h4>\n<p>Holden:</p>\n<blockquote>\n<p>I do not believe that [my objections to SI's apparent views] constitute a sharp/tight case for the idea that SI's work has low/negative value; I believe, instead, that SI's own arguments are too vague for such a rebuttal to be possible. There are many possible responses to my objections, but SI's public arguments (and the private arguments) do not make clear which possible response (if any) SI would choose to take up and defend. Hopefully the dialogue following this post will clarify what SI believes and why.</p>\n</blockquote>\n<p>I agree that SI's arguments are often vague. For example, Chris Hallquist <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6k30\">reported</a>:</p>\n<blockquote>\n<p>I've been trying to write something about Eliezer's debate with Robin Hanson, but the problem I keep running up against is that Eliezer's points are not clearly articulated at all. Even making my best educated guesses about what's supposed to go in the gaps in his arguments, I still ended up with very little.</p>\n</blockquote>\n<p>I know the feeling! That's why I've tried to write as many clarifying documents as I can, including the <a href=\"http://intelligence.org/singularity-faq/\">Singularity FAQ</a>, <a href=\"http://intelligence.org/files/IE-EI.pdf\">Intelligence Explosion: Evidence and Import</a>, <a href=\"http://intelligence.org/files/SaME.pdf\">The Singularity and Machine Ethics</a>, <em><a href=\"http://facingthesingularity.com/\">Facing the Singularity</a></em>, <a href=\"http://lukeprog.com/SaveTheWorld.html\">So You Want to Save the World</a>, and <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a>.</p>\n<p>Unfortunately, it takes lots of <a href=\"http://intelligence.org/donate/\">resources</a> to write up hundreds of arguments and responses to objections in clear and precise language, and we're <a href=\"http://intelligence.org/research/\">working on it</a>. (For comparison, Nick Bostrom's forthcoming book on machine superintelligence will barely scratch the surface of the things SI and FHI researchers have worked out in conversation, and it will probably take him 2+ years to write in total, and Bostrom is <em>already</em> an unusually prolific writer.) Hopefully SI's responses to Holden's post have helped to clarify our positions already.</p>\n<h4 id=\"first_objection\"><br></h4>\n<h4 id=\"Holden_s_objection__1_punts_to_objection__2\">Holden's objection #1 punts to objection #2</h4>\n<p>The first objection on Holden's numbered list was:</p>\n<blockquote>\n<p>it seems to me that any AGI that was set to maximize a \"Friendly\" utility function would be extraordinarily dangerous.</p>\n</blockquote>\n<p>I'm glad Holden agrees with us that successful Friendly AI is <em>very hard</em>. SI has spent much of its effort trying to show people that the first 20 solutions they come up with all fail. See: <a href=\"http://intelligence.org/files/AIRisk.pdf\">AI as a Positive and Negative Factor in Global Risk</a>, <a href=\"http://intelligence.org/files/SaME.pdf\">The Singularity and Machine Ethics</a>, <a href=\"http://intelligence.org/files/ComplexValues.pdf\">Complex Value Systems are Required to Realize Valuable Futures</a>, etc. Holden mentions the standard SI worry about the <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">hidden complexity of wishes</a>, and the one about a friendly utility function still causing havoc because the AI's priors are wrong (problem 3.6 from my <a href=\"http://lukeprog.com/SaveTheWorld.html\">list of open problems in AI risk research</a>).</p>\n<p>There are reasons to think FAI is harder <em>still</em>. What if we get the utility function right and we get the priors right but the AI's values <em>change</em> for the worse when it <a href=\"http://intelligence.org/files/OntologicalCrises.pdf\">updates its ontology</a>? What if the smartest, most careful, most insanely safety-conscious AI researchers humanity can produce <em>just aren't smart enough</em> to solve the problem? What if <em>no</em> humans are altruistic enough to choose to build FAI over an AI that will make them king of the universe? What if the idea of FAI is incoherent? (The human brain is an existence proof for the possibility of general intelligence, but we have <em>no</em> existence proof for the possibility of a decision theoretic agent which stably optimizes the world according to a set of preferences over states of affairs.)</p>\n<p>So, yeah. Friendly AI is <em>hard</em>. But as I said <a href=\"/lw/cck/holden_karnofskys_singularity_institute_objection/\">elsewhere</a>:</p>\n<blockquote>\n<p>The point is that <em>not</em> trying as hard as you can to build Friendly AI is even <em>worse</em>, because then you <em>almost certainly get uFAI</em>. At least by <em>trying</em> to build FAI, we've got some chance of winning.</p>\n</blockquote>\n<p>So Holden's objection #1 objection really just punts to objection #2, about tool-AGI, as the last paragraph in this section of Holden's post seems to indicate:</p>\n<blockquote>\n<p>So far, all I have argued is that the development of \"Friendliness\" theory can achieve at best only a limited reduction in the probability of an unfavorable outcome. However, as I argue in the next section, I believe there is at least one concept - the \"tool-agent\" distinction - that has more potential to reduce risks, and that SI appears to ignore this concept entirely.</p>\n</blockquote>\n<p>So if Holden's objection #2 doesn't work, then objection #1 ends up reducing to \"the development of Friendliness theory can achieve at best a reduction in AI risk,\" which is what SI has been saying all along.</p>\n<h4 id=\"toolAI\"><br></h4>\n<h4 id=\"Tool_AI\">Tool AI</h4>\n<p>Holden's second numbered objection was:</p>\n<blockquote>\n<p>SI appears to neglect the potentially important distinction between \"tool\" and \"agent\" AI.</p>\n</blockquote>\n<p>Eliezer wrote a whole post about this <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">here</a>. To sum up:</p>\n<p>(1) <strong>Whether you're working with Tool AI or Agent AI, you need the \"Friendly AI\" domain experts that SI is trying to recruit</strong>:</p>\n<blockquote>\n<p>A \"Friendly AI programmer\" is somebody who specializes in seeing the correspondence of mathematical structures to What Happens in the Real World. It's somebody who looks at Hutter's specification of AIXI and reads the actual equations - actually stares at the Greek symbols and not just the accompanying English text - and sees, \"Oh, this AI will try to gain control of its reward channel,\" as well as numerous subtler issues like, \"This AI presumes a Cartesian boundary separating itself from the environment; it may drop an anvil on its own head.\" Similarly, working on TDT means e.g. looking at a mathematical specification of decision theory, and seeing \"Oh, this is vulnerable to blackmail\" and coming up with a mathematical counter-specification of an AI that isn't so vulnerable to blackmail.</p>\n<p>Holden's post seems to imply that if you're building a non-self-modifying planning Oracle (aka 'tool AI') rather than an acting-in-the-world agent, you don't need a Friendly AI programmer because FAI programmers only work on agents. But this isn't how the engineering skills are split up. Inside the AI, whether an agent AI or a planning Oracle, there would be similar AGI-challenges like \"build a predictive model of the world\", and similar FAI-conjugates of those challenges like finding the 'user' inside an AI-created model of the universe. The insides would look a lot more similar than the outsides. An analogy would be supposing that a machine learning professional who does sales optimization for an orange company couldn't possibly do sales optimization for a banana company, because their skills must be about oranges rather than bananas.</p>\n</blockquote>\n<p>(2) <strong>Tool AI isn't that much safer than Agent AI, because Tool AIs have lots of hidden \"gotchas\" that cause havoc, too</strong>. (See <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Eliezer's post</a> for examples.)</p>\n<p>These points illustrate something else Eliezer wrote:</p>\n<blockquote>\n<p>What the human species needs from an x-risk perspective is experts on This Whole Damn Problem [of AI risk], who will acquire whatever skills are needed to that end. The Singularity Institute exists to host such people and enable their research\u2014once we have enough funding to find and recruit them.</p>\n</blockquote>\n<p>Indeed. We need places for experts who specialize in seeing the consequences of mathematical objects for things humans value (e.g. the Singularity Institute) just like we need places for experts on efficient charity (e.g. <a href=\"http://www.givewell.org/\">Givewell</a>).</p>\n<p>Anyway, it's worth pointing out that Holden did <em>not</em> make the common (and mistaken) argument that \"We should just build Tool AIs instead of Agent AIs and then we'll be fine.\" This is wrong for many reasons, but one obvious point is that there are incentives to build Agent AIs (because they're powerful), so even if the first 6 teams are careful enough to build only Tool AIs, the 7th team could still build Agent AI and destroy the world.</p>\n<p>Instead, Holden pointed out that <em>you could use Tool AI to increase your chances of successfully building agenty FAI</em>:</p>\n<blockquote>\n<p>if developing \"Friendly AI\" is what we seek, a tool-AGI could likely be helpful enough in thinking through this problem as to render any previous work on \"Friendliness theory\" moot. Among other things, a tool-AGI would allow transparent views into the AGI's reasoning and predictions without any reason to fear being purposefully misled, and would facilitate safe experimental testing of any utility function that one wished to eventually plug into an \"agent.\"</p>\n</blockquote>\n<p>After reading <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Eliezer's reply</a>, however, you can probably guess my replies to this paragraph:</p>\n<ol>\n<li>Tool AI isn't as safe as Holden thinks.</li>\n<li>But yeah, a Friendly AI team may very well use \"Tool AI\" to aid Friendliness research if it can figure out a safe way to do that. This doesn't obviate the need for Friendly AI researchers; it's <em>part</em> of their research toolbox.</li>\n</ol>\n<p>So Holden's Objection #2 doesn't work, which (as explained earlier) means that his Objection #1 (as stated) doesn't work either.</p>\n<h4 id=\"disjunctive\"><br></h4>\n<h4 id=\"SI_s_mission_assumes_a_scenario_that_is_far_less_conjunctive_than_it_initially_appears_\">SI's mission assumes a scenario that is far <em>less</em> conjunctive than it initially appears.</h4>\n<p>Holden's objection #3 is:</p>\n<blockquote>\n<p>SI's envisioned scenario is far more specific and conjunctive than it appears at first glance, and I believe this scenario to be highly unlikely.</p>\n</blockquote>\n<p>His main concern here seemed to be that technological developments and other factors would render earlier FAI work irrelevant. But Eliezer's <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">clarifications</a> about what we mean by \"FAI team\" render this objection moot, at least as it is currently stated. The <em>purpose</em> of an FAI team is not to blindly develop one particular approach to Friendly AI without checking to see whether this work will be obsoleted by future developments. Instead, the purpose of an FAI team is to develop highly specialized expertise on, among other things, which kinds of research are more and less likely to be relevant given future developments.</p>\n<p>Holden's confusion about what SI means by \"FAI team\" is common and understandable, and it is one reason that SI's mission assumes a scenario that is far <em>less</em> conjunctive than it appears to many. We aren't saying we need an FAI team because we know lots of specific things about how AGI will be built 30 years from now. We're saying you need experts on \"the consequences of mathematical objects for things humans value\" (an FAI team) because AGIs are mathematical objects and will have big consequences. That's pretty disjunctive.</p>\n<p>Similarly, many people think SI's mission is predicated on <a href=\"http://wiki.lesswrong.com/wiki/Hard_takeoff\">hard takeoff</a>. After all, we call ourselves the \"Singularity Institute,\" Eliezer has spent a lot of time arguing for hard takeoff, and our <a href=\"http://intelligence.org/summary/\">current research summary</a> frames AI risk in terms of <a href=\"http://wiki.lesswrong.com/wiki/Recursive_self-improvement\">recursive self-improvement</a>.</p>\n<p>But the case for AI as a global risk, and thus the need for dedicated experts on AI risk and \"the consequences of mathematical objects for things humans value\", isn't predicated on hard takeoff. Instead, it looks something like this:</p>\n<p>(1) <strong>Eventually, most tasks are performed by machine intelligences.</strong></p>\n<p>The improved flexibility, copyability, and modifiability of machine intelligences make them economically dominant even without other advantages (<a href=\"http://www.amazon.com/Race-Against-The-Machine-ebook/dp/B005WTR4ZI/\">Brynjolfsson &amp; McAfee 2011</a>; <a href=\"http://maven.smith.edu/~thiebaut/research/singularity/ieee_spectrum__economics_of_the_singularity.pdf\">Hanson 2008</a>). In addition, there is <a href=\"http://facingthesingularity.com/2011/plenty-of-room-above-us/\">plenty of room</a> \"above\" the human brain in terms of hardware and software for general intelligence (<a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon 2012</a>; <a href=\"http://www.xuenay.net/Papers/DigitalAdvantages.pdf\">Sotala 2012</a>; <a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\">Kurzweil 2005</a>).</p>\n<p>(2) <strong>Machine intelligences don't necessarily do things we like.</strong></p>\n<p>We don't necessarily control AIs, since advanced intelligences may be inherently goal-oriented (<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\">Omohundro 2007</a>), and even if we build advanced \"Tool AIs,\" these aren't necessarily safe either (<a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Yudkowsky 2012</a>) and there will be significant economic incentives to transform them into autonomous agents (<a href=\"http://www.amazon.com/Race-Against-The-Machine-ebook/dp/B005WTR4ZI/\">Brynjolfsson &amp; McAfee 2011</a>). We don't value most possible futures, but it's <em>very</em> hard to get an autonomous AI to do exactly what you want (<a href=\"http://intelligence.org/files/AIRisk.pdf\">Yudkowsky 2008</a>, <a href=\"http://intelligence.org/files/ComplexValues.pdf\">2011</a>; <a href=\"http://intelligence.org/files/SaME.pdf\">Muehlhauser &amp; Helm 2012</a>; <a href=\"http://www.amazon.com/Governing-Lethal-Behavior-Autonomous-Robots/dp/1420085948/\">Arkin 2009</a>).</p>\n<p>(3) <strong>There are things we can do to increase the probability that machine intelligences do things we like.</strong></p>\n<p>Further research can clarify (1) the nature and severity of the risk, (2) how to engineer goal-oriented systems safely, (3) how to increase safety with <a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>, (4) how to limit and control machine intelligences (<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. 2012</a>; <a href=\"http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf\">Yampolskiy 2012</a>), (5) solutions to AI development coordination problems, and more.</p>\n<p>(4) <strong>We should do those things now.</strong></p>\n<p>People aren't doing much about these issues now. We could wait until we understand better (e.g.) what kind of AI is likely, but: (1) it might take a long time to resolve the core issues, including difficult technical subproblems that require time-consuming mathematical breakthroughs, (2) incentives <a href=\"http://intelligence.org/files/ArmsControl.pdf\">may be badly aligned</a> (e.g. there seem to be strong economic incentives to build AI, but not to take into account social and global risks for AI), (3) AI may not be that far away (<a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon 2012</a>), and (4) the transition to machine dominance may be surprisingly rapid due to (e.g.) intelligence explosion (<a href=\"http://consc.net/papers/singularityjcs.pdf\">Chalmers 2010</a>, <a href=\"http://consc.net/papers/singreply.pdf\">2012</a>; <a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon 2012</a>) or <a href=\"http://wiki.lesswrong.com/wiki/Computing_overhang\">computing overhang</a>.</p>\n<p>What do I mean by \"computing overhang\"? We may get the hardware needed for AI long before we get the software, such that once software for general intelligence is figured out, there is tons of computing hardware sitting around for running AIs (a \"computing overhang\"). Thus we could switch from a world with one autonomous AI to a world with 10 billion autonomous AIs at the speed of copying software, and thereby transition rapidly from human dominance to AI dominance even without an intelligence explosion. (This is one of the many, many things we haven't yet written up in detail up due to lack of resources.)</p>\n<p><small>(This broad argument is greatly compressed from a paper outline developed by <a href=\"http://ordinaryideas.wordpress.com/\">Paul Christiano</a>, <a href=\"/lw/7ob/timeline_of_carl_shulman_publications/\">Carl Shulman</a>, <a href=\"https://sites.google.com/site/nbeckstead/\">Nick Beckstead</a>, and myself. We'd love to write the paper at some point, but haven't had the resources to do so. The fuller version of this argument is of course more detailed.)</small></p>\n<h4 id=\"argumentation\"><br></h4>\n<h4 id=\"SI_s_public_argumentation\">SI's public argumentation</h4>\n<p>Next, Holden turned to the topic of SI's organizational effectiveness:</p>\n<blockquote>\n<p>when evaluating a group such as SI, I can't avoid placing a heavy weight on (my read on) the general competence, capability and \"intangibles\" of the people and organization, because SI's mission is not about repeating activities that have worked in the past...</p>\n<p>There are several reasons that I currently have a negative impression of SI's general competence, capability and \"intangibles.\"</p>\n</blockquote>\n<p>The first reason Holden gave for his negative impression of SI is:</p>\n<blockquote>\n<p>SI has produced enormous quantities of public argumentation... Yet I have never seen a clear response to any of the three basic objections I listed in the previous section. One of SI's major goals is to raise awareness of AI-related risks; given this, the fact that it has not advanced clear/concise/compelling arguments speaks, in my view, to its general competence.</p>\n</blockquote>\n<p>I agree <em>in part</em>. Here's what I think:</p>\n<ul>\n<li>SI <em>hasn't</em> made its arguments as clear, concise, and compelling as I would like. We're <a href=\"http://intelligence.org/research/\">working on that</a>. It takes time, money, and people who are (1) smart and capable enough to do AI risk research work and yet somehow (2) willing to work for non-profit salaries and (3) willing to <em>not</em> advance their careers like they would if they chose instead to work at a university.</li>\n<li>There are a <em>huge</em> number of possible objections to SI's arguments, and we haven't had the resources to write up clear and compelling replies to <em>all</em> of them. (See <a href=\"http://consc.net/papers/singreply.pdf\">Chalmers 2012</a> for quick rebuttals to many objections to intelligence explosion, but what he covers in that paper barely scratches the surface.) As <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Eliezer wrote</a>, Holden's complaint that SI hasn't addressed <em>his</em> particular objections \"seems to lack perspective on how <em>many</em> different things various people see as the <em>one obvious solution</em> to Friendly AI. Tool AI wasn't the obvious solution to John McCarthy, I.J. Good, or Marvin Minsky. Today's leading AI textbook, <em>Artificial Intelligence: A Modern Approach</em>... discusses Friendly AI and AI risk for 3.5 pages but doesn't mention tool AI as an obvious solution. For Ray Kurzweil, the obvious solution is merging humans and AIs. For Jurgen Schmidhuber, the obvious solution is AIs that value a certain complicated definition of complexity in their sensory inputs. Ben Goertzel, J. Storrs Hall, and Bill Hibbard, among others, have all written about how silly Singinst is to pursue Friendly AI when the solution is obviously X, for various different X. Among current leading people working on serious AGI programs labeled as such, neither Demis Hassabis (VC-funded to the tune of several million dollars) nor Moshe Looks (head of AGI research at Google) nor Henry Markram (Blue Brain at IBM) think that the obvious answer is Tool AI. Vernor Vinge, Isaac Asimov, and any number of other SF writers with technical backgrounds who spent serious time thinking about these issues didn't converge on that solution.\"</li>\n<li>SI <em>has</em> done a decent job of raising awareness of AI risk, I think. Writing <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a> and <em><a href=\"http://hpmor.com/\">HPMoR</a></em> have (indirectly) raised more awareness for AI risk that one can normally expect from, say, writing a bunch of clear and precise academic papers about a subject. (At least, it seems that way to me.)</li>\n</ul>\n<h4 id=\"endorsements\"><br></h4>\n<h4 id=\"SI_s_endorsements\">SI's endorsements</h4>\n<p>The second reason Holden gave for his negative impression of SI is \"a lack of impressive endorsements.\" This one is generally true, despite the three \"celebrity endorsements\" on <a href=\"http://intelligence.org/donate/\">our new donate page</a>. More impressive than these is the fact that, as Eliezer mentioned, the latest edition of the leading AI textbook spend <a href=\"/lw/4rx/singularity_and_friendly_ai_in_the_dominant_ai/\">several pages</a> talking about AI risk and Friendly AI, and discusses the work of SI-associated researchers like <a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> and <a href=\"http://steveomohundro.com/\">Steve Omohundro</a> while <em>completely ignoring</em> the existence of the older, more prestigious, and vastly larger mainstream academic field of \"<a href=\"http://en.wikipedia.org/wiki/Machine_ethics\">machine ethics</a>.\"</p>\n<p>Why don't we have impressive endorsements? To my knowledge, SI hasn't tried very hard to get them. That's <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">another thing</a> we're in the process of changing.</p>\n<h4 id=\"feedback\"><br></h4>\n<h4 id=\"SI_and_feedback_loops\">SI and feedback loops</h4>\n<p>The third reason Holden gave for his negative impression of SI is:</p>\n<blockquote>\n<p>SI seems to have passed up opportunities to test itself and its own rationality by e.g. aiming for objectively impressive accomplishments... Pursuing more impressive endorsements and developing benign but objectively recognizable innovations (particularly commercially viable ones) are two possible ways to impose more demanding feedback loops.</p>\n</blockquote>\n<p>We have thought many times about commercially viable innovations we could develop, but these would generally be large distractions from the work of our core mission. (The <a href=\"http://appliedrationality.org\">Center for Applied Rationality</a>, in contrast, has <em>many</em> opportunities to develop commercially viable innovations in line with its core mission.)</p>\n<p>Still, I <em>do</em> think it's important for the Singularity Institute to test itself with tight feedback loops wherever feasible. This is particularly difficult to do for a research organization doing a <a href=\"http://www.nickbostrom.com/old/predict.html\">philosophy of long-term forecasting</a> (30 years is not a \"tight\" feedback loop in the slightest), but that's what <a href=\"http://www.fhi.ox.ac.uk/\">FHI</a> does and they have more \"objectively impressive\" (that is, \"externally proclaimed\") accomplishments: lots of peer-reviewed publications, some major awards for its top researcher Nick Bostrom, etc.</p>\n<h4 id=\"rationality\"><br></h4>\n<h4 id=\"SI_and_rationality\">SI and rationality</h4>\n<p>Holden's fourth concern about SI is that it is overconfident about the level of its own rationality, and that this seems to show itself in (e.g.) \"insufficient self-skepticism\" and \"being too selective (in terms of looking for people who share its preconceptions) when determining whom to hire and whose feedback to take seriously.\"</p>\n<p>What would provide good evidence of rationality? Holden explains:</p>\n<blockquote>\n<p>I endorse <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Eliezer Yudkowsky's statement</a>, \"Be careful \u2026 any time you find yourself defining the [rationalist] as someone other than the agent who is currently smiling from on top of a giant heap of utility.\" To me, the best evidence of superior general rationality (or of insight into it) would be objectively impressive achievements (successful commercial ventures, highly prestigious awards, clear innovations, etc.) and/or accumulation of wealth and power. As mentioned above, SI staff/supporters/advocates do not seem particularly impressive on these fronts...</p>\n</blockquote>\n<p>Unfortunately, this seems to misunderstand the term \"rationality\" as it is meant in cognitive science. As I explained <a href=\"/lw/c7g/rationality_and_winning/\">elsewhere</a>:</p>\n<blockquote>\n<p>Like intelligence and money, rationality is only a ceteris paribus predictor of success.</p>\n<p>So while it's empirically true (<a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/\">Stanovich 2010</a>) that rationality is a predictor of life success, it's a weak one. (At least, it's a weak predictor of success at the levels of human rationality we are capable of <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">training</a> <em>today</em>.) If you want to more reliably achieve life success, I recommend inheriting a billion dollars or, failing that, being born+raised to have an excellent work ethic and low akrasia.</p>\n<p>The reason you should \"be careful\u2026 any time you find yourself defining the [rationalist] as someone other than the agent who is currently smiling from on top of a giant heap of utility\" is because you should \"<a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">never end up envying someone else's mere <em>choices</em></a>.\" You are still allowed to envy their resources, intelligence, work ethic, mastery over akrasia, and other predictors of success.</p>\n</blockquote>\n<p>But I don't mean to dodge the key issue. I think SIers <em>are</em> generally more rational than most people (and so are LWers, <a href=\"http://commonsenseatheism.com/?p=16199\">it seems</a>), but I think SIers <em>have</em> often overestimated their own rationality, myself included. Certainly, I think SI's leaders have been <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6l4h\">pretty irrational</a> about <em>organizational development</em> at many times in the past. In internal communications about why SI should help launch <a href=\"http://appliedrationality.org\">CFAR</a>, one reason on my list has been: \"We need to improve our own rationality, and figure out how to create better rationalists than exist today.\"</p>\n<h4 id=\"activities\"><br></h4>\n<h4 id=\"SI_s_goals_and_activities\">SI's goals and activities</h4>\n<p>Holden's fifth concern about SI is the apparent disconnect between SI's goals and its activities:</p>\n<blockquote>\n<p>SI seeks to build FAI and/or to develop and promote \"Friendliness theory\" that can be useful to others in building FAI. Yet it seems that most of its time goes to activities other than developing AI or theory.</p>\n</blockquote>\n<p>This one is pretty easy to answer. We've focused mostly on movement-building rather than direct research because, until very recently, there wasn't enough community interest or funding to seriously begin to form an FAI team. To do that you need (1) at least a few million dollars a year, and (2) enough smart, altruistic people to care about AI risk that there exist some potential <a href=\"/r/discussion/lw/cv9/building_toward_a_friendly_ai_team/\">superhero mathematicians</a> for the FAI team. And to get those two things, you've got to do <em>mostly</em> movement-building, e.g. <em><a href=\"/\">Less Wrong</a></em>, <em><a href=\"http://hpmor.com/\">HPMoR</a></em>, the <a href=\"http://singularitysummit.com/\">Singularity Summit</a>, etc.</p>\n<h4 id=\"theft\"><br></h4>\n<h4 id=\"Theft\">Theft</h4>\n<p>And of course, Holden is (rightly) concerned about the <a href=\"/lw/5il/siai_an_examination/\">2009 theft of $118,000</a> from SI, and the lack of public statements from SI on the matter.</p>\n<p>Briefly:</p>\n<ul>\n<li>Two former employees stole $118,000 from SI. Earlier this year we finally won stipulated judgments against both individuals, forcing them to pay back the full amounts they stole. We have already recovered several thousand dollars of this.</li>\n<li>We do have much better financial controls now. We consolidated our accounts so there are fewer accounts to watch, and at least three staff members check them regularly, as does our treasurer, who is <em>not</em> an SI staff member or board member.</li>\n</ul>\n<h4 id=\"mugging\"><br></h4>\n<h4 id=\"Pascal_s_Mugging\">Pascal's Mugging</h4>\n<p>In another section, Holden wrote:</p>\n<blockquote>\n<p>A common argument that SI supporters raise with me is along the lines of, \"Even if SI's arguments are weak and its staff isn't as capable as one would like to see, their goal is so important that they would be a good investment even at a tiny probability of success.\"</p>\n<p>I believe this argument to be a form of <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a> and I have outlined the reasons I believe it to be invalid...</p>\n</blockquote>\n<p>Some problems with Holden's <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">two</a> <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">posts</a>&nbsp;on this subject will be explained in a forthcoming post by Steven Kaas. But as Holden notes, some SI principals like <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4nzy\">Eliezer</a> don't use \"small probability of large impact\" arguments, anyway. We in fact argue that the probability of a large impact is <em>not</em> tiny.</p>\n<h2 id=\"summary\"><br></h2>\n<h2 id=\"Summary_of_my_reply_to_Holden\">Summary of my reply to Holden</h2>\n<p>Now that I have addressed so many details, let us return to the big picture. My summarized reply to Holden goes like this:</p>\n<p>Holden's first two objections can be summarized as arguing that developing the Friendly AI approach is more dangerous than developing non-agent \"Tool\" AI. <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Eliezer's post</a> points out that \"Friendly AI\" domain experts are what you need whether you're working with Tool AI or Agent AI, because (1) both of these approaches require FAI experts (experts in seeing the consequences of mathematical objects for what humans value), and because (2) Tool AI isn't necessarily much safer than Agent AI, because Tool AIs have lots of hidden gotchas, too. Thus, \"What the human species needs from an x-risk perspective is experts on This Whole Damn Problem [of AI risk], who will acquire whatever skills are needed to that end. The Singularity Institute exists to host such people and enable their research \u2014 once we have enough funding to find and recruit them.\"</p>\n<p>Holden's third objection was that the argument behind SI's mission is more conjunctive than it seems. I <a href=\"#disjunctive\">replied</a> that the argument behind SI's mission is actually <em>less</em> conjunctive than it often seems, because an \"FAI team\" works on a broader set of problems than Holden had realized, and because the case for AI risk is more disjunctive than many people realize. These confusions are understandable, however, and they probably are a result of insufficient clear argumentative writing from SI on these matters \u2014 a problem we am trying to fix with several recent and forthcoming <a href=\"http://intelligence.org/research/\">papers</a> and other communications (like this one).</p>\n<p>Holden's next objection concerned SI as an organization: \"SI has, or has had, multiple properties that I associate with ineffective organizations.\" I acknowledged these problems before Holden published his post, and have since outlined the <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">many improvements</a> we've made to organizational effectiveness since I was made Executive Director. I addressed several of Holden's specific worries <a href=\"#argumentation\">here</a>.</p>\n<p>Finally, Holden recommended giving to a donor-advised fund rather than to SI:</p>\n<blockquote>\n<p>I don't think that \"Cause X is the one I care about and Organization Y is the only one working on it\" to be a good reason to support Organization Y. For donors determined to donate within this cause, I encourage you to consider donating to a donor-advised fund while making it clear that you intend to grant out the funds to existential-risk-reduction-related organizations in the future....</p>\n<p>For one who accepts my arguments about SI, I believe withholding funds in this way is likely to be better for SI's mission than donating to SI</p>\n</blockquote>\n<p>By now I've called into question most of Holden's arguments about SI, but I will still address the issue of donating to SI vs. donating to a <a href=\"http://en.wikipedia.org/wiki/Donor_advised_fund\">donor-advised fund</a>.</p>\n<p>First: Which public charity would administer the donor-advised fund? Remember also that in the U.S., the administering charity need not spend from the donor-advised fund as the donor wishes, though they often do.</p>\n<p>Second: As I said <a href=\"#mission\">earlier</a>,</p>\n<blockquote>\n<p>it's probably easier to reform SI into a more effective organization than it is to launch a new one, since SI has successfully concentrated lots of attention, donor support, and human capital. Also, SI has learned many lessons about how to run a very tricky kind of organization. AI risk reduction is a mission that (1) is beyond most people's time horizons for caring, (2) is hard to understand and visualize, (3) pattern-matches to science fiction and apocalyptic religion, (4) suffers under complicated and <em>necessarily</em> uncertain <a href=\"/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">strategic considerations</a> (compare to the simplicity of <a href=\"http://www.givewell.org/international/top-charities/AMF\">bed nets</a>), (5) has a very small pool of people from which to recruit researchers, etc. SI has lots of experience with these issues; experience that probably takes a long time and lots of money to acquire.</p>\n</blockquote>\n<p>The case for funding improvements and growth at SI (as opposed to starving SI as Holden suggests) is bolstered by the fact that <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">SI's productivity and effectiveness have been improving rapidly</a> of late, and many other improvements (and <a href=\"/lw/cs6/how_to_purchase_ai_risk_reduction/\">exciting projects</a>) are on our \"to-do\" list if we can raise sufficient funding to implement them.</p>\n<p>Holden even seems to share some of this optimism:</p>\n<blockquote>\n<p>Luke's... recognition of the problems I raise... increases my estimate of the likelihood that SI will work to address them...</p>\n<p>I'm aware that SI has relatively new leadership that is attempting to address the issues behind some of my complaints. I have a generally positive impression of the new leadership; I believe the Executive Director and Development Director, in particular, to represent a step forward in terms of being interested in transparency and in testing their own general rationality. So I will not be surprised if there is some improvement in the coming years...</p>\n</blockquote>\n<h4 id=\"conclusion\"><br></h4>\n<h4 id=\"Conclusion\">Conclusion</h4>\n<p>For brevity's sake I have skipped many important details. I may also have misinterpreted Holden somewhere. And surely, Holden and other readers have follow-up questions and objections. This is not the end of the conversation; it is closer to the beginning. I invite you to leave your comments, preferably in accordance with <a href=\"#comments\">these guidelines</a> (for improved discussion clarity).</p>", "sections": [{"title": "Contents", "anchor": "Contents", "level": 1}, {"title": "Comments", "anchor": "Comments", "level": 1}, {"title": "Why many people care greatly about existential risk reduction", "anchor": "Why_many_people_care_greatly_about_existential_risk_reduction", "level": 1}, {"title": "AI risk: the most important existential risk", "anchor": "AI_risk__the_most_important_existential_risk", "level": 1}, {"title": "SI can purchase several kinds of AI risk reduction more efficiently than others can", "anchor": "SI_can_purchase_several_kinds_of_AI_risk_reduction_more_efficiently_than_others_can", "level": 1}, {"title": "My replies to Holden, point by point", "anchor": "My_replies_to_Holden__point_by_point", "level": 1}, {"title": "GiveWell Labs", "anchor": "GiveWell_Labs", "level": 2}, {"title": "Three possible outcomes", "anchor": "Three_possible_outcomes", "level": 2}, {"title": "SI's mission is more important than SI as an organization", "anchor": "SI_s_mission_is_more_important_than_SI_as_an_organization", "level": 2}, {"title": "SI's arguments need to be clearer", "anchor": "SI_s_arguments_need_to_be_clearer", "level": 2}, {"title": "Holden's objection #1 punts to objection #2", "anchor": "Holden_s_objection__1_punts_to_objection__2", "level": 2}, {"title": "Tool AI", "anchor": "Tool_AI", "level": 2}, {"title": "SI's mission assumes a scenario that is far less conjunctive than it initially appears.", "anchor": "SI_s_mission_assumes_a_scenario_that_is_far_less_conjunctive_than_it_initially_appears_", "level": 2}, {"title": "SI's public argumentation", "anchor": "SI_s_public_argumentation", "level": 2}, {"title": "SI's endorsements", "anchor": "SI_s_endorsements", "level": 2}, {"title": "SI and feedback loops", "anchor": "SI_and_feedback_loops", "level": 2}, {"title": "SI and rationality", "anchor": "SI_and_rationality", "level": 2}, {"title": "SI's goals and activities", "anchor": "SI_s_goals_and_activities", "level": 2}, {"title": "Theft", "anchor": "Theft", "level": 2}, {"title": "Pascal's Mugging", "anchor": "Pascal_s_Mugging", "level": 2}, {"title": "Summary of my reply to Holden", "anchor": "Summary_of_my_reply_to_Holden", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "214 comments"}], "headingsCount": 24}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 215, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm", "pC47ZTsPNAkjavkXs", "hEqsWLm5zQtsPevd3", "2ftJ38y9SRBCBsCzy", "8LrrHdt8HSvZFKmRb", "mYuMdmMmGM7fFj382", "4dFzBkpjx6aHZpjqL", "YiyAdz2DcmkR7Nz2H", "sizjfDgCgAsuLJQmm", "i2XoqtYEykc4XWp9B", "4ARaTpNX62uaL86j6", "4iqhkNv7woSnkpMRa", "4uQxZonCwCZtz39Hw", "K45SaBaB3D7o9xpAs", "6ddcsdA2c2XpNpE5x", "nEgXQkewAnvX6mYd7", "fkhbBE2ZTSytvsy9x", "p4Gd8pRcbnKo46hus", "qqhdj3W3vSfB5E9ss", "a5JAiTdytou3Jg749"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-11T02:46:10.659Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Big Monthly", "slug": "meetup-vancouver-big-monthly", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.059Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fTQZMNmFaTFtR6BLg/meetup-vancouver-big-monthly", "pageUrlRelative": "/posts/fTQZMNmFaTFtR6BLg/meetup-vancouver-big-monthly", "linkUrl": "https://www.lesswrong.com/posts/fTQZMNmFaTFtR6BLg/meetup-vancouver-big-monthly", "postedAtFormatted": "Wednesday, July 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Big%20Monthly&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Big%20Monthly%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTQZMNmFaTFtR6BLg%2Fmeetup-vancouver-big-monthly%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Big%20Monthly%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTQZMNmFaTFtR6BLg%2Fmeetup-vancouver-big-monthly", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTQZMNmFaTFtR6BLg%2Fmeetup-vancouver-big-monthly", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bt'>Vancouver Big Monthly</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 July 2012 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">885 W Georgia Street, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Lurkers everywhere, come on out for Vancouver's monthly big meetup.</p>\n\n<p>Meet in the lobby of 885 W Georgia, we may move upstairs to 1228 later.</p>\n\n<p>As usual, our mailing list is <a href=\"http://groups.google.com/gorup/vancouver-rationalists\" rel=\"nofollow\">here</a>.</p>\n\n<p>The meetup is at 18:00 on Thursday (yes that's very soon, sorry for not posting this earlier).</p>\n\n<p>See you there. (free hugs and food for incentive if you like that sort of thing).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bt'>Vancouver Big Monthly</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fTQZMNmFaTFtR6BLg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.402194008485125e-07, "legacy": true, "legacyId": "17542", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Big_Monthly\">Discussion article for the meetup : <a href=\"/meetups/bt\">Vancouver Big Monthly</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 July 2012 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">885 W Georgia Street, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Lurkers everywhere, come on out for Vancouver's monthly big meetup.</p>\n\n<p>Meet in the lobby of 885 W Georgia, we may move upstairs to 1228 later.</p>\n\n<p>As usual, our mailing list is <a href=\"http://groups.google.com/gorup/vancouver-rationalists\" rel=\"nofollow\">here</a>.</p>\n\n<p>The meetup is at 18:00 on Thursday (yes that's very soon, sorry for not posting this earlier).</p>\n\n<p>See you there. (free hugs and food for incentive if you like that sort of thing).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Big_Monthly1\">Discussion article for the meetup : <a href=\"/meetups/bt\">Vancouver Big Monthly</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Big Monthly", "anchor": "Discussion_article_for_the_meetup___Vancouver_Big_Monthly", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Big Monthly", "anchor": "Discussion_article_for_the_meetup___Vancouver_Big_Monthly1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-11T06:48:18.332Z", "modifiedAt": null, "url": null, "title": "Rationality and Cancer", "slug": "rationality-and-cancer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.807Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SwingDancerMike", "createdAt": "2012-06-20T19:35:15.919Z", "isAdmin": false, "displayName": "SwingDancerMike"}, "userId": "JjcjwJCRa4fRdFFC3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qxkv7xoDyHpnwPtCM/rationality-and-cancer", "pageUrlRelative": "/posts/Qxkv7xoDyHpnwPtCM/rationality-and-cancer", "linkUrl": "https://www.lesswrong.com/posts/Qxkv7xoDyHpnwPtCM/rationality-and-cancer", "postedAtFormatted": "Wednesday, July 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20and%20Cancer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20and%20Cancer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxkv7xoDyHpnwPtCM%2Frationality-and-cancer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20and%20Cancer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxkv7xoDyHpnwPtCM%2Frationality-and-cancer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxkv7xoDyHpnwPtCM%2Frationality-and-cancer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 505, "htmlBody": "<p>Today, my dentist found a possible oral cancer.<br /><br />I'm 31, a non-smoker, in good health. I know the research showing that doctors ignore base rates and overestimate your chances of cancer. (I asked the doctor the base rate, he didn't know.) I know that we grossly overprescribe biopsies and surgeries, when it would be better to just wait and see. But I'm having it removed and biopsied on Friday, even though I don't have dental insurance and it's costing me $1,000 of my own money.<br /><br />Why?<br /><br />I thought this would be an interesting case study: Introspectively, what's going on to make me ignore my rationalist training, ignore the external data, and choose what I know is probably the less optimal path?<br /><br />My first thought is embarrassment: If I do nothing, and it turns out I have cancer, will my support network roll their eyes and blame me for not being more aggressive? My feeling is yes, that even though they wouldn't do it to my face, they would secretly blame me, and become less available.<br /><br />My second thought is fear of the unknown: I roughly know what the biopsy will entail. It's a light anesthesia, a few stitches, and 1-2 days of recovery. No big deal. And $1,000 isn't tiny, but it's not a big deal for me, either. In contrast, what happens if I don't do the biopsy? Huge, scary unknown. And, even if I know that I only have a 0.01% chance of having cancer (to guess a number), I also know my emotional mind is bad at math, and I'll have great difficulty controlling its worry. And so, it's rational to buy some level of anti-worry insurance -- I don't know what the rational value of that anti-worry insurance is, and I don't know if that value exceeds $1,000, but clearly, anti-worry insurance has some positive value, and probably a fairly high value.<br /><br />There are other considerations. I need some wisdom teeth removed, and we're doing them at the same time, so adding the biopsy doesn't affect the recovery time. And I have a 6-week trip to Australia coming up, and I'd hate to have problems while I'm traveling. But mostly, I think it's embarrassment and worry.</p>\n<p>By the way, this is a personal matter than I'm choosing to share with you. Honest advice about how to handle it, particularly from people who've faced similar decisions, is welcome and appreciated. Flames about how I'm ignoring research are not. Thanks for understanding.</p>\n<p>Update: I talked with a friend who does research in medical decision making. She explained that, for the specific population a doctor serves, he's usually fairly accurate in his estimates of how prevalent a disease is. She also encouraged me that I'm right to get the biopsy. I feel much more relaxed, and I realized that I was feeling guilty about being irrational. I'm sure there's some meta-lesson in there that I'll figure out someday.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qxkv7xoDyHpnwPtCM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 22, "extendedScore": null, "score": 9.403334612558477e-07, "legacy": true, "legacyId": "17547", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-11T06:54:41.480Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Can Counterfactuals Be True?", "slug": "seq-rerun-can-counterfactuals-be-true", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:39.515Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gKnTic56BLYu3Y7hF/seq-rerun-can-counterfactuals-be-true", "pageUrlRelative": "/posts/gKnTic56BLYu3Y7hF/seq-rerun-can-counterfactuals-be-true", "linkUrl": "https://www.lesswrong.com/posts/gKnTic56BLYu3Y7hF/seq-rerun-can-counterfactuals-be-true", "postedAtFormatted": "Wednesday, July 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Can%20Counterfactuals%20Be%20True%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Can%20Counterfactuals%20Be%20True%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgKnTic56BLYu3Y7hF%2Fseq-rerun-can-counterfactuals-be-true%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Can%20Counterfactuals%20Be%20True%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgKnTic56BLYu3Y7hF%2Fseq-rerun-can-counterfactuals-be-true", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgKnTic56BLYu3Y7hF%2Fseq-rerun-can-counterfactuals-be-true", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>Today's post, <a href=\"/lw/sh/can_counterfactuals_be_true/\">Can Counterfactuals Be True?</a> was originally published on 24 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>How can we explain counterfactuals having a truth value, if we don't talk about \"nearby possible worlds\" or any of the other explanations offered by philosophers?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dih/seq_rerun_when_not_to_use_probabilities/\">When (Not) To Use Probabilities</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gKnTic56BLYu3Y7hF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.403364697566134e-07, "legacy": true, "legacyId": "17548", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dhGGnB2oxBP3m5cBc", "CDc6NQ2e7cmBDXjfk", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-11T08:05:20.544Z", "modifiedAt": null, "url": null, "title": "An Intuitive Explanation of Solomonoff Induction", "slug": "an-intuitive-explanation-of-solomonoff-induction", "viewCount": null, "lastCommentedAt": "2022-01-17T09:08:55.599Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alex_Altair", "createdAt": "2010-07-21T18:30:40.806Z", "isAdmin": false, "displayName": "Alex_Altair"}, "userId": "5wu9jG4pm9q6xjZ9R", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kyc5dFDzBg4WccrbK/an-intuitive-explanation-of-solomonoff-induction", "pageUrlRelative": "/posts/Kyc5dFDzBg4WccrbK/an-intuitive-explanation-of-solomonoff-induction", "linkUrl": "https://www.lesswrong.com/posts/Kyc5dFDzBg4WccrbK/an-intuitive-explanation-of-solomonoff-induction", "postedAtFormatted": "Wednesday, July 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20Intuitive%20Explanation%20of%20Solomonoff%20Induction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20Intuitive%20Explanation%20of%20Solomonoff%20Induction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKyc5dFDzBg4WccrbK%2Fan-intuitive-explanation-of-solomonoff-induction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20Intuitive%20Explanation%20of%20Solomonoff%20Induction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKyc5dFDzBg4WccrbK%2Fan-intuitive-explanation-of-solomonoff-induction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKyc5dFDzBg4WccrbK%2Fan-intuitive-explanation-of-solomonoff-induction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7302, "htmlBody": "<p><small>This is the completed article that Luke wrote the <a href=\"/lw/8nr/intuitive_explanation_of_solomonoff_induction/\">first half of</a>. My thanks go to the following for reading, editing, and commenting; Luke Muehlhauser, Louie Helm, Benjamin Noble, and Francelle Wax.</small></p>\n<p><img style=\"vertical-align: middle;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617849/mirroredImages/Kyc5dFDzBg4WccrbK/gc54xxvtafgqkj452b2r.png\" alt width=\"700\" height=\"227\"></p>\n<p>People disagree about things. Some say that television makes you dumber; other say it makes you smarter. Some scientists believe life must exist elsewhere in the universe; others believe it must not. Some say that complicated financial derivatives are essential to a modern competitive economy; others think a nation&apos;s economy will do better without them. It&apos;s hard to know what is true.</p>\n<p>And it&apos;s hard to know <em>how to figure out</em> what is true. Some argue that you should assume the things you are most certain about and then deduce all other beliefs from your original beliefs. Others think you should accept at face value the most intuitive explanations of personal experience. Still others think you should generally agree with the scientific consensus until it is disproved.</p>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617850/mirroredImages/Kyc5dFDzBg4WccrbK/jw1n4ooctjjaus3v1lfr.jpg\" alt width=\"250\" height=\"210\">Wouldn&apos;t it be nice if determining what is true was like baking a cake? What if there was a <em>recipe</em> for finding out what is true? All you&apos;d have to do is follow the written directions exactly, and after the last instruction you&apos;d inevitably find yourself with some sweet, tasty <em>truth</em>!</p>\n<p>In this tutorial, we&apos;ll explain the closest thing we&#x2019;ve found so far to a recipe for finding truth: Solomonoff induction.</p>\n<p>There are some qualifications to make. To describe just one: roughly speaking, you don&apos;t have time to follow the recipe. To find the truth to even a simple question using this recipe would require you to follow one step after another until long after the <a href=\"http://en.wikipedia.org/wiki/Heat_death_of_the_universe\">heat death</a> of the universe, and you can&apos;t do that.</p>\n<p>But we can find shortcuts. Suppose you know that the <em>exact</em> recipe for baking a cake asks you to count out one molecule of H<sub>2</sub>O at a time until you have <em>exactly</em> 0.5 cups of water. If you did that, you might not finish the cake before the heat death of the universe. But you could approximate that part of the recipe by measuring out something very close to 0.5 cups of water, and you&apos;d probably still end up with a pretty good cake.</p>\n<p>Similarly, once we know the exact recipe for finding truth, we can try to approximate it in a way that allows us to finish all the steps sometime before the sun burns out.</p>\n<p>This tutorial explains that best-we&apos;ve-got-so-far recipe for finding truth, Solomonoff induction. Don&#x2019;t worry, we won&#x2019;t be using any equations, just qualitative descriptions.</p>\n<p>Like Eliezer Yudkowsky&apos;s <a href=\"http://yudkowsky.net/rational/bayes\">Intuitive Explanation of Bayes&apos; Theorem</a> and Luke Muehlhauser&apos;s <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">Crash Course in the Neuroscience of Human Motivation</a>, this tutorial is <em>long</em>. You may not have time to read it; that&apos;s fine. But if you do read it, we recommend that you read it in sections.</p>\n<p><a id=\"more\"></a></p>\n<h4>Contents:</h4>\n<p><span style=\"white-space: pre;\"> </span>Background</p>\n<p style=\"padding-left: 30px;\">1. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#algorithms\">Algorithms</a> &#x2014; We&#x2019;re looking for an algorithm to determine truth.</p>\n<p style=\"padding-left: 30px;\">2. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#induction\">Induction</a> &#x2014; By &#x201C;determine truth&#x201D;, we mean induction.</p>\n<p style=\"padding-left: 30px;\">3. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#occams_razor\">Occam&#x2019;s Razor</a> &#x2014; How we judge between many inductive hypotheses.</p>\n<p style=\"padding-left: 30px;\">4. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#probability\">Probability</a> &#x2014; Probability is what we usually use in induction.</p>\n<p style=\"padding-left: 30px;\">5. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#the_problem_of_priors\">The Problem of Priors</a> &#x2014; Probabilities change with evidence, but where do they start?</p>\n<p>The Solution</p>\n<p style=\"padding-left: 30px;\">6. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#binary_sequences\">Binary Sequences</a> &#x2014; Everything can be encoded as binary.</p>\n<p style=\"padding-left: 30px;\">7. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#all_algorithms\">All Algorithms</a> &#x2014; Hypotheses are algorithms. Turing machines describe these.</p>\n<p style=\"padding-left: 30px;\">8. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#solomonoffs_lightsaber\">Solomonoff&apos;s Lightsaber</a> &#x2014; Putting it all together.</p>\n<p style=\"padding-left: 30px;\">9. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#formalized_science\">Formalized Science</a> &#x2014; From intuition to precision.</p>\n<p style=\"padding-left: 30px;\">10. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#approximations\">Approximations</a> &#x2014; Ongoing work towards practicality.</p>\n<p style=\"padding-left: 30px;\">11. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#unresolved_details\">Unresolved Details</a> &#x2014; Problems, philosophical and mathematical.</p>\n<h4><a name=\"algorithms\"></a>Algorithms</h4>\n<p>At an early age you learned a set of precisely-defined steps &#x2014; a &apos;recipe&apos; or, more formally, an <em>algorithm</em> &#x2014; that you could use to find the largest number in a list of numbers like this:</p>\n<p style=\"padding-left: 30px; \">21, 18, 4, 19, 55, 12, 30</p>\n<p>The algorithm you learned probably looked something like this:</p>\n<ol>\n<li>Look at the first item. Note that it is the largest you&apos;ve seen on this list so far. If this is the only item on the list, output it as the largest number on the list. Otherwise, proceed to step 2.</li>\n<li>Look at the next item. If it is larger than the largest item noted so far, note it as the largest you&apos;ve seen in this list so far. Proceed to step 3.</li>\n<li>If you have not reached the end of the list, return to step 2. Otherwise, output the last noted item as the largest number in the list.</li>\n</ol>\n<p>Other algorithms could be used to solve the same problem. For example, you could work your way from right to left instead of from left to right. But the point is that if you follow this algorithm exactly, and you have enough time to complete the task, you can&apos;t <em>fail</em> to solve the problem. You can&apos;t get confused about what one of the steps means or what the next step is. Every instruction tells you exactly what to do next, all the way through to the answer.</p>\n<p><img style=\"float: right;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617850/mirroredImages/Kyc5dFDzBg4WccrbK/uibbsnjfcy00pabk0ddv.png\" alt width=\"330\" height=\"742\">You probably learned other algorithms, too, like how to find the greatest common divisor of any two integers (see image on right).</p>\n<p>But not just any set of instructions is a precisely-defined algorithm. Sometimes, instructions are unclear or incomplete. Consider the following instructions based on <a href=\"http://science.howstuffworks.com/innovation/scientific-experiments/scientific-method6.htm\">an article</a> about the scientific method:</p>\n<ol>\n<li>Make an observation.</li>\n<li>Form a hypothesis that explains the observation.</li>\n<li>Conduct an experiment that will test the hypothesis.</li>\n<li>If the experimental results disconfirm the hypothesis, return to step #2 and form a hypothesis not yet used. If the experimental results confirm the hypothesis, provisionally accept the hypothesis.</li>\n</ol>\n<p>This is not an algorithm.</p>\n<p>First, many of the terms are not clearly defined. What counts as an observation? What counts as a hypothesis? What would a hypothesis need to be like in order to &#x2018;explain&#x2019; the observation? What counts as an experiment that will &#x2018;test&#x2019; the hypothesis? What does it mean for experimental results to &#x2018;confirm&#x2019; or &#x2018;disconfirm&#x2019; a hypothesis?</p>\n<p>Second, the instructions may be incomplete. What do we do if we reach step 4 and the experimental results neither &#x2018;confirm&#x2019; nor &#x2018;disconfirm&#x2019; the hypothesis under consideration, but instead are in some sense &#x2018;neutral&#x2019; toward the hypothesis? These instructions don&#x2019;t tell us what to do in that case.</p>\n<p>An algorithm is a well-defined procedure that takes some value or values as input and, after a finite series of steps, generates some value or values as output.</p>\n<p>For example, the &#x2018;find the largest number&#x2019; algorithm above could take the input {21, 18, 4, 19, 55, 12, 30} and would, after 13 steps, produce the following output: {55}. Or it could take the input {34} and, after 1 step, produce the output: {34}.</p>\n<p>An algorithm is so well written, that we can construct machines that follow them. Today, the machines that follow algorithms are mostly computers. This is why all computer science students take a class in algorithms. If we construct our algorithm for truth, then we can make a computer program that finds truth&#x2014;an Artificial Intelligence.</p>\n<h4><a name=\"induction\"></a>Induction</h4>\n<p>Let&#x2019;s clarify what we mean. In movies, scientists will reveal &#x201C;truth machines&#x201D;. Input a statement, and the truth machine will tell you whether it is true or false. This is <em>not</em> what Solomonoff induction does. Instead, Solomonoff induction is our ultimate &#x201C;induction machine&#x201D;.</p>\n<blockquote>\n<p>Whether we are a detective trying to catch a thief, a scientist trying to discover a new physical law, or a businessman attempting to understand a recent change in demand, we are all in the process of collecting information and trying to infer the underlying causes.</p>\n<p>-Shane Legg</p>\n</blockquote>\n<p>The problem of induction is this: We have a set of <em>observations</em> (or <em>data</em>), and we want to find the underlying causes of those observations. That is, we want to find <em>hypotheses</em> that explain our data. We&#x2019;d like to know which hypothesis is correct, so we can use that knowledge to predict future events. Our algorithm for truth will not listen to questions and answer yes or no. Our algorithm will take in data (observations) and output the rule by which the data was created. That is, it will give us the explanation of the observations; the causes.</p>\n<p>Suppose your data concern a large set of stock market changes and other events in the world. You&#x2019;d like to know the processes responsible for the stock market price changes, because then you can predict what the stock market will do in the future, and make some money.</p>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617851/mirroredImages/Kyc5dFDzBg4WccrbK/xfsjvvlsnnrbwhklytmg.jpg\" alt width=\"220\" height=\"165\">Or, suppose you are a parent. You come home from work to find a chair propped against the refrigerator, with the cookie jar atop the fridge a bit emptier than before. You like cookies, and you don&#x2019;t want them to disappear, so you start thinking. One hypothesis that leaps to mind is that your young daughter used the chair to reach the cookies. However, many other hypotheses explain the data. Perhaps a very short thief broke into your home and stole some cookies. Perhaps your daughter put the chair in front of the fridge because the fridge door is broken and no longer stays shut, and you forgot that your friend ate a few cookies when he visited last night. Perhaps you moved the chair and ate the cookies yourself while sleepwalking the night before.</p>\n<p>All these hypotheses are possible, but intuitively it seems like some hypotheses are more likely than others. If you&#x2019;ve seen your daughter access the cookies this way before but have never been burgled, then the &#x2018;daughter hypothesis&#x2019; seems more plausible. If some expensive things from your bedroom and living room are missing and there is hateful graffiti on your door at the eye level of a very short person, then the &#x2018;short thief&#x2019; hypothesis becomes more plausible than before. If you suddenly remember that your friend ate a few cookies and broke the fridge door last night, the &#x2018;broken fridge door&#x2019; hypothesis gains credibility. If you&#x2019;ve never been burgled and your daughter is out of town and you have a habit of moving and eating things while sleepwalking, the &#x2018;sleepwalking&#x2019; hypothesis becomes less bizarre.</p>\n<p>So the weight you give to each hypothesis depends greatly on your prior knowledge. But what if you had just been hit on the head and lost all past memories, and for some reason the most urgent thing you wanted to do was to solve the mystery of the chair and cookies? Then how would you weigh the likelihood of the available hypotheses?</p>\n<p>When you have very little data but want to compare hypotheses anyway, Occam&apos;s Razor comes to the rescue.</p>\n<h4><a name=\"occams_razor\"></a>Occam&#x2019;s Razor</h4>\n<p>Consider a different inductive problem. A computer program outputs the following sequence of numbers:</p>\n<p style=\"padding-left: 30px; \">1, 3, 5, 7</p>\n<p>Which number comes next? If you guess correctly, you&#x2019;ll win $500.</p>\n<p>In order to predict the next number in the sequence, you make a hypothesis about the process the computer is using to generate these numbers. One obvious hypothesis is that it is simply listing all the odd numbers in ascending order from 1. If that&#x2019;s true, you should guess that &quot;9&quot; will be the next number.&#xA0;</p>\n<p>But perhaps the computer is using a different algorithm to generate the numbers. Suppose that n is the step in the sequence, so that n=1 when it generated &#x2018;1&#x2019;, n=2 when it generated &#x2018;3&#x2019;, and so on. Maybe the computer used this equation to calculate each number in the sequence:</p>\n<p style=\"padding-left: 30px; \">2n &#x2212; 1 + (n &#x2212; 1)(n &#x2212; 2)(n &#x2212; 3)(n &#x2212; 4)</p>\n<p>If so, the next number in the sequence will be 33. (Go ahead, <a href=\"http://www.wolframalpha.com/\">check</a> the calculations.)</p>\n<p>But doesn&#x2019;t the first hypothesis seem more likely?</p>\n<p>The principle behind this intuition, which goes back to <a href=\"http://en.wikipedia.org/wiki/William_of_Ockham\">William of Occam</a>, could be stated:</p>\n<blockquote>\n<p>Among all hypotheses consistent with the observations, the simplest is the most likely.</p>\n</blockquote>\n<p>The principle is called <a href=\"http://en.wikipedia.org/wiki/Occam%27s_razor\">Occam&#x2019;s razor</a> because it &#x2018;shaves away&#x2019; unnecessary assumptions.</p>\n<p>For example, think about the case of the missing cookies again. In most cases, the &#x2018;daughter&#x2019; hypothesis seems to make fewer unnecessary assumptions than the &#x2018;short thief&#x2019; hypothesis does. You already know you have a daughter that likes cookies and knows how to move chairs to reach cookies. But in order for the short thief hypothesis to be plausible, you have to assume that (1) a thief found a way to break in, that (2) the thief wanted inexpensive cookies from your home, that (3) the thief was, unusually, too short to reach the top of the fridge without the help of a chair, and (4) many other unnecessary assumptions.</p>\n<p>Occam&#x2019;s razor sounds right, but can it be made more precise, and can it be justified? How do we find <em>all</em> consistent hypotheses, and how do we judge their simplicity? We will return to those questions later. Before then, we&#x2019;ll describe the area of mathematics that usually deals with reasoning: probability.</p>\n<h4><a name=\"probability\"></a>Probability</h4>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617852/mirroredImages/Kyc5dFDzBg4WccrbK/vetmnhwxt4ucpvvj80ke.png\" alt width=\"250\" height=\"147\">You&#x2019;re a soldier in combat, crouching in a trench. You know for sure there is just one enemy soldier left on the battlefield, about 400 yards away. You also know that if the remaining enemy is a regular army troop, there&#x2019;s only a small chance he could hit you with one shot from that distance. But if the remaining enemy is a sniper, then there&#x2019;s a very good chance he can hit you with one shot from that distance. But snipers are rare, so it&#x2019;s probably just a regular army troop.</p>\n<p>You peek your head out of the trench, trying to get a better look.</p>\n<p>Bam! A bullet glances off your helmet and you duck down again.</p>\n<p>&#x201C;Okay,&#x201D; you think. &#x201C;I know snipers are rare, but that guy just hit me with a bullet from 400 yards away. I suppose it might still be a regular army troop, but there&#x2019;s a seriously good chance it&#x2019;s a sniper, since he hit me from that far away.&#x201D;</p>\n<p>After another minute, you dare to take another look, and peek your head out of the trench again.</p>\n<p>Bam! Another bullet glances off your helmet! You duck down again.</p>\n<p>&#x201C;Whoa,&#x201D; you think. &#x201C;It&#x2019;s definitely a sniper. No matter how rare snipers are, there&#x2019;s no way that guy just hit me twice in a row from that distance if he&#x2019;s a regular army troop. He&#x2019;s gotta be a sniper. I&#x2019;d better call for support.&#x201D;</p>\n<p>This is an example of reasoning under uncertainty, of updating uncertain beliefs in response to evidence. We do it all the time.</p>\n<p>You start with some prior beliefs, and all of them are uncertain. You are 99.99% certain the Earth revolves around the sun, 90% confident your best friend will attend your birthday party, and 40% sure that the song you&apos;re listening to on the radio was played by The Turtles.</p>\n<p>Then, you encounter new evidence&#x2014;new observations&#x2014;and you update your beliefs in response.</p>\n<p>Suppose you start out 85% confident that the one remaining enemy soldier is not a sniper. That leaves only 15% credence to the hypothesis that he <em>is</em> a sniper. But then, a bullet glances off your helmet &#x2014; an event far more likely if the enemy soldier is a sniper than if he is not. So now you&#x2019;re only 40% confident he&#x2019;s a non-sniper, and 60% confident he is a sniper. Another bullet glances off your helmet, and you update again. Now you&#x2019;re only 2% confident he&#x2019;s a non-sniper, and 98% confident he is a sniper.</p>\n<p>Probability theory is the mathematics of reasoning with uncertainty. The keystone of this subject is called Bayes&#x2019; Theorem. It tells you how likely something is given some other knowledge. Understanding this simple theorem is more useful and important for most people than Solomonoff induction. If you haven&#x2019;t learned it already, you may want to read either <a href=\"http://yudkowsky.net/rational/bayes\">tutorial #1</a>, <a href=\"http://commonsenseatheism.com/?p=13156\">tutorial #2</a>, <a href=\"/lw/2b0/bayes_theorem_illustrated_my_way/\">tutorial #3</a>, or <a href=\"http://oscarbonilla.com/2009/05/visualizing-bayes-theorem/\">tutorial #4</a> on Bayes&#x2019; Theorem. The exact math of Bayes&apos; Theorem is not required for this tutorial. We&apos;ll just describe its results qualitatively.</p>\n<p>Bayes&#x2019; Theorem can tell us how likely a hypothesis is, given evidence (or data, or observations). This is helpful because we want to know which model of the world is correct so that we can successfully predict the future. It calculates this probability based on the prior probability of the hypothesis alone, the probability of the evidence alone, and the probability of the evidence <em>given</em> the hypothesis. Now we just plug the numbers in.</p>\n<p>Of course, it&#x2019;s not easy to &#x201C;just plug the numbers in.&#x201D; You aren&#x2019;t an all-knowing god. You don&#x2019;t know <em>exactly</em> how likely it is that the enemy soldier would hit your helmet if he&#x2019;s a sniper, compared to how likely that is if he&#x2019;s not a sniper. But you can do your best. With enough evidence, it will become overwhelmingly clear which hypothesis is correct.</p>\n<p>But guesses are not well-suited to an exact algorithm, and so our quest to find an algorithm for truth-finding must continue. For now, we turn to the problem of choosing priors.</p>\n<h4><a name=\"the_problem_of_priors\"></a>The Problem of Priors</h4>\n<p>In the example above where you&apos;re a soldier in combat, I gave you your starting probabilities: 85% confidence that the enemy soldier was a sniper, and 15% confidence he was not. But what if you don&apos;t know your &quot;priors&quot;? What then?</p>\n<p>Most situations in real life are complex, so that your &#x201C;priors&#x201D; (as used in Bayes&#x2019; Theorem) are actually probabilities that have been updated several times with past evidence. You had an idea that snipers were rare because you saw many soldiers, but only a few of them were snipers. Or you read a reliable report saying that snipers were rare. But what would our ideal reasoning computer do before it knew anything? What would the probabilities be set to before we turned it on? How can we determine the probability of a hypothesis before seeing <em>any</em> data?</p>\n<p>The general answer is Occam&#x2019;s razor; simpler hypotheses are more likely. But this isn&#x2019;t rigorous. It&#x2019;s usually difficult to find a measure of complexity, even for mathematical hypotheses. Is a normal curve simpler than an exponential curve? Bayesian probability theory doesn&#x2019;t have anything to say about choosing priors. Thus, many standard &quot;prior distributions&quot; have been developed. Generally, they distribute probability equally across hypotheses. Of course this is a good approach if all the hypotheses are equally likely. But as we saw above, it seems that some hypotheses are more complex than others, and this makes them less likely than the other hypotheses. So when distributing your probability across several hypotheses, you shouldn&apos;t necessarily distribute it evenly. There&#x2019;s also a growing body of work around an idea called the <a href=\"http://en.wikipedia.org/wiki/Principle_of_maximum_entropy\">Maximum Entropy Principle</a>. This principle helps you choose a prior that makes the least assumptions given the constraints of the problem. But this principle can&#x2019;t be used to handle all possible types of hypotheses, only ones for which &#x201C;<a href=\"http://en.wikipedia.org/wiki/Entropy_(information_theory)\">entropy</a>&#x201D; can be mathematically evaluated.</p>\n<p>We need a method that everyone can agree provides the correct priors in all situations. This helps us perform induction correctly. It also helps everyone be more honest. Since priors partly determine what people believe, they can sometimes choose priors that help &#x201C;prove&#x201D; what they want to prove. This can happen intentionally or unintentionally. It can also happen in formal situations, such as an academic paper defending a proposed program.</p>\n<p>To solve the problem of priors once and for all, we&apos;d like to have an acceptable, <em>universal</em> prior distribution, so that there&apos;s no vagueness in the process of induction. We need a recipe, an <em>algorithm</em>, for selecting our priors. For that, we turn to the subject of binary sequences.</p>\n<h4><a name=\"binary_sequences\"></a>Binary Sequences</h4>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617853/mirroredImages/Kyc5dFDzBg4WccrbK/wuoqjwtugvyyx5zxgxph.jpg\" alt width=\"220\" height=\"220\">At this point, we have collected a lot of background material. We know about algorithms, and we know we need an algorithm that does induction. We know that induction also uses Occam&#x2019;s razor and probability. We know that one of the problems in probability is selecting priors. Now we&#x2019;re ready to formalize it.</p>\n<p>To start, we need a language in which we can express all problems, all data, all hypotheses. <em>Binary</em> is the name for representing information using only the characters &apos;0&apos; and &apos;1&apos;. In a sense, binary is the simplest possible alphabet. A two-character alphabet is the smallest alphabet that can communicate a difference. If we had an alphabet of just one character, our &#x201C;sentences&#x201D; would be uniform. With two, we can begin to encode information. Each 0 or 1 in a binary sequence (e. g. 01001011) can be considered the answer to a yes-or-no question.&#xA0;</p>\n<p>In the above example about sorting numbers, it&apos;s easy to convert it to binary just by writing a computer program to follow the algorithm. All programming languages are based on binary. This also applies to anything you&#x2019;ve ever experienced using a computer. From the greatest movie you&#x2019;ve ever seen to emotional instant messaging conversations, <em>all</em> of it was encoded in binary.</p>\n<p>In principle, all information can be represented in binary sequences. If this seems like an extreme claim, consider the following...</p>\n<p>All your experiences, whether from your eyes, ears, other senses or even memories and muscle movements, occur between neurons (your nerve cells and brain cells). And it was discovered that neurons communicate using a digital signal called the action potential. Because it is digital, a neuron either sends the signal, or it doesn&apos;t. There is no half-sending the action potential. This can be translated directly into binary. An action potential is a 1, no action potential is a 0. All your sensations, thoughts, and actions can be encoded as a binary sequence over time. A really long sequence.</p>\n<p>Or, if neuron communication turns out to be more complicated than that, we can look to a deeper level. All events in the universe follow the laws of physics. We&apos;re not quite done discovering the true laws of physics; there are some inconsistencies and unexplained phenomena. But the currently proposed laws are incredibly accurate. And they can be represented as a single binary sequence.</p>\n<p>You might be thinking, &#x201C;But I see and do multiple things simultaneously, and in the universe there are trillions of stars all burning at the same time. How can parallel events turn into a single sequence of binary?&#x201D;</p>\n<p>This is a perfectly reasonable question. It turns out that, at least formally, this poses no problem at all. The machinery we will use to deal with binary sequences can turn multiple sequences into one just by dovetailing them together and adjusting how it processes them so the results are the same. Because it is easiest to deal with a single sequence, we do this in the formal recipe. Any good implementation of Solomonoff induction will use multiple sequences just to be faster.</p>\n<p>A picture of your daughter can be represented as a sequence of ones and zeros. But a picture is not your daughter. A video of all your daughter&#x2019;s actions can also be represented as a sequence of ones and zeros. But a video isn&#x2019;t your daughter, either; we can&#x2019;t necessarily tell if she&#x2019;s thinking about cookies, or poetry. The position of all the subatomic particles that make up your daughter as she lives her entire life can be represented as a sequence of binary. And that really <em>is</em> your daughter.</p>\n<p>Having a common and simple language can sometimes be the key to progress. The ancient Greek mathematician Archimedes discovered many <em>specific</em> results of calculus, but could not generalize the methods because he did not have the <em>language</em> of calculus. After this language was developed in the late 1600s, hundreds of mathematicians were able to produce new results in the field. Now, calculus forms an important base of our modern civilization.</p>\n<p>Being able to do everything in the language of binary sequences simplifies things greatly, and gives us great power. Now we don&apos;t have to deal with complex concepts like &#x201C;daughter&#x201D; and &#x201C;soldier.&quot; It&apos;s all still there in the data, only as a large sequence of 0s and 1s. We can treat it all the same.</p>\n<h4><a name=\"all_algorithms\"></a>All Algorithms</h4>\n<p>Now that we have a simple way to deal with all types of data, let&apos;s look at hypotheses. Recall that we&#x2019;re looking for a way to assign prior probabilities to hypotheses. (And then, when we encounter new data, we&apos;ll use Bayes&apos; Theorem to update the probabilities we assign to those hypotheses). To be complete, and guarantee we find the <em>real</em> explanation for our data, we have to consider <em>all</em> possible hypotheses. But how could we ever find all possible explanations for our data? We could sit in a room for days, making a list of all the ways the cookies could be missing, and still not think of the possibility that our wife took some to work.</p>\n<p>It turns out that mathematical abstraction can save the day. By using a well-tested model, and the language of binary, we can find all hypotheses.</p>\n<p>This piece of the puzzle was discovered in 1936 by a man named Alan Turing. He created a simple, formal model of computers called &#x201C;Turing machines&#x201D; before anyone had ever built a computer.&#xA0;</p>\n<p>In Turing&apos;s model, each machine&apos;s language is&#x2014;you guessed it&#x2014;binary. There is one binary sequence for the input, a second binary sequence that constantly gets worked on and re-written, and a third binary sequence for output. (This description is called a three-tape Turing machine, and is easiest to think about for Solomonoff induction. The normal description of Turing machines includes only one tape, but it turns out that they are equivalent.)</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617854/mirroredImages/Kyc5dFDzBg4WccrbK/tjtylhd4ru0skqxtujlq.png\" alt width=\"694\" height=\"379\"></p>\n<p>The rules that determine how the machine reacts to and changes the bits on these tapes are very simple. An example is shown on the diagram above. Basically, every Turing machine has a finite number of &#x201C;states&#x201D;, each of which is a little rule. These rules seem bland and boring at first, but in a few paragraphs, you&#x2019;ll find out why they&#x2019;re so exciting. First, the machine will start out in a certain state, with some binary on the input tape, all zeros on the work tape, and all zeros on the output tape. The rules for that first state will tell it to look at the input tape and the work tape. Depending on what binary number is on those two tapes, the rules will say to perform certain actions. It will say to;</p>\n<ol>\n<li>feed the input tape (or not)</li>\n<li>write 0 or 1 to the work tape</li>\n<li>move the work tape left or right</li>\n<li>write 0 or 1 to the output tape</li>\n<li>feed the output tape (or not).</li>\n</ol>\n<p>After that, the rules will say which state to move to next, and the process will repeat. Remember that the rules for these states are fixed; they could be written out on pieces of paper. All that changes are the tapes, and what rule the machine is currently following. The basic mathematics behind this is fairly simple to understand, and can be found in books on computational theory.</p>\n<p>This model is <em>incredibly</em> powerful. Given the right rules, Turing machines can</p>\n<ul>\n<li>calculate the square of a number,</li>\n<li>run a spreadsheet program,</li>\n<li>compress large files,</li>\n<li>estimate the probability of rain tomorrow,</li>\n<li>control the flight of an airplane,</li>\n<li>play chess better than a human,</li>\n<li>and much, much more.</li>\n</ul>\n<p>You may have noticed that this sounds like a list of what regular computers do. And you would be correct; the model of Turing machines came before, and served as an invaluable guide to, the invention of electronic computers. Everything they do is within the model of Turing machines.</p>\n<p>Even more exciting is the fact that <em>all</em> attempts to formalize the intuitive idea of &#x201C;algorithm&#x201D; or &#x201C;process&#x201D; have been proven to be at most equally as powerful as Turing machines. If a system has this property, it is called <em>Turing complete</em>. For example, math equations using algebra have a huge range of algorithms they can express. Multiplying is an algorithm, finding the hypotenuse of a right triangle is an algorithm, and the quadratic formula is an algorithm. Turing machines can run all these algorithms, <em>and more</em>. That is, Turing machines can be used to calculate out all algebraic algorithms, but there are some algorithms Turing machines can run that can&#x2019;t be represented by algebra. This means algebra is not Turing complete. For another example, mathematicians often invent &#x201C;games&#x201D; where sequences of symbols can be rewritten using certain rules. (They will then try to prove things about what sequences these rules can create.) But no matter how creative their rules, every one of them can be simulated on a Turing machine. That is, for every set of re-writing rules, there is a binary sequence you can give a Turing machine so that the machine will rewrite the sequences in the same way.</p>\n<p>Remember how limited the states of a Turing machine are; every machine has only a finite number of states with &#x201C;if&#x201D; rules like in the figure above. But somehow, using these and the tape as memory, they can simulate every set of rules, every algorithm ever thought up. Even the distinctively different theory of quantum computers is at most Turing complete. In the 80 years since Turing&#x2019;s paper, no superior systems have been found. The idea that Turing machines truly capture the idea of &#x201C;algorithm&#x201D; is called the Church-Turing thesis.</p>\n<p>So the model of Turing machines covers regular computers, but that is not all. As mentioned above, the current laws of physics can be represented as a binary sequence. That is, the laws of physics are an algorithm that can be fed into a Turing machine to compute the past, present and future of the universe. This includes stars burning, the climate of the earth, the action of cells, and even the actions and thoughts of humans. Most of the power here is in the laws of physics themselves. What Turing discovered is that these can be computed by a mathematically simple machine.</p>\n<p>As if Turing&#x2019;s model wasn&#x2019;t amazing enough, he went on to prove that <em>one specific</em> set of these rules could simulate all <em>other</em> sets of rules.</p>\n<p>The computer with this special rule set is called a universal Turing machine. We simulate another chosen machine by giving the universal machine a compiler binary sequence. A <em>compiler</em> is a short program that translates between computer languages or, in this case, between machines. Sometimes a compiler doesn&#x2019;t exist. For example, you couldn&#x2019;t translate Super Mario Brothers onto a computer that only plays tic-tac-toe. But there will always be a compiler to translate onto a universal Turing machine. We place this compiler in front of the input which would have been given to the chosen machine. From one perspective, we are just giving the universal machine a single, longer sequence. But from another perspective, the universal machine is using the compiler to set itself up to simulate the chosen machine. While the universal machine (using its own, fixed rules) is processing the compiler, it will write various things on the work tape. By the time it has passed the compiler and gets to the original input sequence, the work tape will have something written on it to help simulate the chosen machine. While processing the input, it will still follow its own, fixed rules, only the binary on the work tape will guide it down a different &#x201C;path&#x201D; through those rules than if we had only given it the original input.</p>\n<p>For example, say that we want to calculate the square of 42 (or in binary, 101010). Assume we know the rule set for the Turing machine which squares numbers when given the number in binary. Given all the specifics, there is an algorithmic way to find the &#x201C;compiler&#x201D; sequence based on these rules. Let&#x2019;s say that the compiler is 1011000. Then, in order to compute the square of 42 on the universal Turing machine, we simply give it the input 1011000101010, which is just the compiler 1011000 next to the number 42 as 101010. If we want to calculate the square of 43, we just change the second part to 101011 (which is 101010 + 1). The compiler sequence doesn&#x2019;t change, because it is a property of the machine we want to simulate, e. g. the squaring machine, but not of the input to that simulated machine, e. g. 42.</p>\n<p>In summary: algorithms are represented by Turing machines, and Turing machines are represented by inputs to the universal Turing machine. Therefore, algorithms are represented by inputs to the universal Turing machine.</p>\n<p>In Solomonoff induction, the assumption we make about our data is that it was generated by some algorithm. That is, the hypothesis that explains the data is an algorithm. &#xA0;Therefore, a universal Turing machine can output the data, as long as you give the machine the correct hypothesis as input. Therefore, the set of all possible inputs to our universal Turing machine is the set of all possible hypotheses. This includes the hypothesis that the data is a list of the odd numbers, the hypothesis that the enemy soldier is a sniper, and the hypothesis that your daughter ate the cookies. This is the power of formalization and mathematics.</p>\n<h4><a name=\"solomonoffs_lightsaber\"></a>Solomonoff&apos;s Lightsaber</h4>\n<p>Now we can find all the hypotheses that would predict the data we have observed. This is much more powerful than the informal statement of Occam&apos;s razor. Because of its precision and completeness, this process has been jokingly dubbed &#x201C;Solomonoff&apos;s Lightsaber&#x201D;. Given our data, we find potential hypotheses to explain it by running every hypothesis, one at a time, through the universal Turing machine. If the output matches our data, we keep it. Otherwise, we throw it away.</p>\n<p>By the way, this is where Solomonoff induction becomes incomputable. It would take an infinite amount of time to check every algorithm. And even more problematic, some of the algorithms will run forever without producing output&#x2014;<em>and we can&apos;t prove they will never stop running</em>. This is known as the halting problem, and it is a deep fact of the theory of computation. It&apos;s the sheer number of algorithms, and these pesky non-halting algorithms that stop us from actually running Solomonoff induction.</p>\n<p>The actual process above might seem a little underwhelming. We just check every single hypothesis? Really? Isn&#x2019;t that a little mindless and inefficient? This will certainly not be how the first true AI operates. But don&#x2019;t forget that before this, nobody had any idea how to do ideal induction, <em>even in principle</em>. Developing fundamental theories, like quantum mechanics, might seem abstract and wasteful. But history has proven that it doesn&#x2019;t take long before such theories and models change the world, as quantum mechanics did with modern electronics. In the future, men and women will develop ways to approximate Solomonoff induction in a second. Perhaps they will develop methods to eliminate large numbers of hypotheses all at once. Maybe hypotheses will be broken into distinct classes. Or maybe they&#x2019;ll use methods to statistically converge toward the right hypotheses.</p>\n<p>So now, at least in theory, we have the whole list of hypotheses that might be the true cause behind our observations. These hypotheses, since they are algorithms, look like binary sequences. For example, the first few might be 01001101, 0011010110000110100100110, and 1000111110111111000111010010100001. That is, for each of these three, when you give them to the universal Turing machine as input, the output is our data. Which of these three do you think is more likely to be the <em>true</em> hypothesis that generated our data in the first place?</p>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617854/mirroredImages/Kyc5dFDzBg4WccrbK/klkmerjenonttm4lwr90.jpg\" alt width=\"228\" height=\"270\">We have a list, but we&apos;re trying to come up with a probability, not just a list of possible explanations. So how do we decide what the probability is of each of these hypotheses? Imagine that the true algorithm is produced in a most unbiased way: by flipping a coin. For each bit of the hypothesis, we flip a coin. Heads will be 0, and tails will be 1. In the example above, 01001101, the coin landed heads, tails, heads, heads, tails, and so on. Because each flip of the coin has a 50% probability, each bit contributes &#xBD; to the final probability.&#xA0;</p>\n<p>Therefore an algorithm that is one bit longer is half as likely to be the true algorithm. Notice that this intuitively fits Occam&apos;s razor; a hypothesis that is 8 bits long is much more likely than a hypothesis that is 34 bits long. Why bother with extra bits? We&#x2019;d need <em>evidence</em> to show that they were necessary.</p>\n<p>So, why not just take the shortest hypothesis, and call that the truth? Because all of the hypotheses predict the data we have so far, and in the future we might get data to rule out the shortest one. We keep all consistent hypotheses, but weigh the shorter ones with higher probability. So in our eight-bit example, the probability of 01001101 being the true algorithm is &#xBD;^8, or 1/256. It&apos;s important to say that this isn&apos;t an absolute probability in the normal sense. It hasn&apos;t been <em>normalized</em>&#x2014;that is, the probabilities haven&apos;t been adjusted so that they add up to 1. This is computationally much more difficult, and might not be necessary in the final implementation of Solomonoff induction. These probabilities can still be used to compare how likely different hypotheses are.</p>\n<p>To find the probability of the evidence alone, all we have to do is add up the probability of all these hypotheses consistent with the evidence. Since any of these hypotheses could be the true one that generates the data, and they&apos;re mutually exclusive, adding them together doesn&apos;t double count any probability.</p>\n<h4><a name=\"formalized_science\"></a>Formalized Science</h4>\n<p>Let&#x2019;s go back to the process above that describes the scientific method. We&#x2019;ll see that Solomonoff induction is <em>this process made into an algorithm</em>.</p>\n<p style=\"padding-left: 30px;\">1. Make an observation.</p>\n<p>Our observation is our binary sequence of data. Only binary sequences are data, and all binary sequences can qualify as data.</p>\n<p style=\"padding-left: 30px;\">2. Form a hypothesis that explains the observation.</p>\n<p>We use a universal Turing machine to find all possible hypotheses, no fuzziness included. The hypothesis &#x201C;explains&#x201D; the observation if the output of the machine matches the data exactly.</p>\n<p style=\"padding-left: 30px;\">3. Conduct an experiment that will test the hypothesis.</p>\n<p>The only &#x201C;experiment&#x201D; is to observe the data sequence for longer, and run the universal machine for longer. The hypotheses whose output continues to match the data are the ones that pass the test.</p>\n<p style=\"padding-left: 30px;\">4. If the experimental results disconfirm the hypothesis, return to step #2 and form a hypothesis not yet used. If the experimental results confirm the hypothesis, provisionally accept the hypothesis.</p>\n<p>This step tells us to repeat the &#x201C;experiment&#x201D; with each binary sequence in our matching collection. However, instead of &#x201C;provisionally&#x201D; accepting the hypothesis, we accept all matching hypotheses with a probability weight according to its length.</p>\n<p>Now we&#x2019;ve found the truth, as best as it can be found. We&#x2019;ve excluded no possibilities. There&#x2019;s no place for a scientist to be biased, and we don&#x2019;t need to depend on our creativity to come up with the right hypothesis or experiment. We know how to measure how complex a hypothesis is. Our only problem left is to efficiently run it.</p>\n<h4><a name=\"approximations\"></a>Approximations</h4>\n<p>As mentioned before, we actually can&#x2019;t run all the hypotheses to see which ones match. There are infinitely many, and some of them will never halt. So, just like our cake recipe, we need some very helpful approximations that still deliver very close to true outputs. Technically, all prediction methods are approximations of Solomonoff induction, because Solomonoff induction tries all possibilities. But most methods use a <em>very</em> small set of hypotheses, and many don&apos;t use good methods for estimating their probabilities. How can we more directly approximate our recipe? At present, there aren&apos;t any outstandingly fast and accurate approximations to Solomonoff induction. If there were, we would be well on our way to true AI. Below are some ideas that have been published.</p>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617855/mirroredImages/Kyc5dFDzBg4WccrbK/jkdxafw8tajusid42gqf.jpg\" alt width=\"300\" height=\"192\">In <em><a href=\"http://www.amazon.com/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3642060528/\">Universal Artificial Intelligence</a></em>, Marcus Hutter provides a full formal approximation of Solomonoff induction which he calls AIXI-<em>tl</em>. This model is optimal in a technical and subtle way. Also it can always return an answer in a finite amount of time, but this time is usually extremely long and <em>doubles</em> with every bit of data. It would still take longer than the life of the universe before we got an answer for most questions. How can we do even better?</p>\n<p>One general method would be to use a Turing machine that you know will always halt. Because of the halting problem, this means that you won&apos;t be testing some halting algorithms that might be the correct hypotheses. But you could still conceivably find a large set of hypotheses that all halted.</p>\n<p>Another popular method of approximating any intractable algorithm is to use randomness. This is called a Monte Carlo method. We can&#x2019;t test <em>all</em> hypotheses, so we have to select a subset. But we don&#x2019;t want our selection process to bias the result. Therefore we could randomly generate a bunch of hypotheses to test. We could use an evolutionary algorithm, where we test this seed set of hypotheses, and keep the ones that generate data closest to ours. Then we would vary these hypotheses, run them again through the Turing machine, keep the closest fits, and continue this process until the hypotheses actually predicted our data exactly.&#xA0;</p>\n<p>The mathematician J&#xFC;rgen Schmidhuber proposes a different probability weighing which also gives high probability to hypotheses that can be quickly computed. He demonstrates how this is &#x201C;near-optimal&#x201D;. This is vastly faster, but risks making another assumption, that faster algorithms are inherently more likely.</p>\n<h4><a name=\"unresolved_details\"></a>Unresolved Details</h4>\n<p>Solomonoff induction is an active area of research in modern mathematics. While it is universal in a broad and impressive sense, some choices can still be made in the mathematical definition. Many people also have philosophical concerns or objections to the claim that Solomonoff induction is ideal and universal.</p>\n<p>The first question many mathematicians ask is, &#x201C;Which universal Turing machine?&#x201D; I have written this tutorial as if there is only one, but there are in fact infinitely many sets of rules that can simulate all other sets of rules. Just as the length of a program will depend on which programming language you write it in, the length of the hypothesis as a binary sequence will depend on which universal Turing machine you use. This means the probabilities will be different. The change, however, is very limited. There are theorems that well-define this effect and it is generally agreed not to be a concern. Specifically, going between two universal machines cannot increase the hypothesis length any more than the length of the compiler from one machine to the other. This length is fixed, independent of the hypothesis, so the more data you use, the less this difference matters.</p>\n<p>Another concern is that the true hypothesis may be incomputable. There are known definitions of binary sequences which make sense, but which no Turing machine can output. Solomonoff induction would converge to this sequence, but would never predict it exactly. It is also generally agreed that <em>nothing</em> could ever predict this sequence, because all known predictors are equivalent to Turing machines. If this is a problem, it is similar to the problem of a finite universe; there is nothing that can be done about it.</p>\n<p>Lastly, many people, mathematicians included, reject the ideas behind the model, such as that the universe can be represented as a binary sequence. This often delves into complex philosophical arguments, and often revolves around consciousness.</p>\n<p>Many more details can be found the more one studies. Many mathematicians work on modified versions and extensions where the computer learns how to act as well as predict. However these open areas are resolved, Solomonoff induction has provided an invaluable model and perspective for research into solving the problem of how to find truth. (Also see <a href=\"/lw/cw1/open_problems_related_to_solomonoff_induction/\">Open Problems Related to Solomonoff Induction</a>.)</p>\n<p>&#xA0;</p>\n<p>Fundamental theories have an effect of unifying previously separate ideas. After Turing discovered the basics of computability theory, it was only a matter of time before these ideas made their way across philosophy and mathematics. Statisticians could only find exact probabilities in simplified situations; now they know how to find them in all situations. Scientists wondered how to really know which hypothesis was simpler; now they can find a number. Philosophers wondered how induction could be justified. Solomonoff induction gives them an answer.</p>\n<p>So, what&#x2019;s next? To build our AI truth-machine, or even to find the precise truth to a single real-world problem, we need considerable work on approximating our recipe. Obviously a scientist cannot presently download a program to tell him the complexity of his hypothesis regarding deep-sea life behavior. But now that we know how to find truth in principle, mathematicians and computer scientists will work on finding it in practice.</p>\n<p>(How does this fit with Bayes&apos; theorem? <a href=\"/r/discussion/lw/di3/how_bayes_theorem_is_consistent_with_solomonoff/\">A followup.</a>)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bTeiZr6YAEaSPQTC8": 20, "TiEFKWDvD3jsKumDx": 7, "EewHHv3ewvQ3mqbyb": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kyc5dFDzBg4WccrbK", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 92, "baseScore": 132, "extendedScore": null, "score": 0.000281, "legacy": true, "legacyId": "17476", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 132, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>This is the completed article that Luke wrote the <a href=\"/lw/8nr/intuitive_explanation_of_solomonoff_induction/\">first half of</a>. My thanks go to the following for reading, editing, and commenting; Luke Muehlhauser, Louie Helm, Benjamin Noble, and Francelle Wax.</small></p>\n<p><img style=\"vertical-align: middle;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617849/mirroredImages/Kyc5dFDzBg4WccrbK/gc54xxvtafgqkj452b2r.png\" alt=\"\" width=\"700\" height=\"227\"></p>\n<p>People disagree about things. Some say that television makes you dumber; other say it makes you smarter. Some scientists believe life must exist elsewhere in the universe; others believe it must not. Some say that complicated financial derivatives are essential to a modern competitive economy; others think a nation's economy will do better without them. It's hard to know what is true.</p>\n<p>And it's hard to know <em>how to figure out</em> what is true. Some argue that you should assume the things you are most certain about and then deduce all other beliefs from your original beliefs. Others think you should accept at face value the most intuitive explanations of personal experience. Still others think you should generally agree with the scientific consensus until it is disproved.</p>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617850/mirroredImages/Kyc5dFDzBg4WccrbK/jw1n4ooctjjaus3v1lfr.jpg\" alt=\"\" width=\"250\" height=\"210\">Wouldn't it be nice if determining what is true was like baking a cake? What if there was a <em>recipe</em> for finding out what is true? All you'd have to do is follow the written directions exactly, and after the last instruction you'd inevitably find yourself with some sweet, tasty <em>truth</em>!</p>\n<p>In this tutorial, we'll explain the closest thing we\u2019ve found so far to a recipe for finding truth: Solomonoff induction.</p>\n<p>There are some qualifications to make. To describe just one: roughly speaking, you don't have time to follow the recipe. To find the truth to even a simple question using this recipe would require you to follow one step after another until long after the <a href=\"http://en.wikipedia.org/wiki/Heat_death_of_the_universe\">heat death</a> of the universe, and you can't do that.</p>\n<p>But we can find shortcuts. Suppose you know that the <em>exact</em> recipe for baking a cake asks you to count out one molecule of H<sub>2</sub>O at a time until you have <em>exactly</em> 0.5 cups of water. If you did that, you might not finish the cake before the heat death of the universe. But you could approximate that part of the recipe by measuring out something very close to 0.5 cups of water, and you'd probably still end up with a pretty good cake.</p>\n<p>Similarly, once we know the exact recipe for finding truth, we can try to approximate it in a way that allows us to finish all the steps sometime before the sun burns out.</p>\n<p>This tutorial explains that best-we've-got-so-far recipe for finding truth, Solomonoff induction. Don\u2019t worry, we won\u2019t be using any equations, just qualitative descriptions.</p>\n<p>Like Eliezer Yudkowsky's <a href=\"http://yudkowsky.net/rational/bayes\">Intuitive Explanation of Bayes' Theorem</a> and Luke Muehlhauser's <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">Crash Course in the Neuroscience of Human Motivation</a>, this tutorial is <em>long</em>. You may not have time to read it; that's fine. But if you do read it, we recommend that you read it in sections.</p>\n<p><a id=\"more\"></a></p>\n<h4 id=\"Contents_\">Contents:</h4>\n<p><span style=\"white-space: pre;\"> </span>Background</p>\n<p style=\"padding-left: 30px;\">1. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#algorithms\">Algorithms</a> \u2014 We\u2019re looking for an algorithm to determine truth.</p>\n<p style=\"padding-left: 30px;\">2. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#induction\">Induction</a> \u2014 By \u201cdetermine truth\u201d, we mean induction.</p>\n<p style=\"padding-left: 30px;\">3. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#occams_razor\">Occam\u2019s Razor</a> \u2014 How we judge between many inductive hypotheses.</p>\n<p style=\"padding-left: 30px;\">4. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#probability\">Probability</a> \u2014 Probability is what we usually use in induction.</p>\n<p style=\"padding-left: 30px;\">5. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#the_problem_of_priors\">The Problem of Priors</a> \u2014 Probabilities change with evidence, but where do they start?</p>\n<p>The Solution</p>\n<p style=\"padding-left: 30px;\">6. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#binary_sequences\">Binary Sequences</a> \u2014 Everything can be encoded as binary.</p>\n<p style=\"padding-left: 30px;\">7. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#all_algorithms\">All Algorithms</a> \u2014 Hypotheses are algorithms. Turing machines describe these.</p>\n<p style=\"padding-left: 30px;\">8. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#solomonoffs_lightsaber\">Solomonoff's Lightsaber</a> \u2014 Putting it all together.</p>\n<p style=\"padding-left: 30px;\">9. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#formalized_science\">Formalized Science</a> \u2014 From intuition to precision.</p>\n<p style=\"padding-left: 30px;\">10. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#approximations\">Approximations</a> \u2014 Ongoing work towards practicality.</p>\n<p style=\"padding-left: 30px;\">11. <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#unresolved_details\">Unresolved Details</a> \u2014 Problems, philosophical and mathematical.</p>\n<h4 id=\"Algorithms\"><a name=\"algorithms\"></a>Algorithms</h4>\n<p>At an early age you learned a set of precisely-defined steps \u2014 a 'recipe' or, more formally, an <em>algorithm</em> \u2014 that you could use to find the largest number in a list of numbers like this:</p>\n<p style=\"padding-left: 30px; \">21, 18, 4, 19, 55, 12, 30</p>\n<p>The algorithm you learned probably looked something like this:</p>\n<ol>\n<li>Look at the first item. Note that it is the largest you've seen on this list so far. If this is the only item on the list, output it as the largest number on the list. Otherwise, proceed to step 2.</li>\n<li>Look at the next item. If it is larger than the largest item noted so far, note it as the largest you've seen in this list so far. Proceed to step 3.</li>\n<li>If you have not reached the end of the list, return to step 2. Otherwise, output the last noted item as the largest number in the list.</li>\n</ol>\n<p>Other algorithms could be used to solve the same problem. For example, you could work your way from right to left instead of from left to right. But the point is that if you follow this algorithm exactly, and you have enough time to complete the task, you can't <em>fail</em> to solve the problem. You can't get confused about what one of the steps means or what the next step is. Every instruction tells you exactly what to do next, all the way through to the answer.</p>\n<p><img style=\"float: right;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617850/mirroredImages/Kyc5dFDzBg4WccrbK/uibbsnjfcy00pabk0ddv.png\" alt=\"\" width=\"330\" height=\"742\">You probably learned other algorithms, too, like how to find the greatest common divisor of any two integers (see image on right).</p>\n<p>But not just any set of instructions is a precisely-defined algorithm. Sometimes, instructions are unclear or incomplete. Consider the following instructions based on <a href=\"http://science.howstuffworks.com/innovation/scientific-experiments/scientific-method6.htm\">an article</a> about the scientific method:</p>\n<ol>\n<li>Make an observation.</li>\n<li>Form a hypothesis that explains the observation.</li>\n<li>Conduct an experiment that will test the hypothesis.</li>\n<li>If the experimental results disconfirm the hypothesis, return to step #2 and form a hypothesis not yet used. If the experimental results confirm the hypothesis, provisionally accept the hypothesis.</li>\n</ol>\n<p>This is not an algorithm.</p>\n<p>First, many of the terms are not clearly defined. What counts as an observation? What counts as a hypothesis? What would a hypothesis need to be like in order to \u2018explain\u2019 the observation? What counts as an experiment that will \u2018test\u2019 the hypothesis? What does it mean for experimental results to \u2018confirm\u2019 or \u2018disconfirm\u2019 a hypothesis?</p>\n<p>Second, the instructions may be incomplete. What do we do if we reach step 4 and the experimental results neither \u2018confirm\u2019 nor \u2018disconfirm\u2019 the hypothesis under consideration, but instead are in some sense \u2018neutral\u2019 toward the hypothesis? These instructions don\u2019t tell us what to do in that case.</p>\n<p>An algorithm is a well-defined procedure that takes some value or values as input and, after a finite series of steps, generates some value or values as output.</p>\n<p>For example, the \u2018find the largest number\u2019 algorithm above could take the input {21, 18, 4, 19, 55, 12, 30} and would, after 13 steps, produce the following output: {55}. Or it could take the input {34} and, after 1 step, produce the output: {34}.</p>\n<p>An algorithm is so well written, that we can construct machines that follow them. Today, the machines that follow algorithms are mostly computers. This is why all computer science students take a class in algorithms. If we construct our algorithm for truth, then we can make a computer program that finds truth\u2014an Artificial Intelligence.</p>\n<h4 id=\"Induction\"><a name=\"induction\"></a>Induction</h4>\n<p>Let\u2019s clarify what we mean. In movies, scientists will reveal \u201ctruth machines\u201d. Input a statement, and the truth machine will tell you whether it is true or false. This is <em>not</em> what Solomonoff induction does. Instead, Solomonoff induction is our ultimate \u201cinduction machine\u201d.</p>\n<blockquote>\n<p>Whether we are a detective trying to catch a thief, a scientist trying to discover a new physical law, or a businessman attempting to understand a recent change in demand, we are all in the process of collecting information and trying to infer the underlying causes.</p>\n<p>-Shane Legg</p>\n</blockquote>\n<p>The problem of induction is this: We have a set of <em>observations</em> (or <em>data</em>), and we want to find the underlying causes of those observations. That is, we want to find <em>hypotheses</em> that explain our data. We\u2019d like to know which hypothesis is correct, so we can use that knowledge to predict future events. Our algorithm for truth will not listen to questions and answer yes or no. Our algorithm will take in data (observations) and output the rule by which the data was created. That is, it will give us the explanation of the observations; the causes.</p>\n<p>Suppose your data concern a large set of stock market changes and other events in the world. You\u2019d like to know the processes responsible for the stock market price changes, because then you can predict what the stock market will do in the future, and make some money.</p>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617851/mirroredImages/Kyc5dFDzBg4WccrbK/xfsjvvlsnnrbwhklytmg.jpg\" alt=\"\" width=\"220\" height=\"165\">Or, suppose you are a parent. You come home from work to find a chair propped against the refrigerator, with the cookie jar atop the fridge a bit emptier than before. You like cookies, and you don\u2019t want them to disappear, so you start thinking. One hypothesis that leaps to mind is that your young daughter used the chair to reach the cookies. However, many other hypotheses explain the data. Perhaps a very short thief broke into your home and stole some cookies. Perhaps your daughter put the chair in front of the fridge because the fridge door is broken and no longer stays shut, and you forgot that your friend ate a few cookies when he visited last night. Perhaps you moved the chair and ate the cookies yourself while sleepwalking the night before.</p>\n<p>All these hypotheses are possible, but intuitively it seems like some hypotheses are more likely than others. If you\u2019ve seen your daughter access the cookies this way before but have never been burgled, then the \u2018daughter hypothesis\u2019 seems more plausible. If some expensive things from your bedroom and living room are missing and there is hateful graffiti on your door at the eye level of a very short person, then the \u2018short thief\u2019 hypothesis becomes more plausible than before. If you suddenly remember that your friend ate a few cookies and broke the fridge door last night, the \u2018broken fridge door\u2019 hypothesis gains credibility. If you\u2019ve never been burgled and your daughter is out of town and you have a habit of moving and eating things while sleepwalking, the \u2018sleepwalking\u2019 hypothesis becomes less bizarre.</p>\n<p>So the weight you give to each hypothesis depends greatly on your prior knowledge. But what if you had just been hit on the head and lost all past memories, and for some reason the most urgent thing you wanted to do was to solve the mystery of the chair and cookies? Then how would you weigh the likelihood of the available hypotheses?</p>\n<p>When you have very little data but want to compare hypotheses anyway, Occam's Razor comes to the rescue.</p>\n<h4 id=\"Occam_s_Razor\"><a name=\"occams_razor\"></a>Occam\u2019s Razor</h4>\n<p>Consider a different inductive problem. A computer program outputs the following sequence of numbers:</p>\n<p style=\"padding-left: 30px; \">1, 3, 5, 7</p>\n<p>Which number comes next? If you guess correctly, you\u2019ll win $500.</p>\n<p>In order to predict the next number in the sequence, you make a hypothesis about the process the computer is using to generate these numbers. One obvious hypothesis is that it is simply listing all the odd numbers in ascending order from 1. If that\u2019s true, you should guess that \"9\" will be the next number.&nbsp;</p>\n<p>But perhaps the computer is using a different algorithm to generate the numbers. Suppose that n is the step in the sequence, so that n=1 when it generated \u20181\u2019, n=2 when it generated \u20183\u2019, and so on. Maybe the computer used this equation to calculate each number in the sequence:</p>\n<p style=\"padding-left: 30px; \">2n \u2212 1 + (n \u2212 1)(n \u2212 2)(n \u2212 3)(n \u2212 4)</p>\n<p>If so, the next number in the sequence will be 33. (Go ahead, <a href=\"http://www.wolframalpha.com/\">check</a> the calculations.)</p>\n<p>But doesn\u2019t the first hypothesis seem more likely?</p>\n<p>The principle behind this intuition, which goes back to <a href=\"http://en.wikipedia.org/wiki/William_of_Ockham\">William of Occam</a>, could be stated:</p>\n<blockquote>\n<p>Among all hypotheses consistent with the observations, the simplest is the most likely.</p>\n</blockquote>\n<p>The principle is called <a href=\"http://en.wikipedia.org/wiki/Occam%27s_razor\">Occam\u2019s razor</a> because it \u2018shaves away\u2019 unnecessary assumptions.</p>\n<p>For example, think about the case of the missing cookies again. In most cases, the \u2018daughter\u2019 hypothesis seems to make fewer unnecessary assumptions than the \u2018short thief\u2019 hypothesis does. You already know you have a daughter that likes cookies and knows how to move chairs to reach cookies. But in order for the short thief hypothesis to be plausible, you have to assume that (1) a thief found a way to break in, that (2) the thief wanted inexpensive cookies from your home, that (3) the thief was, unusually, too short to reach the top of the fridge without the help of a chair, and (4) many other unnecessary assumptions.</p>\n<p>Occam\u2019s razor sounds right, but can it be made more precise, and can it be justified? How do we find <em>all</em> consistent hypotheses, and how do we judge their simplicity? We will return to those questions later. Before then, we\u2019ll describe the area of mathematics that usually deals with reasoning: probability.</p>\n<h4 id=\"Probability\"><a name=\"probability\"></a>Probability</h4>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617852/mirroredImages/Kyc5dFDzBg4WccrbK/vetmnhwxt4ucpvvj80ke.png\" alt=\"\" width=\"250\" height=\"147\">You\u2019re a soldier in combat, crouching in a trench. You know for sure there is just one enemy soldier left on the battlefield, about 400 yards away. You also know that if the remaining enemy is a regular army troop, there\u2019s only a small chance he could hit you with one shot from that distance. But if the remaining enemy is a sniper, then there\u2019s a very good chance he can hit you with one shot from that distance. But snipers are rare, so it\u2019s probably just a regular army troop.</p>\n<p>You peek your head out of the trench, trying to get a better look.</p>\n<p>Bam! A bullet glances off your helmet and you duck down again.</p>\n<p>\u201cOkay,\u201d you think. \u201cI know snipers are rare, but that guy just hit me with a bullet from 400 yards away. I suppose it might still be a regular army troop, but there\u2019s a seriously good chance it\u2019s a sniper, since he hit me from that far away.\u201d</p>\n<p>After another minute, you dare to take another look, and peek your head out of the trench again.</p>\n<p>Bam! Another bullet glances off your helmet! You duck down again.</p>\n<p>\u201cWhoa,\u201d you think. \u201cIt\u2019s definitely a sniper. No matter how rare snipers are, there\u2019s no way that guy just hit me twice in a row from that distance if he\u2019s a regular army troop. He\u2019s gotta be a sniper. I\u2019d better call for support.\u201d</p>\n<p>This is an example of reasoning under uncertainty, of updating uncertain beliefs in response to evidence. We do it all the time.</p>\n<p>You start with some prior beliefs, and all of them are uncertain. You are 99.99% certain the Earth revolves around the sun, 90% confident your best friend will attend your birthday party, and 40% sure that the song you're listening to on the radio was played by The Turtles.</p>\n<p>Then, you encounter new evidence\u2014new observations\u2014and you update your beliefs in response.</p>\n<p>Suppose you start out 85% confident that the one remaining enemy soldier is not a sniper. That leaves only 15% credence to the hypothesis that he <em>is</em> a sniper. But then, a bullet glances off your helmet \u2014 an event far more likely if the enemy soldier is a sniper than if he is not. So now you\u2019re only 40% confident he\u2019s a non-sniper, and 60% confident he is a sniper. Another bullet glances off your helmet, and you update again. Now you\u2019re only 2% confident he\u2019s a non-sniper, and 98% confident he is a sniper.</p>\n<p>Probability theory is the mathematics of reasoning with uncertainty. The keystone of this subject is called Bayes\u2019 Theorem. It tells you how likely something is given some other knowledge. Understanding this simple theorem is more useful and important for most people than Solomonoff induction. If you haven\u2019t learned it already, you may want to read either <a href=\"http://yudkowsky.net/rational/bayes\">tutorial #1</a>, <a href=\"http://commonsenseatheism.com/?p=13156\">tutorial #2</a>, <a href=\"/lw/2b0/bayes_theorem_illustrated_my_way/\">tutorial #3</a>, or <a href=\"http://oscarbonilla.com/2009/05/visualizing-bayes-theorem/\">tutorial #4</a> on Bayes\u2019 Theorem. The exact math of Bayes' Theorem is not required for this tutorial. We'll just describe its results qualitatively.</p>\n<p>Bayes\u2019 Theorem can tell us how likely a hypothesis is, given evidence (or data, or observations). This is helpful because we want to know which model of the world is correct so that we can successfully predict the future. It calculates this probability based on the prior probability of the hypothesis alone, the probability of the evidence alone, and the probability of the evidence <em>given</em> the hypothesis. Now we just plug the numbers in.</p>\n<p>Of course, it\u2019s not easy to \u201cjust plug the numbers in.\u201d You aren\u2019t an all-knowing god. You don\u2019t know <em>exactly</em> how likely it is that the enemy soldier would hit your helmet if he\u2019s a sniper, compared to how likely that is if he\u2019s not a sniper. But you can do your best. With enough evidence, it will become overwhelmingly clear which hypothesis is correct.</p>\n<p>But guesses are not well-suited to an exact algorithm, and so our quest to find an algorithm for truth-finding must continue. For now, we turn to the problem of choosing priors.</p>\n<h4 id=\"The_Problem_of_Priors\"><a name=\"the_problem_of_priors\"></a>The Problem of Priors</h4>\n<p>In the example above where you're a soldier in combat, I gave you your starting probabilities: 85% confidence that the enemy soldier was a sniper, and 15% confidence he was not. But what if you don't know your \"priors\"? What then?</p>\n<p>Most situations in real life are complex, so that your \u201cpriors\u201d (as used in Bayes\u2019 Theorem) are actually probabilities that have been updated several times with past evidence. You had an idea that snipers were rare because you saw many soldiers, but only a few of them were snipers. Or you read a reliable report saying that snipers were rare. But what would our ideal reasoning computer do before it knew anything? What would the probabilities be set to before we turned it on? How can we determine the probability of a hypothesis before seeing <em>any</em> data?</p>\n<p>The general answer is Occam\u2019s razor; simpler hypotheses are more likely. But this isn\u2019t rigorous. It\u2019s usually difficult to find a measure of complexity, even for mathematical hypotheses. Is a normal curve simpler than an exponential curve? Bayesian probability theory doesn\u2019t have anything to say about choosing priors. Thus, many standard \"prior distributions\" have been developed. Generally, they distribute probability equally across hypotheses. Of course this is a good approach if all the hypotheses are equally likely. But as we saw above, it seems that some hypotheses are more complex than others, and this makes them less likely than the other hypotheses. So when distributing your probability across several hypotheses, you shouldn't necessarily distribute it evenly. There\u2019s also a growing body of work around an idea called the <a href=\"http://en.wikipedia.org/wiki/Principle_of_maximum_entropy\">Maximum Entropy Principle</a>. This principle helps you choose a prior that makes the least assumptions given the constraints of the problem. But this principle can\u2019t be used to handle all possible types of hypotheses, only ones for which \u201c<a href=\"http://en.wikipedia.org/wiki/Entropy_(information_theory)\">entropy</a>\u201d can be mathematically evaluated.</p>\n<p>We need a method that everyone can agree provides the correct priors in all situations. This helps us perform induction correctly. It also helps everyone be more honest. Since priors partly determine what people believe, they can sometimes choose priors that help \u201cprove\u201d what they want to prove. This can happen intentionally or unintentionally. It can also happen in formal situations, such as an academic paper defending a proposed program.</p>\n<p>To solve the problem of priors once and for all, we'd like to have an acceptable, <em>universal</em> prior distribution, so that there's no vagueness in the process of induction. We need a recipe, an <em>algorithm</em>, for selecting our priors. For that, we turn to the subject of binary sequences.</p>\n<h4 id=\"Binary_Sequences\"><a name=\"binary_sequences\"></a>Binary Sequences</h4>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617853/mirroredImages/Kyc5dFDzBg4WccrbK/wuoqjwtugvyyx5zxgxph.jpg\" alt=\"\" width=\"220\" height=\"220\">At this point, we have collected a lot of background material. We know about algorithms, and we know we need an algorithm that does induction. We know that induction also uses Occam\u2019s razor and probability. We know that one of the problems in probability is selecting priors. Now we\u2019re ready to formalize it.</p>\n<p>To start, we need a language in which we can express all problems, all data, all hypotheses. <em>Binary</em> is the name for representing information using only the characters '0' and '1'. In a sense, binary is the simplest possible alphabet. A two-character alphabet is the smallest alphabet that can communicate a difference. If we had an alphabet of just one character, our \u201csentences\u201d would be uniform. With two, we can begin to encode information. Each 0 or 1 in a binary sequence (e. g. 01001011) can be considered the answer to a yes-or-no question.&nbsp;</p>\n<p>In the above example about sorting numbers, it's easy to convert it to binary just by writing a computer program to follow the algorithm. All programming languages are based on binary. This also applies to anything you\u2019ve ever experienced using a computer. From the greatest movie you\u2019ve ever seen to emotional instant messaging conversations, <em>all</em> of it was encoded in binary.</p>\n<p>In principle, all information can be represented in binary sequences. If this seems like an extreme claim, consider the following...</p>\n<p>All your experiences, whether from your eyes, ears, other senses or even memories and muscle movements, occur between neurons (your nerve cells and brain cells). And it was discovered that neurons communicate using a digital signal called the action potential. Because it is digital, a neuron either sends the signal, or it doesn't. There is no half-sending the action potential. This can be translated directly into binary. An action potential is a 1, no action potential is a 0. All your sensations, thoughts, and actions can be encoded as a binary sequence over time. A really long sequence.</p>\n<p>Or, if neuron communication turns out to be more complicated than that, we can look to a deeper level. All events in the universe follow the laws of physics. We're not quite done discovering the true laws of physics; there are some inconsistencies and unexplained phenomena. But the currently proposed laws are incredibly accurate. And they can be represented as a single binary sequence.</p>\n<p>You might be thinking, \u201cBut I see and do multiple things simultaneously, and in the universe there are trillions of stars all burning at the same time. How can parallel events turn into a single sequence of binary?\u201d</p>\n<p>This is a perfectly reasonable question. It turns out that, at least formally, this poses no problem at all. The machinery we will use to deal with binary sequences can turn multiple sequences into one just by dovetailing them together and adjusting how it processes them so the results are the same. Because it is easiest to deal with a single sequence, we do this in the formal recipe. Any good implementation of Solomonoff induction will use multiple sequences just to be faster.</p>\n<p>A picture of your daughter can be represented as a sequence of ones and zeros. But a picture is not your daughter. A video of all your daughter\u2019s actions can also be represented as a sequence of ones and zeros. But a video isn\u2019t your daughter, either; we can\u2019t necessarily tell if she\u2019s thinking about cookies, or poetry. The position of all the subatomic particles that make up your daughter as she lives her entire life can be represented as a sequence of binary. And that really <em>is</em> your daughter.</p>\n<p>Having a common and simple language can sometimes be the key to progress. The ancient Greek mathematician Archimedes discovered many <em>specific</em> results of calculus, but could not generalize the methods because he did not have the <em>language</em> of calculus. After this language was developed in the late 1600s, hundreds of mathematicians were able to produce new results in the field. Now, calculus forms an important base of our modern civilization.</p>\n<p>Being able to do everything in the language of binary sequences simplifies things greatly, and gives us great power. Now we don't have to deal with complex concepts like \u201cdaughter\u201d and \u201csoldier.\" It's all still there in the data, only as a large sequence of 0s and 1s. We can treat it all the same.</p>\n<h4 id=\"All_Algorithms\"><a name=\"all_algorithms\"></a>All Algorithms</h4>\n<p>Now that we have a simple way to deal with all types of data, let's look at hypotheses. Recall that we\u2019re looking for a way to assign prior probabilities to hypotheses. (And then, when we encounter new data, we'll use Bayes' Theorem to update the probabilities we assign to those hypotheses). To be complete, and guarantee we find the <em>real</em> explanation for our data, we have to consider <em>all</em> possible hypotheses. But how could we ever find all possible explanations for our data? We could sit in a room for days, making a list of all the ways the cookies could be missing, and still not think of the possibility that our wife took some to work.</p>\n<p>It turns out that mathematical abstraction can save the day. By using a well-tested model, and the language of binary, we can find all hypotheses.</p>\n<p>This piece of the puzzle was discovered in 1936 by a man named Alan Turing. He created a simple, formal model of computers called \u201cTuring machines\u201d before anyone had ever built a computer.&nbsp;</p>\n<p>In Turing's model, each machine's language is\u2014you guessed it\u2014binary. There is one binary sequence for the input, a second binary sequence that constantly gets worked on and re-written, and a third binary sequence for output. (This description is called a three-tape Turing machine, and is easiest to think about for Solomonoff induction. The normal description of Turing machines includes only one tape, but it turns out that they are equivalent.)</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617854/mirroredImages/Kyc5dFDzBg4WccrbK/tjtylhd4ru0skqxtujlq.png\" alt=\"\" width=\"694\" height=\"379\"></p>\n<p>The rules that determine how the machine reacts to and changes the bits on these tapes are very simple. An example is shown on the diagram above. Basically, every Turing machine has a finite number of \u201cstates\u201d, each of which is a little rule. These rules seem bland and boring at first, but in a few paragraphs, you\u2019ll find out why they\u2019re so exciting. First, the machine will start out in a certain state, with some binary on the input tape, all zeros on the work tape, and all zeros on the output tape. The rules for that first state will tell it to look at the input tape and the work tape. Depending on what binary number is on those two tapes, the rules will say to perform certain actions. It will say to;</p>\n<ol>\n<li>feed the input tape (or not)</li>\n<li>write 0 or 1 to the work tape</li>\n<li>move the work tape left or right</li>\n<li>write 0 or 1 to the output tape</li>\n<li>feed the output tape (or not).</li>\n</ol>\n<p>After that, the rules will say which state to move to next, and the process will repeat. Remember that the rules for these states are fixed; they could be written out on pieces of paper. All that changes are the tapes, and what rule the machine is currently following. The basic mathematics behind this is fairly simple to understand, and can be found in books on computational theory.</p>\n<p>This model is <em>incredibly</em> powerful. Given the right rules, Turing machines can</p>\n<ul>\n<li>calculate the square of a number,</li>\n<li>run a spreadsheet program,</li>\n<li>compress large files,</li>\n<li>estimate the probability of rain tomorrow,</li>\n<li>control the flight of an airplane,</li>\n<li>play chess better than a human,</li>\n<li>and much, much more.</li>\n</ul>\n<p>You may have noticed that this sounds like a list of what regular computers do. And you would be correct; the model of Turing machines came before, and served as an invaluable guide to, the invention of electronic computers. Everything they do is within the model of Turing machines.</p>\n<p>Even more exciting is the fact that <em>all</em> attempts to formalize the intuitive idea of \u201calgorithm\u201d or \u201cprocess\u201d have been proven to be at most equally as powerful as Turing machines. If a system has this property, it is called <em>Turing complete</em>. For example, math equations using algebra have a huge range of algorithms they can express. Multiplying is an algorithm, finding the hypotenuse of a right triangle is an algorithm, and the quadratic formula is an algorithm. Turing machines can run all these algorithms, <em>and more</em>. That is, Turing machines can be used to calculate out all algebraic algorithms, but there are some algorithms Turing machines can run that can\u2019t be represented by algebra. This means algebra is not Turing complete. For another example, mathematicians often invent \u201cgames\u201d where sequences of symbols can be rewritten using certain rules. (They will then try to prove things about what sequences these rules can create.) But no matter how creative their rules, every one of them can be simulated on a Turing machine. That is, for every set of re-writing rules, there is a binary sequence you can give a Turing machine so that the machine will rewrite the sequences in the same way.</p>\n<p>Remember how limited the states of a Turing machine are; every machine has only a finite number of states with \u201cif\u201d rules like in the figure above. But somehow, using these and the tape as memory, they can simulate every set of rules, every algorithm ever thought up. Even the distinctively different theory of quantum computers is at most Turing complete. In the 80 years since Turing\u2019s paper, no superior systems have been found. The idea that Turing machines truly capture the idea of \u201calgorithm\u201d is called the Church-Turing thesis.</p>\n<p>So the model of Turing machines covers regular computers, but that is not all. As mentioned above, the current laws of physics can be represented as a binary sequence. That is, the laws of physics are an algorithm that can be fed into a Turing machine to compute the past, present and future of the universe. This includes stars burning, the climate of the earth, the action of cells, and even the actions and thoughts of humans. Most of the power here is in the laws of physics themselves. What Turing discovered is that these can be computed by a mathematically simple machine.</p>\n<p>As if Turing\u2019s model wasn\u2019t amazing enough, he went on to prove that <em>one specific</em> set of these rules could simulate all <em>other</em> sets of rules.</p>\n<p>The computer with this special rule set is called a universal Turing machine. We simulate another chosen machine by giving the universal machine a compiler binary sequence. A <em>compiler</em> is a short program that translates between computer languages or, in this case, between machines. Sometimes a compiler doesn\u2019t exist. For example, you couldn\u2019t translate Super Mario Brothers onto a computer that only plays tic-tac-toe. But there will always be a compiler to translate onto a universal Turing machine. We place this compiler in front of the input which would have been given to the chosen machine. From one perspective, we are just giving the universal machine a single, longer sequence. But from another perspective, the universal machine is using the compiler to set itself up to simulate the chosen machine. While the universal machine (using its own, fixed rules) is processing the compiler, it will write various things on the work tape. By the time it has passed the compiler and gets to the original input sequence, the work tape will have something written on it to help simulate the chosen machine. While processing the input, it will still follow its own, fixed rules, only the binary on the work tape will guide it down a different \u201cpath\u201d through those rules than if we had only given it the original input.</p>\n<p>For example, say that we want to calculate the square of 42 (or in binary, 101010). Assume we know the rule set for the Turing machine which squares numbers when given the number in binary. Given all the specifics, there is an algorithmic way to find the \u201ccompiler\u201d sequence based on these rules. Let\u2019s say that the compiler is 1011000. Then, in order to compute the square of 42 on the universal Turing machine, we simply give it the input 1011000101010, which is just the compiler 1011000 next to the number 42 as 101010. If we want to calculate the square of 43, we just change the second part to 101011 (which is 101010 + 1). The compiler sequence doesn\u2019t change, because it is a property of the machine we want to simulate, e. g. the squaring machine, but not of the input to that simulated machine, e. g. 42.</p>\n<p>In summary: algorithms are represented by Turing machines, and Turing machines are represented by inputs to the universal Turing machine. Therefore, algorithms are represented by inputs to the universal Turing machine.</p>\n<p>In Solomonoff induction, the assumption we make about our data is that it was generated by some algorithm. That is, the hypothesis that explains the data is an algorithm. &nbsp;Therefore, a universal Turing machine can output the data, as long as you give the machine the correct hypothesis as input. Therefore, the set of all possible inputs to our universal Turing machine is the set of all possible hypotheses. This includes the hypothesis that the data is a list of the odd numbers, the hypothesis that the enemy soldier is a sniper, and the hypothesis that your daughter ate the cookies. This is the power of formalization and mathematics.</p>\n<h4 id=\"Solomonoff_s_Lightsaber\"><a name=\"solomonoffs_lightsaber\"></a>Solomonoff's Lightsaber</h4>\n<p>Now we can find all the hypotheses that would predict the data we have observed. This is much more powerful than the informal statement of Occam's razor. Because of its precision and completeness, this process has been jokingly dubbed \u201cSolomonoff's Lightsaber\u201d. Given our data, we find potential hypotheses to explain it by running every hypothesis, one at a time, through the universal Turing machine. If the output matches our data, we keep it. Otherwise, we throw it away.</p>\n<p>By the way, this is where Solomonoff induction becomes incomputable. It would take an infinite amount of time to check every algorithm. And even more problematic, some of the algorithms will run forever without producing output\u2014<em>and we can't prove they will never stop running</em>. This is known as the halting problem, and it is a deep fact of the theory of computation. It's the sheer number of algorithms, and these pesky non-halting algorithms that stop us from actually running Solomonoff induction.</p>\n<p>The actual process above might seem a little underwhelming. We just check every single hypothesis? Really? Isn\u2019t that a little mindless and inefficient? This will certainly not be how the first true AI operates. But don\u2019t forget that before this, nobody had any idea how to do ideal induction, <em>even in principle</em>. Developing fundamental theories, like quantum mechanics, might seem abstract and wasteful. But history has proven that it doesn\u2019t take long before such theories and models change the world, as quantum mechanics did with modern electronics. In the future, men and women will develop ways to approximate Solomonoff induction in a second. Perhaps they will develop methods to eliminate large numbers of hypotheses all at once. Maybe hypotheses will be broken into distinct classes. Or maybe they\u2019ll use methods to statistically converge toward the right hypotheses.</p>\n<p>So now, at least in theory, we have the whole list of hypotheses that might be the true cause behind our observations. These hypotheses, since they are algorithms, look like binary sequences. For example, the first few might be 01001101, 0011010110000110100100110, and 1000111110111111000111010010100001. That is, for each of these three, when you give them to the universal Turing machine as input, the output is our data. Which of these three do you think is more likely to be the <em>true</em> hypothesis that generated our data in the first place?</p>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617854/mirroredImages/Kyc5dFDzBg4WccrbK/klkmerjenonttm4lwr90.jpg\" alt=\"\" width=\"228\" height=\"270\">We have a list, but we're trying to come up with a probability, not just a list of possible explanations. So how do we decide what the probability is of each of these hypotheses? Imagine that the true algorithm is produced in a most unbiased way: by flipping a coin. For each bit of the hypothesis, we flip a coin. Heads will be 0, and tails will be 1. In the example above, 01001101, the coin landed heads, tails, heads, heads, tails, and so on. Because each flip of the coin has a 50% probability, each bit contributes \u00bd to the final probability.&nbsp;</p>\n<p>Therefore an algorithm that is one bit longer is half as likely to be the true algorithm. Notice that this intuitively fits Occam's razor; a hypothesis that is 8 bits long is much more likely than a hypothesis that is 34 bits long. Why bother with extra bits? We\u2019d need <em>evidence</em> to show that they were necessary.</p>\n<p>So, why not just take the shortest hypothesis, and call that the truth? Because all of the hypotheses predict the data we have so far, and in the future we might get data to rule out the shortest one. We keep all consistent hypotheses, but weigh the shorter ones with higher probability. So in our eight-bit example, the probability of 01001101 being the true algorithm is \u00bd^8, or 1/256. It's important to say that this isn't an absolute probability in the normal sense. It hasn't been <em>normalized</em>\u2014that is, the probabilities haven't been adjusted so that they add up to 1. This is computationally much more difficult, and might not be necessary in the final implementation of Solomonoff induction. These probabilities can still be used to compare how likely different hypotheses are.</p>\n<p>To find the probability of the evidence alone, all we have to do is add up the probability of all these hypotheses consistent with the evidence. Since any of these hypotheses could be the true one that generates the data, and they're mutually exclusive, adding them together doesn't double count any probability.</p>\n<h4 id=\"Formalized_Science\"><a name=\"formalized_science\"></a>Formalized Science</h4>\n<p>Let\u2019s go back to the process above that describes the scientific method. We\u2019ll see that Solomonoff induction is <em>this process made into an algorithm</em>.</p>\n<p style=\"padding-left: 30px;\">1. Make an observation.</p>\n<p>Our observation is our binary sequence of data. Only binary sequences are data, and all binary sequences can qualify as data.</p>\n<p style=\"padding-left: 30px;\">2. Form a hypothesis that explains the observation.</p>\n<p>We use a universal Turing machine to find all possible hypotheses, no fuzziness included. The hypothesis \u201cexplains\u201d the observation if the output of the machine matches the data exactly.</p>\n<p style=\"padding-left: 30px;\">3. Conduct an experiment that will test the hypothesis.</p>\n<p>The only \u201cexperiment\u201d is to observe the data sequence for longer, and run the universal machine for longer. The hypotheses whose output continues to match the data are the ones that pass the test.</p>\n<p style=\"padding-left: 30px;\">4. If the experimental results disconfirm the hypothesis, return to step #2 and form a hypothesis not yet used. If the experimental results confirm the hypothesis, provisionally accept the hypothesis.</p>\n<p>This step tells us to repeat the \u201cexperiment\u201d with each binary sequence in our matching collection. However, instead of \u201cprovisionally\u201d accepting the hypothesis, we accept all matching hypotheses with a probability weight according to its length.</p>\n<p>Now we\u2019ve found the truth, as best as it can be found. We\u2019ve excluded no possibilities. There\u2019s no place for a scientist to be biased, and we don\u2019t need to depend on our creativity to come up with the right hypothesis or experiment. We know how to measure how complex a hypothesis is. Our only problem left is to efficiently run it.</p>\n<h4 id=\"Approximations\"><a name=\"approximations\"></a>Approximations</h4>\n<p>As mentioned before, we actually can\u2019t run all the hypotheses to see which ones match. There are infinitely many, and some of them will never halt. So, just like our cake recipe, we need some very helpful approximations that still deliver very close to true outputs. Technically, all prediction methods are approximations of Solomonoff induction, because Solomonoff induction tries all possibilities. But most methods use a <em>very</em> small set of hypotheses, and many don't use good methods for estimating their probabilities. How can we more directly approximate our recipe? At present, there aren't any outstandingly fast and accurate approximations to Solomonoff induction. If there were, we would be well on our way to true AI. Below are some ideas that have been published.</p>\n<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1611617855/mirroredImages/Kyc5dFDzBg4WccrbK/jkdxafw8tajusid42gqf.jpg\" alt=\"\" width=\"300\" height=\"192\">In <em><a href=\"http://www.amazon.com/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3642060528/\">Universal Artificial Intelligence</a></em>, Marcus Hutter provides a full formal approximation of Solomonoff induction which he calls AIXI-<em>tl</em>. This model is optimal in a technical and subtle way. Also it can always return an answer in a finite amount of time, but this time is usually extremely long and <em>doubles</em> with every bit of data. It would still take longer than the life of the universe before we got an answer for most questions. How can we do even better?</p>\n<p>One general method would be to use a Turing machine that you know will always halt. Because of the halting problem, this means that you won't be testing some halting algorithms that might be the correct hypotheses. But you could still conceivably find a large set of hypotheses that all halted.</p>\n<p>Another popular method of approximating any intractable algorithm is to use randomness. This is called a Monte Carlo method. We can\u2019t test <em>all</em> hypotheses, so we have to select a subset. But we don\u2019t want our selection process to bias the result. Therefore we could randomly generate a bunch of hypotheses to test. We could use an evolutionary algorithm, where we test this seed set of hypotheses, and keep the ones that generate data closest to ours. Then we would vary these hypotheses, run them again through the Turing machine, keep the closest fits, and continue this process until the hypotheses actually predicted our data exactly.&nbsp;</p>\n<p>The mathematician J\u00fcrgen Schmidhuber proposes a different probability weighing which also gives high probability to hypotheses that can be quickly computed. He demonstrates how this is \u201cnear-optimal\u201d. This is vastly faster, but risks making another assumption, that faster algorithms are inherently more likely.</p>\n<h4 id=\"Unresolved_Details\"><a name=\"unresolved_details\"></a>Unresolved Details</h4>\n<p>Solomonoff induction is an active area of research in modern mathematics. While it is universal in a broad and impressive sense, some choices can still be made in the mathematical definition. Many people also have philosophical concerns or objections to the claim that Solomonoff induction is ideal and universal.</p>\n<p>The first question many mathematicians ask is, \u201cWhich universal Turing machine?\u201d I have written this tutorial as if there is only one, but there are in fact infinitely many sets of rules that can simulate all other sets of rules. Just as the length of a program will depend on which programming language you write it in, the length of the hypothesis as a binary sequence will depend on which universal Turing machine you use. This means the probabilities will be different. The change, however, is very limited. There are theorems that well-define this effect and it is generally agreed not to be a concern. Specifically, going between two universal machines cannot increase the hypothesis length any more than the length of the compiler from one machine to the other. This length is fixed, independent of the hypothesis, so the more data you use, the less this difference matters.</p>\n<p>Another concern is that the true hypothesis may be incomputable. There are known definitions of binary sequences which make sense, but which no Turing machine can output. Solomonoff induction would converge to this sequence, but would never predict it exactly. It is also generally agreed that <em>nothing</em> could ever predict this sequence, because all known predictors are equivalent to Turing machines. If this is a problem, it is similar to the problem of a finite universe; there is nothing that can be done about it.</p>\n<p>Lastly, many people, mathematicians included, reject the ideas behind the model, such as that the universe can be represented as a binary sequence. This often delves into complex philosophical arguments, and often revolves around consciousness.</p>\n<p>Many more details can be found the more one studies. Many mathematicians work on modified versions and extensions where the computer learns how to act as well as predict. However these open areas are resolved, Solomonoff induction has provided an invaluable model and perspective for research into solving the problem of how to find truth. (Also see <a href=\"/lw/cw1/open_problems_related_to_solomonoff_induction/\">Open Problems Related to Solomonoff Induction</a>.)</p>\n<p>&nbsp;</p>\n<p>Fundamental theories have an effect of unifying previously separate ideas. After Turing discovered the basics of computability theory, it was only a matter of time before these ideas made their way across philosophy and mathematics. Statisticians could only find exact probabilities in simplified situations; now they know how to find them in all situations. Scientists wondered how to really know which hypothesis was simpler; now they can find a number. Philosophers wondered how induction could be justified. Solomonoff induction gives them an answer.</p>\n<p>So, what\u2019s next? To build our AI truth-machine, or even to find the precise truth to a single real-world problem, we need considerable work on approximating our recipe. Obviously a scientist cannot presently download a program to tell him the complexity of his hypothesis regarding deep-sea life behavior. But now that we know how to find truth in principle, mathematicians and computer scientists will work on finding it in practice.</p>\n<p>(How does this fit with Bayes' theorem? <a href=\"/r/discussion/lw/di3/how_bayes_theorem_is_consistent_with_solomonoff/\">A followup.</a>)</p>", "sections": [{"title": "Contents:", "anchor": "Contents_", "level": 1}, {"title": "Algorithms", "anchor": "Algorithms", "level": 1}, {"title": "Induction", "anchor": "Induction", "level": 1}, {"title": "Occam\u2019s Razor", "anchor": "Occam_s_Razor", "level": 1}, {"title": "Probability", "anchor": "Probability", "level": 1}, {"title": "The Problem of Priors", "anchor": "The_Problem_of_Priors", "level": 1}, {"title": "Binary Sequences", "anchor": "Binary_Sequences", "level": 1}, {"title": "All Algorithms", "anchor": "All_Algorithms", "level": 1}, {"title": "Solomonoff's Lightsaber", "anchor": "Solomonoff_s_Lightsaber", "level": 1}, {"title": "Formalized Science", "anchor": "Formalized_Science", "level": 1}, {"title": "Approximations", "anchor": "Approximations", "level": 1}, {"title": "Unresolved Details", "anchor": "Unresolved_Details", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "222 comments"}], "headingsCount": 14}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 222, "af": false, "version": "1.0.1", "pingbacks": {"Posts": ["p3b5T8YbijEXDAfmR", "hN2aRnu798yas5b2k", "CMt3ijXYuCynhPWXa", "fC248GwrWLT4Dkjf6", "5pgsbB5sqC2wLwr4d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-07-11T08:05:20.544Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}