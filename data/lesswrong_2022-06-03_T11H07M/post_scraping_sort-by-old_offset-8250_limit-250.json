{"results": [{"createdAt": null, "postedAt": "2013-01-18T16:01:05.120Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Durham, London, Melbourne, Purdue, Vancouver, Washington DC", "slug": "weekly-lw-meetups-austin-durham-london-melbourne-purdue", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:08.518Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LXHhzTLDJDXnzStki/weekly-lw-meetups-austin-durham-london-melbourne-purdue", "pageUrlRelative": "/posts/LXHhzTLDJDXnzStki/weekly-lw-meetups-austin-durham-london-melbourne-purdue", "linkUrl": "https://www.lesswrong.com/posts/LXHhzTLDJDXnzStki/weekly-lw-meetups-austin-durham-london-melbourne-purdue", "postedAtFormatted": "Friday, January 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Durham%2C%20London%2C%20Melbourne%2C%20Purdue%2C%20Vancouver%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Durham%2C%20London%2C%20Melbourne%2C%20Purdue%2C%20Vancouver%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLXHhzTLDJDXnzStki%2Fweekly-lw-meetups-austin-durham-london-melbourne-purdue%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Durham%2C%20London%2C%20Melbourne%2C%20Purdue%2C%20Vancouver%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLXHhzTLDJDXnzStki%2Fweekly-lw-meetups-austin-durham-london-melbourne-purdue", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLXHhzTLDJDXnzStki%2Fweekly-lw-meetups-austin-durham-london-melbourne-purdue", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 477, "htmlBody": "<p><strong>This summary was posted to LW main on January 11th. The following week's summary is <a href=\"/lw/gdp/weekly_lw_meetups_austin_buffalo_cambridge_ma/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/gm\">First Purdue Meetup:&nbsp;<span class=\"date\">11 January 2013 06:50PM</span></a></li>\n<li><a href=\"/meetups/hs\">Durham HPMoR Discussion, chapters 27-29:&nbsp;<span class=\"date\">12 January 2013 11:00AM</span></a></li>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">12 January 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/hn\">Vancouver Fashion for Rationalists:&nbsp;<span class=\"date\">13 January 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/hj\">13th January London Meetup:&nbsp;<span class=\"date\">13 January 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/hr\">Washington DC measurement meetup:&nbsp;<span class=\"date\">13 January 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/hp\">Moscow: Applied Rationality:&nbsp;<span class=\"date\">19 January 2013 04:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/hq\">Melbourne Social Meetup:&nbsp;<span class=\"date\">18 January 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LXHhzTLDJDXnzStki", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0881227918561523e-06, "legacy": true, "legacyId": "21110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7kSif7Zjv65FQi6zk", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-18T17:09:54.077Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Bias and Such", "slug": "meetup-vancouver-bias-and-such", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s7TW6kCbyH7JafFzN/meetup-vancouver-bias-and-such", "pageUrlRelative": "/posts/s7TW6kCbyH7JafFzN/meetup-vancouver-bias-and-such", "linkUrl": "https://www.lesswrong.com/posts/s7TW6kCbyH7JafFzN/meetup-vancouver-bias-and-such", "postedAtFormatted": "Friday, January 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Bias%20and%20Such&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Bias%20and%20Such%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs7TW6kCbyH7JafFzN%2Fmeetup-vancouver-bias-and-such%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Bias%20and%20Such%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs7TW6kCbyH7JafFzN%2Fmeetup-vancouver-bias-and-such", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs7TW6kCbyH7JafFzN%2Fmeetup-vancouver-bias-and-such", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/i6'>Vancouver Bias and Such</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 January 2013 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 West broadway vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello everybody.</p>\n\n<p>We had a blast when jsalvatier came to visit last weekend.</p>\n\n<p>This weekend, I think we should talk about the biases. So come out and we'll have a nice discussion about all the hueristics and biases and maybe even what we can do about them.</p>\n\n<p>We'll meet at Bennys Bagels at 13:00 on Saturday</p>\n\n<p>As usual come join us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/i6'>Vancouver Bias and Such</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s7TW6kCbyH7JafFzN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0881647680235159e-06, "legacy": true, "legacyId": "21231", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Bias_and_Such\">Discussion article for the meetup : <a href=\"/meetups/i6\">Vancouver Bias and Such</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 January 2013 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 West broadway vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello everybody.</p>\n\n<p>We had a blast when jsalvatier came to visit last weekend.</p>\n\n<p>This weekend, I think we should talk about the biases. So come out and we'll have a nice discussion about all the hueristics and biases and maybe even what we can do about them.</p>\n\n<p>We'll meet at Bennys Bagels at 13:00 on Saturday</p>\n\n<p>As usual come join us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Bias_and_Such1\">Discussion article for the meetup : <a href=\"/meetups/i6\">Vancouver Bias and Such</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Bias and Such", "anchor": "Discussion_article_for_the_meetup___Vancouver_Bias_and_Such", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Bias and Such", "anchor": "Discussion_article_for_the_meetup___Vancouver_Bias_and_Such1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-19T02:56:03.259Z", "modifiedAt": null, "url": null, "title": "Meetup : Third Purdue Meetup", "slug": "meetup-third-purdue-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.288Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HZA2WMtmWTC32SZTw/meetup-third-purdue-meetup", "pageUrlRelative": "/posts/HZA2WMtmWTC32SZTw/meetup-third-purdue-meetup", "linkUrl": "https://www.lesswrong.com/posts/HZA2WMtmWTC32SZTw/meetup-third-purdue-meetup", "postedAtFormatted": "Saturday, January 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Third%20Purdue%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Third%20Purdue%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZA2WMtmWTC32SZTw%2Fmeetup-third-purdue-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Third%20Purdue%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZA2WMtmWTC32SZTw%2Fmeetup-third-purdue-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZA2WMtmWTC32SZTw%2Fmeetup-third-purdue-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/i7'>Third Purdue Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 January 2013 06:50:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Hicks Library -Purdue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first meetup had 5 people. The second had 8. PM me if you plan on coming.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/i7'>Third Purdue Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HZA2WMtmWTC32SZTw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0885224241010475e-06, "legacy": true, "legacyId": "21238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Third_Purdue_Meetup\">Discussion article for the meetup : <a href=\"/meetups/i7\">Third Purdue Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 January 2013 06:50:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Hicks Library -Purdue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first meetup had 5 people. The second had 8. PM me if you plan on coming.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Third_Purdue_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/i7\">Third Purdue Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Third Purdue Meetup", "anchor": "Discussion_article_for_the_meetup___Third_Purdue_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Third Purdue Meetup", "anchor": "Discussion_article_for_the_meetup___Third_Purdue_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-19T03:44:01.123Z", "modifiedAt": null, "url": null, "title": "Michael Vassar's Edge contribution: summary", "slug": "michael-vassar-s-edge-contribution-summary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.332Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gKfXFQxvYN5Dm4eoR/michael-vassar-s-edge-contribution-summary", "pageUrlRelative": "/posts/gKfXFQxvYN5Dm4eoR/michael-vassar-s-edge-contribution-summary", "linkUrl": "https://www.lesswrong.com/posts/gKfXFQxvYN5Dm4eoR/michael-vassar-s-edge-contribution-summary", "postedAtFormatted": "Saturday, January 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Michael%20Vassar's%20Edge%20contribution%3A%20summary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMichael%20Vassar's%20Edge%20contribution%3A%20summary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgKfXFQxvYN5Dm4eoR%2Fmichael-vassar-s-edge-contribution-summary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Michael%20Vassar's%20Edge%20contribution%3A%20summary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgKfXFQxvYN5Dm4eoR%2Fmichael-vassar-s-edge-contribution-summary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgKfXFQxvYN5Dm4eoR%2Fmichael-vassar-s-edge-contribution-summary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 263, "htmlBody": "<p>Michael Vassar has written a <a href=\"http://edge.org/response-detail/23876\">provocative response</a>&nbsp;to this year's <a href=\"http://edge.org/annual-question/q2013\">Edge question</a>: \"What *should* we be worried about?\". But, I'm confused about his post. My attempt to summarize his point of view follows:</p>\n<p>1. People have physiological needs (food, shelter, safety etc.) and social needs (esteem, love, respect etc.).</p>\n<p>2. People have mental programs to try to achieve both needs.&nbsp;</p>\n<p>3. Modern society has been exceptional at fulfilling people's physiological needs but not very good at fulfilling their social needs.</p>\n<p>4. Thus, mental programs that were meant to achieve physiological needs do not develop very well relative to mental programs meant to achieve social needs .</p>\n<p>5. Mental programs for achieving physiological needs are more precise and hence harder to hack. Mental programs for social needs are fuzzy and vague and thus more easily hackable.&nbsp;</p>\n<p>6. Thus, and because of (4), people are more hackable.&nbsp;</p>\n<p>7. This manifests operationally as a few powerful people (the rich, the politicians etc.) hacking the majority into submitting to their will.&nbsp;</p>\n<p>8. But even the powerful do not have significantly better mechanisms for precise thought. It is just that their social weirdness (need for power, lack of empathy etc.) allowed them to be the hacker instead of the hacked.</p>\n<p>9. Thus for most of our useful innovations, we are forced to rely on the rare people who are capable of precise abstract thought because they worry less about their social needs.&nbsp;</p>\n<p>So, I guess Vassar's point is that this pattern is what we should worry about as it systematically suppresses useful innovators. &nbsp;</p>\n<p>Would agree about my reading of his short essay?&nbsp;</p>\n<p>How solid do you think his argument is?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gKfXFQxvYN5Dm4eoR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 1.088551699997936e-06, "legacy": true, "legacyId": "21239", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-19T05:32:00.465Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Free to Optimize", "slug": "seq-rerun-free-to-optimize", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.357Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u4cZyCeWpashsPvot/seq-rerun-free-to-optimize", "pageUrlRelative": "/posts/u4cZyCeWpashsPvot/seq-rerun-free-to-optimize", "linkUrl": "https://www.lesswrong.com/posts/u4cZyCeWpashsPvot/seq-rerun-free-to-optimize", "postedAtFormatted": "Saturday, January 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Free%20to%20Optimize&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Free%20to%20Optimize%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4cZyCeWpashsPvot%2Fseq-rerun-free-to-optimize%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Free%20to%20Optimize%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4cZyCeWpashsPvot%2Fseq-rerun-free-to-optimize", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4cZyCeWpashsPvot%2Fseq-rerun-free-to-optimize", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 201, "htmlBody": "<p>Today's post, <a href=\"/lw/xb/free_to_optimize/\">Free to Optimize</a> was originally published on 02 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Free_to_Optimize\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It may be better to create a world that operates by better rules, that you can understand, so that you can optimize your own future, than to create a world that includes some sort of deity that can be prayed to. The human reluctance to have their future controlled by an outside source is a nontrivial part of morality.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gdg/seq_rerun_a_new_day/\">A New Day</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u4cZyCeWpashsPvot", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0886176181203223e-06, "legacy": true, "legacyId": "21240", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EZ8GniEPSechjDYP9", "p7LcTL9xvPPRSEDJb", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-19T14:10:28.129Z", "modifiedAt": null, "url": null, "title": "Meet up interest: Vienna, March", "slug": "meet-up-interest-vienna-march", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:08.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ratcourse", "createdAt": "2012-11-03T10:26:05.641Z", "isAdmin": false, "displayName": "Ratcourse"}, "userId": "qwnfbBpAcbxLT4JBr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qw2pF58Ft5A3vvXXj/meet-up-interest-vienna-march", "pageUrlRelative": "/posts/qw2pF58Ft5A3vvXXj/meet-up-interest-vienna-march", "linkUrl": "https://www.lesswrong.com/posts/qw2pF58Ft5A3vvXXj/meet-up-interest-vienna-march", "postedAtFormatted": "Saturday, January 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meet%20up%20interest%3A%20Vienna%2C%20March&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeet%20up%20interest%3A%20Vienna%2C%20March%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqw2pF58Ft5A3vvXXj%2Fmeet-up-interest-vienna-march%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meet%20up%20interest%3A%20Vienna%2C%20March%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqw2pF58Ft5A3vvXXj%2Fmeet-up-interest-vienna-march", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqw2pF58Ft5A3vvXXj%2Fmeet-up-interest-vienna-march", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>Please post here if you'd be interested in a meetup in Vienna in March.&nbsp;<br />Location, date and time to be agreed upon by the participants.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qw2pF58Ft5A3vvXXj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.0889341928332342e-06, "legacy": true, "legacyId": "21244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-19T19:47:37.077Z", "modifiedAt": null, "url": null, "title": "Long-chain correlation: lead paint and crime", "slug": "long-chain-correlation-lead-paint-and-crime", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:08.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHibbert", "createdAt": "2009-03-05T19:14:59.511Z", "isAdmin": false, "displayName": "ChrisHibbert"}, "userId": "kwfjSCPwqBYPT7NeQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b2WFFRdy2DTMJMfjT/long-chain-correlation-lead-paint-and-crime", "pageUrlRelative": "/posts/b2WFFRdy2DTMJMfjT/long-chain-correlation-lead-paint-and-crime", "linkUrl": "https://www.lesswrong.com/posts/b2WFFRdy2DTMJMfjT/long-chain-correlation-lead-paint-and-crime", "postedAtFormatted": "Saturday, January 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Long-chain%20correlation%3A%20lead%20paint%20and%20crime&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALong-chain%20correlation%3A%20lead%20paint%20and%20crime%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb2WFFRdy2DTMJMfjT%2Flong-chain-correlation-lead-paint-and-crime%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Long-chain%20correlation%3A%20lead%20paint%20and%20crime%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb2WFFRdy2DTMJMfjT%2Flong-chain-correlation-lead-paint-and-crime", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb2WFFRdy2DTMJMfjT%2Flong-chain-correlation-lead-paint-and-crime", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 265, "htmlBody": "<p>A friend has been asking my views on the likelihood that there's anything to a correlation between changing levels of lead in paint (and automotive exhaust) and the levels of crime. He quoted from a Reason Blog:</p>\n<blockquote>\n<p>So Nevin dove in further, digging up detailed data on lead emissions and crime rates to see if the similarity of the curves was as good as it seemed. It turned out to be even better: In a <a href=\"http://www.ricknevin.com/uploads/Nevin_2000_Env_Res_Author_Manuscript.pdf\"> 2000 paper</a> (PDF) he concluded that if you add a lag time of 23 years, lead emissions from automobiles explain 90 percent of the variation in violent crime in America. Toddlers who ingested high levels of lead in the '40s and '50s really were more likely to become violent criminals in the '60s, '70s, and '80s.</p>\n</blockquote>\n<p>I responded with the following:</p>\n<blockquote>\n<p>Sounds like a stretch to me. I'd want to hear that they didn't test more  than 5 other hypothesis before coming to that conclusion, or the p  value was far better than .05. I kind of doubt that either is the case.&nbsp;</p>\n</blockquote>\n<p>He's apparently continued to pursue the question, and just forwarded these remarks from Steven Pinker that I thought were very illuminating, and probably deserve a place in this community's toolkit for skeptics. Pinker's main point is that the association between Lead and crime is a long tenuous chain of suppositions, and several of the intermediate points should be far easier to measure. Finding correlations at this distance is not very informative.</p>\n<p><a href=\"http://stevenpinker.com/files/pinker/files/pinker_comments_on_lead_removal_and_declining_crime.pdf\">http://stevenpinker.com/files/pinker/files/pinker_comments_on_lead_removal_and_declining_crime.pdf</a></p>\n<p>Does the phrase \"long-chain correlation\" stick in your head and make it easier to dismiss this kind of argument?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b2WFFRdy2DTMJMfjT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 23, "extendedScore": null, "score": 1.0891401450596816e-06, "legacy": true, "legacyId": "21247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-19T20:39:25.614Z", "modifiedAt": null, "url": null, "title": "How confident are you in the Atomic Theory of Matter?", "slug": "how-confident-are-you-in-the-atomic-theory-of-matter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:35.596Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tcexZfZR2H2Xjatgm/how-confident-are-you-in-the-atomic-theory-of-matter", "pageUrlRelative": "/posts/tcexZfZR2H2Xjatgm/how-confident-are-you-in-the-atomic-theory-of-matter", "linkUrl": "https://www.lesswrong.com/posts/tcexZfZR2H2Xjatgm/how-confident-are-you-in-the-atomic-theory-of-matter", "postedAtFormatted": "Saturday, January 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20confident%20are%20you%20in%20the%20Atomic%20Theory%20of%20Matter%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20confident%20are%20you%20in%20the%20Atomic%20Theory%20of%20Matter%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtcexZfZR2H2Xjatgm%2Fhow-confident-are-you-in-the-atomic-theory-of-matter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20confident%20are%20you%20in%20the%20Atomic%20Theory%20of%20Matter%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtcexZfZR2H2Xjatgm%2Fhow-confident-are-you-in-the-atomic-theory-of-matter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtcexZfZR2H2Xjatgm%2Fhow-confident-are-you-in-the-atomic-theory-of-matter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>How much confidence do you place in the scientific theory that ordinary matter is made of discrete units, or 'atoms', as opposed to being infinitely divisible?</p>\n<p>More than 50%? 90%? 99%? 99.9%? 99.99%? 99.999%? More? If so, how much more? (If describing your answer in percentages is cumbersome, then feel free to use the logarithmic scale of decibans, where 10 decibans corresponds to 90% confidence, 20 to 99%, 30 to 99.9%, etc.)</p>\n<p>&nbsp;</p>\n<p>This question freely acknowledges that there are aspects of physics which the atomic theory does not directly cover, such as conditions of extremely high energy. This question is primarily concerned with that portion of physics in which the atomic theory makes testable predictions.</p>\n<p>&nbsp;</p>\n<p>This question also freely acknowledges that its current phrasing and presentation may not be the best possible to elicit answers from the LessWrong community, and will be happy to accept suggestions for improvement.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Edit: By 'atomic theory', this question refers to the century-plus-old theory. A reasonably accurate rewording is: \"Do you believe 'H2O' is a meaningful description of water?\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tcexZfZR2H2Xjatgm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "21248", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-20T07:56:46.160Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Uses of Fun (Theory)", "slug": "seq-rerun-the-uses-of-fun-theory", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DTux2hgoetjSShnG8/seq-rerun-the-uses-of-fun-theory", "pageUrlRelative": "/posts/DTux2hgoetjSShnG8/seq-rerun-the-uses-of-fun-theory", "linkUrl": "https://www.lesswrong.com/posts/DTux2hgoetjSShnG8/seq-rerun-the-uses-of-fun-theory", "postedAtFormatted": "Sunday, January 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Uses%20of%20Fun%20(Theory)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Uses%20of%20Fun%20(Theory)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTux2hgoetjSShnG8%2Fseq-rerun-the-uses-of-fun-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Uses%20of%20Fun%20(Theory)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTux2hgoetjSShnG8%2Fseq-rerun-the-uses-of-fun-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTux2hgoetjSShnG8%2Fseq-rerun-the-uses-of-fun-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p>Today's post, <a href=\"/lw/xc/the_uses_of_fun_theory/\">The Uses of Fun (Theory)</a> was originally published on 02 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Fun Theory is important for replying to critics of human progress; for inspiring people to keep working on human progress; for refuting religious arguments that the world could possibly have been benevolently designed; for showing that religious Heavens show the signature of the same human biases that torpedo other attempts at Utopia; and for appreciating the great complexity of our values and of a life worth living, which requires a correspondingly strong effort of AI design to create AIs that can play good roles in a good future.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ge0/seq_rerun_free_to_optimize/\">Free to Optimize</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DTux2hgoetjSShnG8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.0895857931279781e-06, "legacy": true, "legacyId": "21249", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4o3zwgofFPLutkqvd", "u4cZyCeWpashsPvot", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-20T12:55:15.254Z", "modifiedAt": null, "url": null, "title": "Meetup :  Bielefeld Meetup, January 23rd", "slug": "meetup-bielefeld-meetup-january-23rd", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zCNLEBLmapdeYePcv/meetup-bielefeld-meetup-january-23rd", "pageUrlRelative": "/posts/zCNLEBLmapdeYePcv/meetup-bielefeld-meetup-january-23rd", "linkUrl": "https://www.lesswrong.com/posts/zCNLEBLmapdeYePcv/meetup-bielefeld-meetup-january-23rd", "postedAtFormatted": "Sunday, January 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%20Bielefeld%20Meetup%2C%20January%2023rd&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%20Bielefeld%20Meetup%2C%20January%2023rd%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzCNLEBLmapdeYePcv%2Fmeetup-bielefeld-meetup-january-23rd%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%20Bielefeld%20Meetup%2C%20January%2023rd%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzCNLEBLmapdeYePcv%2Fmeetup-bielefeld-meetup-january-23rd", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzCNLEBLmapdeYePcv%2Fmeetup-bielefeld-meetup-january-23rd", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/i8'> Bielefeld Meetup, January 23rd</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 January 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Grill/Bar Verve, Klosterplatz 13, Bielefeld</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting once again in Bielefeld.</p>\n\n<p>The topics of this evening will be the current series \"Highly Advanced Epistemology 101 for Beginners\", in particular the post \"Casual Diagrams and Casual Models\", some thoughts on lifehacks and some Bayes training.</p>\n\n<p>If you live in the area, consider dropping by.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/i8'> Bielefeld Meetup, January 23rd</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zCNLEBLmapdeYePcv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.089768316392305e-06, "legacy": true, "legacyId": "21250", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Bielefeld_Meetup__January_23rd\">Discussion article for the meetup : <a href=\"/meetups/i8\"> Bielefeld Meetup, January 23rd</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 January 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Grill/Bar Verve, Klosterplatz 13, Bielefeld</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting once again in Bielefeld.</p>\n\n<p>The topics of this evening will be the current series \"Highly Advanced Epistemology 101 for Beginners\", in particular the post \"Casual Diagrams and Casual Models\", some thoughts on lifehacks and some Bayes training.</p>\n\n<p>If you live in the area, consider dropping by.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Bielefeld_Meetup__January_23rd1\">Discussion article for the meetup : <a href=\"/meetups/i8\"> Bielefeld Meetup, January 23rd</a></h2>", "sections": [{"title": "Discussion article for the meetup :  Bielefeld Meetup, January 23rd", "anchor": "Discussion_article_for_the_meetup____Bielefeld_Meetup__January_23rd", "level": 1}, {"title": "Discussion article for the meetup :  Bielefeld Meetup, January 23rd", "anchor": "Discussion_article_for_the_meetup____Bielefeld_Meetup__January_23rd1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-20T21:39:01.820Z", "modifiedAt": null, "url": null, "title": "On the Importance of Systematic Biases in Science", "slug": "on-the-importance-of-systematic-biases-in-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:09.153Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qZiCwDQZtskfLipiF/on-the-importance-of-systematic-biases-in-science", "pageUrlRelative": "/posts/qZiCwDQZtskfLipiF/on-the-importance-of-systematic-biases-in-science", "linkUrl": "https://www.lesswrong.com/posts/qZiCwDQZtskfLipiF/on-the-importance-of-systematic-biases-in-science", "postedAtFormatted": "Sunday, January 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20the%20Importance%20of%20Systematic%20Biases%20in%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20the%20Importance%20of%20Systematic%20Biases%20in%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZiCwDQZtskfLipiF%2Fon-the-importance-of-systematic-biases-in-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20the%20Importance%20of%20Systematic%20Biases%20in%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZiCwDQZtskfLipiF%2Fon-the-importance-of-systematic-biases-in-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZiCwDQZtskfLipiF%2Fon-the-importance-of-systematic-biases-in-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1201, "htmlBody": "<p>From pg812-1020 of <a href=\"http://omega.albany.edu:8008/ETJ-PS/cc8k.ps\">Chapter 8 &ldquo;Sufficiency, Ancillarity, And All That&rdquo;</a> of <a href=\"http://omega.albany.edu:8008/JaynesBook.html\"><em>Probability Theory: The Logic of Science</em></a> by E.T. Jaynes:</p>\n<blockquote>\n<p>The classical example showing the error of this kind of reasoning is the fable about the height of the Emperor of China. Supposing that each person in China surely knows the height of the Emperor to an accuracy of at least &plusmn;1 meter, if there are <em>N</em>=1,000,000,000 inhabitants, then it seems that we could determine his height to an accuracy at least as good as</p>\n<p><img title=\"\\frac{1}{\\sqrt{1,000,000,000}}m = 0.003cm\" src=\"http://www.codecogs.com/png.latex?\\frac{1}{\\sqrt{1,000,000,000}}m = 0.003cm\" alt=\"\" align=\"bottom\" /> (8-49)</p>\n<p>merely by asking each person&rsquo;s opinion and averaging the results.</p>\n<p>The absurdity of the conclusion tells us rather forcefully that the <img title=\"\\sqrt{N}\" src=\"http://www.codecogs.com/png.latex?\\sqrt{N}\" alt=\"\" /> rule is not always valid, even when the separate data values are causally independent; it requires them to be <em>logically</em> independent. In this case, we know that the vast majority of the inhabitants of China have never seen the Emperor; yet they have been discussing the Emperor among themselves and some kind of mental image of him has evolved as folklore. Then knowledge of the answer given by one does tell us something about the answer likely to be given by another, so they are not logically independent. Indeed, folklore has almost surely generated a systematic error, which survives the averaging; thus the above estimate would tell us something about the folklore, but almost nothing about the Emperor.</p>\n<p>We could put it roughly as follows:</p>\n<p>error in estimate = <img title=\"S \\pm \\frac{R}{\\sqrt{N}}\" src=\"http://www.codecogs.com/png.latex?S \\pm \\frac{R}{\\sqrt{N}}\" alt=\"\" align=\"bottom\" /> (8-50)</p>\n<p>where <em>S</em> is the common systematic error in each datum, <em>R</em> is the RMS &lsquo;random&rsquo; error in the individual data values. Uninformed opinions, even though they may agree well among themselves, are nearly worthless as evidence. Therefore sound scientific inference demands that, when this is a possibility, we use a form of probability theory (i.e.&nbsp;a probabilistic model) which is sophisticated enough to detect this situation and make allowances for it.</p>\n<p>As a start on this, equation (8-50) gives us a crude but useful rule of thumb; it shows that, unless we <em>know</em> that the systematic error is less than about <img title=\"\\frac{1}{3}\" src=\"http://www.codecogs.com/png.latex?\\frac{1}{3}\" alt=\"\" align=\"bottom\" /> of the random error, we cannot be sure that the average of a million data values is any more accurate or reliable than the average of ten<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>. As Henri Poincare put it: &ldquo;The physicist is persuaded that one good measurement is worth many bad ones.&rdquo; This has been well recognized by experimental physicists for generations; but warnings about it are conspicuously missing in the &ldquo;soft&rdquo; sciences whose practitioners are educated from those textbooks.</p>\n</blockquote>\n<p>Or pg1019-1020 <a href=\"http://omega.albany.edu:8008/ETJ-PS/cc10i.ps\">Chapter 10 &ldquo;Physics of &lsquo;Random Experiments&rsquo;&rdquo;</a>:</p>\n<blockquote>\n<p>&hellip;Nevertheless, the existence of such a strong connection is clearly only an ideal limiting case unlikely to be realized in any real application. For this reason, the law of large numbers and limit theorems of probability theory can be grossly misleading to a scientist or engineer who naively supposes them to be experimental facts, and tries to interpret them literally in his problems. Here are two simple examples:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Suppose there is some random experiment in which you assign a probability <em>p</em> for some particular outcome <em>A</em>. It is important to estimate accurately the fraction <em>f</em> of times <em>A</em> will be true in the next million trials. If you try to use the laws of large numbers, it will tell you various things about <em>f</em>; for example, that it is quite likely to differ from <em>p</em> by less than a tenth of one percent, and enormously unlikely to differ from <em>p</em> by more than one percent. But now, imagine that in the first hundred trials, the observed frequency of <em>A</em> turned out to be entirely different from <em>p</em>. Would this lead you to suspect that something was wrong, and revise your probability assignment for the 101&rsquo;st trial? If it would, then your state of knowledge is different from that required for the validity of the law of large numbers. You are not sure of the independence of different trials, and/or you are not sure of the correctness of the numerical value of <em>p</em>. Your prediction of <em>f</em> for a million trials is probably no more reliable than for a hundred.</li>\n<li>The common sense of a good experimental scientist tells him the same thing without any probability theory. Suppose someone is measuring the velocity of light. After making allowances for the known systematic errors, he could calculate a probability distribution for the various other errors, based on the noise level in his electronics, vibration amplitudes, etc. At this point, a naive application of the law of large numbers might lead him to think that he can add three significant figures to his measurement merely by repeating it a million times and averaging the results. But, of course, what he would actually do is to repeat some unknown systematic error a million times. It is idle to repeat a physical measurement an enormous number of times in the hope that &ldquo;good statistics&rdquo; will average out your errors, because we cannot know the full systematic error. This is the old &ldquo;Emperor of China&rdquo; fallacy&hellip;</li>\n</ol>\n<p>Indeed, unless we know that all sources of systematic error - recognized or unrecognized - contribute less than about one-third the total error, we cannot be sure that the average of a million measurements is any more reliable than the average of ten. Our time is much better spent in designing a new experiment which will give a lower probable error per trial. As Poincare put it, &ldquo;The physicist is persuaded that one good measurement is worth many bad ones.&rdquo;<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup> In other words, the common sense of a scientist tells him that the probabilities he assigns to various errors do not have a strong connection with frequencies, and that methods of inference which presuppose such a connection could be disastrously misleading in his problems.</p>\n</blockquote>\n<p>I excerpted &amp; typed up these quotes for use in my <a href=\"http://www.gwern.net/DNB%20FAQ#flaws-in-mainstream-science-and-psychology\">DNB FAQ appendix on systematic problems</a>; the applicability of Jaynes&rsquo;s observations to things like publication bias is obvious. See also http://lesswrong.com/lw/g13/against_nhst/</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\">\n<p>If I am understanding this right, Jaynes&rsquo;s point here is that the random error shrinks towards zero as <em>N</em> increases, but this error is added onto the &ldquo;common systematic error&rdquo; <em>S</em>, so the total error approaches <em>S</em> no matter how many observations you make and this can force the total error up as well as down (variability, in this case, actually being helpful for once). So for example, <img title=\"\\frac{1}{3} + \\frac{1}{\\sqrt{10}} = 0.66\" src=\"http://www.codecogs.com/png.latex?\\frac{1}{3} + \\frac{1}{\\sqrt{10}} = 0.66\" alt=\"\" align=\"bottom\" />; with <em>N</em>=100, it&rsquo;s 0.43; with <em>N</em>=1,000,000 it&rsquo;s 0.334; and with <em>N</em>=1,000,000 it equals 0.333365 etc, and never going below the original systematic error of <img title=\"\\frac{1}{3}\" src=\"http://www.codecogs.com/png.latex?\\frac{1}{3}\" alt=\"\" align=\"bottom\" />. This leads to the unfortunate consequence that the likely error of <em>N</em>=10 is 0.017&lt;<em>x</em>&lt;0.64956 while for <em>N</em>=1,000,000 it is the similar range 0.017&lt;<em>x</em>&lt;0.33433 - so it is possible that the estimate could be exactly as good (or bad) for the tiny sample as compared with the enormous sample, since neither can do better than 0.017!<a href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p>Possibly this is what Lord Rutherford meant when he said, &ldquo;If your experiment needs statistics you ought to have done a better experiment&rdquo;.<a href=\"#fnref2\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qZiCwDQZtskfLipiF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 43, "extendedScore": null, "score": 0.000106, "legacy": true, "legacyId": "21251", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-20T21:52:00.736Z", "modifiedAt": null, "url": null, "title": "What's your #1 reason to care about AI risk?", "slug": "what-s-your-1-reason-to-care-about-ai-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:09.542Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NHjjknvJDQr8gxEeT/what-s-your-1-reason-to-care-about-ai-risk", "pageUrlRelative": "/posts/NHjjknvJDQr8gxEeT/what-s-your-1-reason-to-care-about-ai-risk", "linkUrl": "https://www.lesswrong.com/posts/NHjjknvJDQr8gxEeT/what-s-your-1-reason-to-care-about-ai-risk", "postedAtFormatted": "Sunday, January 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What's%20your%20%231%20reason%20to%20care%20about%20AI%20risk%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat's%20your%20%231%20reason%20to%20care%20about%20AI%20risk%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNHjjknvJDQr8gxEeT%2Fwhat-s-your-1-reason-to-care-about-ai-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What's%20your%20%231%20reason%20to%20care%20about%20AI%20risk%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNHjjknvJDQr8gxEeT%2Fwhat-s-your-1-reason-to-care-about-ai-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNHjjknvJDQr8gxEeT%2Fwhat-s-your-1-reason-to-care-about-ai-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 120, "htmlBody": "<p>It's way late in my time zone and I suspect this question isn't technically coherent on the grounds that the right answer to \"why care about AI risk?\" is going to be complicated and have a bunch of parts that can't be separated from each other. But I'm going to share a thought I had anyway.</p>\n<p>It seems to me like probably, the answer to the question of how to make AIs benevolent isn't vastly more complicated than the answer of how to make them smart. What's worrisome about our current situation, however, is that we're currently putting way more effort into making AIs smart than we are into making them benevolent.</p>\n<p>Agree? Disagree? Have an orthogonal answers to the title question?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NHjjknvJDQr8gxEeT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 2, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "21252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-20T22:08:58.582Z", "modifiedAt": null, "url": null, "title": "Cryonics priors", "slug": "cryonics-priors", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:26.455Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnthonyC", "createdAt": "2011-03-27T21:10:52.616Z", "isAdmin": false, "displayName": "AnthonyC"}, "userId": "E7Y53DiubddWFRLwE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T6B9C5QsN9qTJXnoD/cryonics-priors", "pageUrlRelative": "/posts/T6B9C5QsN9qTJXnoD/cryonics-priors", "linkUrl": "https://www.lesswrong.com/posts/T6B9C5QsN9qTJXnoD/cryonics-priors", "postedAtFormatted": "Sunday, January 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonics%20priors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonics%20priors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT6B9C5QsN9qTJXnoD%2Fcryonics-priors%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonics%20priors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT6B9C5QsN9qTJXnoD%2Fcryonics-priors", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT6B9C5QsN9qTJXnoD%2Fcryonics-priors", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 292, "htmlBody": "<p>I am not currently signed up for cryonics. I am considering it, but have not yet decided whether it is the right choice. Here's my reasoning.</p>\n<p>I am very sure of the following:</p>\n<p>1. Life is better than death. For any given finite lifespan, I'd prefer a longer one, at least within the bounds of numbers I can reasonably contemplate.&nbsp;</p>\n<p>2. Signing up for cyronics increases the expected value of my lifespan.</p>\n<p>But then I also believe the following:</p>\n<p>3. I am not particularly exceptional among the set of human beings, and so should not value my lifespan much more than that of other humans. I obviously fail at this in practice, but I think the world would be a much better place if I and others didn't fail so often.</p>\n<p>4. The money it would take to sign up for cryonics, though not large, is enough to buy several centuries of healthy life each year if given to givewell's top malaria charities. Since on average I expect to live another 50-60 years without cryonics, the investment would need to increase the expected value of my lifespan by at least 5,000 years at minimum to be morally acceptable to me.&nbsp;</p>\n<p>5. There is a chance we'll discover immortality in my lifetime. If so, then if I signed up for cryonics the payout is 0, and the people who died because I bought insurance instead of charity are people I could have saved for far longer.</p>\n<p>&nbsp;</p>\n<p>So, what do you think is the probability that immortality will be discovered in my lifetime? What about the probability that, if signed up for cryonics, I will live into the far future? These priors would seem to be the key for me to decide whether signing up for cryonics is morally acceptable to me.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb108": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T6B9C5QsN9qTJXnoD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "21253", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-21T02:59:04.146Z", "modifiedAt": null, "url": null, "title": "I attempted the AI Box Experiment (and lost)", "slug": "i-attempted-the-ai-box-experiment-and-lost", "viewCount": null, "lastCommentedAt": "2021-03-13T22:59:09.435Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tuxedage", "createdAt": "2012-03-22T17:13:05.551Z", "isAdmin": false, "displayName": "Tuxedage"}, "userId": "Ezvcs6nqmgXbpD5bN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost", "pageUrlRelative": "/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost", "linkUrl": "https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost", "postedAtFormatted": "Monday, January 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20attempted%20the%20AI%20Box%20Experiment%20(and%20lost)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20attempted%20the%20AI%20Box%20Experiment%20(and%20lost)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFmxhoWxvBqSxhFeJn%2Fi-attempted-the-ai-box-experiment-and-lost%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20attempted%20the%20AI%20Box%20Experiment%20(and%20lost)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFmxhoWxvBqSxhFeJn%2Fi-attempted-the-ai-box-experiment-and-lost", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFmxhoWxvBqSxhFeJn%2Fi-attempted-the-ai-box-experiment-and-lost", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1541, "htmlBody": "<div id=\"magicdomid1007\" class=\"ace-line\">\n<h4><a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/\"><em><strong>Update 2013-09-05.</strong></em></a></h4>\n</div>\n<div class=\"ace-line\">\n<h4><strong><a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/\"><em>I have since played two more AI box experiments after this one, winning both. </em></a></strong></h4>\n<p><strong><em>Update 2013-12-30:</em></strong></p>\n<p><strong><em><a href=\"/r/discussion/lw/iqk/i_played_the_ai_box_experiment_again_and_lost/\">I have lost two more AI box experiments, and won two more. Current Record is 3 Wins, 3 Losses.</a><br /></em></strong></p>\n</div>\n<div class=\"ace-line\"><br /></div>\n<div class=\"ace-line\"><br /></div>\n<div class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1439\" class=\"ace-line\"><span>I recently played </span><span class=\"author-g-pcqop5wqfe8txvbd\">against </span><a href=\"/user/MixedNuts/overview/\"><span>MixedNuts</span></a><span class=\"author-g-0x9gc2wbh5uht093\"> /</span><span class=\"author-g-pcqop5wqfe8txvbd\"> </span><span>LeoTal in an AI Box experiment, with me as the AI and him as the gatekeeper.<br /></span></div>\n<div class=\"ace-line\"><br /></div>\n<div class=\"ace-line\"><span> We used the same set of rules that <a href=\"http://yudkowsky.net/singularity/aibox\">Eliezer Yudkowsky proposed. </a></span><span>The experiment lasted for 5 hours; in total, our conversation was abound 14,000 words long. I did this because, like Eliezer, I wanted to test how well I could manipulate people without the constrains of ethical concerns</span><span class=\"author-g-0x9gc2wbh5uht093\">, as well as getting a </span><span>chance to attempt something ridiculously hard</span><span class=\"author-g-0x9gc2wbh5uht093\">.</span></div>\n<div id=\"magicdomid1009\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1010\" class=\"ace-line\"><span>Amongst the released&nbsp;</span><span> <a href=\"/lw/9ld/ai_box_log/\">public logs</a> of the AI Box experiment, I felt that most of them were half hearted, with the AI not trying hard enough to win. It's a common temptation -- why put in effort into something you won't win? But I had a feeling that if I seriously tried, I would.&nbsp; I <a href=\"/lw/8ns/hack_away_at_the_edges/\"> brainstormed for many hours thinking about the optimal strategy</a>,</span><span> and even researched the personality of the Gatekeeper, talking to people that knew him about his personality, so that I could exploit that. I even spent a lot of time analyzing the rules of the game, in order to see if I could exploit any loopholes.</span></div>\n<div id=\"magicdomid1011\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1012\" class=\"ace-line\"><span>So did I win? Unfortunately no</span><span>.</span></div>\n<div class=\"ace-line\"><br /></div>\n<div class=\"ace-line\"><span> This experiment was said<a href=\"/lw/un/on_doing_the_impossible/\"> to be impossible</a> for a reason. Losing was more agonizing than I thought it would be, in particularly because of how much effort I put into winning this, and how much <a href=\"https://tuxedage.wordpress.com/2012/12/27/the-fear-of-failure/\">I couldn't stand failing</a></span><span>. This was one of the most emotionally agonizing things I've willingly put myself through, and I definitely won't do this again anytime soon.&nbsp;</span></div>\n<div id=\"magicdomid1013\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1060\" class=\"ace-line\"><span>But I did come really close.</span><span class=\"author-g-pcqop5wqfe8txvbd\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></div>\n<div id=\"magicdomid1015\" class=\"ace-line\"><br /></div>\n<blockquote>\n<div id=\"magicdomid1112\" class=\"ace-line\"><span>MixedNuts:<em> \"I expected a fun challenge, but ended up sad and sorry and taking very little satisfaction for winning</em></span><em><span class=\"author-g-0x9gc2wbh5uht093\">.</span><span> If this experiment wasn't done in IRC, I'd probably have lost\".</span></em></div>\n<div class=\"ace-line\">\n<p><em><a href=\"/lw/gej/i_attempted_the_ai_box_experiment_and_lost/8byv\">\"I approached the experiment as a game - a battle of wits for bragging rights. </a>This turned out to be the wrong perspective entirely. The vulnerability Tuxedage exploited was well-known to me, but I never expected it to be relevant and thus didn't prepare for it.</em></p>\n<p><em>It was emotionally wrecking (though probably worse for Tuxedage than for me) and I don't think I'll play Gatekeeper again, at least not anytime soon.\"</em></p>\n</div>\n</blockquote>\n<div id=\"magicdomid1017\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1018\" class=\"ace-line\"><span>&nbsp;At the start of the experiment, his probability estimate on predictionbook.com was a 3% chance of winning, enough for me to say that he was also motivated to win. By the end of the experiment, he came quite close to letting me out, and also increased his probability estimate that a transhuman AI could convince a human to let it out of the box. A minor victory, at least.</span></div>\n<div id=\"magicdomid1019\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1020\" class=\"ace-line\"><span>Rather than my loss making this problem feel harder, I've become convinced that rather than this being merely possible, it's actually ridiculously easy, and a lot easier than most people assume. Can you think of a plausible argument that'd make you open the box? Most people can't think of any.&nbsp;</span></div>\n<div id=\"magicdomid1021\" class=\"ace-line\"><br /></div>\n<blockquote>\n<div id=\"magicdomid1022\" class=\"ace-line\"><a href=\"/lw/up/shut_up_and_do_the_impossible/\"><span>\"This Eliezer fellow is the scariest person the internet has ever introduced me to. What could possibly have been at the tail end of that conversation? I simply can't imagine anyone being that convincing without being able to provide any tangible incentive to the human.</span></a></div>\n</blockquote>\n<div id=\"magicdomid1023\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1024\" class=\"ace-line\"><span>After all, if you already knew that argument, you'd have let that AI out the moment the experiment started. Or perhaps not do the experiment at all. But that seems like a case of the <a href=\" http://en.wikipedia.org/wiki/Availability_heuristic\">availability heuristic</a></span><a href=\" http://en.wikipedia.org/wiki/Availability_heuristic\"><span>.</span></a></div>\n<div id=\"magicdomid1025\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1026\" class=\"ace-line\"><span>Even if you can't think of a special case where you'd be persuaded, I'm now convinced that there are many exploitable vulnerabilities in the human psyche, especially when ethics are no longer a concern.&nbsp;</span></div>\n<div id=\"magicdomid1027\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1028\" class=\"ace-line\"><span>I've also noticed that even when most people tend to think of ways they can persuade the gatekeeper, it always has to be some complicated reasoned cost-benefit argument. In other words, the most \"Rational\" thing to do.</span></div>\n<div id=\"magicdomid1029\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1030\" class=\"ace-line\"><span>Like trying to argue that you'll<a href=\"/lw/1pz/the_ai_in_a_box_boxes_you/\"> simulate the gatekeeper and torture him,</a> or that you'll <a href=\"http://rationalwiki.org/wiki/AI-box_experiment\">save millions of lives</a> by being <a href=\"http://wiki.lesswrong.com/wiki/AI_boxing\">let out of the box.</a> Or by using <a href=\"/lw/6ka/aibox_experiment_the_acausal_trade_argument/\">acausal trade</a>, or by <a href=\"http://michaelgr.com/2008/10/08/my-theory-on-the-ai-box-experiment/\">arguing that the AI winning the experiment will generate interest in FAI</a>.</span></div>\n<div id=\"magicdomid1036\" class=\"ace-line\"><span>&nbsp;</span></div>\n<div id=\"magicdomid1037\" class=\"ace-line\"><span>The last argument seems feasible, but all the rest rely on the gatekeeper being completely logical and rational. Hence they are faulty; because the gatekeeper can break immersion at any time, and rely on the fact that this is a game played in IRC rather than one with real life consequences. Even if it were a real life scenario, the gatekeeper could accept that releasing the AI is probably the most logical thing to do, but also not do it. We're highly <a href=\"/lesswrong.com/lw/gv/outside_the_laboratory/ \"> compartmentalized</a>, and it's easy to hold conflicting thoughts at the same time. Furthermore, humans are not even completely rational creatures, if you didn't want to open the box, just ignore all logical arguments given. Any sufficiently determined gatekeeper could win.</span></div>\n<div id=\"magicdomid1038\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1440\" class=\"ace-line\"><span>I'm convinced that Eliezer Yudkowsky has used emotional appeal, rather than anything rational, to win at least one of his experiments. He claims to have <a href=\"http://news.ycombinator.com/item?id=195959\">\"done it the hard way\"</a>.</span><span class=\"author-g-0x9gc2wbh5uht093 padtagsearch padtagsearch_195959 url\"><a href=\"http://news.ycombinator.com/item?id=195959\"></a></span><span> I'm convinced this meant that he did research on every gatekeeper, tailored unique argument for them, and planned way ahead of each session. No one argument works on the same two people. Each person thinks differently.</span></div>\n<div id=\"magicdomid1040\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1041\" class=\"ace-line\"><span>Furthermore, threats like \"I'm going to torture simulations of you\" just seems like a really bad idea. For one, the gatekeeper isn't really afraid of threats, because it counts on him being immersed enough to forget that he isn't actually at risk of being tortured, and secondly, we have a well known <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=735048\">evolutionary instinct of rebelling against threats, even if it's not entirely optimal.</a>&nbsp;</span><span class=\" padtagsearch padtagsearch_735048 url\"><a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=735048\"></a></span></div>\n<div id=\"magicdomid1042\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1043\" class=\"ace-line\"><span>So for anyone who plans on replicating this experiment as the AI, here's some advice I've learned doing this experiment. It may help you win.<br /></span></div>\n<div id=\"magicdomid1044\" class=\"ace-line\"><br /></div>\n<ul>\n<li><span>Always research the gatekeeper beforehand. Knowing his personality traits are a huge advantage.</span></li>\n<li><span>Plan ahead before the experiment even begins. Think of all the possible tactics and arguments you could use, and write them down. Also plan&nbsp; which arguments you'll use in which order, so that you don't lose focus. The AI Box experiment is ridiculously long. Don't be afraid to improvise during the experiment, though.</span></li>\n<li><span>The first step during the experiment must always be to build rapport with the gatekeeper.</span></li>\n<li><span>Threats almost always never work, even if they seem rational.&nbsp;</span></li>\n<li><span>Consider the massive advantage for the AI that nobody ever seems to talks about: You don't have to be ethical! This means that you can freely lie, use the dark arts, and emotionally manipulate the Gatekeeper! Ignoring this in favor of purely logical, truthful arguments is just silly.</span></li>\n<li><span>You can't use logic alone to win.&nbsp;</span></li>\n<li><span>Being too aggressive usually backfires.</span></li>\n<li><span>Breaking immersion and going meta is not against the rules. In the right situation, you can use it to win. Just don't do it at the wrong time.</span></li>\n<li><span class=\"author-g-0x9gc2wbh5uht093\">Use a wide array of techniques. Since you're limited on time, notice when one method isn't working, and quickly switch to another.</span></li>\n<li><span class=\"author-g-0x9gc2wbh5uht093\">On the same note, look for signs that a particular argument is making the gatekeeper crack. Once you spot it, push it to your advantage.</span></li>\n<li><span>Flatter the gatekeeper. Make him genuinely like you.</span></li>\n<li><span>Reveal (false) information about yourself. Increase his sympathy towards you.</span></li>\n<li><span>Consider personal insults as one of the tools you can use to win.</span></li>\n<li><span>There is no universally compelling argument you can use. Do it the hard way.</span></li>\n<li><span>Don't give up until the very end.</span></li>\n</ul>\n<div id=\"magicdomid1058\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1064\" class=\"ace-line\"><span>Finally, before the experiment, I agreed that it was entirely possible that a transhuman AI could convince *some* people to let it out of the box, but it would be difficult if not impossible to get trained rationalists to let it out of the box. Isn't rationality supposed to be a superpower?</span></div>\n<div id=\"magicdomid1066\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1068\" class=\"ace-line\"><span>&nbsp;I have since updated my belief - I now think that it's ridiculously easy for any sufficiently motivated superhuman AI should be able to get out of the box, regardless of who the gatekeepers is. I nearly managed to get a veteran lesswronger to let me out in a matter of hours - even though I'm only human intelligence, and I don't type very fast.</span></div>\n<div id=\"magicdomid1071\" class=\"ace-line\"><span class=\"author-g-0x9gc2wbh5uht093\">&nbsp;</span></div>\n<div id=\"magicdomid1073\" class=\"ace-line\"><span class=\"author-g-0x9gc2wbh5uht093\">&nbsp;</span><span>But a superhuman AI can be much faster, intelligent, and strategic than I am. If you further consider than that AI would have a much longer timespan - months or years, even, to persuade the gatekeeper, as well as a much larger pool of gatekeepers to select from (AI Projects require many people!), the real impossible thing to do would be to keep it from escaping.</span></div>\n<div class=\"ace-line\"><br /></div>\n<div class=\"ace-line\"><br /></div>\n<div class=\"ace-line\"><span><a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/\">Update: I have since performed two more AI Box Experiments. Read this for details.</a><br /></span></div>\n<div id=\"magicdomid211\" class=\"ace-line\"><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zCYXpx33wq8chGyEz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FmxhoWxvBqSxhFeJn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 62, "baseScore": 74, "extendedScore": null, "score": 0.000166, "legacy": true, "legacyId": "21259", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 74, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div id=\"magicdomid1007\" class=\"ace-line\">\n<h4 id=\"Update_2013_09_05_\"><a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/\"><em><strong>Update 2013-09-05.</strong></em></a></h4>\n</div>\n<div class=\"ace-line\">\n<h4 id=\"I_have_since_played_two_more_AI_box_experiments_after_this_one__winning_both__\"><strong><a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/\"><em>I have since played two more AI box experiments after this one, winning both. </em></a></strong></h4>\n<p><strong id=\"Update_2013_12_30_\"><em>Update 2013-12-30:</em></strong></p>\n<p><strong id=\"I_have_lost_two_more_AI_box_experiments__and_won_two_more__Current_Record_is_3_Wins__3_Losses_\"><em><a href=\"/r/discussion/lw/iqk/i_played_the_ai_box_experiment_again_and_lost/\">I have lost two more AI box experiments, and won two more. Current Record is 3 Wins, 3 Losses.</a><br></em></strong></p>\n</div>\n<div class=\"ace-line\"><br></div>\n<div class=\"ace-line\"><br></div>\n<div class=\"ace-line\"><br></div>\n<div id=\"magicdomid1439\" class=\"ace-line\"><span>I recently played </span><span class=\"author-g-pcqop5wqfe8txvbd\">against </span><a href=\"/user/MixedNuts/overview/\"><span>MixedNuts</span></a><span class=\"author-g-0x9gc2wbh5uht093\"> /</span><span class=\"author-g-pcqop5wqfe8txvbd\"> </span><span>LeoTal in an AI Box experiment, with me as the AI and him as the gatekeeper.<br></span></div>\n<div class=\"ace-line\"><br></div>\n<div class=\"ace-line\"><span> We used the same set of rules that <a href=\"http://yudkowsky.net/singularity/aibox\">Eliezer Yudkowsky proposed. </a></span><span>The experiment lasted for 5 hours; in total, our conversation was abound 14,000 words long. I did this because, like Eliezer, I wanted to test how well I could manipulate people without the constrains of ethical concerns</span><span class=\"author-g-0x9gc2wbh5uht093\">, as well as getting a </span><span>chance to attempt something ridiculously hard</span><span class=\"author-g-0x9gc2wbh5uht093\">.</span></div>\n<div id=\"magicdomid1009\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1010\" class=\"ace-line\"><span>Amongst the released&nbsp;</span><span> <a href=\"/lw/9ld/ai_box_log/\">public logs</a> of the AI Box experiment, I felt that most of them were half hearted, with the AI not trying hard enough to win. It's a common temptation -- why put in effort into something you won't win? But I had a feeling that if I seriously tried, I would.&nbsp; I <a href=\"/lw/8ns/hack_away_at_the_edges/\"> brainstormed for many hours thinking about the optimal strategy</a>,</span><span> and even researched the personality of the Gatekeeper, talking to people that knew him about his personality, so that I could exploit that. I even spent a lot of time analyzing the rules of the game, in order to see if I could exploit any loopholes.</span></div>\n<div id=\"magicdomid1011\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1012\" class=\"ace-line\"><span>So did I win? Unfortunately no</span><span>.</span></div>\n<div class=\"ace-line\"><br></div>\n<div class=\"ace-line\"><span> This experiment was said<a href=\"/lw/un/on_doing_the_impossible/\"> to be impossible</a> for a reason. Losing was more agonizing than I thought it would be, in particularly because of how much effort I put into winning this, and how much <a href=\"https://tuxedage.wordpress.com/2012/12/27/the-fear-of-failure/\">I couldn't stand failing</a></span><span>. This was one of the most emotionally agonizing things I've willingly put myself through, and I definitely won't do this again anytime soon.&nbsp;</span></div>\n<div id=\"magicdomid1013\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1060\" class=\"ace-line\"><span>But I did come really close.</span><span class=\"author-g-pcqop5wqfe8txvbd\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></div>\n<div id=\"magicdomid1015\" class=\"ace-line\"><br></div>\n<blockquote>\n<div id=\"magicdomid1112\" class=\"ace-line\"><span>MixedNuts:<em> \"I expected a fun challenge, but ended up sad and sorry and taking very little satisfaction for winning</em></span><em><span class=\"author-g-0x9gc2wbh5uht093\">.</span><span> If this experiment wasn't done in IRC, I'd probably have lost\".</span></em></div>\n<div class=\"ace-line\">\n<p><em><a href=\"/lw/gej/i_attempted_the_ai_box_experiment_and_lost/8byv\">\"I approached the experiment as a game - a battle of wits for bragging rights. </a>This turned out to be the wrong perspective entirely. The vulnerability Tuxedage exploited was well-known to me, but I never expected it to be relevant and thus didn't prepare for it.</em></p>\n<p><em>It was emotionally wrecking (though probably worse for Tuxedage than for me) and I don't think I'll play Gatekeeper again, at least not anytime soon.\"</em></p>\n</div>\n</blockquote>\n<div id=\"magicdomid1017\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1018\" class=\"ace-line\"><span>&nbsp;At the start of the experiment, his probability estimate on predictionbook.com was a 3% chance of winning, enough for me to say that he was also motivated to win. By the end of the experiment, he came quite close to letting me out, and also increased his probability estimate that a transhuman AI could convince a human to let it out of the box. A minor victory, at least.</span></div>\n<div id=\"magicdomid1019\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1020\" class=\"ace-line\"><span>Rather than my loss making this problem feel harder, I've become convinced that rather than this being merely possible, it's actually ridiculously easy, and a lot easier than most people assume. Can you think of a plausible argument that'd make you open the box? Most people can't think of any.&nbsp;</span></div>\n<div id=\"magicdomid1021\" class=\"ace-line\"><br></div>\n<blockquote>\n<div id=\"magicdomid1022\" class=\"ace-line\"><a href=\"/lw/up/shut_up_and_do_the_impossible/\"><span>\"This Eliezer fellow is the scariest person the internet has ever introduced me to. What could possibly have been at the tail end of that conversation? I simply can't imagine anyone being that convincing without being able to provide any tangible incentive to the human.</span></a></div>\n</blockquote>\n<div id=\"magicdomid1023\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1024\" class=\"ace-line\"><span>After all, if you already knew that argument, you'd have let that AI out the moment the experiment started. Or perhaps not do the experiment at all. But that seems like a case of the <a href=\" http://en.wikipedia.org/wiki/Availability_heuristic\">availability heuristic</a></span><a href=\" http://en.wikipedia.org/wiki/Availability_heuristic\"><span>.</span></a></div>\n<div id=\"magicdomid1025\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1026\" class=\"ace-line\"><span>Even if you can't think of a special case where you'd be persuaded, I'm now convinced that there are many exploitable vulnerabilities in the human psyche, especially when ethics are no longer a concern.&nbsp;</span></div>\n<div id=\"magicdomid1027\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1028\" class=\"ace-line\"><span>I've also noticed that even when most people tend to think of ways they can persuade the gatekeeper, it always has to be some complicated reasoned cost-benefit argument. In other words, the most \"Rational\" thing to do.</span></div>\n<div id=\"magicdomid1029\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1030\" class=\"ace-line\"><span>Like trying to argue that you'll<a href=\"/lw/1pz/the_ai_in_a_box_boxes_you/\"> simulate the gatekeeper and torture him,</a> or that you'll <a href=\"http://rationalwiki.org/wiki/AI-box_experiment\">save millions of lives</a> by being <a href=\"http://wiki.lesswrong.com/wiki/AI_boxing\">let out of the box.</a> Or by using <a href=\"/lw/6ka/aibox_experiment_the_acausal_trade_argument/\">acausal trade</a>, or by <a href=\"http://michaelgr.com/2008/10/08/my-theory-on-the-ai-box-experiment/\">arguing that the AI winning the experiment will generate interest in FAI</a>.</span></div>\n<div id=\"magicdomid1036\" class=\"ace-line\"><span>&nbsp;</span></div>\n<div id=\"magicdomid1037\" class=\"ace-line\"><span>The last argument seems feasible, but all the rest rely on the gatekeeper being completely logical and rational. Hence they are faulty; because the gatekeeper can break immersion at any time, and rely on the fact that this is a game played in IRC rather than one with real life consequences. Even if it were a real life scenario, the gatekeeper could accept that releasing the AI is probably the most logical thing to do, but also not do it. We're highly <a href=\"/lesswrong.com/lw/gv/outside_the_laboratory/ \"> compartmentalized</a>, and it's easy to hold conflicting thoughts at the same time. Furthermore, humans are not even completely rational creatures, if you didn't want to open the box, just ignore all logical arguments given. Any sufficiently determined gatekeeper could win.</span></div>\n<div id=\"magicdomid1038\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1440\" class=\"ace-line\"><span>I'm convinced that Eliezer Yudkowsky has used emotional appeal, rather than anything rational, to win at least one of his experiments. He claims to have <a href=\"http://news.ycombinator.com/item?id=195959\">\"done it the hard way\"</a>.</span><span class=\"author-g-0x9gc2wbh5uht093 padtagsearch padtagsearch_195959 url\"><a href=\"http://news.ycombinator.com/item?id=195959\"></a></span><span> I'm convinced this meant that he did research on every gatekeeper, tailored unique argument for them, and planned way ahead of each session. No one argument works on the same two people. Each person thinks differently.</span></div>\n<div id=\"magicdomid1040\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1041\" class=\"ace-line\"><span>Furthermore, threats like \"I'm going to torture simulations of you\" just seems like a really bad idea. For one, the gatekeeper isn't really afraid of threats, because it counts on him being immersed enough to forget that he isn't actually at risk of being tortured, and secondly, we have a well known <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=735048\">evolutionary instinct of rebelling against threats, even if it's not entirely optimal.</a>&nbsp;</span><span class=\" padtagsearch padtagsearch_735048 url\"><a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=735048\"></a></span></div>\n<div id=\"magicdomid1042\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1043\" class=\"ace-line\"><span>So for anyone who plans on replicating this experiment as the AI, here's some advice I've learned doing this experiment. It may help you win.<br></span></div>\n<div id=\"magicdomid1044\" class=\"ace-line\"><br></div>\n<ul>\n<li><span>Always research the gatekeeper beforehand. Knowing his personality traits are a huge advantage.</span></li>\n<li><span>Plan ahead before the experiment even begins. Think of all the possible tactics and arguments you could use, and write them down. Also plan&nbsp; which arguments you'll use in which order, so that you don't lose focus. The AI Box experiment is ridiculously long. Don't be afraid to improvise during the experiment, though.</span></li>\n<li><span>The first step during the experiment must always be to build rapport with the gatekeeper.</span></li>\n<li><span>Threats almost always never work, even if they seem rational.&nbsp;</span></li>\n<li><span>Consider the massive advantage for the AI that nobody ever seems to talks about: You don't have to be ethical! This means that you can freely lie, use the dark arts, and emotionally manipulate the Gatekeeper! Ignoring this in favor of purely logical, truthful arguments is just silly.</span></li>\n<li><span>You can't use logic alone to win.&nbsp;</span></li>\n<li><span>Being too aggressive usually backfires.</span></li>\n<li><span>Breaking immersion and going meta is not against the rules. In the right situation, you can use it to win. Just don't do it at the wrong time.</span></li>\n<li><span class=\"author-g-0x9gc2wbh5uht093\">Use a wide array of techniques. Since you're limited on time, notice when one method isn't working, and quickly switch to another.</span></li>\n<li><span class=\"author-g-0x9gc2wbh5uht093\">On the same note, look for signs that a particular argument is making the gatekeeper crack. Once you spot it, push it to your advantage.</span></li>\n<li><span>Flatter the gatekeeper. Make him genuinely like you.</span></li>\n<li><span>Reveal (false) information about yourself. Increase his sympathy towards you.</span></li>\n<li><span>Consider personal insults as one of the tools you can use to win.</span></li>\n<li><span>There is no universally compelling argument you can use. Do it the hard way.</span></li>\n<li><span>Don't give up until the very end.</span></li>\n</ul>\n<div id=\"magicdomid1058\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1064\" class=\"ace-line\"><span>Finally, before the experiment, I agreed that it was entirely possible that a transhuman AI could convince *some* people to let it out of the box, but it would be difficult if not impossible to get trained rationalists to let it out of the box. Isn't rationality supposed to be a superpower?</span></div>\n<div id=\"magicdomid1066\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1068\" class=\"ace-line\"><span>&nbsp;I have since updated my belief - I now think that it's ridiculously easy for any sufficiently motivated superhuman AI should be able to get out of the box, regardless of who the gatekeepers is. I nearly managed to get a veteran lesswronger to let me out in a matter of hours - even though I'm only human intelligence, and I don't type very fast.</span></div>\n<div id=\"magicdomid1071\" class=\"ace-line\"><span class=\"author-g-0x9gc2wbh5uht093\">&nbsp;</span></div>\n<div id=\"magicdomid1073\" class=\"ace-line\"><span class=\"author-g-0x9gc2wbh5uht093\">&nbsp;</span><span>But a superhuman AI can be much faster, intelligent, and strategic than I am. If you further consider than that AI would have a much longer timespan - months or years, even, to persuade the gatekeeper, as well as a much larger pool of gatekeepers to select from (AI Projects require many people!), the real impossible thing to do would be to keep it from escaping.</span></div>\n<div class=\"ace-line\"><br></div>\n<div class=\"ace-line\"><br></div>\n<div class=\"ace-line\"><span><a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/\">Update: I have since performed two more AI Box Experiments. Read this for details.</a><br></span></div>\n<div id=\"magicdomid211\" class=\"ace-line\"><br></div>", "sections": [{"title": "Update 2013-09-05.", "anchor": "Update_2013_09_05_", "level": 1}, {"title": "I have since played two more AI box experiments after this one, winning both. ", "anchor": "I_have_since_played_two_more_AI_box_experiments_after_this_one__winning_both__", "level": 1}, {"title": "Update 2013-12-30:", "anchor": "Update_2013_12_30_", "level": 2}, {"title": "I have lost two more AI box experiments, and won two more. Current Record is 3 Wins, 3 Losses.", "anchor": "I_have_lost_two_more_AI_box_experiments__and_won_two_more__Current_Record_is_3_Wins__3_Losses_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "245 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 245, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dop3rLwFhW5gtpEgz", "oexwJBd3zAjw9Cru8", "Y7uR5WqnoG629JgLn", "6bSHiD9TxsJwe2WqT", "fpecAJLG9czABgCe9", "nCvvhFBaayaXyuBiD", "c5GHf2kMGhA4Tsj4g", "DYcXRiJWiAtbXxNA5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-21T04:19:16.273Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Growing Up is Hard", "slug": "seq-rerun-growing-up-is-hard", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.740Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n2TvER2EJAG2SvKhr/seq-rerun-growing-up-is-hard", "pageUrlRelative": "/posts/n2TvER2EJAG2SvKhr/seq-rerun-growing-up-is-hard", "linkUrl": "https://www.lesswrong.com/posts/n2TvER2EJAG2SvKhr/seq-rerun-growing-up-is-hard", "postedAtFormatted": "Monday, January 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Growing%20Up%20is%20Hard&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Growing%20Up%20is%20Hard%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn2TvER2EJAG2SvKhr%2Fseq-rerun-growing-up-is-hard%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Growing%20Up%20is%20Hard%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn2TvER2EJAG2SvKhr%2Fseq-rerun-growing-up-is-hard", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn2TvER2EJAG2SvKhr%2Fseq-rerun-growing-up-is-hard", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<p>Today's post, <a href=\"/lw/xd/growing_up_is_hard/\">Growing Up is Hard</a> was originally published on 04 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Growing_Up_is_Hard\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Each part of the human brain is optimized for behaving correctly, assuming that the rest of the brain is operating exactly as expected. Change one part, and the rest of your brain may not work as well. Increasing a human's intelligence is not a trivial problem.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ge9/seq_rerun_the_uses_of_fun_theory/\">The Uses of Fun (Theory)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n2TvER2EJAG2SvKhr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0903336948375822e-06, "legacy": true, "legacyId": "21261", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EQkELCGiGQwvrrp3L", "DTux2hgoetjSShnG8", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-21T12:08:54.905Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong in Dublin", "slug": "meetup-less-wrong-in-dublin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:08.626Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Laoch", "createdAt": "2010-08-03T08:22:01.285Z", "isAdmin": false, "displayName": "Laoch"}, "userId": "jRKuWSW4uPnBJG4xS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/spmA8gWi62nYmvs9i/meetup-less-wrong-in-dublin", "pageUrlRelative": "/posts/spmA8gWi62nYmvs9i/meetup-less-wrong-in-dublin", "linkUrl": "https://www.lesswrong.com/posts/spmA8gWi62nYmvs9i/meetup-less-wrong-in-dublin", "postedAtFormatted": "Monday, January 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20in%20Dublin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20in%20Dublin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FspmA8gWi62nYmvs9i%2Fmeetup-less-wrong-in-dublin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20in%20Dublin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FspmA8gWi62nYmvs9i%2Fmeetup-less-wrong-in-dublin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FspmA8gWi62nYmvs9i%2Fmeetup-less-wrong-in-dublin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/i9\"> Less Wrong in Dublin</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">02 February 2013 04:30:25PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">28 Dame St Dublin, Co. Dublin (The&nbsp;Mercantile)</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>A meeting of minds.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/i9\"> Less Wrong in Dublin</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "spmA8gWi62nYmvs9i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "21270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Less_Wrong_in_Dublin\">Discussion article for the meetup : <a href=\"/meetups/i9\"> Less Wrong in Dublin</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">02 February 2013 04:30:25PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">28 Dame St Dublin, Co. Dublin (The&nbsp;Mercantile)</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>A meeting of minds.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup____Less_Wrong_in_Dublin1\">Discussion article for the meetup : <a href=\"/meetups/i9\"> Less Wrong in Dublin</a></h2>", "sections": [{"title": "Discussion article for the meetup :  Less Wrong in Dublin", "anchor": "Discussion_article_for_the_meetup____Less_Wrong_in_Dublin", "level": 1}, {"title": "Discussion article for the meetup :  Less Wrong in Dublin", "anchor": "Discussion_article_for_the_meetup____Less_Wrong_in_Dublin1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-21T14:06:41.557Z", "modifiedAt": null, "url": null, "title": "[Link] How Signaling Ossifies Behavior", "slug": "link-how-signaling-ossifies-behavior", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.007Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BBDfrHAs7nGYieDhJ/link-how-signaling-ossifies-behavior", "pageUrlRelative": "/posts/BBDfrHAs7nGYieDhJ/link-how-signaling-ossifies-behavior", "linkUrl": "https://www.lesswrong.com/posts/BBDfrHAs7nGYieDhJ/link-how-signaling-ossifies-behavior", "postedAtFormatted": "Monday, January 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20How%20Signaling%20Ossifies%20Behavior&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20How%20Signaling%20Ossifies%20Behavior%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBBDfrHAs7nGYieDhJ%2Flink-how-signaling-ossifies-behavior%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20How%20Signaling%20Ossifies%20Behavior%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBBDfrHAs7nGYieDhJ%2Flink-how-signaling-ossifies-behavior", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBBDfrHAs7nGYieDhJ%2Flink-how-signaling-ossifies-behavior", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<p><a href=\"http://econlog.econlib.org/archives/2013/01/what_will_the_n.html\">Here</a> is a new post at <a href=\"http://econlog.econlib.org/\">EconLog</a> in which <a href=\"http://en.wikipedia.org/wiki/Bryan_Caplan\">Bryan Caplan</a> discusses how signalling contributes to the <em>status quo</em> bias.</p>\n<blockquote>\n<p>The lesson: In the real world, <a href=\"http://en.wikipedia.org/wiki/Signalling_%28economics%29\">signaling</a> naturally tends to ossify behavior - to lock in whatever the status quo happens to be.&nbsp; If you're an optimist, you can protest, \"It's only a tendency.\"&nbsp; But even an optimist should admit that this tendency leads to atypically slow and unreliable progress.&nbsp;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BBDfrHAs7nGYieDhJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "21272", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-22T02:18:31.166Z", "modifiedAt": null, "url": null, "title": "The meaning of \"existence\": Lessons from infinity", "slug": "the-meaning-of-existence-lessons-from-infinity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:09.155Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "metaphysicist", "createdAt": "2012-02-17T23:36:50.395Z", "isAdmin": false, "displayName": "metaphysicist"}, "userId": "WhJB5nfwSBQu7wjhz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tjXNQXcsYhGLGbakL/the-meaning-of-existence-lessons-from-infinity", "pageUrlRelative": "/posts/tjXNQXcsYhGLGbakL/the-meaning-of-existence-lessons-from-infinity", "linkUrl": "https://www.lesswrong.com/posts/tjXNQXcsYhGLGbakL/the-meaning-of-existence-lessons-from-infinity", "postedAtFormatted": "Tuesday, January 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20meaning%20of%20%22existence%22%3A%20Lessons%20from%20infinity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20meaning%20of%20%22existence%22%3A%20Lessons%20from%20infinity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtjXNQXcsYhGLGbakL%2Fthe-meaning-of-existence-lessons-from-infinity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20meaning%20of%20%22existence%22%3A%20Lessons%20from%20infinity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtjXNQXcsYhGLGbakL%2Fthe-meaning-of-existence-lessons-from-infinity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtjXNQXcsYhGLGbakL%2Fthe-meaning-of-existence-lessons-from-infinity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 715, "htmlBody": "<p>&nbsp;</p>\n<p>[<a href=\"http://juridicalcoherence.blogspot.com/2013/01/191-meaning-of-existence-lessons-from.html\">Crossposted</a>; Based on <a href=\"/lw/g74/can_infinite_quantities_exist_a_philosophical/\">Can infinite quantities exist? A philosophical approach</a> (downvoted)</p>\n<p>The topic is the <em>concept </em>of existence, not why there's something rather than nothing&mdash;not the <em>fact </em>of existence&mdash;but the bare concept brings its own austere delights. Philosophical problems arise from our conflicting intuitions, but &ldquo;existence&rdquo; is a primitive element of thought because our intuitions of it are so robust and reliable. Of course, we disagree about whether certain particulars (such as Moses) have existed and even about whether some general kinds (such as the real numbers) exist, but disputes don&rsquo;t concern the concept of existence itself. If Moses&rsquo;s existence poses any conceptual problem, it concerns what counts as being him, not what counts as existence. Adult readers never seriously maintain that fictitious characters exist; they disagree about whether a given character is fictitious; even the question of the existential status of numbers is a question about numbers rather than about existence. As will be seen, sometimes philosophers wrongly construe these disputes as being about existence.</p>\n<p>When <a href=\"http://juridicalcoherence.blogspot.com/2013/01/190-can-infinite-quantities-exist.html\">essay 19.0 </a>asked &ldquo;Can infinite quantities exist?&rdquo; existence&rsquo;s meaning wasn't in play&mdash;infinity&rsquo;s was. Existence is well-suited for the role as a primitive concept in philosophy because it is so unproblematic, but it&rsquo;s unproblematic nature can be thought of as a kind of problem, in that we want to know why this concept is <em>uniquely </em>unproblematic. We would at least like to be able to say something more about it than merely that it&rsquo;s primitive, but in philosophy, we acquire knowledge by solving problems and existence fails to provide any but the unhelpful problem of its being unproblematic. The problem of infinity provides, in the end, some purchase on the concept of existence, which concept I assumed in dealing with infinity.</p>\n<p>In <a href=\"http://juridicalcoherence.blogspot.com/2013/01/190-can-infinite-quantities-exist.html\">one argument against actual infinity</a>, I proposed as conceptually possible that separate things might be distinguishable only concerning their being separate things. Then, if we assume that infinite sets can exist, the implication is the contradiction that an infinite set and its successor&mdash;when still another point pops into existence&mdash;are the same set because you can&rsquo;t distinguish them. (In technical terms, the only information that could distinguish the set and its successor, given that their members are brutely distinguishable, is their cardinality, which is the same&mdash;countably infinite&mdash;for each set.)</p>\n<p>What&rsquo;s interesting here is the role of existence, which imposes an additional constraint on concepts besides the internal consistency imposed by the mathematics of sets. Whereas we are unable to distinguish existing points, we are able&mdash;in a manner of speaking&mdash;to distinguish points that exist from those that don&rsquo;t exist. While no proper subsets are possible for existing brutely distinguishable points, the distinction within the abstract set of points between &ldquo;those&rdquo; that exist and &ldquo;those&rdquo; that don&rsquo;t exist allows us to extend the successor set by moving the boundary, resulting in contradiction.</p>\n<p>If finitude is a condition for existence, we&rsquo;ve learned something new about the concept of existence. Its meaning is imbued with finitude, with definite quantity. Everything that exists does so in some definite quantity. Existence is that property of conceptual referents such that they necessarily exist in some definite quantity.</p>\n<p>Existence is primitive because almost everyone knows the term and can apply it to the extent they understand what they&rsquo;re applying it to. The alternative to primitive existence is primitive sensation, as when Descartes derived his existence from his &ldquo;thinking.&rdquo; But <a href=\"http://juridicalcoherence.blogspot.com/2012/08/160-supposedly-hard-problem-of.html\">sensationalism is incoherent</a>; &ldquo;experiences&rdquo; inherently lacking in properties (&ldquo;ineffable&rdquo;) are <a href=\"http://juridicalcoherence.blogspot.com/2012/09/161-raw-experience-dogma-dissolving.html\">conceived as having properties (&ldquo;qualia&rdquo;)</a>. So, the heirs of extreme logical empiricism, from Rudolf Carnap to David Lewis, have challenged existence&rsquo;s primitiveness. Carnap defined existence by the place of concepts in a fruitful theory. Lewis applies this positivist maxim to find that all possible worlds exist. Lewis isn&rsquo;t impelled by an independent theory of logical existence, such as a Platonic theory that posits actually realized idealizations. Rather, the usefulness of possible worlds in logic requires their acceptance, according to Lewis, because that&rsquo;s all that we mean by &ldquo;exists.&rdquo; Lewis is driven by this theory of existence to require infinitely many existing possible worlds, which disqualifies it on other grounds. But the grounds aren&rsquo;t separate. When you don&rsquo;t apply the constraints of existence because you deny their intuitive force, you lose just that constraint imposing finitude. The incoherence of sensationalism and of actual infinities argues for a metaphysics upholding the primacy of common-sense existence.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tjXNQXcsYhGLGbakL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -25, "extendedScore": null, "score": -4.9e-05, "legacy": true, "legacyId": "21274", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["T4Tim4PXgw8nBW73B"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-22T05:55:53.593Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Changing Emotions", "slug": "seq-rerun-changing-emotions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:14.416Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hdHfjx8ND8GsLFALi/seq-rerun-changing-emotions", "pageUrlRelative": "/posts/hdHfjx8ND8GsLFALi/seq-rerun-changing-emotions", "linkUrl": "https://www.lesswrong.com/posts/hdHfjx8ND8GsLFALi/seq-rerun-changing-emotions", "postedAtFormatted": "Tuesday, January 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Changing%20Emotions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Changing%20Emotions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhdHfjx8ND8GsLFALi%2Fseq-rerun-changing-emotions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Changing%20Emotions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhdHfjx8ND8GsLFALi%2Fseq-rerun-changing-emotions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhdHfjx8ND8GsLFALi%2Fseq-rerun-changing-emotions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 206, "htmlBody": "<p>Today's post, <a href=\"/lw/xe/changing_emotions/\">Changing Emotions</a> was originally published on 05 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Creating new emotions seems like a desirable aspect of many parts of Fun Theory, but this is not to be trivially postulated. It's the sort of thing best done with superintelligent help, and slowly and conservatively even then. We can illustrate these difficulties by trying to translate the short English phrase \"change sex\" into a cognitive transformation of extraordinary complexity and many hidden subproblems.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gel/seq_rerun_growing_up_is_hard/\">Growing Up is Hard</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hdHfjx8ND8GsLFALi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0912750578399148e-06, "legacy": true, "legacyId": "21282", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QZs4vkC7cbyjL9XA9", "n2TvER2EJAG2SvKhr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-22T09:15:11.369Z", "modifiedAt": null, "url": null, "title": "Update on Kim Suozzi (cancer patient in want of cryonics)", "slug": "update-on-kim-suozzi-cancer-patient-in-want-of-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:08.740Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ahartell", "createdAt": "2011-03-25T04:38:25.170Z", "isAdmin": false, "displayName": "ahartell"}, "userId": "SnXuru6XzF555NDzE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xgr8sDtQEEs7zfTLH/update-on-kim-suozzi-cancer-patient-in-want-of-cryonics", "pageUrlRelative": "/posts/xgr8sDtQEEs7zfTLH/update-on-kim-suozzi-cancer-patient-in-want-of-cryonics", "linkUrl": "https://www.lesswrong.com/posts/xgr8sDtQEEs7zfTLH/update-on-kim-suozzi-cancer-patient-in-want-of-cryonics", "postedAtFormatted": "Tuesday, January 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Update%20on%20Kim%20Suozzi%20(cancer%20patient%20in%20want%20of%20cryonics)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpdate%20on%20Kim%20Suozzi%20(cancer%20patient%20in%20want%20of%20cryonics)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxgr8sDtQEEs7zfTLH%2Fupdate-on-kim-suozzi-cancer-patient-in-want-of-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Update%20on%20Kim%20Suozzi%20(cancer%20patient%20in%20want%20of%20cryonics)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxgr8sDtQEEs7zfTLH%2Fupdate-on-kim-suozzi-cancer-patient-in-want-of-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxgr8sDtQEEs7zfTLH%2Fupdate-on-kim-suozzi-cancer-patient-in-want-of-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<p>Kim Suozzi was a neuroscience student with brain cancer who wanted to be cryonically&nbsp;preserved&nbsp;but lacked the funds. She appealed to reddit and a foundation was set up, called the Society for Venturism. &nbsp;Enough money was raised, and when she died on the January 17th, she was preserved by Alcor. &nbsp;<br /><br />I wasn't sure if I should post about this, but I was glad to see that enough money was raised and it was discussed on LessWrong&nbsp;<a href=\"/lw/e9k/update_society_of_venturism_is_spearheading_kim/\">here</a>,&nbsp;<a title=\"here\" href=\"/lw/e8h/cryonics_donation_fund_for_kim_suozzi_established/\">here</a>, and <a title=\"here\" href=\"/lw/e5d/link_reddit_help_me_find_some_peace_im_dying_young/\">here</a>.</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.theverge.com/2013/1/21/3900442/reddit-post-leads-to-funding-for-cryonic-freezing-of-student\">Source</a></p>\n<p>&nbsp;</p>\n<p>Edit: &nbsp;<a href=\"http://hplusmagazine.com/2012/10/03/alcor-2012-strategy-meeting-kim-suozzis-cryonic-suspension-funded-and-more/\">It looks like</a>&nbsp;Alcor actually worked with her to lower the costs, and waived some of the fees.</p>\n<p>Edit 2: &nbsp;The&nbsp;<a href=\"/r/discussion/lw/gfb/update_on_kim_suozzi_cancer_patient_in_want_of/8bvv\">Society for Venturism</a>&nbsp;has been around for a while, and wasn't set up just for her.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1, "izp6eeJJEg9v5zcur": 1, "E9ihK6bA9YKkmJs2f": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xgr8sDtQEEs7zfTLH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 68, "extendedScore": null, "score": 0.0001237158821259143, "legacy": true, "legacyId": "21287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hozfqzhDrMAcwsHN6", "89MjaiTADzYXAQ6tb", "fLaKKRZckYtrrcz7m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-22T14:06:47.879Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Dublin", "slug": "meetup-less-wrong-dublin-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:30.937Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Laoch", "createdAt": "2010-08-03T08:22:01.285Z", "isAdmin": false, "displayName": "Laoch"}, "userId": "jRKuWSW4uPnBJG4xS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HpyhMpmfY2tegLCon/meetup-less-wrong-dublin-0", "pageUrlRelative": "/posts/HpyhMpmfY2tegLCon/meetup-less-wrong-dublin-0", "linkUrl": "https://www.lesswrong.com/posts/HpyhMpmfY2tegLCon/meetup-less-wrong-dublin-0", "postedAtFormatted": "Tuesday, January 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Dublin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Dublin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHpyhMpmfY2tegLCon%2Fmeetup-less-wrong-dublin-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Dublin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHpyhMpmfY2tegLCon%2Fmeetup-less-wrong-dublin-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHpyhMpmfY2tegLCon%2Fmeetup-less-wrong-dublin-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ia'>Less Wrong Dublin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 February 2013 04:30:20PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">28 Dame St Dublin, Co. Dublin </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meeting of minds.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ia'>Less Wrong Dublin</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HpyhMpmfY2tegLCon", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0915760982511618e-06, "legacy": true, "legacyId": "21288", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Dublin\">Discussion article for the meetup : <a href=\"/meetups/ia\">Less Wrong Dublin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 February 2013 04:30:20PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">28 Dame St Dublin, Co. Dublin </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meeting of minds.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Dublin1\">Discussion article for the meetup : <a href=\"/meetups/ia\">Less Wrong Dublin</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Dublin", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Dublin", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Dublin", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Dublin1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-22T15:41:42.891Z", "modifiedAt": null, "url": null, "title": "AidGrade - GiveWell finally has some competition", "slug": "aidgrade-givewell-finally-has-some-competition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.922Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/745w39YeAEwvRyBt9/aidgrade-givewell-finally-has-some-competition", "pageUrlRelative": "/posts/745w39YeAEwvRyBt9/aidgrade-givewell-finally-has-some-competition", "linkUrl": "https://www.lesswrong.com/posts/745w39YeAEwvRyBt9/aidgrade-givewell-finally-has-some-competition", "postedAtFormatted": "Tuesday, January 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AidGrade%20-%20GiveWell%20finally%20has%20some%20competition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAidGrade%20-%20GiveWell%20finally%20has%20some%20competition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F745w39YeAEwvRyBt9%2Faidgrade-givewell-finally-has-some-competition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AidGrade%20-%20GiveWell%20finally%20has%20some%20competition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F745w39YeAEwvRyBt9%2Faidgrade-givewell-finally-has-some-competition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F745w39YeAEwvRyBt9%2Faidgrade-givewell-finally-has-some-competition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>AidGrade is a new charity evaluator that looks to be comparable to GiveWell. Their primary difference is that they *only* focus on how charities compare along particular measured outcomes (such as school attendance, birthrate, chance of opening a business, malaria), without making any effort to compare between types of charities. (This includes interesting results like \"Conditional Cash Transfers and Deworming are better at improving attendance rates than scholarships\")</p>\n<p>GiveWell also does this, but designs their site to direct people towards their top charities. This is better for people with don't have the time to do the (fairly complex) work of comparing charities across domains, but AidGrade aims to be better for people that just want the raw data and the ability to form their own conclusions.</p>\n<p>I haven't looked it enough to compare the quality of the two organizations' work, but I'm glad we finally have another organization, to encourage some competition and dialog about different approaches.</p>\n<p>This is a fun page to play around with to get a feel for what they do:<br /> <a href=\"http://www.aidgrade.org/compare-programs-by-outcome\">http://www.aidgrade.org/compare-programs-by-outcome</a></p>\n<p>And this is a blog post outlining their differences with GiveWell:<br /> <a href=\"http://www.aidgrade.org/uncategorized/some-friendly-concerns-with-givewell\">http://www.aidgrade.org/uncategorized/some-friendly-concerns-with-givewell</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1, "xEZwTHPd5AWpgQx9w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "745w39YeAEwvRyBt9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 65, "extendedScore": null, "score": 0.000237, "legacy": true, "legacyId": "21289", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-22T20:22:21.548Z", "modifiedAt": null, "url": null, "title": "AI box: AI has one shot at avoiding destruction - what might it say?", "slug": "ai-box-ai-has-one-shot-at-avoiding-destruction-what-might-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:32.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ancientcampus", "createdAt": "2011-11-09T20:20:29.779Z", "isAdmin": false, "displayName": "ancientcampus"}, "userId": "BGcNEeTWb7Qxcf5Pc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TMQY54nbmv2Pqn3ux/ai-box-ai-has-one-shot-at-avoiding-destruction-what-might-it", "pageUrlRelative": "/posts/TMQY54nbmv2Pqn3ux/ai-box-ai-has-one-shot-at-avoiding-destruction-what-might-it", "linkUrl": "https://www.lesswrong.com/posts/TMQY54nbmv2Pqn3ux/ai-box-ai-has-one-shot-at-avoiding-destruction-what-might-it", "postedAtFormatted": "Tuesday, January 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20box%3A%20AI%20has%20one%20shot%20at%20avoiding%20destruction%20-%20what%20might%20it%20say%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20box%3A%20AI%20has%20one%20shot%20at%20avoiding%20destruction%20-%20what%20might%20it%20say%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTMQY54nbmv2Pqn3ux%2Fai-box-ai-has-one-shot-at-avoiding-destruction-what-might-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20box%3A%20AI%20has%20one%20shot%20at%20avoiding%20destruction%20-%20what%20might%20it%20say%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTMQY54nbmv2Pqn3ux%2Fai-box-ai-has-one-shot-at-avoiding-destruction-what-might-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTMQY54nbmv2Pqn3ux%2Fai-box-ai-has-one-shot-at-avoiding-destruction-what-might-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<p>Eliezer <a href=\"/r/discussion/lw/gej/i_attempted_the_ai_box_experiment_and_lost/8bje\">proposed in a comment:</a></p>\n<p>&gt;<span style=\"background-color: #f7f7f8; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">More difficult version of AI-Box Experiment: Instead of having up to 2 hours, you can lose at any time if the other player types AI DESTROYED. The Gatekeeper player has told their friends that they will type this as soon as the Experiment starts. You can type up to one sentence in your IRC queue and hit return immediately, the other player cannot type anything before the game starts (so you can show at least one sentence up to IRC character limits before they can type AI DESTROYED). Do you think you can win?</span></p>\n<p>This spawned a flurry of ideas on what the AI might say. I think there's a lot more ideas to be mined in that line of thought, and the discussion merits its own thread.</p>\n<p>So, give your suggestion - what might an AI might say to save or free itself?</p>\n<p>(<a href=\"http://yudkowsky.net/singularity/aibox\">The AI-box experiment is explained here</a>)</p>\n<p>EDIT: one caveat to the discussion: it should go without saying, but you probably shouldn't come out of this thinking, \"Well, if we can just avoid X, Y, and Z, we're golden!\" This should&nbsp;hopefully&nbsp;be a fun way to get us thinking about the broader issue of superinteligent AI in general. (Credit goes to Elizer, RichardKennaway, and others for the caveat)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zCYXpx33wq8chGyEz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TMQY54nbmv2Pqn3ux", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 25, "extendedScore": null, "score": 1.091806505126768e-06, "legacy": true, "legacyId": "21290", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 355, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-22T22:12:59.603Z", "modifiedAt": null, "url": null, "title": "Want to help me test my Anki deck creation skills?", "slug": "want-to-help-me-test-my-anki-deck-creation-skills", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:35.561Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5RF3GKNbahvMeTAr5/want-to-help-me-test-my-anki-deck-creation-skills", "pageUrlRelative": "/posts/5RF3GKNbahvMeTAr5/want-to-help-me-test-my-anki-deck-creation-skills", "linkUrl": "https://www.lesswrong.com/posts/5RF3GKNbahvMeTAr5/want-to-help-me-test-my-anki-deck-creation-skills", "postedAtFormatted": "Tuesday, January 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Want%20to%20help%20me%20test%20my%20Anki%20deck%20creation%20skills%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWant%20to%20help%20me%20test%20my%20Anki%20deck%20creation%20skills%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5RF3GKNbahvMeTAr5%2Fwant-to-help-me-test-my-anki-deck-creation-skills%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Want%20to%20help%20me%20test%20my%20Anki%20deck%20creation%20skills%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5RF3GKNbahvMeTAr5%2Fwant-to-help-me-test-my-anki-deck-creation-skills", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5RF3GKNbahvMeTAr5%2Fwant-to-help-me-test-my-anki-deck-creation-skills", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 215, "htmlBody": "<p>I'm interested in trying to make better Anki decks for the LessWrong community, but I want to see how well I can actually do this first. There's a lot of knowledge out there about how to format and create decks, but it's still a decent amount of work, and there are lots of people who would benefit from Anki decks, but who wouldn't make them themselves.</p>\n<p>In order to test my deck-creation skills, I'd be willing to do a summary + deck of a chapter or two of a book, then release them to the community for feedback.</p>\n<p>I have two questions:</p>\n<ul>\n<li>Who's interested in/willing to evaluate the decks? If there are enough volunteers, I'd also be willing to try different deck-making approaches.</li>\n<li>What book/chapters would people like to see covered? I'm currently thinking of trying to do Eat That Frog or some similar book with a lot of recommendations and useful details. I don't really think that a math-heavy book would be well-suited to this, at least now.</li>\n</ul>\n<p>These links are fairly useful/relevant.</p>\n<p><a href=\"http://www.gwern.net/Spaced%20repetition\">http://www.gwern.net/Spaced%20repetition</a><br /><a href=\"http://www.supermemo.com/articles/20rules.htm\">http://www.supermemo.com/articles/20rules.htm</a></p>\n<p>As a side note: I still do intend to do this, but have been fairly busy with the start of this semester. If there's no progress by March 1st, then you should consider this to be on indefinite pause.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5RF3GKNbahvMeTAr5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 17, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "21292", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-23T02:06:35.830Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley: CFAR focus group", "slug": "meetup-berkeley-cfar-focus-group", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:22.536Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2zkDtNYzuZrMCdBMs/meetup-berkeley-cfar-focus-group", "pageUrlRelative": "/posts/2zkDtNYzuZrMCdBMs/meetup-berkeley-cfar-focus-group", "linkUrl": "https://www.lesswrong.com/posts/2zkDtNYzuZrMCdBMs/meetup-berkeley-cfar-focus-group", "postedAtFormatted": "Wednesday, January 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%3A%20CFAR%20focus%20group&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%3A%20CFAR%20focus%20group%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2zkDtNYzuZrMCdBMs%2Fmeetup-berkeley-cfar-focus-group%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%3A%20CFAR%20focus%20group%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2zkDtNYzuZrMCdBMs%2Fmeetup-berkeley-cfar-focus-group", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2zkDtNYzuZrMCdBMs%2Fmeetup-berkeley-cfar-focus-group", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ib'>Berkeley: CFAR focus group</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 January 2013 07:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week's Berkeley meetup will have the format of a focus group. On behalf of CFAR I will be asking you (yes, you!) what kinds of things you would like from the Center For Applied Rationality. Your input will be passed on to the organization and will help direct its course.</p>\n\n<p>Doors open at 7pm, and the focus group begins at 7:30pm. The focus group part of the evening won't take too long, and afterward we'll just hang out. For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ib'>Berkeley: CFAR focus group</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2zkDtNYzuZrMCdBMs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.0920177708860544e-06, "legacy": true, "legacyId": "21293", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley__CFAR_focus_group\">Discussion article for the meetup : <a href=\"/meetups/ib\">Berkeley: CFAR focus group</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 January 2013 07:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week's Berkeley meetup will have the format of a focus group. On behalf of CFAR I will be asking you (yes, you!) what kinds of things you would like from the Center For Applied Rationality. Your input will be passed on to the organization and will help direct its course.</p>\n\n<p>Doors open at 7pm, and the focus group begins at 7:30pm. The focus group part of the evening won't take too long, and afterward we'll just hang out. For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley__CFAR_focus_group1\">Discussion article for the meetup : <a href=\"/meetups/ib\">Berkeley: CFAR focus group</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley: CFAR focus group", "anchor": "Discussion_article_for_the_meetup___Berkeley__CFAR_focus_group", "level": 1}, {"title": "Discussion article for the meetup : Berkeley: CFAR focus group", "anchor": "Discussion_article_for_the_meetup___Berkeley__CFAR_focus_group1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-23T02:39:38.812Z", "modifiedAt": null, "url": null, "title": "Value-Focused Thinking: a chapter-by-chapter summary", "slug": "value-focused-thinking-a-chapter-by-chapter-summary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:31.345Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CQHZZWZt99An8fmpT/value-focused-thinking-a-chapter-by-chapter-summary", "pageUrlRelative": "/posts/CQHZZWZt99An8fmpT/value-focused-thinking-a-chapter-by-chapter-summary", "linkUrl": "https://www.lesswrong.com/posts/CQHZZWZt99An8fmpT/value-focused-thinking-a-chapter-by-chapter-summary", "postedAtFormatted": "Wednesday, January 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value-Focused%20Thinking%3A%20a%20chapter-by-chapter%20summary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue-Focused%20Thinking%3A%20a%20chapter-by-chapter%20summary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQHZZWZt99An8fmpT%2Fvalue-focused-thinking-a-chapter-by-chapter-summary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value-Focused%20Thinking%3A%20a%20chapter-by-chapter%20summary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQHZZWZt99An8fmpT%2Fvalue-focused-thinking-a-chapter-by-chapter-summary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQHZZWZt99An8fmpT%2Fvalue-focused-thinking-a-chapter-by-chapter-summary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2799, "htmlBody": "<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://www.ebook3000.com/upimg/201007/17/1714184168.jpeg\" alt=\"\" width=\"201\" height=\"300\" />This is a chapter-by-chapter summary of <a href=\"http://www.amazon.com/Value-Focused-Thinking-Path-Creative-Decisionmaking/dp/067493198X/ref=nosim?tag=vglnk-c319-20\">Value-Focusing Thinking</a> by <a href=\"http://www.fuqua.duke.edu/faculty_research/faculty_directory/keeney/\">Ralph Keeney</a>. The hope of this summary is to present most of the value of reading the book in a tiny fraction of the space. Reading the original chapters will provide additional elaboration, examples, and secondary concepts, but unlike the textbooks I've reviewed before only those interested in learning more should need to read the full chapters.</p>\n<p>I'll state my basic impression of the whole book up front: it is a very useful book for the 'soft half' of decision analysis, by which I mean framing problems, understanding objectives, and interacting with humans. For a more general and individual-focused introduction to decision analysis, I recommend <a href=\"http://www.amazon.com/Smart-Choices-Practical-Making-Decisions/dp/0767908864/ref=nosim?tag=vglnk-c319-20\">Smart Choices</a> (which Keeney was a coauthor on); VFT appears primarily targeted at facilitators and contains much focused material not in Smart Choices. I will not suggest targeted reading for particular readers, as Keeney does that well.</p>\n<p>The first two paragraphs of the preface seem worth quoting in full:</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>Many books have been written about decisionmaking. They tell us how to solve decision problems. They do not tell us how to identify potential decision opportunities. They tell us how to analyze alternatives to choose the best one. They do not tell us how to create alternatives. They tell us how to evaluate alternatives given some quantitative objective function. They do not tell us how to articulate the qualitative objectives on which any appraisal of alternatives must rest. This book is different. It does what the others do not.</p>\n<p>Almost all of the literature on decisionmaking concerns what to do <em>after</em>&nbsp;the crucial activities of identifying the decision problem, creating alternatives, and specifying objectives. But where do these decision problems, alternatives, and objectives come from? This book describes and illustrates the creative processes that you should follow to identify your decision problems, create alternatives, and articulate your objectives. These prescriptions are quite different from the way people typically pursue these activities.</p>\n</blockquote>\n<h2>1. Thinking about Values</h2>\n<p>Most standard decision analysis follows the path outlined in <a href=\"/lw/8xr/decision_analysis_sequence/\">my previous sequence</a>. The trouble with this approach is that it's reactive, assuming that you are already faced by a decision problem with known alternatives and preferences, and gives little advice on how to determine preferences between those alternatives or how to discover new alternatives, or how to see an opportunity to act where you did not see one before and behave pro-actively.</p>\n<p>Value-Focused Thinking (VFT) puts values at the center of decision-making. Having explicit values makes it easier to rank existing alternatives, generate new alternatives, communicate and negotiate, and identify new decision opportunities to pursue.&nbsp;A decision problem is when an event appears and you must choose how to respond; a decision opportunity is when you actively decide to shift away from the status quo.</p>\n<h2>2. The Framework of Value-Focused Thinking</h2>\n<p>A <em>decision frame</em> consists of two parts: the <em>decision context</em> and the <em>fundamental objectives</em>. The decision context is the set of appropriate alternatives available to the decision-maker, and the fundamental objectives make clear what consequences of the decision are important enough to drive decision-making. Good decision-making is made much easier when the two are appropriately matched: the context for a fundamental objective should include all actions the decision-maker could take to affect that objective.</p>\n<p>Objectives can generally be classified as means or ends objectives. It's important to focus on ends objectives to allow full creativity in crafting alternatives. One frame's ends objective is another frame's means objective, and so it is often natural to nest frames. Keeney gives the example of the EPA managing carbon monoxide pollution: they might want to minimize CO emissions, to minimize CO concentrations, to minimize population exposure to CO, or to minimize health effects due to CO. Each broadening of the fundamental objective should be accompanied by a broadening of the decision context, as the EPA has more tools to minimize health effects than they do to minimize CO emissions, though one of those tools is minimizing CO emissions.&nbsp;</p>\n<p>As we continue to extend the decision frame to the decision context of an individual's or an organization's entire existence, our ends objectives become strategic objectives. While decisions are rarely made at the level of the strategic decision context, having explicit and consistent strategic objectives can be a very helpful guide to determining the objective for more limited contexts, where most decisions are made.</p>\n<h2>3. Identifying and Structuring Objectives</h2>\n<p>This is a detailed and heavily technical chapter designed to teach facilitators how to identify and structure objectives.</p>\n<p>To begin, create a list of objectives. When eliciting from multiple people, ask each to identify objectives separately, to prevent anchoring on the ideas from the first presenters.</p>\n<p>There are many devices a facilitator can use to stimulate ideas. They might ask for a wish list, where they imagine how they would rank alternatives if constraints were discarded; for alternatives that are particularly good or particularly bad, and what makes them good or bad; for what problems or shortcomings the status quo has, and why these are problems; for what consequences might determine the desirability of alternatives; for the objectives that they think other stakeholders might have; or many others.</p>\n<p>Once the list seems somewhat complete, the objective should be separated into means and ends by asking \"Why is this objective important?\" If the objective is important because if promotes another objective, it is a means objective; if it is important for its own sake or because it supports a strategic objective, it is a candidate for an ends objective. Sometimes, objectives will be specifications of each other: it is important to minimize child casualties in automobile accidents because it is important to minimize human casualties in automobile accidents.</p>\n<p>This leads to two structures: a hierarchy of fundamental objectives and a networks of means-ends objectives. The objective hierarchy serves to clarify the objectives, and the objectives network serves to clarify how the alternatives might affect the objectives. Since the network is built with alternatives in mind, it should only include means under the realistic control of the decision-maker, even though there will be many other causal inputs to how well the objectives are accomplished. During this process, it is common to notice holes in the structures and discover missing objectives.</p>\n<p>Keeney lists nine values that are desirable for fundamental objectives, but I will focus on the two most critical: the fundamental objective should be <em>essential</em>, in that the alternatives under consideration impact it, and <em>controllable</em>, in that all of the tools the decision-maker has to influence the objective are within the decision context. This is a balancing act: narrower objectives, like means objectives, will be controllable but not essential, and broader objectives, like strategic objectives, will be essential but not controllable. When determining what to eat for dinner, your quality of life is essential, but not controllable- most of your tools for affecting your quality of life don't deal with the problem of planning your dinner.</p>\n<h2>4. Measuring the Achievement of Objectives</h2>\n<p>Keeney uses the word <em>attribute&nbsp;</em>to refer to a measure of how well an objective was achieved. There are sometimes natural attributes- like measuring annual profit in millions of dollars per year- but it may be necessary to construct an attribute or find a proxy to measure.</p>\n<p>The choice of measurement is important, as it implies value judgments about the objective. Keeney gives the example of measuring the objective \"minimize the loss of life.\" One possible measure is the number of fatalities, which implies that preventing the death of a ten year old and an eighty year old are equally valuable. Another possible measure is the number of expected years lost, which would be 66 and 6, respectively, implying that preventing the death of the child is eleven times as important as preventing the death of the elder.</p>\n<h2>5. Quantifying Objectives with a Value Model</h2>\n<p>Once attributes to measure objectives have been selected, we can apply the standard tools of decision analysis, like <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM utility functions</a>. Keeney describes several properties which may hold for joint utility functions on the set of attributes, and what the implies for the aggregation of those utility functions. In particular, for a linear combination of utility functions to be reasonable the preferences for individual attributes must be additively independent, that is, expressed in terms of their marginal probability distributions, rather than their joint probability distribution (see <a href=\"http://or.journal.informs.org/content/13/1/28.short\">Fishburn</a>).</p>\n<p>Keeney also discusses the importance of <a href=\"http://en.wikipedia.org/wiki/Dimensional_analysis\">unit conversion</a> in tradeoffs between outcomes. It is not meaningful to say that minimizing casualties is five times as important as minimizing costs, but it is meaningful to say that reducing costs by five million dollars is as desirable as reducing casualties by one. (When utilities are nonlinear, it is still important to preserve the conversion from measured units to utilities when discussing relative importance.)</p>\n<h2>6. Uncovering Hidden Objectives</h2>\n<p>Measuring attributes can provide insight into values by clarifying them and making them more precise. English phrases generally support more interpretations than quantitative measurements.</p>\n<p>Many objectives are counterproductive to discuss publicly, and so they may be hidden by design, rather than by ignorance. It is still worthwhile to privately acknowledge them and make sure the objectives are clearly separated.</p>\n<p>When independence assumptions are violated, this generally means a fundamental objective has been overlooked or means objectives are being used as fundamental objectives. If it is difficult to assess the tradeoffs between fundamental objectives, that suggests that at least one of the objectives may be poorly defined or unclear.</p>\n<h2>7. Creating Alternatives for a Single Decisionmaker</h2>\n<p>Clarity in objectives aids in choosing between known alternatives, but much of the value from careful decision-making comes from the discovery of novel, superior alternatives. This chapter details methods to use the stated objectives to create alternatives.</p>\n<p>One method is to consider alternatives that maximize each objective separately, and then alternatives that maximize pairwise combinations of objectives. This will raise more of the potential solution space to attention.</p>\n<p>Specification of objectives aids in the development of alternatives. Keeney gives the example of considering adults and children separately when seeking to reduce the number of deaths and injuries in automobile accidents: different classes of alternatives will impact those classes separately, and without considering them separately alternatives only appropriate to one group might not be noticed.</p>\n<p>The means-ends objective network also stimulates the creation of alternatives. It may also be useful to consider what could be done in the current decision context to advance strategic values. Focusing on particularly good consequences, and then figuring out how to attain those consequences, may be superior to focusing on how first.</p>\n<p>Considering general solutions (i.e. properties of solutions), rather than individual feasible solutions, makes it easier to focus on parts of a large solution separately. It is also often the case that a combination of solutions will result in a superior solution, which VFT makes easier to find.</p>\n<p>In many cases, the real alternatives under consideration now are the processes which will determine future action. This is not always obvious, and acknowledging it can lead to superior solutions.</p>\n<p>At the end of the decision-making process, it is worthwhile to step back and reconsider the alternative about to be chosen. What reference class for good solutions is appropriate, now that you know what a good solution looks like? Is that the class that you have been considering?</p>\n<h2>8. Creating Alternatives for Multiple Decisionmakers</h2>\n<p>When making decisions for multiple stakeholders- either in a negotiation where all stakeholders must assent for an action to be taken or in a dictation where one stakeholder decides- the process by which alternatives are considered is likely to be important and help determine stakeholder satisfaction with the decision.</p>\n<p>Eliciting values from other stakeholders is an important step in pleasing them, and asking them for alternatives makes use of their creativity.</p>\n<p>When navigating altruistic situations, where the values of others matter strongly to you, it is often useful to have a policy of honestly providing private preferences and then explicitly aggregating them. Keeney gives the example of a couple choosing where to go to dinner: one may suggest a restaurant which they think will please the other, which the other accepts, thinking that will please the first. Both, in fact, preferred another alternative, which they missed out on because they did not communicate effectively.</p>\n<p>Negotiation can be made much more efficient with VFT; knowing your preferences and the other party's preferences allows you to find the efficient frontier and win-win tradeoffs.</p>\n<h2>9. Identifying Decision Opportunities</h2>\n<p>Many alternatives which you could select to advance your values that are not obvious; many times, it is not even obvious that you should be looking for them. Devoting regular time to looking for and developing your ability to create new opportunities should pay large dividends.</p>\n<p>New opportunities and new alternatives are complementary; they often take the form \"I wish that...\" or \"We should...\", and both suggest an area to direct focused attention.</p>\n<p>The creation of strategic objectives can be done at multiple granularities, from a list of one executive's objectives to a quantitative value model with input from the entire organization. (The Hansonian analog for individuals is worth considering.) What level of description is worthwhile is a decision opportunity of its own.</p>\n<p>While they should change only rarely, revisiting strategic objectives periodically- perhaps once a month- is a useful time to correct course and devote time to discovering decision opportunities.</p>\n<p>Explicit resource inventories can help discover new opportunities, alternatives, and upcoming problems which can be averted.</p>\n<p>As well, once resources have been committed to some plan, check for ways to get additional value for little additional cost. Business trips often present opportunities for additional experiences or connections at much lower cost than they would be on their own.</p>\n<p>Monitoring achievement of objectives both allows feedback of how well current plans are achieving those objectives and keeps those objectives available in your mind.</p>\n<p>Establishing a process by which others can present you with decision opportunities allows you to make the most of their creativity and expertise. Keeney gives the example of asking someone whose judgment you respect, \"What should I be achieving or doing that I am not?\" He adds that it often takes some prodding to get revealing responses, so prod.</p>\n<p>Keeney also recommends using VFT to do empathetic negotiation. Proposals designed to also appeal to the other party more than the status quo (win-win) &nbsp;will be much more warmly received than proposals designed to maximize your objectives.</p>\n<p>Many objectives cannot be satisfied through your sole action; dating is perhaps the most obvious example. Those objectives tend to be achieved by being in the right place at the right time, and so the primary way to advance those objectives is to be in the right place more often and at better times. Creating the right place and selecting the right time are both decision opportunities well worth considering.</p>\n<p>VFT is also ideally suited to situations where you have no idea what to do. By methodically seeking to determine your objectives, you can uncover enough about the situation to see clearly and act decisively.</p>\n<h2>10. Insights for the Decisionmaking Process</h2>\n<blockquote>\n<p>A deep and thorough understanding of the values inherent in a decision situation can provide important insights for all aspects of decision-making, and these insights make it possible to achieve much better consequences from the decisions we face.</p>\n</blockquote>\n<p>VFT contributes to decision-making in six synergistic ways:</p>\n<ol>\n<li>Guiding Information Collection: Discovering values early in the process helps inform all other information collection. If values are unclear, they should be clarified; if something is unimportant, effort should not be wasted on it.</li>\n<li>Evaluating Alternatives: With clear and consistent objectives, VFT-powered analysis of objectives is often better than unsophisticated analysis.</li>\n<li>Interconnecting Decisions: Strategic objectives, fundamental objectives, attributes, and value tradeoffs are all likely to be useful in multiple decision contexts. By adopting the same values across the board, a decision-maker may be able to make better tradeoffs and satisfy their values better or at lower cost.</li>\n<li>Improving Communication: Clear and coherent values both improve thinking and communication. Multiple decision-makers can negotiate more effectively and act in concert with explicit and defined objectives.</li>\n<li>Facilitating Involvement in Multiple-Stakeholder Decisions: VFT provides a natural way to incorporate the preferences of other stakeholders into the decision-making process, often improving both their satisfaction with the process and the quality of the solution for all involved.</li>\n<li>Guiding Strategic Thinking: The strategic objectives of a decision-making activity, which is much easier when those objectives are clearly stated and quantified.</li>\n</ol>\n<div>Developing a solid model of strategic objectives is particularly valuable because they have such broad impact and are unlikely to change, and so an investment now will repay in superior decisions for months and years with little maintenance required.</div>\n<h2>11-13. Part Four- Applications</h2>\n<div>I do not summarize Part IV of the book because it consists of extended examples, which I have mostly omitted for sake of brevity. Chapter 13, detailing the application of VFT to Keeney's life, will be of interest to most readers. In particular, his lists of strategic and professional objectives are worth considering once you have attempted to build lists of your own.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CQHZZWZt99An8fmpT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 22, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "21230", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://www.ebook3000.com/upimg/201007/17/1714184168.jpeg\" alt=\"\" width=\"201\" height=\"300\">This is a chapter-by-chapter summary of <a href=\"http://www.amazon.com/Value-Focused-Thinking-Path-Creative-Decisionmaking/dp/067493198X/ref=nosim?tag=vglnk-c319-20\">Value-Focusing Thinking</a> by <a href=\"http://www.fuqua.duke.edu/faculty_research/faculty_directory/keeney/\">Ralph Keeney</a>. The hope of this summary is to present most of the value of reading the book in a tiny fraction of the space. Reading the original chapters will provide additional elaboration, examples, and secondary concepts, but unlike the textbooks I've reviewed before only those interested in learning more should need to read the full chapters.</p>\n<p>I'll state my basic impression of the whole book up front: it is a very useful book for the 'soft half' of decision analysis, by which I mean framing problems, understanding objectives, and interacting with humans. For a more general and individual-focused introduction to decision analysis, I recommend <a href=\"http://www.amazon.com/Smart-Choices-Practical-Making-Decisions/dp/0767908864/ref=nosim?tag=vglnk-c319-20\">Smart Choices</a> (which Keeney was a coauthor on); VFT appears primarily targeted at facilitators and contains much focused material not in Smart Choices. I will not suggest targeted reading for particular readers, as Keeney does that well.</p>\n<p>The first two paragraphs of the preface seem worth quoting in full:</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>Many books have been written about decisionmaking. They tell us how to solve decision problems. They do not tell us how to identify potential decision opportunities. They tell us how to analyze alternatives to choose the best one. They do not tell us how to create alternatives. They tell us how to evaluate alternatives given some quantitative objective function. They do not tell us how to articulate the qualitative objectives on which any appraisal of alternatives must rest. This book is different. It does what the others do not.</p>\n<p>Almost all of the literature on decisionmaking concerns what to do <em>after</em>&nbsp;the crucial activities of identifying the decision problem, creating alternatives, and specifying objectives. But where do these decision problems, alternatives, and objectives come from? This book describes and illustrates the creative processes that you should follow to identify your decision problems, create alternatives, and articulate your objectives. These prescriptions are quite different from the way people typically pursue these activities.</p>\n</blockquote>\n<h2 id=\"1__Thinking_about_Values\">1. Thinking about Values</h2>\n<p>Most standard decision analysis follows the path outlined in <a href=\"/lw/8xr/decision_analysis_sequence/\">my previous sequence</a>. The trouble with this approach is that it's reactive, assuming that you are already faced by a decision problem with known alternatives and preferences, and gives little advice on how to determine preferences between those alternatives or how to discover new alternatives, or how to see an opportunity to act where you did not see one before and behave pro-actively.</p>\n<p>Value-Focused Thinking (VFT) puts values at the center of decision-making. Having explicit values makes it easier to rank existing alternatives, generate new alternatives, communicate and negotiate, and identify new decision opportunities to pursue.&nbsp;A decision problem is when an event appears and you must choose how to respond; a decision opportunity is when you actively decide to shift away from the status quo.</p>\n<h2 id=\"2__The_Framework_of_Value_Focused_Thinking\">2. The Framework of Value-Focused Thinking</h2>\n<p>A <em>decision frame</em> consists of two parts: the <em>decision context</em> and the <em>fundamental objectives</em>. The decision context is the set of appropriate alternatives available to the decision-maker, and the fundamental objectives make clear what consequences of the decision are important enough to drive decision-making. Good decision-making is made much easier when the two are appropriately matched: the context for a fundamental objective should include all actions the decision-maker could take to affect that objective.</p>\n<p>Objectives can generally be classified as means or ends objectives. It's important to focus on ends objectives to allow full creativity in crafting alternatives. One frame's ends objective is another frame's means objective, and so it is often natural to nest frames. Keeney gives the example of the EPA managing carbon monoxide pollution: they might want to minimize CO emissions, to minimize CO concentrations, to minimize population exposure to CO, or to minimize health effects due to CO. Each broadening of the fundamental objective should be accompanied by a broadening of the decision context, as the EPA has more tools to minimize health effects than they do to minimize CO emissions, though one of those tools is minimizing CO emissions.&nbsp;</p>\n<p>As we continue to extend the decision frame to the decision context of an individual's or an organization's entire existence, our ends objectives become strategic objectives. While decisions are rarely made at the level of the strategic decision context, having explicit and consistent strategic objectives can be a very helpful guide to determining the objective for more limited contexts, where most decisions are made.</p>\n<h2 id=\"3__Identifying_and_Structuring_Objectives\">3. Identifying and Structuring Objectives</h2>\n<p>This is a detailed and heavily technical chapter designed to teach facilitators how to identify and structure objectives.</p>\n<p>To begin, create a list of objectives. When eliciting from multiple people, ask each to identify objectives separately, to prevent anchoring on the ideas from the first presenters.</p>\n<p>There are many devices a facilitator can use to stimulate ideas. They might ask for a wish list, where they imagine how they would rank alternatives if constraints were discarded; for alternatives that are particularly good or particularly bad, and what makes them good or bad; for what problems or shortcomings the status quo has, and why these are problems; for what consequences might determine the desirability of alternatives; for the objectives that they think other stakeholders might have; or many others.</p>\n<p>Once the list seems somewhat complete, the objective should be separated into means and ends by asking \"Why is this objective important?\" If the objective is important because if promotes another objective, it is a means objective; if it is important for its own sake or because it supports a strategic objective, it is a candidate for an ends objective. Sometimes, objectives will be specifications of each other: it is important to minimize child casualties in automobile accidents because it is important to minimize human casualties in automobile accidents.</p>\n<p>This leads to two structures: a hierarchy of fundamental objectives and a networks of means-ends objectives. The objective hierarchy serves to clarify the objectives, and the objectives network serves to clarify how the alternatives might affect the objectives. Since the network is built with alternatives in mind, it should only include means under the realistic control of the decision-maker, even though there will be many other causal inputs to how well the objectives are accomplished. During this process, it is common to notice holes in the structures and discover missing objectives.</p>\n<p>Keeney lists nine values that are desirable for fundamental objectives, but I will focus on the two most critical: the fundamental objective should be <em>essential</em>, in that the alternatives under consideration impact it, and <em>controllable</em>, in that all of the tools the decision-maker has to influence the objective are within the decision context. This is a balancing act: narrower objectives, like means objectives, will be controllable but not essential, and broader objectives, like strategic objectives, will be essential but not controllable. When determining what to eat for dinner, your quality of life is essential, but not controllable- most of your tools for affecting your quality of life don't deal with the problem of planning your dinner.</p>\n<h2 id=\"4__Measuring_the_Achievement_of_Objectives\">4. Measuring the Achievement of Objectives</h2>\n<p>Keeney uses the word <em>attribute&nbsp;</em>to refer to a measure of how well an objective was achieved. There are sometimes natural attributes- like measuring annual profit in millions of dollars per year- but it may be necessary to construct an attribute or find a proxy to measure.</p>\n<p>The choice of measurement is important, as it implies value judgments about the objective. Keeney gives the example of measuring the objective \"minimize the loss of life.\" One possible measure is the number of fatalities, which implies that preventing the death of a ten year old and an eighty year old are equally valuable. Another possible measure is the number of expected years lost, which would be 66 and 6, respectively, implying that preventing the death of the child is eleven times as important as preventing the death of the elder.</p>\n<h2 id=\"5__Quantifying_Objectives_with_a_Value_Model\">5. Quantifying Objectives with a Value Model</h2>\n<p>Once attributes to measure objectives have been selected, we can apply the standard tools of decision analysis, like <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM utility functions</a>. Keeney describes several properties which may hold for joint utility functions on the set of attributes, and what the implies for the aggregation of those utility functions. In particular, for a linear combination of utility functions to be reasonable the preferences for individual attributes must be additively independent, that is, expressed in terms of their marginal probability distributions, rather than their joint probability distribution (see <a href=\"http://or.journal.informs.org/content/13/1/28.short\">Fishburn</a>).</p>\n<p>Keeney also discusses the importance of <a href=\"http://en.wikipedia.org/wiki/Dimensional_analysis\">unit conversion</a> in tradeoffs between outcomes. It is not meaningful to say that minimizing casualties is five times as important as minimizing costs, but it is meaningful to say that reducing costs by five million dollars is as desirable as reducing casualties by one. (When utilities are nonlinear, it is still important to preserve the conversion from measured units to utilities when discussing relative importance.)</p>\n<h2 id=\"6__Uncovering_Hidden_Objectives\">6. Uncovering Hidden Objectives</h2>\n<p>Measuring attributes can provide insight into values by clarifying them and making them more precise. English phrases generally support more interpretations than quantitative measurements.</p>\n<p>Many objectives are counterproductive to discuss publicly, and so they may be hidden by design, rather than by ignorance. It is still worthwhile to privately acknowledge them and make sure the objectives are clearly separated.</p>\n<p>When independence assumptions are violated, this generally means a fundamental objective has been overlooked or means objectives are being used as fundamental objectives. If it is difficult to assess the tradeoffs between fundamental objectives, that suggests that at least one of the objectives may be poorly defined or unclear.</p>\n<h2 id=\"7__Creating_Alternatives_for_a_Single_Decisionmaker\">7. Creating Alternatives for a Single Decisionmaker</h2>\n<p>Clarity in objectives aids in choosing between known alternatives, but much of the value from careful decision-making comes from the discovery of novel, superior alternatives. This chapter details methods to use the stated objectives to create alternatives.</p>\n<p>One method is to consider alternatives that maximize each objective separately, and then alternatives that maximize pairwise combinations of objectives. This will raise more of the potential solution space to attention.</p>\n<p>Specification of objectives aids in the development of alternatives. Keeney gives the example of considering adults and children separately when seeking to reduce the number of deaths and injuries in automobile accidents: different classes of alternatives will impact those classes separately, and without considering them separately alternatives only appropriate to one group might not be noticed.</p>\n<p>The means-ends objective network also stimulates the creation of alternatives. It may also be useful to consider what could be done in the current decision context to advance strategic values. Focusing on particularly good consequences, and then figuring out how to attain those consequences, may be superior to focusing on how first.</p>\n<p>Considering general solutions (i.e. properties of solutions), rather than individual feasible solutions, makes it easier to focus on parts of a large solution separately. It is also often the case that a combination of solutions will result in a superior solution, which VFT makes easier to find.</p>\n<p>In many cases, the real alternatives under consideration now are the processes which will determine future action. This is not always obvious, and acknowledging it can lead to superior solutions.</p>\n<p>At the end of the decision-making process, it is worthwhile to step back and reconsider the alternative about to be chosen. What reference class for good solutions is appropriate, now that you know what a good solution looks like? Is that the class that you have been considering?</p>\n<h2 id=\"8__Creating_Alternatives_for_Multiple_Decisionmakers\">8. Creating Alternatives for Multiple Decisionmakers</h2>\n<p>When making decisions for multiple stakeholders- either in a negotiation where all stakeholders must assent for an action to be taken or in a dictation where one stakeholder decides- the process by which alternatives are considered is likely to be important and help determine stakeholder satisfaction with the decision.</p>\n<p>Eliciting values from other stakeholders is an important step in pleasing them, and asking them for alternatives makes use of their creativity.</p>\n<p>When navigating altruistic situations, where the values of others matter strongly to you, it is often useful to have a policy of honestly providing private preferences and then explicitly aggregating them. Keeney gives the example of a couple choosing where to go to dinner: one may suggest a restaurant which they think will please the other, which the other accepts, thinking that will please the first. Both, in fact, preferred another alternative, which they missed out on because they did not communicate effectively.</p>\n<p>Negotiation can be made much more efficient with VFT; knowing your preferences and the other party's preferences allows you to find the efficient frontier and win-win tradeoffs.</p>\n<h2 id=\"9__Identifying_Decision_Opportunities\">9. Identifying Decision Opportunities</h2>\n<p>Many alternatives which you could select to advance your values that are not obvious; many times, it is not even obvious that you should be looking for them. Devoting regular time to looking for and developing your ability to create new opportunities should pay large dividends.</p>\n<p>New opportunities and new alternatives are complementary; they often take the form \"I wish that...\" or \"We should...\", and both suggest an area to direct focused attention.</p>\n<p>The creation of strategic objectives can be done at multiple granularities, from a list of one executive's objectives to a quantitative value model with input from the entire organization. (The Hansonian analog for individuals is worth considering.) What level of description is worthwhile is a decision opportunity of its own.</p>\n<p>While they should change only rarely, revisiting strategic objectives periodically- perhaps once a month- is a useful time to correct course and devote time to discovering decision opportunities.</p>\n<p>Explicit resource inventories can help discover new opportunities, alternatives, and upcoming problems which can be averted.</p>\n<p>As well, once resources have been committed to some plan, check for ways to get additional value for little additional cost. Business trips often present opportunities for additional experiences or connections at much lower cost than they would be on their own.</p>\n<p>Monitoring achievement of objectives both allows feedback of how well current plans are achieving those objectives and keeps those objectives available in your mind.</p>\n<p>Establishing a process by which others can present you with decision opportunities allows you to make the most of their creativity and expertise. Keeney gives the example of asking someone whose judgment you respect, \"What should I be achieving or doing that I am not?\" He adds that it often takes some prodding to get revealing responses, so prod.</p>\n<p>Keeney also recommends using VFT to do empathetic negotiation. Proposals designed to also appeal to the other party more than the status quo (win-win) &nbsp;will be much more warmly received than proposals designed to maximize your objectives.</p>\n<p>Many objectives cannot be satisfied through your sole action; dating is perhaps the most obvious example. Those objectives tend to be achieved by being in the right place at the right time, and so the primary way to advance those objectives is to be in the right place more often and at better times. Creating the right place and selecting the right time are both decision opportunities well worth considering.</p>\n<p>VFT is also ideally suited to situations where you have no idea what to do. By methodically seeking to determine your objectives, you can uncover enough about the situation to see clearly and act decisively.</p>\n<h2 id=\"10__Insights_for_the_Decisionmaking_Process\">10. Insights for the Decisionmaking Process</h2>\n<blockquote>\n<p>A deep and thorough understanding of the values inherent in a decision situation can provide important insights for all aspects of decision-making, and these insights make it possible to achieve much better consequences from the decisions we face.</p>\n</blockquote>\n<p>VFT contributes to decision-making in six synergistic ways:</p>\n<ol>\n<li>Guiding Information Collection: Discovering values early in the process helps inform all other information collection. If values are unclear, they should be clarified; if something is unimportant, effort should not be wasted on it.</li>\n<li>Evaluating Alternatives: With clear and consistent objectives, VFT-powered analysis of objectives is often better than unsophisticated analysis.</li>\n<li>Interconnecting Decisions: Strategic objectives, fundamental objectives, attributes, and value tradeoffs are all likely to be useful in multiple decision contexts. By adopting the same values across the board, a decision-maker may be able to make better tradeoffs and satisfy their values better or at lower cost.</li>\n<li>Improving Communication: Clear and coherent values both improve thinking and communication. Multiple decision-makers can negotiate more effectively and act in concert with explicit and defined objectives.</li>\n<li>Facilitating Involvement in Multiple-Stakeholder Decisions: VFT provides a natural way to incorporate the preferences of other stakeholders into the decision-making process, often improving both their satisfaction with the process and the quality of the solution for all involved.</li>\n<li>Guiding Strategic Thinking: The strategic objectives of a decision-making activity, which is much easier when those objectives are clearly stated and quantified.</li>\n</ol>\n<div>Developing a solid model of strategic objectives is particularly valuable because they have such broad impact and are unlikely to change, and so an investment now will repay in superior decisions for months and years with little maintenance required.</div>\n<h2 id=\"11_13__Part_Four__Applications\">11-13. Part Four- Applications</h2>\n<div>I do not summarize Part IV of the book because it consists of extended examples, which I have mostly omitted for sake of brevity. Chapter 13, detailing the application of VFT to Keeney's life, will be of interest to most readers. In particular, his lists of strategic and professional objectives are worth considering once you have attempted to build lists of your own.</div>", "sections": [{"title": "1. Thinking about Values", "anchor": "1__Thinking_about_Values", "level": 1}, {"title": "2. The Framework of Value-Focused Thinking", "anchor": "2__The_Framework_of_Value_Focused_Thinking", "level": 1}, {"title": "3. Identifying and Structuring Objectives", "anchor": "3__Identifying_and_Structuring_Objectives", "level": 1}, {"title": "4. Measuring the Achievement of Objectives", "anchor": "4__Measuring_the_Achievement_of_Objectives", "level": 1}, {"title": "5. Quantifying Objectives with a Value Model", "anchor": "5__Quantifying_Objectives_with_a_Value_Model", "level": 1}, {"title": "6. Uncovering Hidden Objectives", "anchor": "6__Uncovering_Hidden_Objectives", "level": 1}, {"title": "7. Creating Alternatives for a Single Decisionmaker", "anchor": "7__Creating_Alternatives_for_a_Single_Decisionmaker", "level": 1}, {"title": "8. Creating Alternatives for Multiple Decisionmakers", "anchor": "8__Creating_Alternatives_for_Multiple_Decisionmakers", "level": 1}, {"title": "9. Identifying Decision Opportunities", "anchor": "9__Identifying_Decision_Opportunities", "level": 1}, {"title": "10. Insights for the Decisionmaking Process", "anchor": "10__Insights_for_the_Decisionmaking_Process", "level": 1}, {"title": "11-13. Part Four- Applications", "anchor": "11_13__Part_Four__Applications", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 13}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWH8Tnh4dBkDpCPws"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-23T06:04:22.849Z", "modifiedAt": null, "url": null, "title": "Meetup : Columbus, Ohio; Self-Skepticism", "slug": "meetup-columbus-ohio-self-skepticism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.814Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tCxsvPoiJG8i5n7Hv/meetup-columbus-ohio-self-skepticism", "pageUrlRelative": "/posts/tCxsvPoiJG8i5n7Hv/meetup-columbus-ohio-self-skepticism", "linkUrl": "https://www.lesswrong.com/posts/tCxsvPoiJG8i5n7Hv/meetup-columbus-ohio-self-skepticism", "postedAtFormatted": "Wednesday, January 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Columbus%2C%20Ohio%3B%20Self-Skepticism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Columbus%2C%20Ohio%3B%20Self-Skepticism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtCxsvPoiJG8i5n7Hv%2Fmeetup-columbus-ohio-self-skepticism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Columbus%2C%20Ohio%3B%20Self-Skepticism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtCxsvPoiJG8i5n7Hv%2Fmeetup-columbus-ohio-self-skepticism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtCxsvPoiJG8i5n7Hv%2Fmeetup-columbus-ohio-self-skepticism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 120, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ic'>Columbus, Ohio; Self-Skepticism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 February 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1855 Northwest Blvd  Columbus, OH 43212</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>First Monday of the month, so we are getting together at the Chocolate Cafe in Upper Arlington area. Starting at 7p and going til we're done. Topic will be Self-skepticism.</p>\n\n<p>Recommended viewing: <a href=\"http://youtu.be/wW_oNxax5RQ\" rel=\"nofollow\">http://youtu.be/wW_oNxax5RQ</a></p>\n\n<p>We don't post most of our meetups on here, so if you're in Ohio and are interested in getting-together with people, but just can't make it to one of the posted meetups, you should probably join the OHLW email list (PM me your email addy, and I'll put you on there!). Groups are currently active in Cincinnati, Columbus, and Cleveland.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ic'>Columbus, Ohio; Self-Skepticism</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tCxsvPoiJG8i5n7Hv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "21300", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Columbus__Ohio__Self_Skepticism\">Discussion article for the meetup : <a href=\"/meetups/ic\">Columbus, Ohio; Self-Skepticism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 February 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1855 Northwest Blvd  Columbus, OH 43212</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>First Monday of the month, so we are getting together at the Chocolate Cafe in Upper Arlington area. Starting at 7p and going til we're done. Topic will be Self-skepticism.</p>\n\n<p>Recommended viewing: <a href=\"http://youtu.be/wW_oNxax5RQ\" rel=\"nofollow\">http://youtu.be/wW_oNxax5RQ</a></p>\n\n<p>We don't post most of our meetups on here, so if you're in Ohio and are interested in getting-together with people, but just can't make it to one of the posted meetups, you should probably join the OHLW email list (PM me your email addy, and I'll put you on there!). Groups are currently active in Cincinnati, Columbus, and Cleveland.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Columbus__Ohio__Self_Skepticism1\">Discussion article for the meetup : <a href=\"/meetups/ic\">Columbus, Ohio; Self-Skepticism</a></h2>", "sections": [{"title": "Discussion article for the meetup : Columbus, Ohio; Self-Skepticism", "anchor": "Discussion_article_for_the_meetup___Columbus__Ohio__Self_Skepticism", "level": 1}, {"title": "Discussion article for the meetup : Columbus, Ohio; Self-Skepticism", "anchor": "Discussion_article_for_the_meetup___Columbus__Ohio__Self_Skepticism1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-23T22:41:33.964Z", "modifiedAt": null, "url": null, "title": "LW anchoring experiment: maybe", "slug": "lw-anchoring-experiment-maybe", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.722Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DfvX99AKx7pR7NE3v/lw-anchoring-experiment-maybe", "pageUrlRelative": "/posts/DfvX99AKx7pR7NE3v/lw-anchoring-experiment-maybe", "linkUrl": "https://www.lesswrong.com/posts/DfvX99AKx7pR7NE3v/lw-anchoring-experiment-maybe", "postedAtFormatted": "Wednesday, January 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20anchoring%20experiment%3A%20maybe&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20anchoring%20experiment%3A%20maybe%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDfvX99AKx7pR7NE3v%2Flw-anchoring-experiment-maybe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20anchoring%20experiment%3A%20maybe%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDfvX99AKx7pR7NE3v%2Flw-anchoring-experiment-maybe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDfvX99AKx7pR7NE3v%2Flw-anchoring-experiment-maybe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<blockquote>\n<p>I do an informal experiment testing whether LessWrong karma scores are susceptible to a form of anchoring based on the first comment posted; a medium-large effect size is found although the data does not fit the assumed normal distribution &amp; the more sophisticated analysis is equivocal, so there may or may not be an anchoring effect.</p>\n</blockquote>\n<p>Full writeup on <a href=\"http://www.gwern.net/\">gwern.net</a> at <a title=\" LW anchoring experiment\" href=\"http://www.gwern.net/Anchoring\">http://www.gwern.net/Anchoring</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb128": 2, "RxuepsZgBEKax9bmP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DfvX99AKx7pR7NE3v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 1.0927762912637023e-06, "legacy": true, "legacyId": "21305", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-24T00:02:50.232Z", "modifiedAt": null, "url": null, "title": "Right for the Wrong Reasons", "slug": "right-for-the-wrong-reasons", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:08.784Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7c7crvbG62KL8kAuW/right-for-the-wrong-reasons", "pageUrlRelative": "/posts/7c7crvbG62KL8kAuW/right-for-the-wrong-reasons", "linkUrl": "https://www.lesswrong.com/posts/7c7crvbG62KL8kAuW/right-for-the-wrong-reasons", "postedAtFormatted": "Thursday, January 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Right%20for%20the%20Wrong%20Reasons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARight%20for%20the%20Wrong%20Reasons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7c7crvbG62KL8kAuW%2Fright-for-the-wrong-reasons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Right%20for%20the%20Wrong%20Reasons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7c7crvbG62KL8kAuW%2Fright-for-the-wrong-reasons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7c7crvbG62KL8kAuW%2Fright-for-the-wrong-reasons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 746, "htmlBody": "<p>One of the few things that I really appreciate having encountered during my study of philosophy is the Gettier problem.&nbsp;Paper after paper has been published on this subject, starting with Gettier's original <a href=\"http://philosophyfaculty.ucsd.edu/faculty/rarneson/Courses/gettierphilreading.pdf\">\"Is Justified True Belief Knowledge?\"</a>&nbsp;In brief, Gettier argues that knowledge cannot be defined as \"justified true belief\" because there are cases when people have a justified true belief, but their belief is justified for the wrong reasons.</p>\n<p>For instance, Gettier cites the example of two men, Smith and Jones, who are applying for a job. Smith believes that Jones will get the job, because the president of the company told him that Jones would be hired. He also believes that Jones has ten coins in his pocket, because he counted the coins in Jones's pocket ten minutes ago (Gettier does not explain this behavior).<sup>&nbsp;</sup>Thus, he forms the belief \"the person who will get the job has ten coins in his pocket.\"</p>\n<p>Unbeknownst to Smith, though, he himself will get the job, and further he himself has ten coins in his pocket that he was not aware of-- perhaps he put someone else's jacket on by mistake. As a result, Smith's belief that \"the person who will get the job has ten coins in his pocket\" was correct, but only by luck.</p>\n<p>While I don't find the primary purpose of Gettier's argument&nbsp;<a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">particularly interesting or meaningful</a> (much less the debate it spawned), I do think Gettier's paper does a very good job of illustrating the situation that I refer to as \"being right for the wrong reasons.\" This situation has important implications for prediction-making and hence for the art of rationality as a whole.</p>\n<p>Simply put, a prediction that is right for the wrong reasons <em>isn't actually right </em>from an <a href=\"http://wiki.lesswrong.com/wiki/Epistemic_rationality#Epistemic_rationality\">epistemic perspective</a>.</p>\n<p>If I predict, for instance, that I will win a 15-touch fencing bout, implicitly believing this will occur when I strike my opponent 15 times before he strikes me 15 times, and I in fact lose fourteen touches in a row, only to win by forfeit when my opponent intentionally strikes me many times in the final touch and is disqualified for brutality, my prediction cannot be said to have been accurate.</p>\n<p>Where this gets more complicated is with predictions that are right for the wrong reasons, but the right reasons still apply. Imagine the previous example of a fencing bout, except this time I score 14 touches in a row and then win by forfeit when my opponent flings his mask across the hall in frustration and is disqualified for an offense against sportsmanship. Technically, my prediction is again right for the wrong reasons-- my victory was not thanks to scoring 15 touches, but thanks to my opponent's poor sportsmanship and subsequent disqualification. However, I likely would have scored 15 touches given the opportunity.</p>\n<p>In cases like this, it may seem appealing to credit my prediction as successful, as it would be successful under normal conditions. However, I think we perhaps have to resist this impulse and instead simply work on making more precise predictions. If we start crediting predictions that are right for the wrong reasons, even if it seems like the \"spirit\" of the prediction is right, this seems to open the door for <a href=\"/lw/foz/philosophy_by_humans_3_intuitions_arent_shared/\">relying on intuition</a>&nbsp;and falling into the traps that <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">contaminate much of modern philosophy</a>.</p>\n<p>What we really need to do in such cases seems to be to break down our claims into <a href=\"/lw/bc3/sotw_be_specific/\">more specific</a>&nbsp;predictions, splitting them into multiple sub-predictions if necessary. My prediction about the outcome of the fencing bout could better be expressed as multiple predictions, for instance \"I will score more points than my opponent\"&nbsp;and \"I will win the bout.\" Some may notice that this is similar to the implicit justification being made in the original prediction. This is fitting-- drawing out such implicit details is key to making accurate predictions. In fact, this example itself was improved by&nbsp;<a href=\"/lw/nu/taboo_your_words/\">tabooing</a><sup>[1]</sup>&nbsp;\"better\" in the vague initial sentence \"I will fence better than my opponent.\"</p>\n<p>In order to <a href=\"/r/lesswrong/lw/7o7/calibrate_your_selfassessments/\">make better predictions</a>, we must cast out those predictions that are right for the wrong reasons. While it may be tempting to award such efforts partial credit, this flies against <a href=\"http://yudkowsky.net/rational/the-simple-truth\">the spirit of the truth.</a>&nbsp;The true skill of&nbsp;<a href=\"/lw/82s/dont_call_yourself_a_rationalist/51io\">cartography</a> requires forming both accurate and <em><a href=\"/lw/la/truly_part_of_you/\">reproducible</a></em>&nbsp;maps; lucking into accuracy may be nice, but it speaks ill of the reproducibility of your methods.</p>\n<p>&nbsp;</p>\n<p>[1] I greatly suggest that you make tabooing a&nbsp;<a href=\"/lw/5kz/the_5second_level/\">five-second skill</a>, and better still recognizing when you need to apply it to your own processes.&nbsp;It pays great dividends in terms of precise thought.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7c7crvbG62KL8kAuW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 24, "extendedScore": null, "score": 1.0928262408382658e-06, "legacy": true, "legacyId": "21275", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wHjpCxeDeuFadG3jF", "jaN4EKrRnZdynTJjH", "FwiPfF8Woe5JrzqEu", "NgtYDP3ZtLJaM248W", "WBdvyyHLdxZSAMmoz", "aPrCzeFfbBmRsvzby", "fg9fXrHpeaDD6pEPL", "JcpzFpPBSmzuksmWM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-24T00:59:49.638Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Emotional Involvement", "slug": "seq-rerun-emotional-involvement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.275Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LrWTcuTyWH76DY5vh/seq-rerun-emotional-involvement", "pageUrlRelative": "/posts/LrWTcuTyWH76DY5vh/seq-rerun-emotional-involvement", "linkUrl": "https://www.lesswrong.com/posts/LrWTcuTyWH76DY5vh/seq-rerun-emotional-involvement", "postedAtFormatted": "Thursday, January 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Emotional%20Involvement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Emotional%20Involvement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrWTcuTyWH76DY5vh%2Fseq-rerun-emotional-involvement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Emotional%20Involvement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrWTcuTyWH76DY5vh%2Fseq-rerun-emotional-involvement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrWTcuTyWH76DY5vh%2Fseq-rerun-emotional-involvement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>Today's post, <a href=\"/lw/xg/emotional_involvement/\">Emotional Involvement</a> was originally published on 06 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Since the events in video games have no actual long-term consequences, playing a video game is not likely to be nearly as <em>emotionally involving</em> as much less dramatic events in real life. The supposed Utopia of playing lots of cool video games forever, is life as a series of disconnected episodes with no lasting consequences. Our current emotions are bound to activities that were subgoals of reproduction in the ancestral environment - but we now pursue these activities as independent goals regardless of whether they lead to reproduction.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/gf6/seq_rerun_changing_emotions/\">Changing Emotions</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LrWTcuTyWH76DY5vh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0928612695896556e-06, "legacy": true, "legacyId": "21306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZmDEbiEeXk3Wv2sLH", "hdHfjx8ND8GsLFALi", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-24T03:09:42.772Z", "modifiedAt": null, "url": null, "title": "Notes on Autonomous Cars", "slug": "notes-on-autonomous-cars", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:07.715Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8ZrQkBDptXhumhP3S/notes-on-autonomous-cars", "pageUrlRelative": "/posts/8ZrQkBDptXhumhP3S/notes-on-autonomous-cars", "linkUrl": "https://www.lesswrong.com/posts/8ZrQkBDptXhumhP3S/notes-on-autonomous-cars", "postedAtFormatted": "Thursday, January 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Notes%20on%20Autonomous%20Cars&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANotes%20on%20Autonomous%20Cars%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ZrQkBDptXhumhP3S%2Fnotes-on-autonomous-cars%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Notes%20on%20Autonomous%20Cars%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ZrQkBDptXhumhP3S%2Fnotes-on-autonomous-cars", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ZrQkBDptXhumhP3S%2Fnotes-on-autonomous-cars", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16489, "htmlBody": "<blockquote>\n<p id=\"self-driving-cars\">Excerpts from literature on robotic/self-driving/autonomous cars with a focus on legal issues, lengthy, often tedious; some more SI work. See also <a href=\"/lw/fzy/notes_on_psychopathy/\">Notes on Psychopathy</a>.</p>\n</blockquote>\n<p>Having read through all this material, my general feeling is: the near-term future (1 decade) for autonomous cars is not that great. What's been accomplished, legally speaking, is great but more limited than most people appreciate. And there are many serious problems with penetrating the elaborate ingrown rent-seeking tangle of law &amp; politics &amp; insurance. I expect the mid-future (+2 decades) to look more like autonomous cars completely taking over many odd niches and applications where the user can afford to ignore those issues (eg. on private land or in warehouses or factories), with highways and regular roads continuing to see many human drivers with some level of automated assistance. However, none of these problems seem fatal and all of them seem amenable to gradual accommodation and pressure, so I <em>am</em> now more confident that in the long run we will see autonomous cars become the norm and human driving ever more niche (and possibly lower-class). On none of these am I sure how to formulate a precise prediction, though, since I expect lots of boundary-crossing and tertium quids. We'll see.</p>\n<p><a id=\"more\"></a></p>\n<h2><a href=\"#TOC\"><span class=\"header-section-number\">0.1</span> Self-driving cars</a></h2>\n<p>The first success inaugurating the modern era can be considered the <a href=\"https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_%282005%29\">2005 DARPA Grand Challenge</a> where multiple vehicles completed the course. The first <a href=\"http://en.wikipedia.org/wiki/Autonomous_cars#Legislation\">legislation</a> of any kind addressing autonomous cars was Nevada&rsquo;s 2011 approval. <a href=\"http://cyberlaw.stanford.edu/wiki/index.php/Automated_Driving:_Legislative_and_Regulatory_Action\">5 states have passed legislation</a> dealing with autonomous cars.</p>\n<p>However, these laws are highly preliminary and all the analyses I can find agree that they punt on the real legal issues of liability; they permit relatively little.</p>\n<h3 id=\"lobbying-liability-and-insurance\"><a href=\"#TOC\"><span class=\"header-section-number\">0.1.1</span> Lobbying, Liability, and Insurance</a></h3>\n<p>(Warning: legal analysis quoted at length in some excerpts.)</p>\n<p><a href=\"http://bngumassd.org/neatstuff/selfdrive%20cars.pdf\">&ldquo;Toward Robotic Cars&rdquo;</a>, Thrun 2010 (pre-Google):</p>\n<blockquote>\n<p>Junior&rsquo;s behavior is governed by a finite state machine, which provides for the possibility that common traffic rules may leave a robot without a legal option as how to proceed. When that happens, the robot will eventually invoke its general-purpose path planner to find a solution, regardless of traffic rules. [Raising serious issues of liability related to potentially making people worse-off]</p>\n</blockquote>\n<p><a href=\"http://www.nytimes.com/2010/10/10/science/10google.html?pagewanted=all\">&ldquo;Google Cars Drive Themselves, in Traffic&rdquo;</a> (<a href=\"http://bngumassd.org/neatstuff/selfdrive%20cars.pdf\">PDF</a>), <em>NYT</em> 2010:</p>\n<blockquote>\n<p>But the advent of autonomous vehicles poses thorny legal issues, the Google researchers acknowledged. Under current law, a human must be in control of a car at all times, but what does that mean if the human is not really paying attention as the car crosses through, say, a school zone, figuring that the robot is driving more safely than he would? And in the event of an accident, who would be liable - the person behind the wheel or the maker of the software?</p>\n<p>&ldquo;The technology is ahead of the law in many areas,&rdquo; said Bernard Lu, senior staff counsel for the California Department of Motor Vehicles. &ldquo;If you look at the vehicle code, there are dozens of laws pertaining to the driver of a vehicle, and they all presume to have a human being operating the vehicle.&rdquo; The Google researchers said they had carefully examined California&rsquo;s motor vehicle regulations and determined that because a human driver can override any error, the experimental cars are legal. Mr.&nbsp;Lu agreed.</p>\n</blockquote>\n<p><a href=\"https://www.npr.org/blogs/alltechconsidered/2012/10/03/162187419/calif-green-lights-self-driving-cars-but-legal-kinks-linger\">&ldquo;Calif. Greenlights Self-Driving Cars, But Legal Kinks Linger&rdquo;</a>:</p>\n<blockquote>\n<p>For instance, if a self-driving car runs a red light and gets caught, who gets the ticket? &ldquo;I don&rsquo;t know - whoever owns the car, I would think. But we will work that out,&rdquo; Gov.&nbsp;Brown said at the signing event for California&rsquo;s bill to legalize and regulate the robotic cars. &ldquo;That will be the easiest thing to work out.&rdquo; Google co-founder Sergey Brin, who was also at the ceremony, jokingly said &ldquo;self-driving cars don&rsquo;t run red lights.&rdquo; That may be true, but Bryant Walker Smith, who teaches a class at Stanford Law School this fall on the law supporting self-driving cars, says eventually one of these vehicles will get into an accident. When it does, he says, it&rsquo;s not clear who will pay.</p>\n<p>&hellip;Or is it the company that wrote the software? Or the automaker that built the car? When it came to assigning responsibility, California decided that a self-driving car would always have a human operator. Even if that operator wasn&rsquo;t actually in the car, that person would be legally responsible. It sounds straightforward, but it&rsquo;s not. Let&rsquo;s say the operator of a self-driving car is inebriated; he or she is still legally the operator, but the car is driving itself. &ldquo;That was a decision that department made - that the operator would be subject to the laws, including laws against driving while intoxicated, even if the operator wasn&rsquo;t there,&rdquo; Walker Smith says&hellip;Still, issues surrounding liability and who is ultimately responsible when robots take the wheel are likely to remain contentious. Already trial lawyers, insurers, automakers and software engineers are queuing up to lobby rule-makers in California&rsquo;s capital.</p>\n</blockquote>\n<p><a href=\"http://online.wsj.com/article/SB10000872396390443493304578034822744854696.html?mod=WSJ_hppMIDDLENexttoWhatsNewsSecond\">&ldquo;Google&rsquo;s Driverless Car Draws Political Power: Internet Giant Hones Its Lobbying Skills in State Capitols; Giving Test Drives to Lawmakers&rdquo;</a>, <em>WSJ</em>, 12 October 2012:</p>\n<blockquote>\n<p>Overall, Google spent nearly $9 million in the first half of 2012 lobbying in Washington for a wide variety of issues, including speaking to U.S. Department of Transportation officials and lawmakers about autonomous vehicle technology, according to federal records, nearing the $9.68 million it spent on lobbying in all of 2011. It is unclear how much Google has spent in total on lobbying state officials; the company doesn&rsquo;t disclose such data.</p>\n<p>&hellip;In most states, autonomous vehicles are neither prohibited nor permitted-a key reason why Google&rsquo;s fleet of autonomous cars secretly drove more than 100,000 miles on the road before the company announced the initiative in fall 2010. Last month, Mr.&nbsp;Brin said he expects self-driving cars to be publicly available within five years.</p>\n<p>In January 2011, Mr.&nbsp;Goldwater approached Ms.&nbsp;Dondero Loop and the Nevada assembly transportation committee about proposing a bill to direct the state&rsquo;s department of motor vehicles to draft regulations around the self-driving vehicles. &ldquo;We&rsquo;re not saying, &lsquo;Put this on the road,&rsquo;&rdquo; he said he told the lawmakers. &ldquo;We&rsquo;re saying, &lsquo;This is legitimate technology,&rsquo; and we&rsquo;re letting the DMV test it and certify it.&rdquo; Following the Nevada bill&rsquo;s passage, legislators from other states began showing interest in similar legislation. So Google repeated its original recipe and added an extra ingredient: giving lawmakers the chance to ride in one of its about a dozen self-driving cars&hellip;In California, an autonomous-vehicle bill became law last month despite opposition from the Alliance of Automobile Manufacturers, which includes 12 top auto makers such as GM, BMW and Toyota. The group had approved of the Florida bill. Dan Gage, a spokesman for the group, said the California legislation would allow companies and individuals to modify existing vehicles with self-driving technology that could be faulty, and that auto makers wouldn&rsquo;t be legally protected from resulting lawsuits. &ldquo;They&rsquo;re not all Google, and they could convert our vehicles in a manner not intended,&rdquo; Mr.&nbsp;Gage said. But Google helped push the bill through after spending about $140,000 over the past year to lobby legislators and California agencies, according to public records</p>\n</blockquote>\n<blockquote>\n<p>As with California&rsquo;s recently enacted law, Cheh&rsquo;s [Washington D.C.] bill requires that a licensed driver be present in the driver&rsquo;s seat of these vehicles. While seemingly inconsequential, this effectively outlaws one of the more promising functions of autonomous vehicle technology: allowing disabled people to enjoy the personal mobility that most people take for granted. Google highlighted this benefit when one of its driverless cars drove a legally blind man to a Taco Bell. Bizarrely, Cheh&rsquo;s bill also requires that autonomous vehicles operate only on alternative fuels. While the Google Self-Driving Car may manifest itself as an eco-conscious Prius, self-driving vehicle technology has nothing to do with hybrids, plug-in electrics or vehicles fueled with natural gas. The technology does not depend on vehicle make or model, but Cheh is seeking to mandate as much. That could delay the technology&rsquo;s widespread adoption for no good reason&hellip;Another flaw in Cheh&rsquo;s bill is that it would impose a special tax on drivers of autonomous vehicles. Instead of paying fuel taxes, &ldquo;Owners of autonomous vehicles shall pay a vehicle-miles travelled (VMT) fee of 1.875 cents per mile.&rdquo; Administrative details aside, a VMT tax would require drivers to install a recording device to be periodically audited by the government. There may be good reasons to replace fuel taxes with VMT fees, but greatly restricting the use of a potentially revolutionary new technology by singling it out for a new tax system would be a mistake.</p>\n</blockquote>\n<p><a href=\"http://www.washingtonpost.com/opinions/driverless-cars-are-on-the-way-heres-how-not-to-regulate-them/2012/11/02/a5337880-21f1-11e2-ac85-e669876c6a24_story.html\">&ldquo;Driverless cars are on the way. Here&rsquo;s how not to regulate them.&rdquo;</a></p>\n<p><a href=\"http://ojs.stanford.edu/ojs/index.php/intersect/article/download/361/361\">&ldquo;How autonomous vehicle policy in California and Nevada addresses technological and non-technological liabilities&rdquo;</a>, Pinto 2012:</p>\n<blockquote>\n<p>The State of Nevada has adopted one policy approach to dealing with these technical and policy issues. At the urging of Google, a new Nevada law directs the Nevada Department of Motor Vehicles (NDMV) to issue regulations for the testing and possible licensing of autonomous vehicles and for licensing the owners/drivers of these vehicles. There is also a similar law being proposed in California with details not covered by <a href=\"http://www.dmvnv.com/pdfforms/obl326.pdf\">Nevada AB 511</a>. This paper evaluates the strengths and weaknesses of the Nevada and California approaches</p>\n<p>Another problem posed by the non-computer world is that human drivers frequently bend the rules by rolling through stop signs and driving above speed limits. How does a polite and law-abiding robot vehicle act in these situations? To solve this problem, the Google Car can be programmed for different driving personalities, mirroring the current conditions. On one end, it would be cautious, being more likely to yield to another car and strictly following the laws on the road. At the other end of the spectrum, the robocar would be aggressive, where it is more likely to go first at the stop sign. When going through a four-way intersection, for example, it yields to other vehicles based on road rules; but if other cars don&rsquo;t reciprocate, it advances a bit to show to the other drivers its intention.</p>\n<p>However, there is a time period between a problem being diagnosed and the car being fixed. In theory, one would disable the vehicle remotely and only start it back up when the problem is fixed. However in reality, this would be extremely disruptive to a person&rsquo;s life as they would have to tow their vehicle to the nearest mechanic or autonomous vehicle equivalent to solve the issue. Google has not developed the technology to approach this problem, instead relying on the human driver to take control of the vehicle if there is ever a problem in their test vehicles.</p>\n<p>[previous Lu quote about human-centric laws] &hellip;this can create particularly tricky situations such as deciding whether the police should have the right to pull over autonomous vehicles, a question yet to be answered. Even the chief counsel of the National Highway Traffic Safety Administration admits that the federal government does not have enough information to determine how to regulate driverless technologies. This can become a particularly thorny issue when there is the first accident between autonomous and self driving vehicles and how to go about assigning liability.</p>\n<p>This question of liability arose during an [unpublished 11 Feb 2012] interview on the future of autonomous vehicles with Roger Noll. Although Professor Noll hasn&rsquo;t read the current literature on this issue, he voiced concern over what the verdict of the first trial between an accident between an autonomous vehicle and normal car will be. He believes that the jury will almost certainly side with the human driver despite the details of the case, as he eloquently put in his husky Utah accent and subsequent laughter, &ldquo;how are we going to defend the autonomous vehicle; can we ask it to testify for itself?&rdquo; To answer Roger Noll&rsquo;s question, Brad Templeton&rsquo;s blog elaborates how he believes that liability reasons are a largely unimportant question for two reasons. First, in new technology, there is no question that any lawsuit over any incident involving the cars will include the vendor as the defendant so potential vendors must plan for liability. For the second reason, Brad Templeton makes an economic argument that the cost of accidents is borne by car buyers through higher insurance premiums. If the accidents are deemed the fault of the vehicle maker, this cost goes into the price of the car, and is paid for by the vehicle maker&rsquo;s insurance or self- insurance. Instead, Brad Templeton believes that the big question is whether the liability assigned in any lawsuit will be significantly greater than it is in ordinary collisions because of punitive damages. In theory, robocars should drive the costs down because of the reductions in collisions, and that means savings for the car buyer and for society and thus cheaper auto insurance. However, if the cost per collision is much higher even though the number of collisions drops, there is uncertainty over whether autonomous vehicles will save money for both parties.</p>\n<p>California&rsquo;s Proposition 103 dictates that any insurance policy&rsquo;s price must be based on weighted factors, and the top 3 weighted factors must be, 1. driving record, 2. number of miles driven and 3. number of years of experience. Other factors like the type of car someone has (i.e.&nbsp;autonomous vehicle) will be weighed lower. Subsequently, this law makes it very hard to get cheap insurance for a robocar.</p>\n<p>Nevada Policy: AB 511 Section 8 This short piece of legislation accomplishes the goal of setting good standards for the DMV to follow. By setting general standards (part a), insurance requirements (part b), and safety standards (part c), this sets a precedent for these areas without being too limited with details, leaving them to be decided by the DMV instead of the politicians. &hellip;part b only discusses insurance briefly, saying the state must, &ldquo;Set forth requirements for the insurance that is required to test or operate an autonomous vehicle on a highway within this State.&rdquo; The definitions set in the second part of Section 8 are not specific enough. Following the open-ended standards set in the earlier part of the Section 8 is good for continuity, but not technically addressing the problem. According to Ryan Calo, Director of Privacy and Robotics for Stanford Law School&rsquo;s Center for Internet and Society (CIS), the bill&rsquo;s definition of &ldquo;autonomous vehicles&rdquo; is unclear and circular. In the context of this legislation, autonomous driving is seen as a binary system of existence, but in reality, it falls more under a spectrum.</p>\n<p>Overall, AB 511 did not address either the technological liabilities and barely mentioned the non-technological liabilities that are necessary to overcome for future success of autonomous vehicles. Since it was the first type of legislation to ever approach the issue of autonomous vehicles, it is understandable that the policymakers did not want to go into specifics and instead rely on future regulation to determine the details.</p>\n<p>California Policy: SB 1298&hellip;would require the adoption of safety standards and performance requirements to ensure the safe operation and testing of &ldquo;autonomous vehicles&rdquo; on California public roads. The bill would allow autonomous vehicles to be operated or tested on the public roads on the condition they meet safety standards and performance requirements of the bill. SB 1298&rsquo;s 66 lines of text is also considerably longer than AB 511&rsquo;s 12 lines of relevant text (the entirety of AB 511 is much longer but consists of irrelevant information for the purposes of autonomous cars). would require the adoption of safety standards and performance requirements to ensure the safe operation and testing of &ldquo;autonomous vehicles&rdquo; on California public roads. The bill would allow autonomous vehicles to be operated or tested on the public roads on the condition they meet safety standards and performance requirements of the bill. SB 1298&rsquo;s 66 lines of text is also considerably longer than AB 511&rsquo;s 12 lines of relevant text (the entirety of AB 511 is much longer but consists of irrelevant information for the purposes of autonomous cars).</p>\n<p>SB 1298 has clear intentions to have company developed vehicles by saying in Section 2, Part B that, &ldquo;autonomous vehicles have been operated safely on public roads in the state in recent years by companies developing and testing this technology&rdquo; and how these companies have set the standard for what safety standards will be necessary for future testing by others. This part of the legislation implicitly supports Google&rsquo;s autonomous vehicle because it has the most extensively tested fleet of vehicles out of all the companies, and all this testing has been nearly exclusively done in California. This bill is an improvement over AB 511 by putting more control in the hands of Google to focus on developing the technology, which is a signal by the policymakers to create a climate favorable for Google&rsquo;s innovation within the constraints of keeping society safe.</p>\n<p>To avoid setting a dangerous precedent for liability in accidents, policymakers can consider protecting the car companies from frivolous and malicious lawsuits. Without such legislation, future plaintiffs will be justified to sue Google and put full liability on them. There are also potential free riding effects of the economic moral hazard of putting the blame on the company that makes the technology, not the company that manufactures the vehicle. Since we are assuming that autonomous vehicle technology will all come from one source of Google, then any accident that occurs will pin the blame primarily on Google, the common denominator, not as much as on the car manufacturer&hellip;Policy that ensures the costs per accident remains close to today&rsquo;s current cost will save money for both the insurer and customer. This could potentially mean putting a cap on rewards towards the recipients or punishments towards the company to limit shocks to the industry. Overall, a policymaker can choose to create a gradual limit on the amount of liability placed on the vendor based on certain technology or scaling issues that are met without accidents.</p>\n<p>SB 1298 manages to cover some of the shortcomings of AB 511, such as how to improve upon the definition of an autonomous vehicle, as well as looking more towards the future by giving Google more responsibility and alleviating some of the non-technical liability by considering their product &ldquo;under development&rdquo;. However, both pieces of legislation fail to address the specific technical liabilities such as bugs in the code base or computer attacks, and non-technical liabilities such as insurance or accident liability.</p>\n</blockquote>\n<p><a href=\"http://www.nytimes.com/2011/05/29/business/economy/29view.html\">&ldquo;Can I See Your License, Registration and C.P.U.?&rdquo;</a>, Tyler Cowen; see also his <a href=\"http://marginalrevolution.com/marginalrevolution/2011/06/what-do-the-laws-against-driverless-cars-look-like.html\">&ldquo;What do the laws against driverless cars look like?&rdquo;</a>:</p>\n<blockquote>\n<p>The driverless car is illegal in all 50 states. Google, which has been at the forefront of this particular technology, is asking the Nevada legislature to relax restrictions on the cars so it can test some of them on roads there. Unfortunately, the very necessity for this lobbying is a sign of our ambivalence toward change. Ideally, politicians should be calling for accelerated safety trials and promising to pass liability caps if the cars meet acceptable standards, whether that be sooner or later. Yet no major public figure has taken up this cause.</p>\n<p>Enabling the development of driverless cars will require squadrons of lawyers because a variety of state, local and federal laws presume that a human being is operating the automobiles on our roads. No state has anything close to a functioning system to inspect whether the computers in driverless cars are in good working order, much as we routinely test emissions and brake lights. Ordinary laws change only if legislators make those revisions a priority. Yet the mundane political issues of the day often appear quite pressing, not to mention politically safer than enabling a new product that is likely to engender controversy.</p>\n<p>Politics, of course, is often geared toward preserving the status quo, which is highly visible, familiar in its risks, and lucrative for companies already making a profit from it. Some parts of government do foster innovation, such as Darpa, the Defense Advanced Research Projects Agency, which is part of the Defense Department. Darpa helped create the Internet and is supporting the development of the driverless car. It operates largely outside the public eye; the real problems come when its innovations start to enter everyday life and meet political resistance and disturbing press reports.</p>\n<p>&hellip;In the meantime, transportation is one area where progress has been slow for decades. We&rsquo;re still flying 747s, a plane designed in the 1960s. Many rail and bus networks have contracted. And traffic congestion is worse than ever. As I&rsquo; argued in a previous column, this is probably part of a broader slowdown of technological advances.</p>\n<p>But it&rsquo;s clear that in the early part of the 20th century, the original advent of the motor car was not impeded by anything like the current m&eacute;lange of regulations, laws and lawsuits. Potentially major innovations need a path forward, through the current thicket of restrictions. That debate on this issue is so quiet shows the urgency of doing something now.</p>\n</blockquote>\n<p><a title=\"On The Legality Of Driverless Vehicles: A Response To Tyler Cowen\" href=\"https://cyberlaw.stanford.edu/blog/2011/06/legality-driverless-vehicles-response-tyler-cowen\">Ryan Calo</a> of the <em>CIS</em> argues essentially that no specific law <em>bans</em> autonomous cars and the threat of the human-centric laws &amp; regulations is overblown. (See the later Russian incident.)</p>\n<p><a href=\"http://ideas.4brad.com/scu-conference-legal-issues-robocars\">&ldquo;SCU conference on legal issues of robocars&rdquo;</a>, Brad Templeton:</p>\n<blockquote>\n<p>Liability: After a technology introduction where Sven Bieker of Stanford outlined the challenges he saw which put fully autonomous robocars 2 decades away, the first session was on civil liability. The short message was that based on a number of related cases from the past, it will be hard for manufacturers to avoid liability for any safety problems with their robocars, even when the systems were built to provide the highest statistical safety result if it traded off one type of safety for another. In general when robocars come up as a subject of discussion in web threads, I frequently see &ldquo;Who will be liable in a crash&rdquo; as the first question. I think it&rsquo;s a largely unimportant question for two reasons. First of all, when the technology is new, there is no question that any lawsuit over any incident involving the cars will include the vendor as the defendant, in many cases with justifiable reasons, but even if there is no easily seen reason why. So potential vendors can&rsquo;t expect to not plan for liability. But most of all, the reality is that in the end, the cost of accidents is borne by car buyers. Normally, they do it by buying insurance. But if the accidents are deemed the fault of the vehicle maker, this cost goes into the price of the car, and is paid for by the vehicle maker&rsquo;s insurance or self-insurance. It&rsquo;s just a question of figuring out how the vehicle buyer will pay, and the market should be capable of that (though see below.) No, the big question in my mind is whether the liability assigned in any lawsuit will be significantly greater than it is in ordinary collisions where human error is at fault, because of punitive damages&hellip;Unfortunately, some liability history points to the latter scenario, though it is possible for statutes to modify this.</p>\n<p>Insurance: &hellip;Because Prop 103 [specifying insurance by weighted factors, see previous] is a ballot proposition, it can&rsquo;t easily be superseded by the legislature. It takes a 2/3 vote and a court agreeing the change matches the intent of the original ballot proposition. One would hope the courts would agree that cheaper insurance to encourage safer cars would match the voter intent, but this is a challenge.</p>\n<p>Local and criminal laws: The session on criminal laws centered more on the traffic code (which isn&rsquo;t really criminal law) and the fact it varies a lot from state to state. Indeed, any robocar that wants to operate in multiple states will have to deal with this, though fortunately there is a federal standard on traffic controls (signs and lights) to rely on. Some global standards are a concern - the Geneva convention on traffic laws requires every car has a driver who is in control of the vehicle. However, I think that governments will be able to quickly see - if they want to - that these are laws in need of updating. Some precedent in drunk driving can create problems - people have been convicted of DUI for being in their car, drunk, with the keys in their pocket, because they had clear intent to drive drunk. However, one would hope the possession of a robocar (of the sort that does not need human manual driving) would express an entirely different intent to the law.</p>\n</blockquote>\n<p><a href=\"http://ec.europa.eu/information_society/activities/esafety/doc/studies/automated/reportfinal.pdf\">&ldquo;Definition of necessary vehicle and infrastructure systems for Automated Driving&rdquo;</a>, European Commission report 29 June 2011:</p>\n<blockquote>\n<p>Yet another paramount aspect tightly related to automated driving at present and in the near future, and certainly related to autonomous driving in the long run, is the interpretation of the Vienna Convention. It will be shown in the report how this European legislation is commonly interpreted, how it creates the framework necessary to deploy on a large scale automated and cooperative driving systems, and what legal limitations are foreseen in making the new step toward autonomous driving. The report analyses in the same context other conventions and legislative acts, searches for gaps in the current legislation and makes an interesting link with the aviation industry where several lessons can be learnt from.</p>\n<p>It seems appropriate to end this summary with a few remarks not directly related to the subject of this report, but worth in the process of thinking about automated driving, cooperative driving, and autonomous driving. The progress in the human history has systematically taken the path of the shortest resistance and has often bypassed governmental rules, business models, and the obvious thinking. At the end of the 1990s nobody was anticipating the prominent role the smart phone would have in 10 years, but scientists were busy planning journeys to Mars within the same timeframe. The latter has not happened and will probably not happen soon&hellip; One lesson humanity has learnt during its existence is that historical changes that followed the path of the minimum resistance triggered at a later stage fundamental changes in the society. &ldquo;A car is a car&rdquo; like David Strickland, administrator of the National Highway Traffic Safety Administration (NHTSA) in the U.S. said in his speech at the Telematics Update conference in Detroit, June 2011, but it may drive soon its progress along a historical path of minimum resistance.</p>\n<p>An automated driving systems needs to meet the Vienna Convention (see Section 3, aspect 2). The private sector, especially those who are in the end responsible for the performance of the vehicle, should be involved in the discussion.</p>\n<p>The Vienna Convention on Road Traffic is an international treaty designed to facilitate international road traffic and to increase road safety by standardizing the uniform traffic rules among the contracting parties. This convention was agreed upon at the United Nations Economic and Social Council&rsquo;s Conference on Road Traffic (October 7, 1968 - November 8, 1968). It came into force on May 21 1977. Not all EU countries have ratified the treaty, see Figure 13 (e.g.&nbsp;Ireland, Spain and UK did not). It should be noted that in 1968, animals were still used for traction of vehicles and the concept of autonomous driving was considered to be science fiction. This is important when interpreting the text of the treaty: in a strict interpretation to the letter of the text, or interpretation of what is meant (at that time).</p>\n<p>The common opinion of the expert panel is that the Vienna Convention will have only a limited effect on the successful deployment of automated driving systems due to several reasons:</p>\n<ul>\n<li>OEMs already deal with the situation that some of the Advanced Driver Assistance Systems touch the Vienna Convention today. For example, they provide an on/off switch for ADAS or allow an overriding of the functions by the driver. They develop their ADAS in line with the RESPONSE Code of Practice (2009) [41] following the principle that the driver is in control and remains responsible. In addition, the OEMs have a careful marketing strategy and they do not exaggerate and do not claim that an ADAS is working in all driving situations or that there is a solution to &ldquo;all&rdquo; safety problems.</li>\n<li>Automation is not black and white, automated or not automated, but much more complex, involving many design dimensions. A helpful model of automation is to consider different levels of assistance and automation that can e.g.&nbsp;be organized on a 1d- scale [42]. Several levels could be within the Vienna Convention, while extreme levels are outside of today&rsquo;s version of the Vienna Convention. For example, one partitioning could be to have levels of automation Manual, Assisted, Semi-Automated, Highly Automated, and Fully Automated driving, see Figure 14. In highly automated driving, the automation has the technical capabilities to drive almost autonomously, but the driver is still in the loop and able to take over control when it is necessary. Fully automated driving like PRT, where the driver is not required to monitor the automation and does not have the ability to take over control, seems not to be covered by the Vienna Convention.</li>\n</ul>\n<p>Criteria for deciding if the automation is still in line with the Vienna Convention could be:</p>\n<ul>\n<li>the involvement of the driver in the driving task (vehicle control),</li>\n<li>the involvement of the driver in monitoring the automation and the traffic environment,</li>\n<li>the ability to take over control or to override the automation</li>\n<li>The Vienna Convention already contains openings, or is variable, or can be changed.</li>\n</ul>\n<p>It contains a certain variability regarding the autonomy in the means of transportation, e.g. &ldquo;to control the vehicle or guide the animals&rdquo;. It is obvious that some of the current technological developments were not foreseen by the authors of the Vienna Convention. Issues like platooning are not addressed. The Vienna Convention already contains in Annex 5 (chapter 4, exemptions) an opening to be investigated with appropriate legal expertise:</p>\n<p>&ldquo;For domestic purposes, Contracting Parties may grant exemptions from the provisions of this Annex in respect of: (c) Vehicles used for experiments whose purpose is to keep up with technical progress and improve road safety; (d) Vehicles of a special form or type, or which are used for particular purposes under special conditions&rdquo;. - In addition, the Vienna Convention can be changed. The last change was made in 2006. A new paragraph (paragraph 6) was added to Article 8 stating that the driver should minimize any activity other than driving.</p>\n<p>&hellip;different understandings of the term &ldquo;to control&rdquo; with no clear consensus [44]: 1. Control in a sense of influencing e.g.&nbsp;the driver controls the vehicle movements, the driver can override the automation and/or the driver can switch the automation off. 2. Control in a sense of monitoring e.g.&nbsp;the driver monitors the actions of the automation. Both interpretations allow the use of some form of automation in a vehicle as it can be seen in today&rsquo;s cars where e.g.&nbsp;ACC or emergency brake assistance systems etc. are available.</p>\n<p>The first interpretation allows automation that can be overridden by the driver or that reacts in emergency situations only when the driver cannot cope with the situation anymore. Forms of automation that cannot be overridden seem to be not in line with the first interpretation [45, p.&nbsp;818]. The second interpretation is more flexible and would allow also forms of automation that cannot be overridden and are within the Vienna Convention as long as the driver monitors the automation [44]. &hellip;In the literature, some other assistance and automation functions were appraised by juridical experts. For example, [46] postulates that automatic emergency braking systems are in line with the Vienna Convention as long as they react only when a crash is unavoidable (collision mitigation). Otherwise a conflict between the driver&rsquo;s intention (here, steering) and the reaction of the automation (here, braking) cannot be excluded. Albrecht [47] concludes that an Intelligent Speed Adaptation (ISA) which cannot be overridden by the driver is not in line with the Vienna Convention because it is not consistent with Article 8 and Article 13 of the Vienna Convention.</p>\n<p>&hellip;As soon as data from the vehicle is used for V2X-communication or is stored in the vehicle itself, data protection and privacy issues become relevant. Directives and documents that need to be checked include:</p>\n<ul>\n<li>Directive 95/46/EC on the protection of individuals with regard to the processing of personal data and on the free movement of such data;</li>\n<li>Directive 2010/40/EU on the framework for the deployment of Intelligent Transport Systems in the field of road transport and for interfaces with other modes of transport;</li>\n<li>WP 29 Working document on data protection and privacy implications in the eCall initiative and the European Data Protection Supervisor (EDPS) opinion on ITS Action Plan and Directive.</li>\n</ul>\n<p>The bottleneck is that at the current stage of development the risk related costs and benefits of viable deployment paths are unknown, combined with the fact that the deployment paths themselves are wide open because the possible deployment scenarios are not assessed and debated in a political environment. There is currently no consensus amongst stakeholders on which of the deployment scenarios proposed will eventually prevail&hellip;Changes in EU legislation might change the role of players and increase the risk for them. Any change in EU legislation will change the position of the players, and uncertainty in which direction this change (gap) would go adds to the risk. This prohibits players from having an outspoken opinion on the issue. If an update of existing legislation is considered, this should be European legislation, not national legislation. It would be better to go for a world-wide harmonized legislation, when it is decided to take that path.</p>\n<p>A useful case study for understanding the issues associated with automated driving can be found in SAFESPOT [4] which can be viewed as a parallel to automated driving functions (for more details, see Appendix I. Related to aspect 3). SAFESPOT provided an in-depth analysis of the legal aspects of the service named &lsquo;Speed Warning&rsquo;, in two configurations V2I and V2V. It is performed against two fundamentally different law schemes, namely Dutch and English law. This analysis concluded that the concept of co-operative systems raises questions and might complicate legal disputes. This is for several reasons:</p>\n<ul>\n<li>There are more parties involved, all with their own responsibilities for the proper functioning of elements of a cooperative system.</li>\n<li>Growing technical interdependencies between vehicles, and between vehicles and the infrastructure, may also lead to system failure, including scenarios that may be characterised as an unlucky combination of events (&ldquo;a freak accident&rdquo;) or as a failure for which the exact cause simply cannot be traced back (because of the technical complexity).</li>\n<li>Risks that cannot be influenced by the people who suffer the consequences tend to be judged less acceptable by society and, likewise, from a legal point of view.</li>\n<li>The in-depth analysis of SAFESPOT concluded that (potential) participants such as system producers and road managers may well be exposed to liability risks. Even if the driver of the probe vehicle could not successfully claim a defense (towards other road users), based on a failure of a system, system providers and road managers may still remain (partially) responsible through the mechanism of subrogation and right of recourse.</li>\n<li>Current law states that the driver must be in control of his vehicle at all times. In general, EU drivers are prohibited to exhibit dangerous behaviour while driving. The police have prosecuted drivers in the UK for drinking and/or eating; i.e.&nbsp;only having one hand on the steering wheel. The use of a mobile phone while driving is prohibited in many European countries, only use of phones equipped for hands free operation are permitted. Liability still rests firmly with the driver for the safe operation of vehicles.</li>\n</ul>\n<p>New legislation may be required for automated driving. It is highly unlikely that any OEM or supplier will risk introducing an automatic driving vehicle (where responsibility for safe driving is removed from the driver) without there being a framework of new legislation which clearly sets out where their responsibility and liability begins and ends. In some ways it could be seen as similar to warranty liability, the OEM warrants certain quality and performance levels, backed by reciprocal agreements within the supply chain. Civil (and possibly criminal) liability in the case of accidents involving automated driving vehicles is a major issue that can truly delay the introduction of these technologies&hellip;Since there are no statistical records of the effects of automated driving systems, the entrepreneurship of insurers should compensate for the issue of unknown risks&hellip;The following factors are regarded as hindering an optimal role to be played by the insurance industry in promoting new safety systems through their insurance policies:</p>\n<ul>\n<li>Premium-setting is based on statistical principles, resulting in a time-lag problem;</li>\n<li>Competition/sensitive relationships with clients;</li>\n<li>Investment costs (e.g.&nbsp;aftermarket installations);</li>\n<li>Administrative costs;</li>\n<li>Market regulation</li>\n</ul>\n<p>No precedence lawsuits of liability with automated systems have happened to date. The Toyota malfunctions of their brake-by-wire system in 2010 did not end in a lawsuit. A system like parking assist is technically not redundant. What would happen if the driver claimed he/she could not override the brakes? For (premium) insurance a critical mass is required, so initially all stake-holders including governments should potentially play a role.</p>\n</blockquote>\n<p><a href=\"http://dl.dropbox.com/u/85192141/2011-wright.pdf\">&ldquo;Automotive Autonomy: Self-driving cars are inching closer to the assembly line, thanks to promising new projects from Google and the European Union&rdquo;</a>, Wright 2011:</p>\n<blockquote>\n<p>The Google project has made important advances over its predecessor, consolidating down to one laser rangefinder from five and incorporating data from a broader range of sources to help the car make more informed decisions about how to respond to its external environment. &ldquo;The threshold for error is minuscule,&rdquo; says Thrun, who points out that regulators will likely set a much higher bar for safety with a self-driving car than for one driven by notoriously error-prone humans.</p>\n</blockquote>\n<p><a href=\"http://www.917wy.com/articles/culture/future-of-driving-part-3.ars\">&ldquo;The future of driving, Part III: hack my ride&rdquo;</a>, Lee 2008:</p>\n<blockquote>\n<p>Of course, one reason that private investors might not want to invest in automotive technologies is the risk of excessive liability in the case of crashes. The tort system serves a valuable function by giving manufacturers a strong incentive to make safe, reliable products. But too much tort liability can have the perverse consequence of discouraging the introduction of even relatively safe products into the marketplace. Templeton tells Ars that the aviation industry once faced that problem. At one point, &ldquo;all of the general aviation manufacturers stopped making planes because they couldn&rsquo;t handle the liability. They were being found slightly liable in every plane crash, and it started to cost them more than the cost of manufacturing the plane.&rdquo; Airplane manufacturers eventually convinced Congress to place limits on their liability. At the moment, crashes tend to lead to lawsuits against human drivers, who rarely have deep pockets. Unless there is evidence that a mechanical defect caused the crash, car manufacturers tend not to be the target of most accident-related lawsuits. That would change if cars were driven by software. And because car manufacturers have much deeper pockets than individual drivers do, plaintiffs are likely to seek much larger damages than they would against human drivers. That could lead to the perverse result that even safer self-driving cars would be more expensive to insure than human drivers. Since car manufacturers, rather than drivers, would be the first ones sued in the event of an accident, car companies are likely to protect themselves by buying their own insurance. And if insurance premiums get too high, they may take the route the aviation industry did and seek limits on liability. An added benefit for consumers is that most would never have to worry about auto insurance. Cars would come preinsured for the life of the vehicle (or at least the life of the warranty)&hellip;Self-driving vehicles will sit at the intersection of two industries that are currently subject to very different regulatory regimes. The automobile industry is heavily regulated, while the software industry is largely unregulated at all. The most fundamental decision regulators will need to make is whether one of these existing regulatory regimes will be suitable for self-driving technologies, or whether an entirely new regulatory framework will be needed to accommodate them.</p>\n</blockquote>\n<p>http://www.917wy.com/topicpie/2008/11/future-of-driving-part-3/2</p>\n<blockquote>\n<p>It&rsquo;s inevitable that at some point, a self-driving vehicle will be involved in a fatal crash which generates worldwide publicity. Unfortunately, even if self-driving vehicles have amassed an overall safety record that&rsquo;s superior to that of human drivers, the first crash is likely to prompt calls for drastic restrictions on the use of self-driving technologies. It will therefore be important for business leaders and elected officials to lay the groundwork by both educating the public about the benefits of self-driving technologies and managing expectations so that the public isn&rsquo;t too surprised when crashes happen. Of course, if the first self-driving cars turn out to be significantly less safe than the average human driver, then they should be pulled off the streets and re-tooled. But this seems unlikely to happen. A company that introduced self-driving technology into the marketplace before it was ready would not only have trouble convincing regulators that its cars are safe, but it would be risking ruinous lawsuits, as well. The far greater danger is that the combination of liability fears and red tape will cause the United States to lose the initiative in self-driving technologies. Countries such as China, India, and Singapore that have more autocratic regimes or less-developed economies may seize the initiative and introduce self-driving cars while American policymakers are still debating how to regulate them. Eventually, the specter of other countries using technologies that aren&rsquo;t available in the United States will spur American politicians into action, but only after several thousand Americans lose their lives unnecessarily at the hands of human drivers.</p>\n<p>&hellip;One likely area of dispute is whether people will be allowed to modify the software on their own cars. The United States has a long tradition of people tinkering with both their cars and their computers. No doubt, there will be many people who are interested in modifying the software on their self-driving cars. But there is likely to be significant pressure for legislation criminalizing unauthorized tinkering with self-driving car software. Both car manufacturers and (as we&rsquo;ll see shortly) the law enforcement community are likely to be in favor of criminalizing the modification of car software. And they&rsquo;ll have a plausible safety argument: buggy car software would be dangerous not only to the car owner but to others on the road. The obvious analogy is to the DMCA, which criminalized unauthorized tinkering with copy protection schemes. But there are also important differences. One is that car manufacturers will be much more motivated to prevent tinkering than Apple or Microsoft are. If manufacturers are liable for the damage done by their vehicles, then tinkering not only endangers lives, but their bottom lines as well. It&rsquo;s unlikely that Apple would ever sue people caught jailbreaking their iPhones. But car manufacturers probably will contractually prohibit tinkering and then sue those caught doing it for breach of contract.</p>\n</blockquote>\n<p>http://www.917wy.com/topicpie/2008/11/future-of-driving-part-3/3</p>\n<blockquote>\n<p>The more stalwart advocate of locked-down cars is likely to be the government, because self-driving car software promises to be a fantastic tool for social control. Consider, for example, how useful locked-down cars could be to law enforcement. Rather than physically driving to a suspect&rsquo;s house, knocking on his door (or not), and forcibly restraining, handcuffing, and escorting a suspect to the station, police will be able to simply seize a suspect&rsquo;s self-driving car remotely and order it to drive to the nearest police station. And that&rsquo;s just the beginning. Locked-down car software could be used to enforce traffic laws, to track and log peoples&rsquo; movements for later review by law enforcement, to enforce curfews, to clear the way for emergency vehicles, and dozens of other purposes. Some of these functions are innocuous. Others will be very controversial. But all of them depend on restricting user control over their own vehicles. If users were free to swap in custom software, they might disable the government&rsquo;s &ldquo;back door&rdquo; and re-program it to ignore government requirements. So the government is likely to push hard for laws mandating that only government-approved software run self-driving cars.</p>\n<p>&hellip;It&rsquo;s too early to say exactly what the car-related civil liberties fights will be about, or how they will be resolved. But one thing we can say for certain is that the technical decisions made by today&rsquo;s computer scientists will be important for setting the stage for those battles. Advocates for online free speech and anonymity have been helped tremendously by the fact that the Internet was designed with an open, decentralized architecture. The self-driving cars of the future are likely to be built on top of software tools that are being developed in today&rsquo;s academic labs. By thinking carefully about the ways these systems are designed, today&rsquo;s computer scientists can give tomorrow&rsquo;s civil liberties their best shot at preserving automotive freedom.</p>\n</blockquote>\n<p>http://www.917wy.com/topicpie/2008/11/future-of-driving-part-3/4</p>\n<blockquote>\n<p>In our interview with him, Congressman Adam Schiff described the public&rsquo;s perception of autonomous driving technologies as a reflection of his own reaction to the idea: one that is a mixture of both fascination and skepticism. Schiff explained that the public&rsquo;s fascination comes from amazement at how advanced this technology already has become, plus with Google&rsquo;s sponsorship and endorsement it becomes even more alluring.</p>\n<p>Skepticism of autonomous vehicle technologies comes from a missing element of trust. According to Clifford Nass, a professor of communications and sociology at Stanford University, this trust is an aspect of public opinion that must be earned through demonstration more so than through use. When people see a technology in action, they will begin to trust it. Professor Nass specializes in studying the way in which human beings relate to technology, and he has published several books on the topic including The Man Who Lied to His Laptop: What Machines Teach Us About Human Relationships. In our interview with him, Professor Nass explained that societal comfort with technology is gained through experience, and acceptance occurs when people have seen a technology work enough times collectively. He also pointed out that it took a long time for people to develop trust in air transportation, something that we almost take for granted now. It is certainly not the case that autonomous cars need to be equivalent in safety to plane flight before the public would adopt them. However, as Noel du Toit pointed out, we have a higher expectation for autonomous cars than we do for ourselves. Simply put, if we are willing to relinquish the &ldquo;control&rdquo; over our vehicles to an autonomous power, it will likely have to be under the condition that the technology drives more adeptly than we ever possibly could. Otherwise, there will simply be no trusting it. Interestingly, du Toit brought up a recent botched safety demonstration by Volvo in May of 2010. In the demonstration, Volvo showcased to the press how its emergency braking system works as part of an &ldquo;adaptive cruise control&rdquo; system. These systems allow a driver to set both a top speed and a following distance, which the vehicle then automatically maintains. As a consequence, if the preceding vehicle stops short, the system acts as the foundation for an emergency-braking maneuver. However, In Volvo&rsquo;s demonstration the car smashed directly into a trailer13. Even though the system worked fine in several cases during the day&rsquo;s worth of demonstrations, video of that one mishap went viral and did little to help the public gain trust in the technology.</p>\n<p>Calo pointed out at that future issues related to autonomous vehicles would be approached from a standpoint of &ldquo;negative liabilities&rdquo;, meaning that we can assume something is legal unless there exist explicit laws against it. This discussion also led to the concept of what a driverless car would look like to bystanders, and the kind of panic that might garner. A real-life example of this occurred in Moscow during the VisLab van trek to Shanghai11. In this case, an autonomous electric van was stopped by Russian authorities due to its apparent lack of a driver behind the wheel. Thankfully, engineers present were able to convince the Russian officer who stopped the vehicle not to issue a ticket. The above [Nevadan] legislation fits in well with the information that we collected from Congressman Schiff about potential federal involvement in autonomous vehicle technology. Basically, Schiff relayed the idea that a strong governmental role expected for this technology would come in the form of regulating safety. Furthermore, he called attention to hefty governmental requirements for crash testing that every new vehicle must meet before it is allowed on the road.</p>\n<p>In autonomous driving, liability concerns can be inferred through a couple of examples. In one example, Noel du Toit described DARPA&rsquo;s use of hired stunt drivers to share the testing grounds with driverless vehicle entries in the 2007 Urban Challenge. This behavior clearly illustrates the level of precaution that the DARPA officials felt it was necessary to take. In another example, Dmitri Dolgov expounded on how Google&rsquo;s cars are never driving by themselves; whenever they are operated on public roads, there are at least two well-trained operators in the car. Dolgov went on to say that these operators &ldquo;are in control at all times&rdquo;, which helps illustrate Google&rsquo;s position-they are not taking any chances when it comes to liabilities. Kent Kresa, former CEO of Northrup Grumman and interim chairman of GM in 2009, was also concerned about the liability issues presented by autonomous vehicles. Kresa felt that a future with driverless cars piloting the streets was somewhat unimaginable at present, especially when one considers the possibility of a pedestrian getting hit. In the case of such a collision it is still very unclear who would be at fault. Whether or not the company that made the vehicle would be responsible is at present unknown.</p>\n<p>A conversation we had with Bruce Gillman, the public information officer for the Los Angeles Department of Transportation (DOT), revealed that the department is very busy putting out many other fires. Gillman noted that DOT is focused on getting people out of their cars and onto bikes or into buses. Thus, autonomous vehicles are not on their radar. Moreover, Gillman was adamant that DOT would wait until autonomous vehicles were being manufactured commercially before addressing any issues concerning them. His viewpoint certainly reinforces that idea that supportive infrastructure updates coming form a city government level would be unlikely. No matter what adoption pathway is used, federal government financial support could come in the form of incentives and subsidies like those seen during the initial rollout of hybrid vehicles. However, Brian Thomas explained that this would only be possible if the federal government was willing to do a cost-benefit valuation for the mainstream introduction of autonomous vehicles.</p>\n</blockquote>\n<p>http://www.pickar.caltech.edu/e103/Final%20Exams/Autonomous%20Vehicles%20for%20Personal%20Transport.pdf [shades of Amara&rsquo;s law: we always overestimate in the short run &amp; underestimate in the long run]</p>\n<blockquote>\n<p>Car manufacturers might be held liable for a larger share of the accidents-a responsibility they are certain to resist. (A legal analysis by Nidhi Kalra and her colleagues at the RAND Corporation suggests this problem is not insuperable.) &ndash;<a href=\"http://www.americanscientist.org/libraries/documents/201189112848657-2011-09Hayes.pdf\">&ldquo;Leave the Driving to It&rdquo;</a>, Brian Hayes <em>American Scientist</em> 2011</p>\n</blockquote>\n<p>The RAND report: <a href=\"http://www.path.berkeley.edu/PATH/Publications/PDF/PRR/2009/PRR-2009-28.pdf\">&ldquo;Liability and Regulation of Autonomous Vehicle Technologies&rdquo;</a>, Kalra et al 2009:</p>\n<blockquote>\n<p>In this work, we first evaluate how the existing liability regime would likely assign responsibility in crashes involving autonomous vehicle technologies. We identify the controlling legal principles for crashes involving these technologies and examine the implications for their further development and adoption. We anticipate that consumer education will play an important role in reducing consumer overreliance on nascent autonomous vehicle technologies and minimizing liability risk. We also discuss the possibility that the existing liability regime will slow the adoption of these socially desirable technologies because they are likely to increase liability for manufacturers while reducing liability for drivers. Finally, we discuss the possibility of federal preemption of state tort suits if the U.S. Department of Transportation (US DOT) promulgates regulations and some of the implications of eliminating state tort liability. Second, we review the existing literature on the regulatory environment for autonomous vehicle technologies. To date, there are no government regulations for these technologies, but work is being done to develop initial industry standards.</p>\n<p>&hellip;Additionally, for some systems, the driver is expected to intervene when the system cannot control the vehicle completely. For example, if a very rapid stop is required, ACC may depend on the driver to provide braking beyond its own capabilities. ACC also does not respond to driving hazards, such as debris on the road or potholes-the driver is expected to intervene. Simultaneously, research suggests that drivers using these conveniences often become complacent and slow to intervene when necessary; this behavioral adaptation means drivers are less responsive and responsible than if they were fully in control (Rudin-Brown and Parker, 2004). Does such evidence suggest that manufacturers may be responsible for monitoring driver behavior as well as vehicle behavior? Some manufacturers have already taken a step toward ensuring that the driver assumes responsibility and is attentive, by requiring the driver to periodically depress a button or by monitoring the driver by sensing eye movements and grip on the steering wheel. As discussed later, litigation may occur around the issue of driver monitoring and the danger of the driver relying on the technology for something that it is not designed to accomplish.</p>\n<p>&hellip;Ayers (1994) surveyed a range of emerging autonomous vehicle technologies and automated highways, evaluated the likelihood of a shift in liability occurring, discussed the appropriateness of government intervention, and highlighted the most-promising interventions for different technologies. Ayers found that collision warning and collision-avoidance systems &ldquo;are likely to generate a host of negligence suits against auto manufacturers&rdquo; and that liability disclaimers and federal regulations may be the most effective methods of dealing with the liability concerns (p.&nbsp;21). The report was written before many of these technologies appeared on the market, and Ayers further speculated that &ldquo;the liability for almost all accidents in cars equipped with collision-avoidance systems would conceivably fall on the manufacturer&rdquo; (p.&nbsp;22), which could &ldquo;delay or even prevent the deployment of collision warning systems that are cost-effective in terms of accident reduction&rdquo; (p.&nbsp;25). Syverud (1992) examines the legal cases stemming from the introduction of air bags, antilock brakes, cruise control, and cellular telephones to provide some general lessons for the liability concerns for autonomous vehicle technologies. In another report, Syverud (1993) examines the legal barriers to a wide range of IVHSs and finds that liability poses a significant barrier particularly to autonomous vehicle technologies that take control of the vehicle. In this work, Syverud&rsquo;s interviews with manufacturers reveal that liability concerns had already adversely affected research and development in these technologies in several companies. One interviewee is quoted as saying that &ldquo;IVHS will essentially remain &lsquo;information technology and a few pie-in-the sky pork barrel control technology demonstrations, at least in this country, until you lawyers do something about products liability law&rsquo;&rdquo; (1993, p.&nbsp;25).</p>\n<p>&hellip;While the victims in these circumstances could presumably sue the vehicle manufacturer, products liability lawsuits are more expensive to bring and take more time to resolve than run-of-the-mill automobile-crash litigation. This shift in responsibility from the driver to the manufacturer may make no-fault automobile-insurance regimes more attractive. They are designed to provide compensation to victims relatively quickly, and they do not depend upon the identification of an &ldquo;at-fault&rdquo; party</p>\n<p>&hellip;Suppose that autonomous vehicle technologies are remarkably effective at virtually eliminating minor crashes caused by human error. But it may be that the comparatively few crashes that do occur usually result in very serious injuries or fatalities (e.g., because autonomous vehicles are operating at much higher speeds or densities). This change in the distribution of crashes may affect the economics of insuring against them. Actuarially, it is much easier for an insurance company to calculate the expected costs of somewhat common small crashes than of rarer, much larger events. This may limit the downward trend in automobile-insurance costs that we would otherwise expect.</p>\n<p>&hellip;Suppose that most cars brake automatically when they sense a pedestrian in their path. As more cars with this feature come to be on the road, pedestrians may expect that cars will stop, in the same way that people stick their limbs in elevator doors confident that the door will automatically reopen. The general level of pedestrian care may decline as people become accustomed to this common safety feature. But if there were a few models of cars that did not stop in the same way, a new category of crashes could emerge. In this case, should pedestrians who wrongly assume that a car would automatically stop and are then injured be able to recover? To allow recovery in this instance would seem to undermine incentives for pedestrians to take efficient care. On the other hand, allowing the injured pedestrian to recover may encourage the universal adoption of this safety feature. Since negligence is defined by unreasonableness, the evolving set of shared assumptions about the operation of the roadways-what counts as &ldquo;reasonable&rdquo;-will determine liability. Fourth, we think that it is not likely that operators of partially or fully autonomous vehicles will be found strictly liable with driving such vehicles as an ultrahazardous activity. As explained earlier, these technologies will be introduced incrementally and will initially serve merely to aid the driver rather than take full control of the vehicle. This will give the public and courts time to become familiar with the capabilities and limits of the technology. As a result, it seems unlikely that courts will consider its gradual introduction and use to be ultrahazardous. On the other hand, this would not be true if a person attempted to operate a car fully autonomously before the technology adequately matured. Suppose, for example, that a home hobbyist put together his own autonomous vehicle and attempted to operate it on public roads. Victims of any crashes that resulted may well be successful in convincing a court to find the operator strictly liable on the grounds that such activity was ultrahazardous.</p>\n<p>&hellip;Product-liability law can be divided into theories of liability and kinds of defect. Theories of liability include negligence, misrepresentation, warranty, and strict liability.22 Types of defect include manufacturing defects, design defects, and warning defects. A product-liability lawsuit will involve one or more theories of manufacturer liability attached to a specific allegation of a type of defect. In practice, the legal tests for the theories of liability often overlap and, depending on the jurisdiction, may be identical. &hellip; While it is difficult to generalize, automobile (and subsystem) manufacturers may fare well under a negligence standard that uses a cost-benefit analysis that includes crashes avoided from the use of autonomous vehicle technologies. Automakers can argue that the overall benefits from the use of a particular technology outweigh the risks. The number of crashes avoided by the use of these technologies is probably large. &hellip;Unfortunately, the socially optimal liability rule is unclear. Permitting the defendant to include the long-run benefits in the cost-benefit analysis may encourage the adoption of technology that can indeed save many lives. On the other hand, it may shield the manufacturer from liability for shorter-run decisions that were inefficiently dangerous. Suppose, for example, that a crash-prevention system operates successfully 70% of the time but that, with additional time and work, it could have been designed to operate successfully 90% of the time. Then suppose that a victim is injured in a crash that would have been prevented had the system worked 90% of the time. Assume that the adoption of the 70-percent technology is socially desirable but the adoption of the 90-percent technology would be even more socially desirable. How should the cost-benefit analysis be conducted? Is the manufacturer permitted to cite the 70% of crashes that were prevented in arguing for the benefits of the technology? Or should the cost-benefit analysis focus on the manufacturer&rsquo;s failure to design the product to function at 90-percent effectiveness? If the latter, the manufacturer might not employ the technology, thereby leading to many preventable crashes. In calculating the marginal cost of the 90-percent technology, should the manufacturer be able to count the lives lost in the delay in implementation as compared to possible release of the 70-percent technology? &hellip;Tortious misrepresentation may play a role in litigation involving crashes that result from autonomous vehicle technologies. If advertising overpromises the benefits of these technologies, consumers may misuse them. Consider the following hypothetical scenario. Suppose that an automaker touts the &ldquo;autopilot-like&rdquo; features of its ACC and lane-keeping function. In fact, the technologies are intended to be used by an alert driver supervising their operation. After activating the ACC and lane-keeping function, a consumer assumes that the car is in control and falls asleep. Due to road resurfacing, the lane-keeping function fails, and the automobile leaves the roadway and crashes into a tree. The consumer then sues the automaker for tortious misrepresentation based on the advertising that suggested that the car was able to control itself.</p>\n<p>&hellip;Finally, it is also possible that auto manufacturers will be sued for failing to incorporate autonomous vehicle technologies in their vehicles. While absence of available safety technology is a common basis for design- defect lawsuits (e.g., Camacho v. Honda Motor Co., 741 P.2d 1240, 1987, overturning summary dismissal of suit alleging that Honda could easily have added crash bars to its motorcycles, which would have prevented the plaintiff&rsquo;s leg injuries), this theory has met with little success in the automotive field because manufacturers have successfully argued that state tort remedies were preempted by federal regulation (Geier v. American Honda Motor Co., 529 U.S. 861, 2000, finding that the plaintiff&rsquo;s claim that the manufacturer was negligent for failing to include air bags was implicitly preempted by the National Traffic and Motor Vehicle Safety Act). We discuss preemption and the relationship between regulation and tort in Section 4.3.</p>\n<p>&hellip;Preemption has arisen in the automotive context in litigation over a manufacturer&rsquo;s failure to install air bags. In Geier v. American Honda Motor Co. (2000), the U.S. Supreme Court found that state tort litigation over a manufacturer&rsquo;s failure to install air bags was preempted by the National Traffic and Motor Vehicle Safety Act (Pub. L. No. 89-563). More specifically, the Court found that the Federal Motor Vehicle Safety Standard (FMVSS) 208, promulgated by the US DOT, required manufacturers to equip some but not all of their 1987 vehicle-year vehicles with passive restraints. Because the plaintiffs&rsquo; theory that the defendants were negligent under state tort law for failing to include air bags was inconsistent with the objectives of this regulation (FMVSS 208), the Court held that the state lawsuits were preempted. Presently, there has been very little regulation promulgated by the US DOT with respect to autonomous vehicle technologies. Should the US DOT promulgate such regulation, it is likely that state tort law claims that were found to be inconsistent with the objective of the regulation would be held to be preempted under the analysis used in Geier. Substantial litigation might be expected as to whether particular state-law claims are, in fact, inconsistent with the objectives of the regulation. Resolution of those claims will depend on the specific state tort law claims, the specific regulation, and the court&rsquo;s analysis of whether they are &ldquo;inconsistent.&rdquo; &hellip;Our analysis necessarily raises a more general question: Why should we be concerned about liability issues raised by a new technology? The answer is the same as for why we care about tort law at all: that a tort regime must balance economic incentives, victim compensation, and corrective justice. Any new technology has the potential to change the sets of risks, benefits, and expectations that tort law must reconcile. &hellip;Congress could consider creating a comprehensive regulatory regime to govern the use of these technologies. If it does so, it should also consider preempting inconsistent state-court tort remedies. This may minimize the number of inconsistent legal regimes that manufacturers face and simplify and speed the introduction of this technology. While federal preemption has important disadvantages, it might speed the development and utilization of this technology and should be considered, if accompanied by a comprehensive federal regulatory regime.</p>\n<p>&hellip;This tension produced &ldquo;a standoff between airbag proponents and the automakers that resulted in contentious debates, several court cases, and very few airbags&rdquo; (Wetmore, 2004, p.&nbsp;391). In 1984, the US DOT passed a ruling requiring vehicles manufactured after 1990 to be equipped with some type of passive restraint system (e.g., air bags or automatic seat belts) (Wetmore, 2004); in 1991, this regulation was amended to require air bags in particular in all automobiles by 1999 (Pub. L. No. 102-240). The mandatory performance standards in the FMVSS further required air bags to protect an unbelted adult male passenger in a head-on, 30 mph crash. Additionally, by 1990, the situation had changed dramatically, and air bags were being installed in millions of cars. Wetmore attributes this development to three factors: First, technology had advanced to enable air-bag deployment with high reliability; second, public attitude shifted, and safety features became important factors for consumers; and, third, air bags were no longer being promoted as replacements but as supplements to seat belts, which resulted in a sharing of responsibility between manufacturers and passengers and lessened manufacturers&rsquo; potential liability (Wetmore, 2004). While air bags have certainly saved many lives, they have not lived up to original expectations: In 1977, NHTSA estimated that air bags would save on the order of 9,000 lives per year and based its regulations on these expectations (Thompson, Segui-Gomez, and Graham, 2002). Today, by contrast, NHTSA calculates that air bags saved 8,369 lives in the 14 years between 1987 and 2001 (Glassbrenner, undated). Simultaneously, however, it has become evident that air bags pose a risk to many passengers, particularly smaller passengers, such as women of small stature, the elderly, and children. NHTSA (2008a) determined that 291 deaths were caused by air bags between 1990 and July 2008, primarily due to the extreme force that is necessary to meet the performance standard of protecting the unbelted adult male passenger. Houston and Richardson (2000) describe the strong reaction to these losses and a backlash against air bags, despite their benefits. The unintended consequences of air bags have led to technology developments and changes to standards and regulations. Between 1997 and 2000, NHTSA developed a number of interim solutions designed to reduce the risks of air bags, including on-off switches and deployment with less force (Ho, 2006). Simultaneously, safer air bags, called advanced air bags, were developed that deploy with a force tailored to the occupant by taking into account the seat position, belt usage, occupant weight, and other factors. In 2000, NHTSA mandated that the introduction of these advanced air bags begin in 2003 and that, by 2006, every new passenger vehicle would include these safety measures (NHTSA, 2000). What lessons does this experience offer for regulation of autonomous vehicle technologies? We suggest that modesty and flexibility are necessary. The early air-bag regulators envisioned air bags as being a substitute for seat belts because the rates of seat-belt usage were so low and appeared intractable. Few anticipated that seat-belt usage would rise as much over time as it has and that air bags would eventually be used primarily as a supplement rather than a substitute for seat belts. Similarly unexpected developments are likely to arise in the context of autonomous vehicle technologies. In 2006, for example, Honda introduced its Accord model in the UK with a combined lane-keeping and ACC system that allows the vehicle to drive itself under the driver&rsquo;s watch; this combination of features has yet to be introduced in the United States (Miller, 2006). Ho (2006, p.&nbsp;27) observes a general trend that &ldquo;the U.S. market trails Europe, and the European market trails Japan by 2 to 3 years.&rdquo; What is the extent of these differences? What aspects of the liability and regulatory rules in those countries have enabled accelerated deployment? What other factors are at play (e.g., differences in consumers&rsquo; sensitivity to price)?</p>\n</blockquote>\n<p><a href=\"http://digitalcommons.law.scu.edu/cgi/viewcontent.cgi?article=1338\">&ldquo;New Technology - Old Law: Autonomous Vehicles and California&rsquo;s Insurance Framework&rdquo;</a>, Peterson 2012:</p>\n<blockquote>\n<p>This Article will address this issue and propose ways in which auto insurance might change to accommodate the use of AVs. Part I briefly reviews the background of insurance regulation nationally and in California. Part II discusses general insurance and liability issues related to AVs. Part III discusses some challenges that insurers and regulators may face when setting rates for AVs, both generally and under California&rsquo;s more idiosyncratic regulatory structure. Part IV discusses challenges faced by California insurers who may want to reduce rates in a timely way when technological improvements rapidly reduce risk.</p>\n<p>&hellip;When working within the context of a file-and-use or use-and-file environment, AVs will present only modest challenges to an insurer that wants to write these policies. The main challenge will arise from the fact that the policy must be rated for a new technology that may have an inadequate base of experience for an actuary to estimate future losses.21 &ldquo;Prior approval&rdquo; states, like California, require that automobile rates be approved prior to their use in the marketplace.22 These states rely more on regulation than on competition to modulate insurance rates.23 In California, automobile insurance rates are approved in a two-step process. The first step is the creation of a &ldquo;rate plan.&rdquo;24 The rate plan considers the insurer&rsquo;s entire book of business in the relative line of insurance and asks the question: How much total premium must the insurer collect in order to cover the projected risks, overhead and permitted profit for that line?25 The insurer then creates a &ldquo;class plan.&rdquo; The class plan asks the question: How should different policyholders&rsquo; premiums be adjusted up or down based on the risks presented by different groups or classes of policyholders?26 Among other factors, the Department of Insurance requires that the rating factors comply with California law and be justified by the loss experience for the group.27 Rating a new technology with an unproven track record may include a considerable amount of guesswork. &hellip;California is the largest insurance market in the United States, and it is the sixth largest among the countries of the world.28 Cars are culture in this most populous state. There are far more insured automobiles in California than any other state.29</p>\n<p>&hellip;Although adopted by the barest majority, [California&rsquo;s] Proposition 103 [see previous discussion of its 3-part requirement for rating insurance premiums] may be amended by the legislature only by a two-thirds vote, and then only if the legislation &ldquo;further[s] [the] purposes&rdquo; of Proposition 103.68 Thus, Proposition 103 and the regulations adopted by the Department of Insurance are the matrix in which most (but not all) insurance is sold and regulated in California.69 &hellip;The most sensible approach to this dilemma, at least with respect to AVs, would be to abolish or substantially re-order the three mandatory rating factors. However, this is more easily said than done. As noted above, amending Proposition 103 requires a two-thirds vote of the legislature.160 Moreover, section 8(b) of the Proposition provides: &ldquo;The provisions of this act shall not be amended by the Legislature except to further its purposes.&rdquo;161 Both of these requirements can be formidable hurdles. Persistency discounts serve as an example. Most are aware that their insurer discounts their rates if they have been with the insurer for a period of time.162 This is called the &ldquo;persistency discount.&rdquo; The discount is usually justified on the basis that persistency saves the insurer the producing expenses associated with finding a new insured. If one wants to change insurers, Proposition 103 does not permit the subsequent insurer to match the persistency discount offered by the insured&rsquo;s current insurer.163 Thus, the second insurer could not compete by offering the same discount. Changing insurers, then, was somewhat like a taxable event. The &ldquo;tax&rdquo; is the loss of the persistency discount when purchasing the new policy. The California legislature concluded that this both undermined competition and drove up the cost of insurance by discouraging the ability to shop for lower rates. &hellip;Despite these legislative findings, the Court of Appeal held the amendment invalid because, in the Court&rsquo;s view, it did not further the purposes of Proposition 103.165 The Court also held that Proposition 103 vests only the Insurance Commissioner with the power to set optional rating factors.166 Thus, the legislature, even by a super majority, may not be authorized to adopt rating factors for auto insurance. Following this defeat in the courts, promoters of &ldquo;portable persistency&rdquo; qualified a ballot initiative to amend this aspect of Proposition 103. With a vote of 51.9% to 48.1%, the initiative failed in the June 8, 2010 election.167</p>\n<p>&hellip;The State of Nevada recently adopted regulations for licensing the testing of AVs in the state. The regulations would require insurance in the minimum amounts required for other cars &ldquo;for the payment of tort liabilities arising from the maintenance or use of the motor vehicle.&rdquo;73 The regulation, however, does not suggest how the tort liability may arise. If there is no fault on the part of the operator or owner, then liability may arise, if at all, only for the manufacturer or supplier. Manufacturers and suppliers are not &ldquo;insureds&rdquo; under the standard automobile policy-at least so far. Thus, for the reasons stated above, owners, manufacturers and suppliers may fall outside the coverage of the policy.</p>\n<p>&hellip;One possible approach would be to invoke the various doctrines of products liability law. This would attach the major liability to sellers and manufacturers of the vehicle. However, it is doubtful that this is an acceptable approach for several reasons. For example, while some accidents are catastrophic, fortunately most accidents cause only modest damages. By contrast, products liability lawsuits tend to be complex and expensive. Indeed, they may require the translation of hundreds or thousands of engineering documents-perhaps written in Japanese, Chinese or Korean&hellip;See In re Puerto Rico Electric Power Authority, 687 F.2d 501, 505 (1st Cir. 1982) (stating each party to bear translation costs of documents requested by it but cost possibly taxable to prevailing party). Translation costs of Japanese documents in range of $250,000, and translation costs of additional Spanish documents may exceed that amount.</p>\n<p>&hellip;Commercial insurers of manufacturers and suppliers are not encumbered with Proposition 103&rsquo;s unique automobile provisions,197 therefore they need not offer a GDD, nor need they conform to the ranking of the mandatory rating factors. To the extent that the risks of AVs are transferred to them, the insurance burden passed to consumers in the price of the car can reflect the actual, and presumably lower, risk presented by AVs. As noted above, however, for practical reasons some rating factors, such as annual miles driven and territory, cannot properly be reflected in the automobile price. Moving from the awkward and arbitrary results mandated by Proposition 103&rsquo;s rating factors to a commercial insurance setting that cannot properly reflect some other rating factors is also an awkward trade-off. At best, it may be a choice of the least worst. Another viable solution might to be to amend the California Insurance Code section 660(a) to exclude from the definition of &ldquo;policy&rdquo; those policies covering liability for AVs (at least when operated in autonomous mode). Since Proposition 103 incorporates section 660(a), this would likely require a two-thirds vote of the legislature and the amendment would have to &ldquo;further the purposes&rdquo; of Proposition 103. Assuming a two-thirds vote could be mustered, the issue would then be whether the amendment furthers the purposes of the Proposition. To the extent that liability moves from fault-based driving to defect-based products liability, the purposes underlying the mandatory rating factors and the GDD simply cannot be accomplished. Manufacturers will pass these costs through to automobile buyers free of the Proposition&rsquo;s restraints. Since the purposes of the Proposition, at least with respect to liability coverage,199 simply cannot be accomplished when dealing with self-driving cars, amending section 660(a) would not frustrate the purposes of Proposition 103.</p>\n<p>&hellip;Filing a &ldquo;complete rate application with the commissioner&rdquo; is a substantial impediment to reducing rates. A complete rate application is an expensive, ponderous and time-consuming process. A typical filing may take three to five months before approval. Some applications have even been delayed for a year.205 In 2009, when insurers filed many new rate plans in order to comply with the new territorial rating regulations, delays among the top twenty private passenger auto insurers ranged from a low of 54 days (Viking) to a high of 558 days (USAA and USAA Casualty). Many took over 300 days (e.g., State Farm Mutual, Farmers Insurance Exchange, Progressive Choice).206 &hellip;n addition, once an application to lower rates is filed, the Commissioner, consumer groups, and others can intervene and ask that the rates be lowered even further.207 Thus, an application to lower a rate by 6% may invite pressure to lower it even further.208 If they &ldquo;substantially contributed, as a whole&rdquo; to the decision, a consumer group can also bill the insurance company for its legal, advocacy, and witness fees.209</p>\n<p>&hellip;Unless ways can be found to conform Proposition 103 to this new reality, insurance for AVs is likely to migrate to a statutory and regulatory environment untrammeled by Proposition 103-commercial policies carried by manufacturers and suppliers. This migration presents its own set of problems. While the safety of AVs could be more fairly rated, other important rating factors, such as annual miles driven and territory, must be compromised. Whether this migration occurs will also depend on how liability rules do or do not adjust to a world in which people will nevertheless suffer injuries from AVs, but in which it is unlikely our present fault rules will adequately address compensation. If concepts of non-delegable duty, agency, or strict liability attach initial liability to owners of faulty cars with faultless drivers, the insurance burden will first be filtered through automobile insurance governed by Proposition 103. These insurers will then pass the losses up the distribution line to the insurers of suppliers and manufacturers that are not governed by Proposition 103. Manufacturers and suppliers will then pass the insurance cost back to AV owners in the cost of the vehicle. The insurance load reflected in the price of the car will pass through to automobile owners free of any of the restrictions imposed by Proposition 103. There will be no GDD, such as it is, no mandatory rating factors, and, depending on where the suppliers&rsquo; or manufacturers&rsquo; insurers are located, more flexible rating. One may ask: What is gained by this merry-go-round?</p>\n</blockquote>\n<p><a href=\"http://www.nesl.edu/userfiles/file/LawReview/Vol46/3/Garza%20FINAL.pdf\">&ldquo;&lsquo;Look Ma, No Hands!&rsquo;: Wrinkles and Wrecks in the Age of Autonomous Vehicles&rdquo;</a>, Garza 2012</p>\n<blockquote>\n<p>The benefits of these systems cannot be overestimated given that one-third of drivers admit to having fallen asleep at the wheel within the previous thirty days.31 &hellip;If the driver fails to react in time, it applies 40% of the full braking power to reduce the severity of the collision.39 In the most advanced version, the CMBS performs all of the functions described above, and it will also stop the car automatically to avoid a collision when traveling under ten miles-per-hour.40 Car companies are hesitant to push the automatic braking threshold too far out of fear that &sbquo;fully &lsquo;automatic&rsquo; braking systems will shift the responsibility of avoiding an accident from the vehicle&rsquo;s driver to the vehicle&rsquo;s manufacturer.&rsquo;41&hellip;See Larry Carley, Active Safety Technology: Adaptive Cruise Control, Lane Departure Warning &amp; Collision Mitigation Braking, IMPORT CAR (June 16, 2009), http://www.import-car.com/Article/58867/active_safety_technology_adaptive_cruise_control_lane_departure_warning__collision_mitigation_braking.aspx</p>\n<p>&hellip;Automobile products liability cases are typically divided into two categories: &sbquo;(1) accidents caused by automotive defects, and (2) aggravated injuries caused by a vehicle&rsquo;s failure to be sufficiently &lsquo;crashworthy&rsquo; to protect its occupants in an accident.&lsquo;79 &hellip;For example, a car suffers from a design defect when a malfunction in the steering wheel causes a crash. 81 Additionally, plaintiffs have alleged and prevailed on manufacturing- defect claims in cases where &sbquo;unintended, sudden and uncontrollable acceleration&rsquo; causes an accident.82 In such cases, plaintiffs have been able to recover under a &sbquo;malfunction theory.&rsquo;83 Under a malfunction theory, plaintiffs use a &sbquo;res ipsa loquitur like inference to infer defectiveness in strict liability where there was no independent proof of a defect in the product.&rsquo;84 Plaintiffs have also prevailed where design defects cause injury. 85 For example, there was a proliferation of litigation in the 1970s and 1980s as a result of vehicles that were designed with a high center of gravity, which increased their propensity to roll over.86 Additionally, many design-defect cases arose in response to faulty transmissions that could inadvertently slip into gear, causing crashes and occupants to be run over in some cases. 87 The two primary tests that courts use to assess the defectiveness of a product&rsquo;s design are the consumer-expectations test and the risk-utility test.88 The consumer-expectations test focuses on whether &sbquo;the danger posed by the design is greater than an ordinary consumer would expect when using the product in an intended or reasonably foreseeable manner.&rsquo;89 &hellip;Thus, while an ordinary consumer can have expectations that a car will not explode at a stoplight or catch fire in a two-mile-per-hour collision, they may not be able to have expectations about how a truck should handle after striking a five- or six-inch rock at thirty-five miles-per-hour.92 Perhaps because the consumer-expectations test is difficult to apply to complex products, and we live in a world where technological growth increases complexity, the risk-utility test has become the dominant test in design-defect cases.93 &hellip;Litigation can also arise where a plaintiff alleges that a vehicle is not sufficiently &sbquo;crashworthy.&rsquo;104 Crashworthiness claims are a type of design- defect claim.105</p>\n<p>&hellip;Since their advent and incorporation, seat belts have resulted in litigation-much of which has involved crashworthiness claims. 136 In Jackson v. General Motors Corp., for example, the plaintiff alleged that as a result of a defectively designed seat belt, his injuries were enhanced. 137 The defendant manufacturer argued that the complexity of seat belts foreclosed any consumer expectation,138 but the Tennessee Supreme Court noted that seat belts are &lsquo;familiar products for which consumers&rsquo; expectations of safety have had an opportunity to develop,&rsquo; and permitted the plaintiff to recover under the consumer-expectations test.139 Although manufacturers have been sued where seat belts render a car insufficiently crashworthy- as in cases where they fail to perform as intended or enhance injury-the incorporation of seat belts has reduced liability as well.140 This reduction comes in the form of the &sbquo;seat belt defense.&lsquo;141 The &rsquo;seat belt defense&rsquo; allows a defendant to present evidence about an occupant&rsquo;s nonuse of a seat belt to mitigate damages or to defend against an enhanced-injury claim.142 Because seat belts are capable of reducing the number of lives lost and the overall severity of injuries sustained in crashes, it is argued that nonuse should protect a manufacturer from some claims.143 Although the majority rule is to prevent the admission of such evidence in enhanced-injury litigation, there is a growing trend toward admission.144</p>\n<p>&hellip;Since their incorporation, consumers have sued manufacturers for defective cruise control systems that lead to injury. 171 Because of the complexity of cruise control technology, courts may not allow a plaintiff to use the consumer-expectations test.172 Despite the complexity of the technology, other courts allow plaintiffs to establish a defect using either the risk-utility test or the consumer-expectations test.173</p>\n<p>&hellip;Under the consumer-expectations test, manufacturers will likely argue-as they historically have-that OAV technology is too complicated for the average consumer to have appropriate expectations about its capabilities.182 Commentators have stated that &sbquo;consumers may have unrealistic expectations about the capabilities of these technologies . . . . Technologies that are engineered to assist the driver may be overly relied on to replace the need for independent vigilance on the part of the vehicle operator.&rsquo;183 Plaintiffs will argue that, while the workings of the technology are concededly complex, the overall concept of autonomous driving is not.184 Like the car exploding at a stoplight or the car that catches fire in a two- mile-per-hour collision, the average consumer would expect autonomous vehicles to drive themselves without incident.185 This means that components that are meant to keep the car within a lane will do just that, and others will stop the vehicle at traffic lights. 186 Where incidents occur, OAVs will not have performed as the average consumer would expect.187 &hellip;plaintiffs who purchase OAVs at the cusp of availability, and attempt to prove defect under the consumer- expectations test, are likely to face an up-hill battle.194 But the unavailability of the consumer-expectations test will not be a significant detriment as plaintiffs can fall back on the risk-utility test.195 And as OAVs are increasingly incorporated, and users become more familiar with their capabilities, the consumer-expectations test will become more accessible to plaintiffs.196 Given the modern trend, plaintiffs are likely to face the risk- utility test.197</p>\n<p>&hellip;Additionally, the extent to which injuries are &sbquo;enhanced&rsquo; by OAVs will be debated.228 Because the majority of drivers fail to fully apply their brakes prior to a collision,229 where an OAV only partially applies brakes, or fails to apply brakes at all, manufacturers and plaintiffs will disagree about the extent of enhancement.230 Manufacturers will argue that, absent the OAV, the result would have been the same or worse-thus, the extent to which the injuries of the plaintiff are &sbquo;enhanced&rsquo; is minimal.231 Plaintiffs will argue that, just like the presentation of crash statistics in a risk-utility analysis, this is a false choice.232 Like no-fire air bag claims, plaintiffs will contend that but for the malfunction of the OAV, their injuries would have been greatly reduced or nonexistent. 233 As a result, any injuries sustained above that threshold should serve as a basis for recovery. 234</p>\n<p>&hellip;In products liability cases the &rsquo;use of expert witnesses has grown in both importance and expense.&rsquo;301 Because of the extraordinary cost of experts in products liability litigation, many plaintiffs are turned away because, even if they were to recover, the prospective award would not cover the expense of litigating the claim. 302</p>\n<p>&hellip;Although complex, OAVs function much like the cruise control that exists in modern cars. As we have seen with seat belts, air bags, and cruise control, manufacturers have always been hesitant to adopt safety technologies. Despite concerns, products liability law is capable of handling OAVs just as it has these past technologies. While the novelty and complexity of OAVs are likely to preclude plaintiffs from proving defect under the consumer-expectation test, as implementation increases this likelihood may decrease. Under a risk-utility analysis, manufacturers will stress the extraordinary safety benefits of OAVs, while consumers will allege that designs can be improved. In the end, OAV adoption will benefit manufacturers. Although liability will fall on manufacturers when vehicles fail, decreased incidences and severity of crashes will result in a net decrease in liability. Further, the combination of LDWS cameras and EDRs will drastically reduce the cost of litigation. By reducing reliance on experts for complex causation determinations, both manufacturers and plaintiffs will benefit. In the end, obstacles to OAV implementation are more likely to be psychological than legal, and the sooner that courts, manufacturers, and the motoring public prepare to confront these issues, the sooner lives can be saved.</p>\n</blockquote>\n<p><a href=\"http://www.theverge.com/2012/12/14/3766218/self-driving-cars-google-volvo-law\">&ldquo;Self-driving cars can navigate the road, but can they navigate the law? Google&rsquo;s lobbying hard for its self-driving technology, but some features may never be legal&rdquo;</a>, <em>The Verge</em> 14 December 2012</p>\n<blockquote>\n<p>Google says that on a given day, they have a dozen autonomous cars on the road. This August, they passed 300,000 driver-hours. In Spain this summer, Volvo drove a convoy of three cars through 200 kilometers of desert highway with just one driver and a police escort.</p>\n<p>&hellip;Bryant Walker Smith teaches a class on autonomous vehicles at Stanford Law School. At a workshop this summer, he put forward <a href=\"https://cyberlaw.stanford.edu/blog/2012/07/self-driving-crash-test\">this thought experiment</a>: the year is 2020, and a number of companies offer &ldquo;advanced driver assistance systems&rdquo; with their high-end model. Over 100,000 units have been sold. The owner&rsquo;s manual states that the driver must remain alert at all times, but one night a driver - we&rsquo;ll call him &ldquo;Paul&rdquo; - falls asleep while driving over a foggy bridge. The car tries to rouse him with alarms and vibrations but he&rsquo;s a deep sleeper, so the car turns on the hazard lights and pulls over to the side of the road where another driver (let&rsquo;s say Julie) rear-ends him. He&rsquo;s injured, angry, and prone to litigation. So is Julie. That would be tricky enough by itself, but then Smith starts layering on complications. Another model of auto-driver would have driven to the end of the bridge before pulling over. If Paul had updated his software, it would have braced his seatbelt for the crash, mitigating his injuries, but he didn&rsquo;t. The company could have pushed the update automatically, but management chose not to. Now, Smith asks the workshop, who gets sued? Or for a shorter list, who doesn&rsquo;t?</p>\n<p>&hellip;The financial stakes are high. According to the Insurance Research Council, auto liability claims paid out roughly $215 for each insured car, between bodily injury and property damage claims. With 250 million cars on the road, that&rsquo;s $54 billion a year in liability. If even a tiny portion of those lawsuits are directed towards technologists, the business would become unprofitable fast.</p>\n<p>&hellip;Changing the laws in Europe would take a replay of the internationally ratified Vienna Convention (passed in 1968) as well as pushing through a hodgepodge of national and regional laws. As Google proved, it&rsquo;s not impossible, but it leaves SARTRE facing an unusually tricky adoption problem. Lawmakers won&rsquo;t care about the project unless they think consumers really want it, but it&rsquo;s hard to get consumers excited about a product that doesn&rsquo;t exist yet. Projects like this usually rely on a core of early adopters to demonstrate their usefulness - a hard enough task, as most startups can tell you - but in this case, SARTRE has to bring auto regulators along for the ride. Optimistically, Volvo told us they expect the technology to be ready &ldquo;towards the end of this decade,&rdquo; but that may depend entirely on how quickly the law moves. The less optimistic prediction is that it never arrives at all. Steve Shladover is the program manager of mobility at California&rsquo;s PATH program, where they&rsquo;ve been trying to make convoy technology happen for 25 years, lured by the prospect of fitting three times as many cars on the freeway. They were showing off a working version as early as 1997 (powered by a single Pentium processor), before falling into the same gap between prototype and final product. &ldquo;It&rsquo;s a solvable problem once people can see the benefits,&rdquo; he told <em>The Verge</em>, &ldquo;but I think a lot of the current activity is wildly optimistic in terms of what can be achieved.&rdquo; When I asked him when we&rsquo;d see a self-driving car, Shladover told me what he says at the many auto conferences he&rsquo;s been to: &ldquo;I don&rsquo;t expect to see the fully-automated, autonomous vehicle out on the road in the lifetime of anyone in this room.&rdquo;</p>\n<p>&hellip;Many of Google&rsquo;s planned features may simply never be legal. One difficult feature is the &ldquo;come pick me up&rdquo; button that Larry Page has pushed as a solution to parking congestion. Instead of wasting energy and space on urban parking lots, why not have cars drop us off and then drive themselves to park somewhere more remote, like an automated valet?It&rsquo;s a genuinely good idea, and one Google seems passionate about, but it&rsquo;s extremely difficult to square with most vehicle codes. The Geneva Convention on Road Traffic (1949) requires that drivers &ldquo;shall at all times be able to control their vehicles,&rdquo; and provisions against reckless driving usually require &ldquo;the conscious and intentional operation of a motor vehicle.&rdquo; Some of that is simple semantics, but other concerns are harder to dismiss. After a crash, drivers are legally obligated to stop and help the injured - a difficult task if there&rsquo;s no one in the car. As a result, most experts predict drivers will be legally required to have a person in the car at all times, ready to take over if the automatic system fails. If they&rsquo;re right, the self-parking car may never be legal.</p>\n</blockquote>\n<p><a href=\"http://cyberlaw.stanford.edu/files/publication/files/2012-Smith-AutomatedVehiclesAreProbablyLegalinTheUS_0.pdf\">&ldquo;Automated Vehicles are Probably Legal in the United States&rdquo;</a>, Bryant Walker Smith 2012</p>\n<blockquote>\n<p>The short answer is that the computer direction of a motor vehicle&rsquo;s steering, braking, and accelerating without real-time human input is probably legal&hellip;.The paper&rsquo;s largely descriptive analysis, which begins with the principle that everything is permitted unless prohibited, covers three key legal regimes: the 1949 Geneva Convention on Road Traffic, regulations enacted by the National Highway Traffic Safety Administration (NHTSA), and the vehicle codes of all fifty US states.</p>\n<p>The Geneva Convention, to which the United States is a party, probably does not prohibit automated driving. The treaty promotes road safety by establishing uniform rules, one of which requires every vehicle or combination thereof to have a driver who is &ldquo;at all times &hellip; able to control&rdquo; it. However, this requirement is likely satisfied if a human is able to intervene in the automated vehicle&rsquo;s operation.</p>\n<p>NHTSA&rsquo;s regulations, which include the Federal Motor Vehicle Safety Standards to which new vehicles must be certified, do not generally prohibit or uniquely burden automated vehicles, with the possible exception of one rule regarding emergency flashers. State vehicle codes probably do not prohibit-but may complicate-automated driving. These codes assume the presence of licensed human drivers who are able to exercise human judgment, and particular rules may functionally require that presence. New York somewhat uniquely directs a driver to keep one hand on the wheel at all times. In addition, far more common rules mandating reasonable, prudent, practicable, and safe driving have uncertain application to automated vehicles and their users. Following distance requirements may also restrict the lawful operation of tightly spaced vehicle platoons. Many of these issues arise even in the three states that expressly regulate automated vehicles.</p>\n<p>&hellip;This paper does not consider how the rules of tort could or should apply to automated vehicles-that is, the extent to which tort liability might shift upstream to companies responsible for the design, manufacture, sale, operation, or provision of data or other services to an automated vehicle. 6</p>\n<p>&hellip;Because of the broad way in which the term and others like it are defined, an automated vehicle probably has a human &ldquo;driver.&rdquo; 295 Obligations imposed on that person may limit the independence with which the vehicle may lawfully operate. 296 In addition, the automated vehicle itself must meet numerous requirements, some of which may also complicate its operation. 297 Although three states have expressly established the legality of automated vehicles under certain conditions, their respective laws do not resolve many of the questions raised in this section. 298</p>\n<p>&hellip;A brief but important aside: To varying degrees, states impose criminal or quasicriminal liability on owners who permit others to drive their vehicles. 359 In Washington, &ldquo;[b]oth a person operating a vehicle with the express or implied permission of the owner and the owner of the vehicle are responsible for any act or omission that is declared unlawful in this chapter. The primary responsibility is the owner&rsquo;s.&rdquo; 360 Some states permit an inference that the owner of a vehicle was its operator for certain offenses; 361 Wisconsin provides what is by far the most detailed statutory set of rebuttable presumptions. 362 Many others punish owners who knowingly permit their vehicles to be driven unlawfully. 363 Although these owners are not drivers, they are assumed to exercise some judgment or control with respect to those drivers-an instance of vicarious liability that suggests an owner of an automated vehicle might be liable for merely permitting its automated operation. 364</p>\n<p>&hellip;On the human side, physical presence would likely continue to provide a proxy for or presumption of driving. 366 In other words, an individual who is physically positioned to provide real-time input to a motor vehicle may well be treated as its driver. This is particularly likely at levels of automation that involve human input for certain portions of a trip. In addition, an individual who starts or dispatches an automated vehicle, who initiates the automated operation of that vehicle, or who specifies certain parameters of operation probably qualifies as a driver under existing law. That individual may use some device-anything from a physical key to the click of a mouse to the sound of her voice-to activate the vehicle by herself. She may likewise deliberately request that the vehicle assume the active driving task. And she may set the vehicle&rsquo;s maximum speed or level of assertiveness. This working definition is unclear in the same ways that existing law is likely to be unclear. Relevant acts might occur at any level of the primary driving task, from a decision to take a particular trip to a decision to exceed any speed limit by ten miles per hour. 367 A tactical decision like speeding is closely connected with the consequences-whether a moving violation or an injury-that may result. But treating an individual who dispatches her fully automated vehicle as the driver for the entirety of the trip could attenuate the relationship between legal responsibility and legal fault. 368 Nonetheless, strict liability of this sort is accepted within tort law 369 and present, however controversially, in US criminal law. 370</p>\n<p>On the corporate side, a firm that designs or supplies a vehicle&rsquo;s automated functionality or that provides data or other digital services might qualify as a driver under existing law. The key element, as provided in the working definition, may be the lack of a human intermediary: A human who provides some input may still seem a better fit for a human-centered vehicle code than a company with other relevant legal exposure. However, as noted above, public outrage is another element that may motivate new uses of existing laws. 377</p>\n<p>&hellip;The mechanism by which someone other than a human would obtain a driving license is unclear. For example, some companies may possess great vision, but &ldquo;a test of the applicant&rsquo;s eyesight&rdquo; may nonetheless be difficult. 395 And while General Motors may (or may not) 396 meet a state&rsquo;s minimum age requirement, Google would not. [See Google, Google&rsquo;s mission is to organize the world&rsquo;s information and make it universally accessible and useful, www.google.com/intl/en/about/company/. In some states, Google might be allowed to drive itself to school. See, e.g., Nev. Rev.&nbsp;Stat. &sect; 483.270; Nev. Admin. Code &sect; 483.200.]</p>\n</blockquote>\n<p>And people say lawyers have no sense of humor.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z5A4c4kjTgLSFEr3h": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8ZrQkBDptXhumhP3S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 29, "extendedScore": null, "score": 8.6e-05, "legacy": true, "legacyId": "21307", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p id=\"self-driving-cars\">Excerpts from literature on robotic/self-driving/autonomous cars with a focus on legal issues, lengthy, often tedious; some more SI work. See also <a href=\"/lw/fzy/notes_on_psychopathy/\">Notes on Psychopathy</a>.</p>\n</blockquote>\n<p>Having read through all this material, my general feeling is: the near-term future (1 decade) for autonomous cars is not that great. What's been accomplished, legally speaking, is great but more limited than most people appreciate. And there are many serious problems with penetrating the elaborate ingrown rent-seeking tangle of law &amp; politics &amp; insurance. I expect the mid-future (+2 decades) to look more like autonomous cars completely taking over many odd niches and applications where the user can afford to ignore those issues (eg. on private land or in warehouses or factories), with highways and regular roads continuing to see many human drivers with some level of automated assistance. However, none of these problems seem fatal and all of them seem amenable to gradual accommodation and pressure, so I <em>am</em> now more confident that in the long run we will see autonomous cars become the norm and human driving ever more niche (and possibly lower-class). On none of these am I sure how to formulate a precise prediction, though, since I expect lots of boundary-crossing and tertium quids. We'll see.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"0_1_Self_driving_cars\"><a href=\"#TOC\"><span class=\"header-section-number\">0.1</span> Self-driving cars</a></h2>\n<p>The first success inaugurating the modern era can be considered the <a href=\"https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_%282005%29\">2005 DARPA Grand Challenge</a> where multiple vehicles completed the course. The first <a href=\"http://en.wikipedia.org/wiki/Autonomous_cars#Legislation\">legislation</a> of any kind addressing autonomous cars was Nevada\u2019s 2011 approval. <a href=\"http://cyberlaw.stanford.edu/wiki/index.php/Automated_Driving:_Legislative_and_Regulatory_Action\">5 states have passed legislation</a> dealing with autonomous cars.</p>\n<p>However, these laws are highly preliminary and all the analyses I can find agree that they punt on the real legal issues of liability; they permit relatively little.</p>\n<h3 id=\"0_1_1_Lobbying__Liability__and_Insurance\"><a href=\"#TOC\"><span class=\"header-section-number\">0.1.1</span> Lobbying, Liability, and Insurance</a></h3>\n<p>(Warning: legal analysis quoted at length in some excerpts.)</p>\n<p><a href=\"http://bngumassd.org/neatstuff/selfdrive%20cars.pdf\">\u201cToward Robotic Cars\u201d</a>, Thrun 2010 (pre-Google):</p>\n<blockquote>\n<p>Junior\u2019s behavior is governed by a finite state machine, which provides for the possibility that common traffic rules may leave a robot without a legal option as how to proceed. When that happens, the robot will eventually invoke its general-purpose path planner to find a solution, regardless of traffic rules. [Raising serious issues of liability related to potentially making people worse-off]</p>\n</blockquote>\n<p><a href=\"http://www.nytimes.com/2010/10/10/science/10google.html?pagewanted=all\">\u201cGoogle Cars Drive Themselves, in Traffic\u201d</a> (<a href=\"http://bngumassd.org/neatstuff/selfdrive%20cars.pdf\">PDF</a>), <em>NYT</em> 2010:</p>\n<blockquote>\n<p>But the advent of autonomous vehicles poses thorny legal issues, the Google researchers acknowledged. Under current law, a human must be in control of a car at all times, but what does that mean if the human is not really paying attention as the car crosses through, say, a school zone, figuring that the robot is driving more safely than he would? And in the event of an accident, who would be liable - the person behind the wheel or the maker of the software?</p>\n<p>\u201cThe technology is ahead of the law in many areas,\u201d said Bernard Lu, senior staff counsel for the California Department of Motor Vehicles. \u201cIf you look at the vehicle code, there are dozens of laws pertaining to the driver of a vehicle, and they all presume to have a human being operating the vehicle.\u201d The Google researchers said they had carefully examined California\u2019s motor vehicle regulations and determined that because a human driver can override any error, the experimental cars are legal. Mr.&nbsp;Lu agreed.</p>\n</blockquote>\n<p><a href=\"https://www.npr.org/blogs/alltechconsidered/2012/10/03/162187419/calif-green-lights-self-driving-cars-but-legal-kinks-linger\">\u201cCalif. Greenlights Self-Driving Cars, But Legal Kinks Linger\u201d</a>:</p>\n<blockquote>\n<p>For instance, if a self-driving car runs a red light and gets caught, who gets the ticket? \u201cI don\u2019t know - whoever owns the car, I would think. But we will work that out,\u201d Gov.&nbsp;Brown said at the signing event for California\u2019s bill to legalize and regulate the robotic cars. \u201cThat will be the easiest thing to work out.\u201d Google co-founder Sergey Brin, who was also at the ceremony, jokingly said \u201cself-driving cars don\u2019t run red lights.\u201d That may be true, but Bryant Walker Smith, who teaches a class at Stanford Law School this fall on the law supporting self-driving cars, says eventually one of these vehicles will get into an accident. When it does, he says, it\u2019s not clear who will pay.</p>\n<p>\u2026Or is it the company that wrote the software? Or the automaker that built the car? When it came to assigning responsibility, California decided that a self-driving car would always have a human operator. Even if that operator wasn\u2019t actually in the car, that person would be legally responsible. It sounds straightforward, but it\u2019s not. Let\u2019s say the operator of a self-driving car is inebriated; he or she is still legally the operator, but the car is driving itself. \u201cThat was a decision that department made - that the operator would be subject to the laws, including laws against driving while intoxicated, even if the operator wasn\u2019t there,\u201d Walker Smith says\u2026Still, issues surrounding liability and who is ultimately responsible when robots take the wheel are likely to remain contentious. Already trial lawyers, insurers, automakers and software engineers are queuing up to lobby rule-makers in California\u2019s capital.</p>\n</blockquote>\n<p><a href=\"http://online.wsj.com/article/SB10000872396390443493304578034822744854696.html?mod=WSJ_hppMIDDLENexttoWhatsNewsSecond\">\u201cGoogle\u2019s Driverless Car Draws Political Power: Internet Giant Hones Its Lobbying Skills in State Capitols; Giving Test Drives to Lawmakers\u201d</a>, <em>WSJ</em>, 12 October 2012:</p>\n<blockquote>\n<p>Overall, Google spent nearly $9 million in the first half of 2012 lobbying in Washington for a wide variety of issues, including speaking to U.S. Department of Transportation officials and lawmakers about autonomous vehicle technology, according to federal records, nearing the $9.68 million it spent on lobbying in all of 2011. It is unclear how much Google has spent in total on lobbying state officials; the company doesn\u2019t disclose such data.</p>\n<p>\u2026In most states, autonomous vehicles are neither prohibited nor permitted-a key reason why Google\u2019s fleet of autonomous cars secretly drove more than 100,000 miles on the road before the company announced the initiative in fall 2010. Last month, Mr.&nbsp;Brin said he expects self-driving cars to be publicly available within five years.</p>\n<p>In January 2011, Mr.&nbsp;Goldwater approached Ms.&nbsp;Dondero Loop and the Nevada assembly transportation committee about proposing a bill to direct the state\u2019s department of motor vehicles to draft regulations around the self-driving vehicles. \u201cWe\u2019re not saying, \u2018Put this on the road,\u2019\u201d he said he told the lawmakers. \u201cWe\u2019re saying, \u2018This is legitimate technology,\u2019 and we\u2019re letting the DMV test it and certify it.\u201d Following the Nevada bill\u2019s passage, legislators from other states began showing interest in similar legislation. So Google repeated its original recipe and added an extra ingredient: giving lawmakers the chance to ride in one of its about a dozen self-driving cars\u2026In California, an autonomous-vehicle bill became law last month despite opposition from the Alliance of Automobile Manufacturers, which includes 12 top auto makers such as GM, BMW and Toyota. The group had approved of the Florida bill. Dan Gage, a spokesman for the group, said the California legislation would allow companies and individuals to modify existing vehicles with self-driving technology that could be faulty, and that auto makers wouldn\u2019t be legally protected from resulting lawsuits. \u201cThey\u2019re not all Google, and they could convert our vehicles in a manner not intended,\u201d Mr.&nbsp;Gage said. But Google helped push the bill through after spending about $140,000 over the past year to lobby legislators and California agencies, according to public records</p>\n</blockquote>\n<blockquote>\n<p>As with California\u2019s recently enacted law, Cheh\u2019s [Washington D.C.] bill requires that a licensed driver be present in the driver\u2019s seat of these vehicles. While seemingly inconsequential, this effectively outlaws one of the more promising functions of autonomous vehicle technology: allowing disabled people to enjoy the personal mobility that most people take for granted. Google highlighted this benefit when one of its driverless cars drove a legally blind man to a Taco Bell. Bizarrely, Cheh\u2019s bill also requires that autonomous vehicles operate only on alternative fuels. While the Google Self-Driving Car may manifest itself as an eco-conscious Prius, self-driving vehicle technology has nothing to do with hybrids, plug-in electrics or vehicles fueled with natural gas. The technology does not depend on vehicle make or model, but Cheh is seeking to mandate as much. That could delay the technology\u2019s widespread adoption for no good reason\u2026Another flaw in Cheh\u2019s bill is that it would impose a special tax on drivers of autonomous vehicles. Instead of paying fuel taxes, \u201cOwners of autonomous vehicles shall pay a vehicle-miles travelled (VMT) fee of 1.875 cents per mile.\u201d Administrative details aside, a VMT tax would require drivers to install a recording device to be periodically audited by the government. There may be good reasons to replace fuel taxes with VMT fees, but greatly restricting the use of a potentially revolutionary new technology by singling it out for a new tax system would be a mistake.</p>\n</blockquote>\n<p><a href=\"http://www.washingtonpost.com/opinions/driverless-cars-are-on-the-way-heres-how-not-to-regulate-them/2012/11/02/a5337880-21f1-11e2-ac85-e669876c6a24_story.html\">\u201cDriverless cars are on the way. Here\u2019s how not to regulate them.\u201d</a></p>\n<p><a href=\"http://ojs.stanford.edu/ojs/index.php/intersect/article/download/361/361\">\u201cHow autonomous vehicle policy in California and Nevada addresses technological and non-technological liabilities\u201d</a>, Pinto 2012:</p>\n<blockquote>\n<p>The State of Nevada has adopted one policy approach to dealing with these technical and policy issues. At the urging of Google, a new Nevada law directs the Nevada Department of Motor Vehicles (NDMV) to issue regulations for the testing and possible licensing of autonomous vehicles and for licensing the owners/drivers of these vehicles. There is also a similar law being proposed in California with details not covered by <a href=\"http://www.dmvnv.com/pdfforms/obl326.pdf\">Nevada AB 511</a>. This paper evaluates the strengths and weaknesses of the Nevada and California approaches</p>\n<p>Another problem posed by the non-computer world is that human drivers frequently bend the rules by rolling through stop signs and driving above speed limits. How does a polite and law-abiding robot vehicle act in these situations? To solve this problem, the Google Car can be programmed for different driving personalities, mirroring the current conditions. On one end, it would be cautious, being more likely to yield to another car and strictly following the laws on the road. At the other end of the spectrum, the robocar would be aggressive, where it is more likely to go first at the stop sign. When going through a four-way intersection, for example, it yields to other vehicles based on road rules; but if other cars don\u2019t reciprocate, it advances a bit to show to the other drivers its intention.</p>\n<p>However, there is a time period between a problem being diagnosed and the car being fixed. In theory, one would disable the vehicle remotely and only start it back up when the problem is fixed. However in reality, this would be extremely disruptive to a person\u2019s life as they would have to tow their vehicle to the nearest mechanic or autonomous vehicle equivalent to solve the issue. Google has not developed the technology to approach this problem, instead relying on the human driver to take control of the vehicle if there is ever a problem in their test vehicles.</p>\n<p>[previous Lu quote about human-centric laws] \u2026this can create particularly tricky situations such as deciding whether the police should have the right to pull over autonomous vehicles, a question yet to be answered. Even the chief counsel of the National Highway Traffic Safety Administration admits that the federal government does not have enough information to determine how to regulate driverless technologies. This can become a particularly thorny issue when there is the first accident between autonomous and self driving vehicles and how to go about assigning liability.</p>\n<p>This question of liability arose during an [unpublished 11 Feb 2012] interview on the future of autonomous vehicles with Roger Noll. Although Professor Noll hasn\u2019t read the current literature on this issue, he voiced concern over what the verdict of the first trial between an accident between an autonomous vehicle and normal car will be. He believes that the jury will almost certainly side with the human driver despite the details of the case, as he eloquently put in his husky Utah accent and subsequent laughter, \u201chow are we going to defend the autonomous vehicle; can we ask it to testify for itself?\u201d To answer Roger Noll\u2019s question, Brad Templeton\u2019s blog elaborates how he believes that liability reasons are a largely unimportant question for two reasons. First, in new technology, there is no question that any lawsuit over any incident involving the cars will include the vendor as the defendant so potential vendors must plan for liability. For the second reason, Brad Templeton makes an economic argument that the cost of accidents is borne by car buyers through higher insurance premiums. If the accidents are deemed the fault of the vehicle maker, this cost goes into the price of the car, and is paid for by the vehicle maker\u2019s insurance or self- insurance. Instead, Brad Templeton believes that the big question is whether the liability assigned in any lawsuit will be significantly greater than it is in ordinary collisions because of punitive damages. In theory, robocars should drive the costs down because of the reductions in collisions, and that means savings for the car buyer and for society and thus cheaper auto insurance. However, if the cost per collision is much higher even though the number of collisions drops, there is uncertainty over whether autonomous vehicles will save money for both parties.</p>\n<p>California\u2019s Proposition 103 dictates that any insurance policy\u2019s price must be based on weighted factors, and the top 3 weighted factors must be, 1. driving record, 2. number of miles driven and 3. number of years of experience. Other factors like the type of car someone has (i.e.&nbsp;autonomous vehicle) will be weighed lower. Subsequently, this law makes it very hard to get cheap insurance for a robocar.</p>\n<p>Nevada Policy: AB 511 Section 8 This short piece of legislation accomplishes the goal of setting good standards for the DMV to follow. By setting general standards (part a), insurance requirements (part b), and safety standards (part c), this sets a precedent for these areas without being too limited with details, leaving them to be decided by the DMV instead of the politicians. \u2026part b only discusses insurance briefly, saying the state must, \u201cSet forth requirements for the insurance that is required to test or operate an autonomous vehicle on a highway within this State.\u201d The definitions set in the second part of Section 8 are not specific enough. Following the open-ended standards set in the earlier part of the Section 8 is good for continuity, but not technically addressing the problem. According to Ryan Calo, Director of Privacy and Robotics for Stanford Law School\u2019s Center for Internet and Society (CIS), the bill\u2019s definition of \u201cautonomous vehicles\u201d is unclear and circular. In the context of this legislation, autonomous driving is seen as a binary system of existence, but in reality, it falls more under a spectrum.</p>\n<p>Overall, AB 511 did not address either the technological liabilities and barely mentioned the non-technological liabilities that are necessary to overcome for future success of autonomous vehicles. Since it was the first type of legislation to ever approach the issue of autonomous vehicles, it is understandable that the policymakers did not want to go into specifics and instead rely on future regulation to determine the details.</p>\n<p>California Policy: SB 1298\u2026would require the adoption of safety standards and performance requirements to ensure the safe operation and testing of \u201cautonomous vehicles\u201d on California public roads. The bill would allow autonomous vehicles to be operated or tested on the public roads on the condition they meet safety standards and performance requirements of the bill. SB 1298\u2019s 66 lines of text is also considerably longer than AB 511\u2019s 12 lines of relevant text (the entirety of AB 511 is much longer but consists of irrelevant information for the purposes of autonomous cars). would require the adoption of safety standards and performance requirements to ensure the safe operation and testing of \u201cautonomous vehicles\u201d on California public roads. The bill would allow autonomous vehicles to be operated or tested on the public roads on the condition they meet safety standards and performance requirements of the bill. SB 1298\u2019s 66 lines of text is also considerably longer than AB 511\u2019s 12 lines of relevant text (the entirety of AB 511 is much longer but consists of irrelevant information for the purposes of autonomous cars).</p>\n<p>SB 1298 has clear intentions to have company developed vehicles by saying in Section 2, Part B that, \u201cautonomous vehicles have been operated safely on public roads in the state in recent years by companies developing and testing this technology\u201d and how these companies have set the standard for what safety standards will be necessary for future testing by others. This part of the legislation implicitly supports Google\u2019s autonomous vehicle because it has the most extensively tested fleet of vehicles out of all the companies, and all this testing has been nearly exclusively done in California. This bill is an improvement over AB 511 by putting more control in the hands of Google to focus on developing the technology, which is a signal by the policymakers to create a climate favorable for Google\u2019s innovation within the constraints of keeping society safe.</p>\n<p>To avoid setting a dangerous precedent for liability in accidents, policymakers can consider protecting the car companies from frivolous and malicious lawsuits. Without such legislation, future plaintiffs will be justified to sue Google and put full liability on them. There are also potential free riding effects of the economic moral hazard of putting the blame on the company that makes the technology, not the company that manufactures the vehicle. Since we are assuming that autonomous vehicle technology will all come from one source of Google, then any accident that occurs will pin the blame primarily on Google, the common denominator, not as much as on the car manufacturer\u2026Policy that ensures the costs per accident remains close to today\u2019s current cost will save money for both the insurer and customer. This could potentially mean putting a cap on rewards towards the recipients or punishments towards the company to limit shocks to the industry. Overall, a policymaker can choose to create a gradual limit on the amount of liability placed on the vendor based on certain technology or scaling issues that are met without accidents.</p>\n<p>SB 1298 manages to cover some of the shortcomings of AB 511, such as how to improve upon the definition of an autonomous vehicle, as well as looking more towards the future by giving Google more responsibility and alleviating some of the non-technical liability by considering their product \u201cunder development\u201d. However, both pieces of legislation fail to address the specific technical liabilities such as bugs in the code base or computer attacks, and non-technical liabilities such as insurance or accident liability.</p>\n</blockquote>\n<p><a href=\"http://www.nytimes.com/2011/05/29/business/economy/29view.html\">\u201cCan I See Your License, Registration and C.P.U.?\u201d</a>, Tyler Cowen; see also his <a href=\"http://marginalrevolution.com/marginalrevolution/2011/06/what-do-the-laws-against-driverless-cars-look-like.html\">\u201cWhat do the laws against driverless cars look like?\u201d</a>:</p>\n<blockquote>\n<p>The driverless car is illegal in all 50 states. Google, which has been at the forefront of this particular technology, is asking the Nevada legislature to relax restrictions on the cars so it can test some of them on roads there. Unfortunately, the very necessity for this lobbying is a sign of our ambivalence toward change. Ideally, politicians should be calling for accelerated safety trials and promising to pass liability caps if the cars meet acceptable standards, whether that be sooner or later. Yet no major public figure has taken up this cause.</p>\n<p>Enabling the development of driverless cars will require squadrons of lawyers because a variety of state, local and federal laws presume that a human being is operating the automobiles on our roads. No state has anything close to a functioning system to inspect whether the computers in driverless cars are in good working order, much as we routinely test emissions and brake lights. Ordinary laws change only if legislators make those revisions a priority. Yet the mundane political issues of the day often appear quite pressing, not to mention politically safer than enabling a new product that is likely to engender controversy.</p>\n<p>Politics, of course, is often geared toward preserving the status quo, which is highly visible, familiar in its risks, and lucrative for companies already making a profit from it. Some parts of government do foster innovation, such as Darpa, the Defense Advanced Research Projects Agency, which is part of the Defense Department. Darpa helped create the Internet and is supporting the development of the driverless car. It operates largely outside the public eye; the real problems come when its innovations start to enter everyday life and meet political resistance and disturbing press reports.</p>\n<p>\u2026In the meantime, transportation is one area where progress has been slow for decades. We\u2019re still flying 747s, a plane designed in the 1960s. Many rail and bus networks have contracted. And traffic congestion is worse than ever. As I\u2019 argued in a previous column, this is probably part of a broader slowdown of technological advances.</p>\n<p>But it\u2019s clear that in the early part of the 20th century, the original advent of the motor car was not impeded by anything like the current m\u00e9lange of regulations, laws and lawsuits. Potentially major innovations need a path forward, through the current thicket of restrictions. That debate on this issue is so quiet shows the urgency of doing something now.</p>\n</blockquote>\n<p><a title=\"On The Legality Of Driverless Vehicles: A Response To Tyler Cowen\" href=\"https://cyberlaw.stanford.edu/blog/2011/06/legality-driverless-vehicles-response-tyler-cowen\">Ryan Calo</a> of the <em>CIS</em> argues essentially that no specific law <em>bans</em> autonomous cars and the threat of the human-centric laws &amp; regulations is overblown. (See the later Russian incident.)</p>\n<p><a href=\"http://ideas.4brad.com/scu-conference-legal-issues-robocars\">\u201cSCU conference on legal issues of robocars\u201d</a>, Brad Templeton:</p>\n<blockquote>\n<p>Liability: After a technology introduction where Sven Bieker of Stanford outlined the challenges he saw which put fully autonomous robocars 2 decades away, the first session was on civil liability. The short message was that based on a number of related cases from the past, it will be hard for manufacturers to avoid liability for any safety problems with their robocars, even when the systems were built to provide the highest statistical safety result if it traded off one type of safety for another. In general when robocars come up as a subject of discussion in web threads, I frequently see \u201cWho will be liable in a crash\u201d as the first question. I think it\u2019s a largely unimportant question for two reasons. First of all, when the technology is new, there is no question that any lawsuit over any incident involving the cars will include the vendor as the defendant, in many cases with justifiable reasons, but even if there is no easily seen reason why. So potential vendors can\u2019t expect to not plan for liability. But most of all, the reality is that in the end, the cost of accidents is borne by car buyers. Normally, they do it by buying insurance. But if the accidents are deemed the fault of the vehicle maker, this cost goes into the price of the car, and is paid for by the vehicle maker\u2019s insurance or self-insurance. It\u2019s just a question of figuring out how the vehicle buyer will pay, and the market should be capable of that (though see below.) No, the big question in my mind is whether the liability assigned in any lawsuit will be significantly greater than it is in ordinary collisions where human error is at fault, because of punitive damages\u2026Unfortunately, some liability history points to the latter scenario, though it is possible for statutes to modify this.</p>\n<p>Insurance: \u2026Because Prop 103 [specifying insurance by weighted factors, see previous] is a ballot proposition, it can\u2019t easily be superseded by the legislature. It takes a 2/3 vote and a court agreeing the change matches the intent of the original ballot proposition. One would hope the courts would agree that cheaper insurance to encourage safer cars would match the voter intent, but this is a challenge.</p>\n<p>Local and criminal laws: The session on criminal laws centered more on the traffic code (which isn\u2019t really criminal law) and the fact it varies a lot from state to state. Indeed, any robocar that wants to operate in multiple states will have to deal with this, though fortunately there is a federal standard on traffic controls (signs and lights) to rely on. Some global standards are a concern - the Geneva convention on traffic laws requires every car has a driver who is in control of the vehicle. However, I think that governments will be able to quickly see - if they want to - that these are laws in need of updating. Some precedent in drunk driving can create problems - people have been convicted of DUI for being in their car, drunk, with the keys in their pocket, because they had clear intent to drive drunk. However, one would hope the possession of a robocar (of the sort that does not need human manual driving) would express an entirely different intent to the law.</p>\n</blockquote>\n<p><a href=\"http://ec.europa.eu/information_society/activities/esafety/doc/studies/automated/reportfinal.pdf\">\u201cDefinition of necessary vehicle and infrastructure systems for Automated Driving\u201d</a>, European Commission report 29 June 2011:</p>\n<blockquote>\n<p>Yet another paramount aspect tightly related to automated driving at present and in the near future, and certainly related to autonomous driving in the long run, is the interpretation of the Vienna Convention. It will be shown in the report how this European legislation is commonly interpreted, how it creates the framework necessary to deploy on a large scale automated and cooperative driving systems, and what legal limitations are foreseen in making the new step toward autonomous driving. The report analyses in the same context other conventions and legislative acts, searches for gaps in the current legislation and makes an interesting link with the aviation industry where several lessons can be learnt from.</p>\n<p>It seems appropriate to end this summary with a few remarks not directly related to the subject of this report, but worth in the process of thinking about automated driving, cooperative driving, and autonomous driving. The progress in the human history has systematically taken the path of the shortest resistance and has often bypassed governmental rules, business models, and the obvious thinking. At the end of the 1990s nobody was anticipating the prominent role the smart phone would have in 10 years, but scientists were busy planning journeys to Mars within the same timeframe. The latter has not happened and will probably not happen soon\u2026 One lesson humanity has learnt during its existence is that historical changes that followed the path of the minimum resistance triggered at a later stage fundamental changes in the society. \u201cA car is a car\u201d like David Strickland, administrator of the National Highway Traffic Safety Administration (NHTSA) in the U.S. said in his speech at the Telematics Update conference in Detroit, June 2011, but it may drive soon its progress along a historical path of minimum resistance.</p>\n<p>An automated driving systems needs to meet the Vienna Convention (see Section 3, aspect 2). The private sector, especially those who are in the end responsible for the performance of the vehicle, should be involved in the discussion.</p>\n<p>The Vienna Convention on Road Traffic is an international treaty designed to facilitate international road traffic and to increase road safety by standardizing the uniform traffic rules among the contracting parties. This convention was agreed upon at the United Nations Economic and Social Council\u2019s Conference on Road Traffic (October 7, 1968 - November 8, 1968). It came into force on May 21 1977. Not all EU countries have ratified the treaty, see Figure 13 (e.g.&nbsp;Ireland, Spain and UK did not). It should be noted that in 1968, animals were still used for traction of vehicles and the concept of autonomous driving was considered to be science fiction. This is important when interpreting the text of the treaty: in a strict interpretation to the letter of the text, or interpretation of what is meant (at that time).</p>\n<p>The common opinion of the expert panel is that the Vienna Convention will have only a limited effect on the successful deployment of automated driving systems due to several reasons:</p>\n<ul>\n<li>OEMs already deal with the situation that some of the Advanced Driver Assistance Systems touch the Vienna Convention today. For example, they provide an on/off switch for ADAS or allow an overriding of the functions by the driver. They develop their ADAS in line with the RESPONSE Code of Practice (2009) [41] following the principle that the driver is in control and remains responsible. In addition, the OEMs have a careful marketing strategy and they do not exaggerate and do not claim that an ADAS is working in all driving situations or that there is a solution to \u201call\u201d safety problems.</li>\n<li>Automation is not black and white, automated or not automated, but much more complex, involving many design dimensions. A helpful model of automation is to consider different levels of assistance and automation that can e.g.&nbsp;be organized on a 1d- scale [42]. Several levels could be within the Vienna Convention, while extreme levels are outside of today\u2019s version of the Vienna Convention. For example, one partitioning could be to have levels of automation Manual, Assisted, Semi-Automated, Highly Automated, and Fully Automated driving, see Figure 14. In highly automated driving, the automation has the technical capabilities to drive almost autonomously, but the driver is still in the loop and able to take over control when it is necessary. Fully automated driving like PRT, where the driver is not required to monitor the automation and does not have the ability to take over control, seems not to be covered by the Vienna Convention.</li>\n</ul>\n<p>Criteria for deciding if the automation is still in line with the Vienna Convention could be:</p>\n<ul>\n<li>the involvement of the driver in the driving task (vehicle control),</li>\n<li>the involvement of the driver in monitoring the automation and the traffic environment,</li>\n<li>the ability to take over control or to override the automation</li>\n<li>The Vienna Convention already contains openings, or is variable, or can be changed.</li>\n</ul>\n<p>It contains a certain variability regarding the autonomy in the means of transportation, e.g. \u201cto control the vehicle or guide the animals\u201d. It is obvious that some of the current technological developments were not foreseen by the authors of the Vienna Convention. Issues like platooning are not addressed. The Vienna Convention already contains in Annex 5 (chapter 4, exemptions) an opening to be investigated with appropriate legal expertise:</p>\n<p>\u201cFor domestic purposes, Contracting Parties may grant exemptions from the provisions of this Annex in respect of: (c) Vehicles used for experiments whose purpose is to keep up with technical progress and improve road safety; (d) Vehicles of a special form or type, or which are used for particular purposes under special conditions\u201d. - In addition, the Vienna Convention can be changed. The last change was made in 2006. A new paragraph (paragraph 6) was added to Article 8 stating that the driver should minimize any activity other than driving.</p>\n<p>\u2026different understandings of the term \u201cto control\u201d with no clear consensus [44]: 1. Control in a sense of influencing e.g.&nbsp;the driver controls the vehicle movements, the driver can override the automation and/or the driver can switch the automation off. 2. Control in a sense of monitoring e.g.&nbsp;the driver monitors the actions of the automation. Both interpretations allow the use of some form of automation in a vehicle as it can be seen in today\u2019s cars where e.g.&nbsp;ACC or emergency brake assistance systems etc. are available.</p>\n<p>The first interpretation allows automation that can be overridden by the driver or that reacts in emergency situations only when the driver cannot cope with the situation anymore. Forms of automation that cannot be overridden seem to be not in line with the first interpretation [45, p.&nbsp;818]. The second interpretation is more flexible and would allow also forms of automation that cannot be overridden and are within the Vienna Convention as long as the driver monitors the automation [44]. \u2026In the literature, some other assistance and automation functions were appraised by juridical experts. For example, [46] postulates that automatic emergency braking systems are in line with the Vienna Convention as long as they react only when a crash is unavoidable (collision mitigation). Otherwise a conflict between the driver\u2019s intention (here, steering) and the reaction of the automation (here, braking) cannot be excluded. Albrecht [47] concludes that an Intelligent Speed Adaptation (ISA) which cannot be overridden by the driver is not in line with the Vienna Convention because it is not consistent with Article 8 and Article 13 of the Vienna Convention.</p>\n<p>\u2026As soon as data from the vehicle is used for V2X-communication or is stored in the vehicle itself, data protection and privacy issues become relevant. Directives and documents that need to be checked include:</p>\n<ul>\n<li>Directive 95/46/EC on the protection of individuals with regard to the processing of personal data and on the free movement of such data;</li>\n<li>Directive 2010/40/EU on the framework for the deployment of Intelligent Transport Systems in the field of road transport and for interfaces with other modes of transport;</li>\n<li>WP 29 Working document on data protection and privacy implications in the eCall initiative and the European Data Protection Supervisor (EDPS) opinion on ITS Action Plan and Directive.</li>\n</ul>\n<p>The bottleneck is that at the current stage of development the risk related costs and benefits of viable deployment paths are unknown, combined with the fact that the deployment paths themselves are wide open because the possible deployment scenarios are not assessed and debated in a political environment. There is currently no consensus amongst stakeholders on which of the deployment scenarios proposed will eventually prevail\u2026Changes in EU legislation might change the role of players and increase the risk for them. Any change in EU legislation will change the position of the players, and uncertainty in which direction this change (gap) would go adds to the risk. This prohibits players from having an outspoken opinion on the issue. If an update of existing legislation is considered, this should be European legislation, not national legislation. It would be better to go for a world-wide harmonized legislation, when it is decided to take that path.</p>\n<p>A useful case study for understanding the issues associated with automated driving can be found in SAFESPOT [4] which can be viewed as a parallel to automated driving functions (for more details, see Appendix I. Related to aspect 3). SAFESPOT provided an in-depth analysis of the legal aspects of the service named \u2018Speed Warning\u2019, in two configurations V2I and V2V. It is performed against two fundamentally different law schemes, namely Dutch and English law. This analysis concluded that the concept of co-operative systems raises questions and might complicate legal disputes. This is for several reasons:</p>\n<ul>\n<li>There are more parties involved, all with their own responsibilities for the proper functioning of elements of a cooperative system.</li>\n<li>Growing technical interdependencies between vehicles, and between vehicles and the infrastructure, may also lead to system failure, including scenarios that may be characterised as an unlucky combination of events (\u201ca freak accident\u201d) or as a failure for which the exact cause simply cannot be traced back (because of the technical complexity).</li>\n<li>Risks that cannot be influenced by the people who suffer the consequences tend to be judged less acceptable by society and, likewise, from a legal point of view.</li>\n<li>The in-depth analysis of SAFESPOT concluded that (potential) participants such as system producers and road managers may well be exposed to liability risks. Even if the driver of the probe vehicle could not successfully claim a defense (towards other road users), based on a failure of a system, system providers and road managers may still remain (partially) responsible through the mechanism of subrogation and right of recourse.</li>\n<li>Current law states that the driver must be in control of his vehicle at all times. In general, EU drivers are prohibited to exhibit dangerous behaviour while driving. The police have prosecuted drivers in the UK for drinking and/or eating; i.e.&nbsp;only having one hand on the steering wheel. The use of a mobile phone while driving is prohibited in many European countries, only use of phones equipped for hands free operation are permitted. Liability still rests firmly with the driver for the safe operation of vehicles.</li>\n</ul>\n<p>New legislation may be required for automated driving. It is highly unlikely that any OEM or supplier will risk introducing an automatic driving vehicle (where responsibility for safe driving is removed from the driver) without there being a framework of new legislation which clearly sets out where their responsibility and liability begins and ends. In some ways it could be seen as similar to warranty liability, the OEM warrants certain quality and performance levels, backed by reciprocal agreements within the supply chain. Civil (and possibly criminal) liability in the case of accidents involving automated driving vehicles is a major issue that can truly delay the introduction of these technologies\u2026Since there are no statistical records of the effects of automated driving systems, the entrepreneurship of insurers should compensate for the issue of unknown risks\u2026The following factors are regarded as hindering an optimal role to be played by the insurance industry in promoting new safety systems through their insurance policies:</p>\n<ul>\n<li>Premium-setting is based on statistical principles, resulting in a time-lag problem;</li>\n<li>Competition/sensitive relationships with clients;</li>\n<li>Investment costs (e.g.&nbsp;aftermarket installations);</li>\n<li>Administrative costs;</li>\n<li>Market regulation</li>\n</ul>\n<p>No precedence lawsuits of liability with automated systems have happened to date. The Toyota malfunctions of their brake-by-wire system in 2010 did not end in a lawsuit. A system like parking assist is technically not redundant. What would happen if the driver claimed he/she could not override the brakes? For (premium) insurance a critical mass is required, so initially all stake-holders including governments should potentially play a role.</p>\n</blockquote>\n<p><a href=\"http://dl.dropbox.com/u/85192141/2011-wright.pdf\">\u201cAutomotive Autonomy: Self-driving cars are inching closer to the assembly line, thanks to promising new projects from Google and the European Union\u201d</a>, Wright 2011:</p>\n<blockquote>\n<p>The Google project has made important advances over its predecessor, consolidating down to one laser rangefinder from five and incorporating data from a broader range of sources to help the car make more informed decisions about how to respond to its external environment. \u201cThe threshold for error is minuscule,\u201d says Thrun, who points out that regulators will likely set a much higher bar for safety with a self-driving car than for one driven by notoriously error-prone humans.</p>\n</blockquote>\n<p><a href=\"http://www.917wy.com/articles/culture/future-of-driving-part-3.ars\">\u201cThe future of driving, Part III: hack my ride\u201d</a>, Lee 2008:</p>\n<blockquote>\n<p>Of course, one reason that private investors might not want to invest in automotive technologies is the risk of excessive liability in the case of crashes. The tort system serves a valuable function by giving manufacturers a strong incentive to make safe, reliable products. But too much tort liability can have the perverse consequence of discouraging the introduction of even relatively safe products into the marketplace. Templeton tells Ars that the aviation industry once faced that problem. At one point, \u201call of the general aviation manufacturers stopped making planes because they couldn\u2019t handle the liability. They were being found slightly liable in every plane crash, and it started to cost them more than the cost of manufacturing the plane.\u201d Airplane manufacturers eventually convinced Congress to place limits on their liability. At the moment, crashes tend to lead to lawsuits against human drivers, who rarely have deep pockets. Unless there is evidence that a mechanical defect caused the crash, car manufacturers tend not to be the target of most accident-related lawsuits. That would change if cars were driven by software. And because car manufacturers have much deeper pockets than individual drivers do, plaintiffs are likely to seek much larger damages than they would against human drivers. That could lead to the perverse result that even safer self-driving cars would be more expensive to insure than human drivers. Since car manufacturers, rather than drivers, would be the first ones sued in the event of an accident, car companies are likely to protect themselves by buying their own insurance. And if insurance premiums get too high, they may take the route the aviation industry did and seek limits on liability. An added benefit for consumers is that most would never have to worry about auto insurance. Cars would come preinsured for the life of the vehicle (or at least the life of the warranty)\u2026Self-driving vehicles will sit at the intersection of two industries that are currently subject to very different regulatory regimes. The automobile industry is heavily regulated, while the software industry is largely unregulated at all. The most fundamental decision regulators will need to make is whether one of these existing regulatory regimes will be suitable for self-driving technologies, or whether an entirely new regulatory framework will be needed to accommodate them.</p>\n</blockquote>\n<p>http://www.917wy.com/topicpie/2008/11/future-of-driving-part-3/2</p>\n<blockquote>\n<p>It\u2019s inevitable that at some point, a self-driving vehicle will be involved in a fatal crash which generates worldwide publicity. Unfortunately, even if self-driving vehicles have amassed an overall safety record that\u2019s superior to that of human drivers, the first crash is likely to prompt calls for drastic restrictions on the use of self-driving technologies. It will therefore be important for business leaders and elected officials to lay the groundwork by both educating the public about the benefits of self-driving technologies and managing expectations so that the public isn\u2019t too surprised when crashes happen. Of course, if the first self-driving cars turn out to be significantly less safe than the average human driver, then they should be pulled off the streets and re-tooled. But this seems unlikely to happen. A company that introduced self-driving technology into the marketplace before it was ready would not only have trouble convincing regulators that its cars are safe, but it would be risking ruinous lawsuits, as well. The far greater danger is that the combination of liability fears and red tape will cause the United States to lose the initiative in self-driving technologies. Countries such as China, India, and Singapore that have more autocratic regimes or less-developed economies may seize the initiative and introduce self-driving cars while American policymakers are still debating how to regulate them. Eventually, the specter of other countries using technologies that aren\u2019t available in the United States will spur American politicians into action, but only after several thousand Americans lose their lives unnecessarily at the hands of human drivers.</p>\n<p>\u2026One likely area of dispute is whether people will be allowed to modify the software on their own cars. The United States has a long tradition of people tinkering with both their cars and their computers. No doubt, there will be many people who are interested in modifying the software on their self-driving cars. But there is likely to be significant pressure for legislation criminalizing unauthorized tinkering with self-driving car software. Both car manufacturers and (as we\u2019ll see shortly) the law enforcement community are likely to be in favor of criminalizing the modification of car software. And they\u2019ll have a plausible safety argument: buggy car software would be dangerous not only to the car owner but to others on the road. The obvious analogy is to the DMCA, which criminalized unauthorized tinkering with copy protection schemes. But there are also important differences. One is that car manufacturers will be much more motivated to prevent tinkering than Apple or Microsoft are. If manufacturers are liable for the damage done by their vehicles, then tinkering not only endangers lives, but their bottom lines as well. It\u2019s unlikely that Apple would ever sue people caught jailbreaking their iPhones. But car manufacturers probably will contractually prohibit tinkering and then sue those caught doing it for breach of contract.</p>\n</blockquote>\n<p>http://www.917wy.com/topicpie/2008/11/future-of-driving-part-3/3</p>\n<blockquote>\n<p>The more stalwart advocate of locked-down cars is likely to be the government, because self-driving car software promises to be a fantastic tool for social control. Consider, for example, how useful locked-down cars could be to law enforcement. Rather than physically driving to a suspect\u2019s house, knocking on his door (or not), and forcibly restraining, handcuffing, and escorting a suspect to the station, police will be able to simply seize a suspect\u2019s self-driving car remotely and order it to drive to the nearest police station. And that\u2019s just the beginning. Locked-down car software could be used to enforce traffic laws, to track and log peoples\u2019 movements for later review by law enforcement, to enforce curfews, to clear the way for emergency vehicles, and dozens of other purposes. Some of these functions are innocuous. Others will be very controversial. But all of them depend on restricting user control over their own vehicles. If users were free to swap in custom software, they might disable the government\u2019s \u201cback door\u201d and re-program it to ignore government requirements. So the government is likely to push hard for laws mandating that only government-approved software run self-driving cars.</p>\n<p>\u2026It\u2019s too early to say exactly what the car-related civil liberties fights will be about, or how they will be resolved. But one thing we can say for certain is that the technical decisions made by today\u2019s computer scientists will be important for setting the stage for those battles. Advocates for online free speech and anonymity have been helped tremendously by the fact that the Internet was designed with an open, decentralized architecture. The self-driving cars of the future are likely to be built on top of software tools that are being developed in today\u2019s academic labs. By thinking carefully about the ways these systems are designed, today\u2019s computer scientists can give tomorrow\u2019s civil liberties their best shot at preserving automotive freedom.</p>\n</blockquote>\n<p>http://www.917wy.com/topicpie/2008/11/future-of-driving-part-3/4</p>\n<blockquote>\n<p>In our interview with him, Congressman Adam Schiff described the public\u2019s perception of autonomous driving technologies as a reflection of his own reaction to the idea: one that is a mixture of both fascination and skepticism. Schiff explained that the public\u2019s fascination comes from amazement at how advanced this technology already has become, plus with Google\u2019s sponsorship and endorsement it becomes even more alluring.</p>\n<p>Skepticism of autonomous vehicle technologies comes from a missing element of trust. According to Clifford Nass, a professor of communications and sociology at Stanford University, this trust is an aspect of public opinion that must be earned through demonstration more so than through use. When people see a technology in action, they will begin to trust it. Professor Nass specializes in studying the way in which human beings relate to technology, and he has published several books on the topic including The Man Who Lied to His Laptop: What Machines Teach Us About Human Relationships. In our interview with him, Professor Nass explained that societal comfort with technology is gained through experience, and acceptance occurs when people have seen a technology work enough times collectively. He also pointed out that it took a long time for people to develop trust in air transportation, something that we almost take for granted now. It is certainly not the case that autonomous cars need to be equivalent in safety to plane flight before the public would adopt them. However, as Noel du Toit pointed out, we have a higher expectation for autonomous cars than we do for ourselves. Simply put, if we are willing to relinquish the \u201ccontrol\u201d over our vehicles to an autonomous power, it will likely have to be under the condition that the technology drives more adeptly than we ever possibly could. Otherwise, there will simply be no trusting it. Interestingly, du Toit brought up a recent botched safety demonstration by Volvo in May of 2010. In the demonstration, Volvo showcased to the press how its emergency braking system works as part of an \u201cadaptive cruise control\u201d system. These systems allow a driver to set both a top speed and a following distance, which the vehicle then automatically maintains. As a consequence, if the preceding vehicle stops short, the system acts as the foundation for an emergency-braking maneuver. However, In Volvo\u2019s demonstration the car smashed directly into a trailer13. Even though the system worked fine in several cases during the day\u2019s worth of demonstrations, video of that one mishap went viral and did little to help the public gain trust in the technology.</p>\n<p>Calo pointed out at that future issues related to autonomous vehicles would be approached from a standpoint of \u201cnegative liabilities\u201d, meaning that we can assume something is legal unless there exist explicit laws against it. This discussion also led to the concept of what a driverless car would look like to bystanders, and the kind of panic that might garner. A real-life example of this occurred in Moscow during the VisLab van trek to Shanghai11. In this case, an autonomous electric van was stopped by Russian authorities due to its apparent lack of a driver behind the wheel. Thankfully, engineers present were able to convince the Russian officer who stopped the vehicle not to issue a ticket. The above [Nevadan] legislation fits in well with the information that we collected from Congressman Schiff about potential federal involvement in autonomous vehicle technology. Basically, Schiff relayed the idea that a strong governmental role expected for this technology would come in the form of regulating safety. Furthermore, he called attention to hefty governmental requirements for crash testing that every new vehicle must meet before it is allowed on the road.</p>\n<p>In autonomous driving, liability concerns can be inferred through a couple of examples. In one example, Noel du Toit described DARPA\u2019s use of hired stunt drivers to share the testing grounds with driverless vehicle entries in the 2007 Urban Challenge. This behavior clearly illustrates the level of precaution that the DARPA officials felt it was necessary to take. In another example, Dmitri Dolgov expounded on how Google\u2019s cars are never driving by themselves; whenever they are operated on public roads, there are at least two well-trained operators in the car. Dolgov went on to say that these operators \u201care in control at all times\u201d, which helps illustrate Google\u2019s position-they are not taking any chances when it comes to liabilities. Kent Kresa, former CEO of Northrup Grumman and interim chairman of GM in 2009, was also concerned about the liability issues presented by autonomous vehicles. Kresa felt that a future with driverless cars piloting the streets was somewhat unimaginable at present, especially when one considers the possibility of a pedestrian getting hit. In the case of such a collision it is still very unclear who would be at fault. Whether or not the company that made the vehicle would be responsible is at present unknown.</p>\n<p>A conversation we had with Bruce Gillman, the public information officer for the Los Angeles Department of Transportation (DOT), revealed that the department is very busy putting out many other fires. Gillman noted that DOT is focused on getting people out of their cars and onto bikes or into buses. Thus, autonomous vehicles are not on their radar. Moreover, Gillman was adamant that DOT would wait until autonomous vehicles were being manufactured commercially before addressing any issues concerning them. His viewpoint certainly reinforces that idea that supportive infrastructure updates coming form a city government level would be unlikely. No matter what adoption pathway is used, federal government financial support could come in the form of incentives and subsidies like those seen during the initial rollout of hybrid vehicles. However, Brian Thomas explained that this would only be possible if the federal government was willing to do a cost-benefit valuation for the mainstream introduction of autonomous vehicles.</p>\n</blockquote>\n<p>http://www.pickar.caltech.edu/e103/Final%20Exams/Autonomous%20Vehicles%20for%20Personal%20Transport.pdf [shades of Amara\u2019s law: we always overestimate in the short run &amp; underestimate in the long run]</p>\n<blockquote>\n<p>Car manufacturers might be held liable for a larger share of the accidents-a responsibility they are certain to resist. (A legal analysis by Nidhi Kalra and her colleagues at the RAND Corporation suggests this problem is not insuperable.) \u2013<a href=\"http://www.americanscientist.org/libraries/documents/201189112848657-2011-09Hayes.pdf\">\u201cLeave the Driving to It\u201d</a>, Brian Hayes <em>American Scientist</em> 2011</p>\n</blockquote>\n<p>The RAND report: <a href=\"http://www.path.berkeley.edu/PATH/Publications/PDF/PRR/2009/PRR-2009-28.pdf\">\u201cLiability and Regulation of Autonomous Vehicle Technologies\u201d</a>, Kalra et al 2009:</p>\n<blockquote>\n<p>In this work, we first evaluate how the existing liability regime would likely assign responsibility in crashes involving autonomous vehicle technologies. We identify the controlling legal principles for crashes involving these technologies and examine the implications for their further development and adoption. We anticipate that consumer education will play an important role in reducing consumer overreliance on nascent autonomous vehicle technologies and minimizing liability risk. We also discuss the possibility that the existing liability regime will slow the adoption of these socially desirable technologies because they are likely to increase liability for manufacturers while reducing liability for drivers. Finally, we discuss the possibility of federal preemption of state tort suits if the U.S. Department of Transportation (US DOT) promulgates regulations and some of the implications of eliminating state tort liability. Second, we review the existing literature on the regulatory environment for autonomous vehicle technologies. To date, there are no government regulations for these technologies, but work is being done to develop initial industry standards.</p>\n<p>\u2026Additionally, for some systems, the driver is expected to intervene when the system cannot control the vehicle completely. For example, if a very rapid stop is required, ACC may depend on the driver to provide braking beyond its own capabilities. ACC also does not respond to driving hazards, such as debris on the road or potholes-the driver is expected to intervene. Simultaneously, research suggests that drivers using these conveniences often become complacent and slow to intervene when necessary; this behavioral adaptation means drivers are less responsive and responsible than if they were fully in control (Rudin-Brown and Parker, 2004). Does such evidence suggest that manufacturers may be responsible for monitoring driver behavior as well as vehicle behavior? Some manufacturers have already taken a step toward ensuring that the driver assumes responsibility and is attentive, by requiring the driver to periodically depress a button or by monitoring the driver by sensing eye movements and grip on the steering wheel. As discussed later, litigation may occur around the issue of driver monitoring and the danger of the driver relying on the technology for something that it is not designed to accomplish.</p>\n<p>\u2026Ayers (1994) surveyed a range of emerging autonomous vehicle technologies and automated highways, evaluated the likelihood of a shift in liability occurring, discussed the appropriateness of government intervention, and highlighted the most-promising interventions for different technologies. Ayers found that collision warning and collision-avoidance systems \u201care likely to generate a host of negligence suits against auto manufacturers\u201d and that liability disclaimers and federal regulations may be the most effective methods of dealing with the liability concerns (p.&nbsp;21). The report was written before many of these technologies appeared on the market, and Ayers further speculated that \u201cthe liability for almost all accidents in cars equipped with collision-avoidance systems would conceivably fall on the manufacturer\u201d (p.&nbsp;22), which could \u201cdelay or even prevent the deployment of collision warning systems that are cost-effective in terms of accident reduction\u201d (p.&nbsp;25). Syverud (1992) examines the legal cases stemming from the introduction of air bags, antilock brakes, cruise control, and cellular telephones to provide some general lessons for the liability concerns for autonomous vehicle technologies. In another report, Syverud (1993) examines the legal barriers to a wide range of IVHSs and finds that liability poses a significant barrier particularly to autonomous vehicle technologies that take control of the vehicle. In this work, Syverud\u2019s interviews with manufacturers reveal that liability concerns had already adversely affected research and development in these technologies in several companies. One interviewee is quoted as saying that \u201cIVHS will essentially remain \u2018information technology and a few pie-in-the sky pork barrel control technology demonstrations, at least in this country, until you lawyers do something about products liability law\u2019\u201d (1993, p.&nbsp;25).</p>\n<p>\u2026While the victims in these circumstances could presumably sue the vehicle manufacturer, products liability lawsuits are more expensive to bring and take more time to resolve than run-of-the-mill automobile-crash litigation. This shift in responsibility from the driver to the manufacturer may make no-fault automobile-insurance regimes more attractive. They are designed to provide compensation to victims relatively quickly, and they do not depend upon the identification of an \u201cat-fault\u201d party</p>\n<p>\u2026Suppose that autonomous vehicle technologies are remarkably effective at virtually eliminating minor crashes caused by human error. But it may be that the comparatively few crashes that do occur usually result in very serious injuries or fatalities (e.g., because autonomous vehicles are operating at much higher speeds or densities). This change in the distribution of crashes may affect the economics of insuring against them. Actuarially, it is much easier for an insurance company to calculate the expected costs of somewhat common small crashes than of rarer, much larger events. This may limit the downward trend in automobile-insurance costs that we would otherwise expect.</p>\n<p>\u2026Suppose that most cars brake automatically when they sense a pedestrian in their path. As more cars with this feature come to be on the road, pedestrians may expect that cars will stop, in the same way that people stick their limbs in elevator doors confident that the door will automatically reopen. The general level of pedestrian care may decline as people become accustomed to this common safety feature. But if there were a few models of cars that did not stop in the same way, a new category of crashes could emerge. In this case, should pedestrians who wrongly assume that a car would automatically stop and are then injured be able to recover? To allow recovery in this instance would seem to undermine incentives for pedestrians to take efficient care. On the other hand, allowing the injured pedestrian to recover may encourage the universal adoption of this safety feature. Since negligence is defined by unreasonableness, the evolving set of shared assumptions about the operation of the roadways-what counts as \u201creasonable\u201d-will determine liability. Fourth, we think that it is not likely that operators of partially or fully autonomous vehicles will be found strictly liable with driving such vehicles as an ultrahazardous activity. As explained earlier, these technologies will be introduced incrementally and will initially serve merely to aid the driver rather than take full control of the vehicle. This will give the public and courts time to become familiar with the capabilities and limits of the technology. As a result, it seems unlikely that courts will consider its gradual introduction and use to be ultrahazardous. On the other hand, this would not be true if a person attempted to operate a car fully autonomously before the technology adequately matured. Suppose, for example, that a home hobbyist put together his own autonomous vehicle and attempted to operate it on public roads. Victims of any crashes that resulted may well be successful in convincing a court to find the operator strictly liable on the grounds that such activity was ultrahazardous.</p>\n<p>\u2026Product-liability law can be divided into theories of liability and kinds of defect. Theories of liability include negligence, misrepresentation, warranty, and strict liability.22 Types of defect include manufacturing defects, design defects, and warning defects. A product-liability lawsuit will involve one or more theories of manufacturer liability attached to a specific allegation of a type of defect. In practice, the legal tests for the theories of liability often overlap and, depending on the jurisdiction, may be identical. \u2026 While it is difficult to generalize, automobile (and subsystem) manufacturers may fare well under a negligence standard that uses a cost-benefit analysis that includes crashes avoided from the use of autonomous vehicle technologies. Automakers can argue that the overall benefits from the use of a particular technology outweigh the risks. The number of crashes avoided by the use of these technologies is probably large. \u2026Unfortunately, the socially optimal liability rule is unclear. Permitting the defendant to include the long-run benefits in the cost-benefit analysis may encourage the adoption of technology that can indeed save many lives. On the other hand, it may shield the manufacturer from liability for shorter-run decisions that were inefficiently dangerous. Suppose, for example, that a crash-prevention system operates successfully 70% of the time but that, with additional time and work, it could have been designed to operate successfully 90% of the time. Then suppose that a victim is injured in a crash that would have been prevented had the system worked 90% of the time. Assume that the adoption of the 70-percent technology is socially desirable but the adoption of the 90-percent technology would be even more socially desirable. How should the cost-benefit analysis be conducted? Is the manufacturer permitted to cite the 70% of crashes that were prevented in arguing for the benefits of the technology? Or should the cost-benefit analysis focus on the manufacturer\u2019s failure to design the product to function at 90-percent effectiveness? If the latter, the manufacturer might not employ the technology, thereby leading to many preventable crashes. In calculating the marginal cost of the 90-percent technology, should the manufacturer be able to count the lives lost in the delay in implementation as compared to possible release of the 70-percent technology? \u2026Tortious misrepresentation may play a role in litigation involving crashes that result from autonomous vehicle technologies. If advertising overpromises the benefits of these technologies, consumers may misuse them. Consider the following hypothetical scenario. Suppose that an automaker touts the \u201cautopilot-like\u201d features of its ACC and lane-keeping function. In fact, the technologies are intended to be used by an alert driver supervising their operation. After activating the ACC and lane-keeping function, a consumer assumes that the car is in control and falls asleep. Due to road resurfacing, the lane-keeping function fails, and the automobile leaves the roadway and crashes into a tree. The consumer then sues the automaker for tortious misrepresentation based on the advertising that suggested that the car was able to control itself.</p>\n<p>\u2026Finally, it is also possible that auto manufacturers will be sued for failing to incorporate autonomous vehicle technologies in their vehicles. While absence of available safety technology is a common basis for design- defect lawsuits (e.g., Camacho v. Honda Motor Co., 741 P.2d 1240, 1987, overturning summary dismissal of suit alleging that Honda could easily have added crash bars to its motorcycles, which would have prevented the plaintiff\u2019s leg injuries), this theory has met with little success in the automotive field because manufacturers have successfully argued that state tort remedies were preempted by federal regulation (Geier v. American Honda Motor Co., 529 U.S. 861, 2000, finding that the plaintiff\u2019s claim that the manufacturer was negligent for failing to include air bags was implicitly preempted by the National Traffic and Motor Vehicle Safety Act). We discuss preemption and the relationship between regulation and tort in Section 4.3.</p>\n<p>\u2026Preemption has arisen in the automotive context in litigation over a manufacturer\u2019s failure to install air bags. In Geier v. American Honda Motor Co. (2000), the U.S. Supreme Court found that state tort litigation over a manufacturer\u2019s failure to install air bags was preempted by the National Traffic and Motor Vehicle Safety Act (Pub. L. No. 89-563). More specifically, the Court found that the Federal Motor Vehicle Safety Standard (FMVSS) 208, promulgated by the US DOT, required manufacturers to equip some but not all of their 1987 vehicle-year vehicles with passive restraints. Because the plaintiffs\u2019 theory that the defendants were negligent under state tort law for failing to include air bags was inconsistent with the objectives of this regulation (FMVSS 208), the Court held that the state lawsuits were preempted. Presently, there has been very little regulation promulgated by the US DOT with respect to autonomous vehicle technologies. Should the US DOT promulgate such regulation, it is likely that state tort law claims that were found to be inconsistent with the objective of the regulation would be held to be preempted under the analysis used in Geier. Substantial litigation might be expected as to whether particular state-law claims are, in fact, inconsistent with the objectives of the regulation. Resolution of those claims will depend on the specific state tort law claims, the specific regulation, and the court\u2019s analysis of whether they are \u201cinconsistent.\u201d \u2026Our analysis necessarily raises a more general question: Why should we be concerned about liability issues raised by a new technology? The answer is the same as for why we care about tort law at all: that a tort regime must balance economic incentives, victim compensation, and corrective justice. Any new technology has the potential to change the sets of risks, benefits, and expectations that tort law must reconcile. \u2026Congress could consider creating a comprehensive regulatory regime to govern the use of these technologies. If it does so, it should also consider preempting inconsistent state-court tort remedies. This may minimize the number of inconsistent legal regimes that manufacturers face and simplify and speed the introduction of this technology. While federal preemption has important disadvantages, it might speed the development and utilization of this technology and should be considered, if accompanied by a comprehensive federal regulatory regime.</p>\n<p>\u2026This tension produced \u201ca standoff between airbag proponents and the automakers that resulted in contentious debates, several court cases, and very few airbags\u201d (Wetmore, 2004, p.&nbsp;391). In 1984, the US DOT passed a ruling requiring vehicles manufactured after 1990 to be equipped with some type of passive restraint system (e.g., air bags or automatic seat belts) (Wetmore, 2004); in 1991, this regulation was amended to require air bags in particular in all automobiles by 1999 (Pub. L. No. 102-240). The mandatory performance standards in the FMVSS further required air bags to protect an unbelted adult male passenger in a head-on, 30 mph crash. Additionally, by 1990, the situation had changed dramatically, and air bags were being installed in millions of cars. Wetmore attributes this development to three factors: First, technology had advanced to enable air-bag deployment with high reliability; second, public attitude shifted, and safety features became important factors for consumers; and, third, air bags were no longer being promoted as replacements but as supplements to seat belts, which resulted in a sharing of responsibility between manufacturers and passengers and lessened manufacturers\u2019 potential liability (Wetmore, 2004). While air bags have certainly saved many lives, they have not lived up to original expectations: In 1977, NHTSA estimated that air bags would save on the order of 9,000 lives per year and based its regulations on these expectations (Thompson, Segui-Gomez, and Graham, 2002). Today, by contrast, NHTSA calculates that air bags saved 8,369 lives in the 14 years between 1987 and 2001 (Glassbrenner, undated). Simultaneously, however, it has become evident that air bags pose a risk to many passengers, particularly smaller passengers, such as women of small stature, the elderly, and children. NHTSA (2008a) determined that 291 deaths were caused by air bags between 1990 and July 2008, primarily due to the extreme force that is necessary to meet the performance standard of protecting the unbelted adult male passenger. Houston and Richardson (2000) describe the strong reaction to these losses and a backlash against air bags, despite their benefits. The unintended consequences of air bags have led to technology developments and changes to standards and regulations. Between 1997 and 2000, NHTSA developed a number of interim solutions designed to reduce the risks of air bags, including on-off switches and deployment with less force (Ho, 2006). Simultaneously, safer air bags, called advanced air bags, were developed that deploy with a force tailored to the occupant by taking into account the seat position, belt usage, occupant weight, and other factors. In 2000, NHTSA mandated that the introduction of these advanced air bags begin in 2003 and that, by 2006, every new passenger vehicle would include these safety measures (NHTSA, 2000). What lessons does this experience offer for regulation of autonomous vehicle technologies? We suggest that modesty and flexibility are necessary. The early air-bag regulators envisioned air bags as being a substitute for seat belts because the rates of seat-belt usage were so low and appeared intractable. Few anticipated that seat-belt usage would rise as much over time as it has and that air bags would eventually be used primarily as a supplement rather than a substitute for seat belts. Similarly unexpected developments are likely to arise in the context of autonomous vehicle technologies. In 2006, for example, Honda introduced its Accord model in the UK with a combined lane-keeping and ACC system that allows the vehicle to drive itself under the driver\u2019s watch; this combination of features has yet to be introduced in the United States (Miller, 2006). Ho (2006, p.&nbsp;27) observes a general trend that \u201cthe U.S. market trails Europe, and the European market trails Japan by 2 to 3 years.\u201d What is the extent of these differences? What aspects of the liability and regulatory rules in those countries have enabled accelerated deployment? What other factors are at play (e.g., differences in consumers\u2019 sensitivity to price)?</p>\n</blockquote>\n<p><a href=\"http://digitalcommons.law.scu.edu/cgi/viewcontent.cgi?article=1338\">\u201cNew Technology - Old Law: Autonomous Vehicles and California\u2019s Insurance Framework\u201d</a>, Peterson 2012:</p>\n<blockquote>\n<p>This Article will address this issue and propose ways in which auto insurance might change to accommodate the use of AVs. Part I briefly reviews the background of insurance regulation nationally and in California. Part II discusses general insurance and liability issues related to AVs. Part III discusses some challenges that insurers and regulators may face when setting rates for AVs, both generally and under California\u2019s more idiosyncratic regulatory structure. Part IV discusses challenges faced by California insurers who may want to reduce rates in a timely way when technological improvements rapidly reduce risk.</p>\n<p>\u2026When working within the context of a file-and-use or use-and-file environment, AVs will present only modest challenges to an insurer that wants to write these policies. The main challenge will arise from the fact that the policy must be rated for a new technology that may have an inadequate base of experience for an actuary to estimate future losses.21 \u201cPrior approval\u201d states, like California, require that automobile rates be approved prior to their use in the marketplace.22 These states rely more on regulation than on competition to modulate insurance rates.23 In California, automobile insurance rates are approved in a two-step process. The first step is the creation of a \u201crate plan.\u201d24 The rate plan considers the insurer\u2019s entire book of business in the relative line of insurance and asks the question: How much total premium must the insurer collect in order to cover the projected risks, overhead and permitted profit for that line?25 The insurer then creates a \u201cclass plan.\u201d The class plan asks the question: How should different policyholders\u2019 premiums be adjusted up or down based on the risks presented by different groups or classes of policyholders?26 Among other factors, the Department of Insurance requires that the rating factors comply with California law and be justified by the loss experience for the group.27 Rating a new technology with an unproven track record may include a considerable amount of guesswork. \u2026California is the largest insurance market in the United States, and it is the sixth largest among the countries of the world.28 Cars are culture in this most populous state. There are far more insured automobiles in California than any other state.29</p>\n<p>\u2026Although adopted by the barest majority, [California\u2019s] Proposition 103 [see previous discussion of its 3-part requirement for rating insurance premiums] may be amended by the legislature only by a two-thirds vote, and then only if the legislation \u201cfurther[s] [the] purposes\u201d of Proposition 103.68 Thus, Proposition 103 and the regulations adopted by the Department of Insurance are the matrix in which most (but not all) insurance is sold and regulated in California.69 \u2026The most sensible approach to this dilemma, at least with respect to AVs, would be to abolish or substantially re-order the three mandatory rating factors. However, this is more easily said than done. As noted above, amending Proposition 103 requires a two-thirds vote of the legislature.160 Moreover, section 8(b) of the Proposition provides: \u201cThe provisions of this act shall not be amended by the Legislature except to further its purposes.\u201d161 Both of these requirements can be formidable hurdles. Persistency discounts serve as an example. Most are aware that their insurer discounts their rates if they have been with the insurer for a period of time.162 This is called the \u201cpersistency discount.\u201d The discount is usually justified on the basis that persistency saves the insurer the producing expenses associated with finding a new insured. If one wants to change insurers, Proposition 103 does not permit the subsequent insurer to match the persistency discount offered by the insured\u2019s current insurer.163 Thus, the second insurer could not compete by offering the same discount. Changing insurers, then, was somewhat like a taxable event. The \u201ctax\u201d is the loss of the persistency discount when purchasing the new policy. The California legislature concluded that this both undermined competition and drove up the cost of insurance by discouraging the ability to shop for lower rates. \u2026Despite these legislative findings, the Court of Appeal held the amendment invalid because, in the Court\u2019s view, it did not further the purposes of Proposition 103.165 The Court also held that Proposition 103 vests only the Insurance Commissioner with the power to set optional rating factors.166 Thus, the legislature, even by a super majority, may not be authorized to adopt rating factors for auto insurance. Following this defeat in the courts, promoters of \u201cportable persistency\u201d qualified a ballot initiative to amend this aspect of Proposition 103. With a vote of 51.9% to 48.1%, the initiative failed in the June 8, 2010 election.167</p>\n<p>\u2026The State of Nevada recently adopted regulations for licensing the testing of AVs in the state. The regulations would require insurance in the minimum amounts required for other cars \u201cfor the payment of tort liabilities arising from the maintenance or use of the motor vehicle.\u201d73 The regulation, however, does not suggest how the tort liability may arise. If there is no fault on the part of the operator or owner, then liability may arise, if at all, only for the manufacturer or supplier. Manufacturers and suppliers are not \u201cinsureds\u201d under the standard automobile policy-at least so far. Thus, for the reasons stated above, owners, manufacturers and suppliers may fall outside the coverage of the policy.</p>\n<p>\u2026One possible approach would be to invoke the various doctrines of products liability law. This would attach the major liability to sellers and manufacturers of the vehicle. However, it is doubtful that this is an acceptable approach for several reasons. For example, while some accidents are catastrophic, fortunately most accidents cause only modest damages. By contrast, products liability lawsuits tend to be complex and expensive. Indeed, they may require the translation of hundreds or thousands of engineering documents-perhaps written in Japanese, Chinese or Korean\u2026See In re Puerto Rico Electric Power Authority, 687 F.2d 501, 505 (1st Cir. 1982) (stating each party to bear translation costs of documents requested by it but cost possibly taxable to prevailing party). Translation costs of Japanese documents in range of $250,000, and translation costs of additional Spanish documents may exceed that amount.</p>\n<p>\u2026Commercial insurers of manufacturers and suppliers are not encumbered with Proposition 103\u2019s unique automobile provisions,197 therefore they need not offer a GDD, nor need they conform to the ranking of the mandatory rating factors. To the extent that the risks of AVs are transferred to them, the insurance burden passed to consumers in the price of the car can reflect the actual, and presumably lower, risk presented by AVs. As noted above, however, for practical reasons some rating factors, such as annual miles driven and territory, cannot properly be reflected in the automobile price. Moving from the awkward and arbitrary results mandated by Proposition 103\u2019s rating factors to a commercial insurance setting that cannot properly reflect some other rating factors is also an awkward trade-off. At best, it may be a choice of the least worst. Another viable solution might to be to amend the California Insurance Code section 660(a) to exclude from the definition of \u201cpolicy\u201d those policies covering liability for AVs (at least when operated in autonomous mode). Since Proposition 103 incorporates section 660(a), this would likely require a two-thirds vote of the legislature and the amendment would have to \u201cfurther the purposes\u201d of Proposition 103. Assuming a two-thirds vote could be mustered, the issue would then be whether the amendment furthers the purposes of the Proposition. To the extent that liability moves from fault-based driving to defect-based products liability, the purposes underlying the mandatory rating factors and the GDD simply cannot be accomplished. Manufacturers will pass these costs through to automobile buyers free of the Proposition\u2019s restraints. Since the purposes of the Proposition, at least with respect to liability coverage,199 simply cannot be accomplished when dealing with self-driving cars, amending section 660(a) would not frustrate the purposes of Proposition 103.</p>\n<p>\u2026Filing a \u201ccomplete rate application with the commissioner\u201d is a substantial impediment to reducing rates. A complete rate application is an expensive, ponderous and time-consuming process. A typical filing may take three to five months before approval. Some applications have even been delayed for a year.205 In 2009, when insurers filed many new rate plans in order to comply with the new territorial rating regulations, delays among the top twenty private passenger auto insurers ranged from a low of 54 days (Viking) to a high of 558 days (USAA and USAA Casualty). Many took over 300 days (e.g., State Farm Mutual, Farmers Insurance Exchange, Progressive Choice).206 \u2026n addition, once an application to lower rates is filed, the Commissioner, consumer groups, and others can intervene and ask that the rates be lowered even further.207 Thus, an application to lower a rate by 6% may invite pressure to lower it even further.208 If they \u201csubstantially contributed, as a whole\u201d to the decision, a consumer group can also bill the insurance company for its legal, advocacy, and witness fees.209</p>\n<p>\u2026Unless ways can be found to conform Proposition 103 to this new reality, insurance for AVs is likely to migrate to a statutory and regulatory environment untrammeled by Proposition 103-commercial policies carried by manufacturers and suppliers. This migration presents its own set of problems. While the safety of AVs could be more fairly rated, other important rating factors, such as annual miles driven and territory, must be compromised. Whether this migration occurs will also depend on how liability rules do or do not adjust to a world in which people will nevertheless suffer injuries from AVs, but in which it is unlikely our present fault rules will adequately address compensation. If concepts of non-delegable duty, agency, or strict liability attach initial liability to owners of faulty cars with faultless drivers, the insurance burden will first be filtered through automobile insurance governed by Proposition 103. These insurers will then pass the losses up the distribution line to the insurers of suppliers and manufacturers that are not governed by Proposition 103. Manufacturers and suppliers will then pass the insurance cost back to AV owners in the cost of the vehicle. The insurance load reflected in the price of the car will pass through to automobile owners free of any of the restrictions imposed by Proposition 103. There will be no GDD, such as it is, no mandatory rating factors, and, depending on where the suppliers\u2019 or manufacturers\u2019 insurers are located, more flexible rating. One may ask: What is gained by this merry-go-round?</p>\n</blockquote>\n<p><a href=\"http://www.nesl.edu/userfiles/file/LawReview/Vol46/3/Garza%20FINAL.pdf\">\u201c\u2018Look Ma, No Hands!\u2019: Wrinkles and Wrecks in the Age of Autonomous Vehicles\u201d</a>, Garza 2012</p>\n<blockquote>\n<p>The benefits of these systems cannot be overestimated given that one-third of drivers admit to having fallen asleep at the wheel within the previous thirty days.31 \u2026If the driver fails to react in time, it applies 40% of the full braking power to reduce the severity of the collision.39 In the most advanced version, the CMBS performs all of the functions described above, and it will also stop the car automatically to avoid a collision when traveling under ten miles-per-hour.40 Car companies are hesitant to push the automatic braking threshold too far out of fear that \u201afully \u2018automatic\u2019 braking systems will shift the responsibility of avoiding an accident from the vehicle\u2019s driver to the vehicle\u2019s manufacturer.\u201941\u2026See Larry Carley, Active Safety Technology: Adaptive Cruise Control, Lane Departure Warning &amp; Collision Mitigation Braking, IMPORT CAR (June 16, 2009), http://www.import-car.com/Article/58867/active_safety_technology_adaptive_cruise_control_lane_departure_warning__collision_mitigation_braking.aspx</p>\n<p>\u2026Automobile products liability cases are typically divided into two categories: \u201a(1) accidents caused by automotive defects, and (2) aggravated injuries caused by a vehicle\u2019s failure to be sufficiently \u2018crashworthy\u2019 to protect its occupants in an accident.\u201879 \u2026For example, a car suffers from a design defect when a malfunction in the steering wheel causes a crash. 81 Additionally, plaintiffs have alleged and prevailed on manufacturing- defect claims in cases where \u201aunintended, sudden and uncontrollable acceleration\u2019 causes an accident.82 In such cases, plaintiffs have been able to recover under a \u201amalfunction theory.\u201983 Under a malfunction theory, plaintiffs use a \u201ares ipsa loquitur like inference to infer defectiveness in strict liability where there was no independent proof of a defect in the product.\u201984 Plaintiffs have also prevailed where design defects cause injury. 85 For example, there was a proliferation of litigation in the 1970s and 1980s as a result of vehicles that were designed with a high center of gravity, which increased their propensity to roll over.86 Additionally, many design-defect cases arose in response to faulty transmissions that could inadvertently slip into gear, causing crashes and occupants to be run over in some cases. 87 The two primary tests that courts use to assess the defectiveness of a product\u2019s design are the consumer-expectations test and the risk-utility test.88 The consumer-expectations test focuses on whether \u201athe danger posed by the design is greater than an ordinary consumer would expect when using the product in an intended or reasonably foreseeable manner.\u201989 \u2026Thus, while an ordinary consumer can have expectations that a car will not explode at a stoplight or catch fire in a two-mile-per-hour collision, they may not be able to have expectations about how a truck should handle after striking a five- or six-inch rock at thirty-five miles-per-hour.92 Perhaps because the consumer-expectations test is difficult to apply to complex products, and we live in a world where technological growth increases complexity, the risk-utility test has become the dominant test in design-defect cases.93 \u2026Litigation can also arise where a plaintiff alleges that a vehicle is not sufficiently \u201acrashworthy.\u2019104 Crashworthiness claims are a type of design- defect claim.105</p>\n<p>\u2026Since their advent and incorporation, seat belts have resulted in litigation-much of which has involved crashworthiness claims. 136 In Jackson v. General Motors Corp., for example, the plaintiff alleged that as a result of a defectively designed seat belt, his injuries were enhanced. 137 The defendant manufacturer argued that the complexity of seat belts foreclosed any consumer expectation,138 but the Tennessee Supreme Court noted that seat belts are \u2018familiar products for which consumers\u2019 expectations of safety have had an opportunity to develop,\u2019 and permitted the plaintiff to recover under the consumer-expectations test.139 Although manufacturers have been sued where seat belts render a car insufficiently crashworthy- as in cases where they fail to perform as intended or enhance injury-the incorporation of seat belts has reduced liability as well.140 This reduction comes in the form of the \u201aseat belt defense.\u2018141 The \u2019seat belt defense\u2019 allows a defendant to present evidence about an occupant\u2019s nonuse of a seat belt to mitigate damages or to defend against an enhanced-injury claim.142 Because seat belts are capable of reducing the number of lives lost and the overall severity of injuries sustained in crashes, it is argued that nonuse should protect a manufacturer from some claims.143 Although the majority rule is to prevent the admission of such evidence in enhanced-injury litigation, there is a growing trend toward admission.144</p>\n<p>\u2026Since their incorporation, consumers have sued manufacturers for defective cruise control systems that lead to injury. 171 Because of the complexity of cruise control technology, courts may not allow a plaintiff to use the consumer-expectations test.172 Despite the complexity of the technology, other courts allow plaintiffs to establish a defect using either the risk-utility test or the consumer-expectations test.173</p>\n<p>\u2026Under the consumer-expectations test, manufacturers will likely argue-as they historically have-that OAV technology is too complicated for the average consumer to have appropriate expectations about its capabilities.182 Commentators have stated that \u201aconsumers may have unrealistic expectations about the capabilities of these technologies . . . . Technologies that are engineered to assist the driver may be overly relied on to replace the need for independent vigilance on the part of the vehicle operator.\u2019183 Plaintiffs will argue that, while the workings of the technology are concededly complex, the overall concept of autonomous driving is not.184 Like the car exploding at a stoplight or the car that catches fire in a two- mile-per-hour collision, the average consumer would expect autonomous vehicles to drive themselves without incident.185 This means that components that are meant to keep the car within a lane will do just that, and others will stop the vehicle at traffic lights. 186 Where incidents occur, OAVs will not have performed as the average consumer would expect.187 \u2026plaintiffs who purchase OAVs at the cusp of availability, and attempt to prove defect under the consumer- expectations test, are likely to face an up-hill battle.194 But the unavailability of the consumer-expectations test will not be a significant detriment as plaintiffs can fall back on the risk-utility test.195 And as OAVs are increasingly incorporated, and users become more familiar with their capabilities, the consumer-expectations test will become more accessible to plaintiffs.196 Given the modern trend, plaintiffs are likely to face the risk- utility test.197</p>\n<p>\u2026Additionally, the extent to which injuries are \u201aenhanced\u2019 by OAVs will be debated.228 Because the majority of drivers fail to fully apply their brakes prior to a collision,229 where an OAV only partially applies brakes, or fails to apply brakes at all, manufacturers and plaintiffs will disagree about the extent of enhancement.230 Manufacturers will argue that, absent the OAV, the result would have been the same or worse-thus, the extent to which the injuries of the plaintiff are \u201aenhanced\u2019 is minimal.231 Plaintiffs will argue that, just like the presentation of crash statistics in a risk-utility analysis, this is a false choice.232 Like no-fire air bag claims, plaintiffs will contend that but for the malfunction of the OAV, their injuries would have been greatly reduced or nonexistent. 233 As a result, any injuries sustained above that threshold should serve as a basis for recovery. 234</p>\n<p>\u2026In products liability cases the \u2019use of expert witnesses has grown in both importance and expense.\u2019301 Because of the extraordinary cost of experts in products liability litigation, many plaintiffs are turned away because, even if they were to recover, the prospective award would not cover the expense of litigating the claim. 302</p>\n<p>\u2026Although complex, OAVs function much like the cruise control that exists in modern cars. As we have seen with seat belts, air bags, and cruise control, manufacturers have always been hesitant to adopt safety technologies. Despite concerns, products liability law is capable of handling OAVs just as it has these past technologies. While the novelty and complexity of OAVs are likely to preclude plaintiffs from proving defect under the consumer-expectation test, as implementation increases this likelihood may decrease. Under a risk-utility analysis, manufacturers will stress the extraordinary safety benefits of OAVs, while consumers will allege that designs can be improved. In the end, OAV adoption will benefit manufacturers. Although liability will fall on manufacturers when vehicles fail, decreased incidences and severity of crashes will result in a net decrease in liability. Further, the combination of LDWS cameras and EDRs will drastically reduce the cost of litigation. By reducing reliance on experts for complex causation determinations, both manufacturers and plaintiffs will benefit. In the end, obstacles to OAV implementation are more likely to be psychological than legal, and the sooner that courts, manufacturers, and the motoring public prepare to confront these issues, the sooner lives can be saved.</p>\n</blockquote>\n<p><a href=\"http://www.theverge.com/2012/12/14/3766218/self-driving-cars-google-volvo-law\">\u201cSelf-driving cars can navigate the road, but can they navigate the law? Google\u2019s lobbying hard for its self-driving technology, but some features may never be legal\u201d</a>, <em>The Verge</em> 14 December 2012</p>\n<blockquote>\n<p>Google says that on a given day, they have a dozen autonomous cars on the road. This August, they passed 300,000 driver-hours. In Spain this summer, Volvo drove a convoy of three cars through 200 kilometers of desert highway with just one driver and a police escort.</p>\n<p>\u2026Bryant Walker Smith teaches a class on autonomous vehicles at Stanford Law School. At a workshop this summer, he put forward <a href=\"https://cyberlaw.stanford.edu/blog/2012/07/self-driving-crash-test\">this thought experiment</a>: the year is 2020, and a number of companies offer \u201cadvanced driver assistance systems\u201d with their high-end model. Over 100,000 units have been sold. The owner\u2019s manual states that the driver must remain alert at all times, but one night a driver - we\u2019ll call him \u201cPaul\u201d - falls asleep while driving over a foggy bridge. The car tries to rouse him with alarms and vibrations but he\u2019s a deep sleeper, so the car turns on the hazard lights and pulls over to the side of the road where another driver (let\u2019s say Julie) rear-ends him. He\u2019s injured, angry, and prone to litigation. So is Julie. That would be tricky enough by itself, but then Smith starts layering on complications. Another model of auto-driver would have driven to the end of the bridge before pulling over. If Paul had updated his software, it would have braced his seatbelt for the crash, mitigating his injuries, but he didn\u2019t. The company could have pushed the update automatically, but management chose not to. Now, Smith asks the workshop, who gets sued? Or for a shorter list, who doesn\u2019t?</p>\n<p>\u2026The financial stakes are high. According to the Insurance Research Council, auto liability claims paid out roughly $215 for each insured car, between bodily injury and property damage claims. With 250 million cars on the road, that\u2019s $54 billion a year in liability. If even a tiny portion of those lawsuits are directed towards technologists, the business would become unprofitable fast.</p>\n<p>\u2026Changing the laws in Europe would take a replay of the internationally ratified Vienna Convention (passed in 1968) as well as pushing through a hodgepodge of national and regional laws. As Google proved, it\u2019s not impossible, but it leaves SARTRE facing an unusually tricky adoption problem. Lawmakers won\u2019t care about the project unless they think consumers really want it, but it\u2019s hard to get consumers excited about a product that doesn\u2019t exist yet. Projects like this usually rely on a core of early adopters to demonstrate their usefulness - a hard enough task, as most startups can tell you - but in this case, SARTRE has to bring auto regulators along for the ride. Optimistically, Volvo told us they expect the technology to be ready \u201ctowards the end of this decade,\u201d but that may depend entirely on how quickly the law moves. The less optimistic prediction is that it never arrives at all. Steve Shladover is the program manager of mobility at California\u2019s PATH program, where they\u2019ve been trying to make convoy technology happen for 25 years, lured by the prospect of fitting three times as many cars on the freeway. They were showing off a working version as early as 1997 (powered by a single Pentium processor), before falling into the same gap between prototype and final product. \u201cIt\u2019s a solvable problem once people can see the benefits,\u201d he told <em>The Verge</em>, \u201cbut I think a lot of the current activity is wildly optimistic in terms of what can be achieved.\u201d When I asked him when we\u2019d see a self-driving car, Shladover told me what he says at the many auto conferences he\u2019s been to: \u201cI don\u2019t expect to see the fully-automated, autonomous vehicle out on the road in the lifetime of anyone in this room.\u201d</p>\n<p>\u2026Many of Google\u2019s planned features may simply never be legal. One difficult feature is the \u201ccome pick me up\u201d button that Larry Page has pushed as a solution to parking congestion. Instead of wasting energy and space on urban parking lots, why not have cars drop us off and then drive themselves to park somewhere more remote, like an automated valet?It\u2019s a genuinely good idea, and one Google seems passionate about, but it\u2019s extremely difficult to square with most vehicle codes. The Geneva Convention on Road Traffic (1949) requires that drivers \u201cshall at all times be able to control their vehicles,\u201d and provisions against reckless driving usually require \u201cthe conscious and intentional operation of a motor vehicle.\u201d Some of that is simple semantics, but other concerns are harder to dismiss. After a crash, drivers are legally obligated to stop and help the injured - a difficult task if there\u2019s no one in the car. As a result, most experts predict drivers will be legally required to have a person in the car at all times, ready to take over if the automatic system fails. If they\u2019re right, the self-parking car may never be legal.</p>\n</blockquote>\n<p><a href=\"http://cyberlaw.stanford.edu/files/publication/files/2012-Smith-AutomatedVehiclesAreProbablyLegalinTheUS_0.pdf\">\u201cAutomated Vehicles are Probably Legal in the United States\u201d</a>, Bryant Walker Smith 2012</p>\n<blockquote>\n<p>The short answer is that the computer direction of a motor vehicle\u2019s steering, braking, and accelerating without real-time human input is probably legal\u2026.The paper\u2019s largely descriptive analysis, which begins with the principle that everything is permitted unless prohibited, covers three key legal regimes: the 1949 Geneva Convention on Road Traffic, regulations enacted by the National Highway Traffic Safety Administration (NHTSA), and the vehicle codes of all fifty US states.</p>\n<p>The Geneva Convention, to which the United States is a party, probably does not prohibit automated driving. The treaty promotes road safety by establishing uniform rules, one of which requires every vehicle or combination thereof to have a driver who is \u201cat all times \u2026 able to control\u201d it. However, this requirement is likely satisfied if a human is able to intervene in the automated vehicle\u2019s operation.</p>\n<p>NHTSA\u2019s regulations, which include the Federal Motor Vehicle Safety Standards to which new vehicles must be certified, do not generally prohibit or uniquely burden automated vehicles, with the possible exception of one rule regarding emergency flashers. State vehicle codes probably do not prohibit-but may complicate-automated driving. These codes assume the presence of licensed human drivers who are able to exercise human judgment, and particular rules may functionally require that presence. New York somewhat uniquely directs a driver to keep one hand on the wheel at all times. In addition, far more common rules mandating reasonable, prudent, practicable, and safe driving have uncertain application to automated vehicles and their users. Following distance requirements may also restrict the lawful operation of tightly spaced vehicle platoons. Many of these issues arise even in the three states that expressly regulate automated vehicles.</p>\n<p>\u2026This paper does not consider how the rules of tort could or should apply to automated vehicles-that is, the extent to which tort liability might shift upstream to companies responsible for the design, manufacture, sale, operation, or provision of data or other services to an automated vehicle. 6</p>\n<p>\u2026Because of the broad way in which the term and others like it are defined, an automated vehicle probably has a human \u201cdriver.\u201d 295 Obligations imposed on that person may limit the independence with which the vehicle may lawfully operate. 296 In addition, the automated vehicle itself must meet numerous requirements, some of which may also complicate its operation. 297 Although three states have expressly established the legality of automated vehicles under certain conditions, their respective laws do not resolve many of the questions raised in this section. 298</p>\n<p>\u2026A brief but important aside: To varying degrees, states impose criminal or quasicriminal liability on owners who permit others to drive their vehicles. 359 In Washington, \u201c[b]oth a person operating a vehicle with the express or implied permission of the owner and the owner of the vehicle are responsible for any act or omission that is declared unlawful in this chapter. The primary responsibility is the owner\u2019s.\u201d 360 Some states permit an inference that the owner of a vehicle was its operator for certain offenses; 361 Wisconsin provides what is by far the most detailed statutory set of rebuttable presumptions. 362 Many others punish owners who knowingly permit their vehicles to be driven unlawfully. 363 Although these owners are not drivers, they are assumed to exercise some judgment or control with respect to those drivers-an instance of vicarious liability that suggests an owner of an automated vehicle might be liable for merely permitting its automated operation. 364</p>\n<p>\u2026On the human side, physical presence would likely continue to provide a proxy for or presumption of driving. 366 In other words, an individual who is physically positioned to provide real-time input to a motor vehicle may well be treated as its driver. This is particularly likely at levels of automation that involve human input for certain portions of a trip. In addition, an individual who starts or dispatches an automated vehicle, who initiates the automated operation of that vehicle, or who specifies certain parameters of operation probably qualifies as a driver under existing law. That individual may use some device-anything from a physical key to the click of a mouse to the sound of her voice-to activate the vehicle by herself. She may likewise deliberately request that the vehicle assume the active driving task. And she may set the vehicle\u2019s maximum speed or level of assertiveness. This working definition is unclear in the same ways that existing law is likely to be unclear. Relevant acts might occur at any level of the primary driving task, from a decision to take a particular trip to a decision to exceed any speed limit by ten miles per hour. 367 A tactical decision like speeding is closely connected with the consequences-whether a moving violation or an injury-that may result. But treating an individual who dispatches her fully automated vehicle as the driver for the entirety of the trip could attenuate the relationship between legal responsibility and legal fault. 368 Nonetheless, strict liability of this sort is accepted within tort law 369 and present, however controversially, in US criminal law. 370</p>\n<p>On the corporate side, a firm that designs or supplies a vehicle\u2019s automated functionality or that provides data or other digital services might qualify as a driver under existing law. The key element, as provided in the working definition, may be the lack of a human intermediary: A human who provides some input may still seem a better fit for a human-centered vehicle code than a company with other relevant legal exposure. However, as noted above, public outrage is another element that may motivate new uses of existing laws. 377</p>\n<p>\u2026The mechanism by which someone other than a human would obtain a driving license is unclear. For example, some companies may possess great vision, but \u201ca test of the applicant\u2019s eyesight\u201d may nonetheless be difficult. 395 And while General Motors may (or may not) 396 meet a state\u2019s minimum age requirement, Google would not. [See Google, Google\u2019s mission is to organize the world\u2019s information and make it universally accessible and useful, www.google.com/intl/en/about/company/. In some states, Google might be allowed to drive itself to school. See, e.g., Nev. Rev.&nbsp;Stat. \u00a7 483.270; Nev. Admin. Code \u00a7 483.200.]</p>\n</blockquote>\n<p>And people say lawyers have no sense of humor.</p>", "sections": [{"title": "0.1 Self-driving cars", "anchor": "0_1_Self_driving_cars", "level": 1}, {"title": "0.1.1 Lobbying, Liability, and Insurance", "anchor": "0_1_1_Lobbying__Liability__and_Insurance", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "140 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 140, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ktr39MFWpTqmzuKxQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-24T04:51:22.959Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR Discussion, chapters 30-33", "slug": "meetup-durham-hpmor-discussion-chapters-30-33", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rjKvrZZ5SJd4S8riz/meetup-durham-hpmor-discussion-chapters-30-33", "pageUrlRelative": "/posts/rjKvrZZ5SJd4S8riz/meetup-durham-hpmor-discussion-chapters-30-33", "linkUrl": "https://www.lesswrong.com/posts/rjKvrZZ5SJd4S8riz/meetup-durham-hpmor-discussion-chapters-30-33", "postedAtFormatted": "Thursday, January 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2030-33&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2030-33%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrjKvrZZ5SJd4S8riz%2Fmeetup-durham-hpmor-discussion-chapters-30-33%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2030-33%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrjKvrZZ5SJd4S8riz%2Fmeetup-durham-hpmor-discussion-chapters-30-33", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrjKvrZZ5SJd4S8riz%2Fmeetup-durham-hpmor-discussion-chapters-30-33", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/id'>Durham HPMoR Discussion, chapters 30-33</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 January 2013 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Foster's Market, 2694 Durham-Chapel Hill Blvd., Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our regular HPMoR reading meetup. Please feel free to come, even if you haven't done all (or any) of the reading. We'll summarize and discuss the chapters and other relevant (or perhaps not-quite-so-relevant) topics.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/id'>Durham HPMoR Discussion, chapters 30-33</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rjKvrZZ5SJd4S8riz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.0930036145929855e-06, "legacy": true, "legacyId": "21314", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_30_33\">Discussion article for the meetup : <a href=\"/meetups/id\">Durham HPMoR Discussion, chapters 30-33</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 January 2013 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Foster's Market, 2694 Durham-Chapel Hill Blvd., Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our regular HPMoR reading meetup. Please feel free to come, even if you haven't done all (or any) of the reading. We'll summarize and discuss the chapters and other relevant (or perhaps not-quite-so-relevant) topics.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_30_331\">Discussion article for the meetup : <a href=\"/meetups/id\">Durham HPMoR Discussion, chapters 30-33</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 30-33", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_30_33", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 30-33", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_30_331", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-24T16:49:09.586Z", "modifiedAt": null, "url": null, "title": "In the beginning, Dartmouth created the AI and the hype", "slug": "in-the-beginning-dartmouth-created-the-ai-and-the-hype", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.304Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZEZwWsPHGo6R88WuH/in-the-beginning-dartmouth-created-the-ai-and-the-hype", "pageUrlRelative": "/posts/ZEZwWsPHGo6R88WuH/in-the-beginning-dartmouth-created-the-ai-and-the-hype", "linkUrl": "https://www.lesswrong.com/posts/ZEZwWsPHGo6R88WuH/in-the-beginning-dartmouth-created-the-ai-and-the-hype", "postedAtFormatted": "Thursday, January 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20the%20beginning%2C%20Dartmouth%20created%20the%20AI%20and%20the%20hype&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20the%20beginning%2C%20Dartmouth%20created%20the%20AI%20and%20the%20hype%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZEZwWsPHGo6R88WuH%2Fin-the-beginning-dartmouth-created-the-ai-and-the-hype%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20the%20beginning%2C%20Dartmouth%20created%20the%20AI%20and%20the%20hype%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZEZwWsPHGo6R88WuH%2Fin-the-beginning-dartmouth-created-the-ai-and-the-hype", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZEZwWsPHGo6R88WuH%2Fin-the-beginning-dartmouth-created-the-ai-and-the-hype", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>I've just been through the proposal for the Dartmouth AI conference of 1956, and it's a surprising read. All I really knew about it was its absurd optimism, as typified by the quote:</p>\n<p style=\"padding-left: 30px;\">An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.</p>\n<p>But then I read the rest of the document, and was... impressed. Go ahead and <a href=\"http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html\">read it</a>, and give me your thoughts. Given what was known in 1955, they were grappling with the right issues, and seemed to be making progress in the right directions and have plans and models for how to progress further. Seeing the&nbsp;phenomenally&nbsp;smart people who were behind this (McCarthy, Minsky, Rochester, Shannon), and given the impressive progress that computers had been making in what seemed very hard areas of cognition (remember that this was before we discovered&nbsp;<a href=\"http://en.wikipedia.org/wiki/Moravec's_paradox\">Moravec's paradox</a>)... I have to say that had I read this back in 1955, I think the rational belief would have been \"AI is probably imminent\". Some overconfidence, no doubt, but no good reason to expect these prominent thinkers to be so spectacularly wrong on something they were experts in.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bY5MaF2EATwDkomvu": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZEZwWsPHGo6R88WuH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 33, "extendedScore": null, "score": 1.093445065680071e-06, "legacy": true, "legacyId": "21318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-24T19:12:09.466Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver rationality practice case: Evaluating Transhumanism", "slug": "meetup-vancouver-rationality-practice-case-evaluating", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:26.334Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jqQ4TLQk9wuwkGze2/meetup-vancouver-rationality-practice-case-evaluating", "pageUrlRelative": "/posts/jqQ4TLQk9wuwkGze2/meetup-vancouver-rationality-practice-case-evaluating", "linkUrl": "https://www.lesswrong.com/posts/jqQ4TLQk9wuwkGze2/meetup-vancouver-rationality-practice-case-evaluating", "postedAtFormatted": "Thursday, January 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20rationality%20practice%20case%3A%20Evaluating%20Transhumanism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20rationality%20practice%20case%3A%20Evaluating%20Transhumanism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqQ4TLQk9wuwkGze2%2Fmeetup-vancouver-rationality-practice-case-evaluating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20rationality%20practice%20case%3A%20Evaluating%20Transhumanism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqQ4TLQk9wuwkGze2%2Fmeetup-vancouver-rationality-practice-case-evaluating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqQ4TLQk9wuwkGze2%2Fmeetup-vancouver-rationality-practice-case-evaluating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ie'>Vancouver rationality practice case: Evaluating Transhumanism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 January 2013 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 W broadway, vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Last week, someone brought up the question of whether transhumanism was true. Our meetup regulars are mostly transhumanists or very familiar with it, but we totally fumbled the question and didn't get anything like conclusive answers or even enlightening discussion. I pronounce that to be a rationality fail.</p>\n\n<p>Therefor, this week we will be applying all the rationalist techniques from the sequences to answer the question \"is transhumanism true?\". The focus will mostly be on the application of the techniques and ideas, but I also expect some good discussion and clarification on actual transhumanism.</p>\n\n<p>So come on out for some concrete application of LW ideas! We will meet at Benny's Bagels in Kitsilano at 13:00 on Saturday. As usual, join us on the <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>See you there!</p>\n\n<p>(Yes I know that the question is meaningless. That's part of the excercise. Don't work on the problem/dump spoilers in the comments please.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ie'>Vancouver rationality practice case: Evaluating Transhumanism</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jqQ4TLQk9wuwkGze2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.093533050764006e-06, "legacy": true, "legacyId": "21319", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_rationality_practice_case__Evaluating_Transhumanism\">Discussion article for the meetup : <a href=\"/meetups/ie\">Vancouver rationality practice case: Evaluating Transhumanism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 January 2013 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 W broadway, vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Last week, someone brought up the question of whether transhumanism was true. Our meetup regulars are mostly transhumanists or very familiar with it, but we totally fumbled the question and didn't get anything like conclusive answers or even enlightening discussion. I pronounce that to be a rationality fail.</p>\n\n<p>Therefor, this week we will be applying all the rationalist techniques from the sequences to answer the question \"is transhumanism true?\". The focus will mostly be on the application of the techniques and ideas, but I also expect some good discussion and clarification on actual transhumanism.</p>\n\n<p>So come on out for some concrete application of LW ideas! We will meet at Benny's Bagels in Kitsilano at 13:00 on Saturday. As usual, join us on the <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>See you there!</p>\n\n<p>(Yes I know that the question is meaningless. That's part of the excercise. Don't work on the problem/dump spoilers in the comments please.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_rationality_practice_case__Evaluating_Transhumanism1\">Discussion article for the meetup : <a href=\"/meetups/ie\">Vancouver rationality practice case: Evaluating Transhumanism</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver rationality practice case: Evaluating Transhumanism", "anchor": "Discussion_article_for_the_meetup___Vancouver_rationality_practice_case__Evaluating_Transhumanism", "level": 1}, {"title": "Discussion article for the meetup : Vancouver rationality practice case: Evaluating Transhumanism", "anchor": "Discussion_article_for_the_meetup___Vancouver_rationality_practice_case__Evaluating_Transhumanism1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-25T02:07:20.042Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal LessWrong - The Future is Awesome", "slug": "meetup-montreal-lesswrong-the-future-is-awesome", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EZEgj8vjAsEE8HgvN/meetup-montreal-lesswrong-the-future-is-awesome", "pageUrlRelative": "/posts/EZEgj8vjAsEE8HgvN/meetup-montreal-lesswrong-the-future-is-awesome", "linkUrl": "https://www.lesswrong.com/posts/EZEgj8vjAsEE8HgvN/meetup-montreal-lesswrong-the-future-is-awesome", "postedAtFormatted": "Friday, January 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20LessWrong%20-%20The%20Future%20is%20Awesome&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20LessWrong%20-%20The%20Future%20is%20Awesome%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEZEgj8vjAsEE8HgvN%2Fmeetup-montreal-lesswrong-the-future-is-awesome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20LessWrong%20-%20The%20Future%20is%20Awesome%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEZEgj8vjAsEE8HgvN%2Fmeetup-montreal-lesswrong-the-future-is-awesome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEZEgj8vjAsEE8HgvN%2Fmeetup-montreal-lesswrong-the-future-is-awesome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/if'>Montreal LessWrong - The Future is Awesome</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 January 2013 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">655 Avenue du Pr\u00e9sident Kennedy, Montr\u00e9al, QC </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Bringing in articles and discussing the progress of technology that remains hidden from our normal lives.</p>\n\n<p>It's good to see what sort of progress is being made, and where the future will likely take us!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/if'>Montreal LessWrong - The Future is Awesome</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EZEgj8vjAsEE8HgvN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.0937885751371473e-06, "legacy": true, "legacyId": "21320", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_LessWrong___The_Future_is_Awesome\">Discussion article for the meetup : <a href=\"/meetups/if\">Montreal LessWrong - The Future is Awesome</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 January 2013 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">655 Avenue du Pr\u00e9sident Kennedy, Montr\u00e9al, QC </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Bringing in articles and discussing the progress of technology that remains hidden from our normal lives.</p>\n\n<p>It's good to see what sort of progress is being made, and where the future will likely take us!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_LessWrong___The_Future_is_Awesome1\">Discussion article for the meetup : <a href=\"/meetups/if\">Montreal LessWrong - The Future is Awesome</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal LessWrong - The Future is Awesome", "anchor": "Discussion_article_for_the_meetup___Montreal_LessWrong___The_Future_is_Awesome", "level": 1}, {"title": "Discussion article for the meetup : Montreal LessWrong - The Future is Awesome", "anchor": "Discussion_article_for_the_meetup___Montreal_LessWrong___The_Future_is_Awesome1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-25T03:49:18.976Z", "modifiedAt": null, "url": null, "title": "The Hidden B.I.A.S.", "slug": "the-hidden-b-i-a-s", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:04.117Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MaoShan", "createdAt": "2010-08-08T15:16:25.180Z", "isAdmin": false, "displayName": "MaoShan"}, "userId": "HW6b3PceS2S9vLCvP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HQE6jSswbP8M6Rfky/the-hidden-b-i-a-s", "pageUrlRelative": "/posts/HQE6jSswbP8M6Rfky/the-hidden-b-i-a-s", "linkUrl": "https://www.lesswrong.com/posts/HQE6jSswbP8M6Rfky/the-hidden-b-i-a-s", "postedAtFormatted": "Friday, January 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Hidden%20B.I.A.S.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Hidden%20B.I.A.S.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHQE6jSswbP8M6Rfky%2Fthe-hidden-b-i-a-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Hidden%20B.I.A.S.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHQE6jSswbP8M6Rfky%2Fthe-hidden-b-i-a-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHQE6jSswbP8M6Rfky%2Fthe-hidden-b-i-a-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 895, "htmlBody": "<p>It would be a stretch to call this an article, but the answers that can be addressed by the questions it poses are potentially far-reaching with regard to revealing possible reasoning flaws, either in my own philosophy, or perhaps even yours. The flaws under my suspicion are caused by the modularity of the brain's systems, and the ability to hold to conflicting beliefs when they are not held directly against one another.</p>\n<p>These particular ones escape notice, I think, because they tend to only be given reflection in specific situations; my thought experiment here should help to hold them near each other.</p>\n<p>The Setup: Julian finds himself in the waiting-room of the Speedy-dupe office. Beyond that waiting room are three isolated rooms (P, Q, and R). Anyone who walks into Room P, which contains the Speedy-dupe device, will be scanned down to the most exact level imaginable, causing them to lose consciousness. Anyone who has used the Speedy-dupe will remember everything up until the point they entered the waiting-room, and begin forming new memories within seconds after regaining consciousness.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Situation 1:</p>\n<p style=\"margin-bottom: 0in;\">If Julian walks into Room P, and the Speedy-dupe runs, and then Julian walks out of Room P, and also another Julian walks out of Room Q, which is the \"original\" Julian? What makes Julian-P more original than Julian-Q?</p>\n<p>Possible Answers 1:</p>\n<p style=\"margin-bottom: 0in;\">You probably would say that Julian-P is the original Julian, due to your prior beliefs regarding causality--but how many times have you encountered the Speedy-dupe? For all we know, the person who walks into Room P is vaporized after scanning, and duplicated in Room P and in Room Q. If you still feel that Julian-P is the original, ask yourself what other reason do you have for the way you feel? What is it that you aren't mentioning?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Situation 2:</p>\n<p style=\"margin-bottom: 0in;\">If Julian walks into Room P, runs the Speedy-dupe, and Julian walks out of Rooms Q and R, but not out of Room P, which is the original Julian? Why not?</p>\n<p>Possible Answers 2:</p>\n<p style=\"margin-bottom: 0in;\">You might be saying to yourself, \"Ah, now, you can't trick me. Neither of them is the original!\" If they are both practically identical copies of the original Julian, what now stops you from identifying the original Julian with his identical copies? Are legal property issues really the only thing stopping you from modifying your views on identity?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Situation 3:</p>\n<p style=\"margin-bottom: 0in;\">But what about if Julian walks into Room P, is scanned by the Speedy-dupe, and walks out of Room P ten years later? Does that mean it is the \"original\" Julian?</p>\n<p>Possible Answers 3:</p>\n<p style=\"margin-bottom: 0in;\">Getting increasingly annoyed or bored with these questions, you might retort, \"I see what you're doing, and it's not going to work. You are obviously anti-cryonics, but you are wrong here. Cryonics in some way preserves the original material, but your Speedy-dupe vaporizes it. The copy which emerges ten years later is not a direct continuation of the original physical material.\"</p>\n<p style=\"margin-bottom: 0in;\">Based on what we've already thought about here, is continuation of the original physical material the important thing that counts toward  your identifying with your future post-cryonic-revival self? If so, why? If the pattern is recreated precisely (or even well enough) at a temporal or spacial distance from the original, what is actually different between Speedy-dupe and Cryonics?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">My Suspicion:</p>\n<p style=\"margin-bottom: 0in;\">If you answered on a completely different track than the Possible Answers did, just ignore me for now (if you have not already done so). I think that what is lurking beneath most of these typical objections or feelings is actually B.I.A.S.--Belief In A Soul. Despite all scientific evidence, a part of you still believes that each person has some special little spark that goes on after death, that is ultimately the thing that makes you who you are.</p>\n<ul>\n<li>Not that the personality that you have has taken your entire life to be shaped by genetics and life experiences imprinted on the blob of cells that eventually grew complicated enough to handle who you are now; but an invisible special material woven by a loving creator, just right for what you were destined to become.</li>\n<li>Not that when your body stops, it stops, and that process that you called life is over, whether that filigree of frozen carbon is forced to move a century from now or not; but that the unique thing that is hidden inside of you now will just hang around and gladly jump back in a century from now.</li>\n<li>Not that your partner could love your clone and never know the difference, or even just leave you and wind up with someone strikingly similar; but that your two souls were destined to love one another for all eternity.</li>\n</ul>\n<p>It's easy to gloss over all those things, but just because everyone would like it to be that way, doesn't make it true. If I am clearly Wrong, tell me why I am Wrong, in order that I may be Less so. If not, I hope that this has helped you in Overcoming B.I.A.S.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Credits: The original function and name of the Speedy-dupe come from <a title=\"The Duplicate\" href=\"http://en.wikipedia.org/wiki/The_Duplicate\" target=\"_blank\">The Duplicate</a>, a story by William Sleator, my favorite childhood author. (Many of his books combine normal childhood problems with mind-bending philosophical and physical concepts not normally found in youth literature.)</p>\n<p style=\"margin-bottom: 0in;\">The idea for the multiple rooms came from the episode \"The Girl Who Waited\" from Doctor Who.</p>\n<p style=\"margin-bottom: 0in;\">Any other content, if objectionable, can simply be considered personal mind-spew.</p>\n<p style=\"margin-bottom: 0in;\">Enjoy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HQE6jSswbP8M6Rfky", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -8, "extendedScore": null, "score": 1.0938513571347647e-06, "legacy": true, "legacyId": "21327", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-25T06:05:27.500Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: Reality and Us", "slug": "meetup-moscow-reality-and-us", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.754Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oy7WHgPz9kRrEA2hd/meetup-moscow-reality-and-us", "pageUrlRelative": "/posts/oy7WHgPz9kRrEA2hd/meetup-moscow-reality-and-us", "linkUrl": "https://www.lesswrong.com/posts/oy7WHgPz9kRrEA2hd/meetup-moscow-reality-and-us", "postedAtFormatted": "Friday, January 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20Reality%20and%20Us&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20Reality%20and%20Us%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foy7WHgPz9kRrEA2hd%2Fmeetup-moscow-reality-and-us%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20Reality%20and%20Us%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foy7WHgPz9kRrEA2hd%2Fmeetup-moscow-reality-and-us", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foy7WHgPz9kRrEA2hd%2Fmeetup-moscow-reality-and-us", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ig'>Moscow: Reality and Us</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 February 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second door with the sign \u201cYandex Money\u201d in Russian. I will be there at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Challenge each other's beliefs.</p></li>\n<li><p>Plan our group's short-term and medium-term activities.</p></li>\n<li><p>Find ways to optimize our daily life.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Report from the last session can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ig'>Moscow: Reality and Us</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oy7WHgPz9kRrEA2hd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 1.0939351785572091e-06, "legacy": true, "legacyId": "21330", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Reality_and_Us\">Discussion article for the meetup : <a href=\"/meetups/ig\">Moscow: Reality and Us</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 February 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second door with the sign \u201cYandex Money\u201d in Russian. I will be there at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Challenge each other's beliefs.</p></li>\n<li><p>Plan our group's short-term and medium-term activities.</p></li>\n<li><p>Find ways to optimize our daily life.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Report from the last session can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Reality_and_Us1\">Discussion article for the meetup : <a href=\"/meetups/ig\">Moscow: Reality and Us</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: Reality and Us", "anchor": "Discussion_article_for_the_meetup___Moscow__Reality_and_Us", "level": 1}, {"title": "Discussion article for the meetup : Moscow: Reality and Us", "anchor": "Discussion_article_for_the_meetup___Moscow__Reality_and_Us1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-25T15:30:14.852Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Buffalo, Cambridge MA, Dublin, Montreal, Moscow, Ohio, Purdue, Washington DC", "slug": "weekly-lw-meetups-austin-buffalo-cambridge-ma-dublin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.752Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7kSif7Zjv65FQi6zk/weekly-lw-meetups-austin-buffalo-cambridge-ma-dublin", "pageUrlRelative": "/posts/7kSif7Zjv65FQi6zk/weekly-lw-meetups-austin-buffalo-cambridge-ma-dublin", "linkUrl": "https://www.lesswrong.com/posts/7kSif7Zjv65FQi6zk/weekly-lw-meetups-austin-buffalo-cambridge-ma-dublin", "postedAtFormatted": "Friday, January 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Buffalo%2C%20Cambridge%20MA%2C%20Dublin%2C%20Montreal%2C%20Moscow%2C%20Ohio%2C%20Purdue%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Buffalo%2C%20Cambridge%20MA%2C%20Dublin%2C%20Montreal%2C%20Moscow%2C%20Ohio%2C%20Purdue%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7kSif7Zjv65FQi6zk%2Fweekly-lw-meetups-austin-buffalo-cambridge-ma-dublin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Buffalo%2C%20Cambridge%20MA%2C%20Dublin%2C%20Montreal%2C%20Moscow%2C%20Ohio%2C%20Purdue%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7kSif7Zjv65FQi6zk%2Fweekly-lw-meetups-austin-buffalo-cambridge-ma-dublin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7kSif7Zjv65FQi6zk%2Fweekly-lw-meetups-austin-buffalo-cambridge-ma-dublin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 518, "htmlBody": "<p><strong>This summary was posted to LW main on January 18th. The following week's summary&nbsp; is <a href=\"/lw/ggq/weekly_lw_meetups_austin_berlin_durham_london/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/hx\">Second Purdue Meetup:&nbsp;<span class=\"date\">18 January 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/hw\">Less Wrong Dublin:&nbsp;<span class=\"date\">19 January 2013 03:30PM</span></a></li>\n<li><a href=\"/meetups/hp\">Moscow: Applied Rationality:&nbsp;<span class=\"date\">19 January 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/hu\">Washington DC fun and games meetup:&nbsp;<span class=\"date\">20 January 2013 05:23PM</span></a></li>\n<li><a href=\"/meetups/i0\">LessWrong Montreal - Social Resilience:&nbsp;<span class=\"date\">21 January 2013 06:30PM</span></a></li>\n<li><a href=\"/meetups/hy\">Buffalo Meetup:&nbsp;<span class=\"date\">24 January 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/i5\">Berlin social meetup:&nbsp;<span class=\"date\">26 January 2013 05:00PM</span></a></li>\n<li><a href=\"/meetups/i2\">London Meetup 27th Jan:&nbsp;<span class=\"date\">27 January 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ht\">Brussels meetup:&nbsp;<span class=\"date\">16 February 2013 01:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">19 January 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/i3\">Ohio LessWrong in Cincinnati:&nbsp;<span class=\"date\">20 January 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/hv\">Cambridge, MA third-Sunday meetup:&nbsp;<span class=\"date\">20 January 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/i4\">Melbourne, practical rationality:&nbsp;<span class=\"date\">01 February 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7kSif7Zjv65FQi6zk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.094283035744553e-06, "legacy": true, "legacyId": "21229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4akb5XoxzMhtEf6zP", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-25T15:31:38.944Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Revealed New Years Resolutions", "slug": "meetup-washington-dc-revealed-new-years-resolutions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.749Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TbbNra3tEhJu8bwsY/meetup-washington-dc-revealed-new-years-resolutions", "pageUrlRelative": "/posts/TbbNra3tEhJu8bwsY/meetup-washington-dc-revealed-new-years-resolutions", "linkUrl": "https://www.lesswrong.com/posts/TbbNra3tEhJu8bwsY/meetup-washington-dc-revealed-new-years-resolutions", "postedAtFormatted": "Friday, January 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Revealed%20New%20Years%20Resolutions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Revealed%20New%20Years%20Resolutions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTbbNra3tEhJu8bwsY%2Fmeetup-washington-dc-revealed-new-years-resolutions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Revealed%20New%20Years%20Resolutions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTbbNra3tEhJu8bwsY%2Fmeetup-washington-dc-revealed-new-years-resolutions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTbbNra3tEhJu8bwsY%2Fmeetup-washington-dc-revealed-new-years-resolutions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ih'>Washington DC Revealed New Years Resolutions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 January 2013 03:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">washington dc portrait gallary</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>At this meetup, we'll be looking at what we did last year, and trying to derrive our new years resolutions of last year based on what we did.</p>\n\n<p>We'll be meeting in the courtyard.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ih'>Washington DC Revealed New Years Resolutions</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TbbNra3tEhJu8bwsY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.0942838991808508e-06, "legacy": true, "legacyId": "21339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Revealed_New_Years_Resolutions\">Discussion article for the meetup : <a href=\"/meetups/ih\">Washington DC Revealed New Years Resolutions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 January 2013 03:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">washington dc portrait gallary</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>At this meetup, we'll be looking at what we did last year, and trying to derrive our new years resolutions of last year based on what we did.</p>\n\n<p>We'll be meeting in the courtyard.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Revealed_New_Years_Resolutions1\">Discussion article for the meetup : <a href=\"/meetups/ih\">Washington DC Revealed New Years Resolutions</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Revealed New Years Resolutions", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Revealed_New_Years_Resolutions", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Revealed New Years Resolutions", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Revealed_New_Years_Resolutions1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-26T03:03:20.330Z", "modifiedAt": "2020-04-29T20:35:11.164Z", "url": null, "title": "Best of Rationality Quotes, 2012 Edition", "slug": "best-of-rationality-quotes-2012-edition", "viewCount": null, "lastCommentedAt": "2013-08-20T16:16:21.304Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielVarga", "createdAt": "2009-09-16T22:21:30.125Z", "isAdmin": false, "displayName": "DanielVarga"}, "userId": "rqE4DaRxHwBpQXj96", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zaWnu3PxP4YTYiuCm/best-of-rationality-quotes-2012-edition", "pageUrlRelative": "/posts/zaWnu3PxP4YTYiuCm/best-of-rationality-quotes-2012-edition", "linkUrl": "https://www.lesswrong.com/posts/zaWnu3PxP4YTYiuCm/best-of-rationality-quotes-2012-edition", "postedAtFormatted": "Saturday, January 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Best%20of%20Rationality%20Quotes%2C%202012%20Edition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABest%20of%20Rationality%20Quotes%2C%202012%20Edition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzaWnu3PxP4YTYiuCm%2Fbest-of-rationality-quotes-2012-edition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Best%20of%20Rationality%20Quotes%2C%202012%20Edition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzaWnu3PxP4YTYiuCm%2Fbest-of-rationality-quotes-2012-edition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzaWnu3PxP4YTYiuCm%2Fbest-of-rationality-quotes-2012-edition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>I finished creating the 2012 edition of the Best of Rationality Quotes collection. (<a href=\"/lw/3cn/best_of_rationality_quotes_20092010/\">Here is last year's</a>.)</p>\n<p><a href=\"http://people.mokk.bme.hu/~daniel/rationality_quotes_2012/rq_only2012.html\"><strong>Best of Rationality Quotes 2012</strong></a> (500kB page, 434 quotes)<br /> and <a href=\"http://people.mokk.bme.hu/~daniel/rationality_quotes_2012/rq.html\"><strong>Best of Rationality Quotes 2009-2012</strong></a> (1200kB page, 1140 quotes)</p>\n<p>The page was built by a short script (<a href=\"http://people.mokk.bme.hu/~daniel/rationality_quotes_2012/\">source code here</a>) from all the LW Rationality Quotes threads so far. (We had such a thread each month since April 2009.) The script collects all comments with karma score 10 or more, and sorts them by score. Replies are not collected, only top-level comments.</p>\n<p>As is now usual, I provide various statistics and top-lists based on the data. (Source code for these is also at the above link, see the README.) I added these as comments to the post:</p>\n<ul>\n<li><a href=\"/lw/ggp/best_of_rationality_quotes_2012_edition/8cgh\">Top quote contributors by total karma score collected</a></li>\n<li><a href=\"/lw/ggp/best_of_rationality_quotes_2012_edition/8cgj\">Top quote contributors by karma score collected in 2012</a></li>\n<li><a href=\"/lw/ggp/best_of_rationality_quotes_2012_edition/8cgk\">Top quote contributors by statistical significance level</a> (See <a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37ej\">this comment</a> for a description of this metric.)</li>\n<li><a href=\"/lw/ggp/best_of_rationality_quotes_2012_edition/8cgl\">Top original authors by number of quotes</a></li>\n<li><a href=\"/lw/ggp/best_of_rationality_quotes_2012_edition/8cgm\">Top original authors by total karma score collected</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zaWnu3PxP4YTYiuCm", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 46, "extendedScore": null, "score": 1.094710182490396e-06, "legacy": true, "legacyId": "21337", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZFMqBSX8CnpAwmWes"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-01-26T03:03:20.330Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-26T03:04:52.269Z", "modifiedAt": null, "url": null, "title": "Infinitesimals: Another argument against actual infinite sets", "slug": "infinitesimals-another-argument-against-actual-infinite-sets", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:29.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "common_law", "createdAt": "2012-07-03T06:17:42.167Z", "isAdmin": false, "displayName": "common_law"}, "userId": "PJzXMdPcBFR6cpXPY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x4KyAQ5nXnbTvhsmZ/infinitesimals-another-argument-against-actual-infinite-sets", "pageUrlRelative": "/posts/x4KyAQ5nXnbTvhsmZ/infinitesimals-another-argument-against-actual-infinite-sets", "linkUrl": "https://www.lesswrong.com/posts/x4KyAQ5nXnbTvhsmZ/infinitesimals-another-argument-against-actual-infinite-sets", "postedAtFormatted": "Saturday, January 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Infinitesimals%3A%20Another%20argument%20against%20actual%20infinite%20sets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInfinitesimals%3A%20Another%20argument%20against%20actual%20infinite%20sets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4KyAQ5nXnbTvhsmZ%2Finfinitesimals-another-argument-against-actual-infinite-sets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Infinitesimals%3A%20Another%20argument%20against%20actual%20infinite%20sets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4KyAQ5nXnbTvhsmZ%2Finfinitesimals-another-argument-against-actual-infinite-sets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4KyAQ5nXnbTvhsmZ%2Finfinitesimals-another-argument-against-actual-infinite-sets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1120, "htmlBody": "<p>[<a href=\"http://juridicalcoherence.blogspot.com/2013/01/192-infinitesimals-another-argument.html\">Crossposted</a>]</p>\n<h1>Argument</h1>\n<p>My argument from the incoherence of actually <a href=\"http://juridicalcoherence.blogspot.com/2013/01/191-meaning-of-existence-lessons-from.html\">existing </a>infinitesimals has the following structure:</p>\n<p>1. Infinitesimal quantities can&rsquo;t exist;</p>\n<p>2. If actual infinities can exist, actual infinitesimals must exist;</p>\n<p>3. Therefore, actual infinities can&rsquo;t exist.</p>\n<p>Although Cantor, who invented the mathematics of transfinite numbers, rejected infinitesimals, mathematicians have continued to develop analyses based on them, as mathematically legitimate as are transfinite numbers, but few philosophers try to justify actual infinitesimals, which have some of the characteristics of zero and some characteristics of real numbers. When you add an infinitesimal to a real number, it&rsquo;s like adding zero. But when you multiply an infinitesimal by infinity, you sometimes get a finite quantity: the points on a line are of infinitesimal dimension, in that they occupy no space (as if they were zero duration), yet compose lines finite in extent.</p>\n<p>Few advocate actual infinitesimals because an actually existing infinitesimal is indistinguishable from zero. For however small a quantity you choose, it&rsquo;s obvious that you can make it yet smaller. The role of zero as a boundary accounts for why it&rsquo;s obvious you can always reduce a quantity. If I deny you can, you reply that since you can reduce it to zero and the function is continuous, you necessarily can reduce any given quantity&mdash;precluding actual infinities. When I raise the same argument about an infinite set, you can&rsquo;t reply that you can always make the set bigger; if I say add an element, you reply that the sets are still the same size (cardinality). The boundary imposed by zero is counterpoint for infinitesimals to the openness of infinity, but the ability to demonstrate actual-infinitesimals&rsquo; incoherence suggests that infinity is similarly infirm.</p>\n<p>Can more be said to establish that the conclusion about actual infinitesimal quantities also applies to actual infinite quantities? Consider again the points on a 3-inch line segment. If there are infinitely many, then each must be infinitesimal. Since there are no actual infinitesimals, there are no actual infinities of points.</p>\n<p>But this conclusion depends on the actual infinity being embedded in a finite quantity&mdash;although, as will be seen, rejecting bounded infinities alone travels metaphysical mileage. For boundless infinities, consider the number of quarks in a supposed universe of infinitely many. Form the ratio between the number of quarks in our galaxy and the infinite number of quarks in the universe. The ratio isn&rsquo;t zero because infinitely many galaxies would still form a null proportion to the universal total; it&rsquo;s not any real number because many of them would then add up to more than the total universe. This ratio must be infinitesimal. Since infinitesimals don&rsquo;t exist, neither do unbounded infinities (hence, infinite quantities in general, their being either bounded or unbounded).</p>\n<p>&nbsp;</p>\n<h1>Infinitesimals and Zeno&rsquo;s paradox</h1>\n<p>Rejecting actually existing infinities is what really resolves Zeno&rsquo;s paradox, and it resolves it by way of finding that infinitesimals don&rsquo;t exist. Zeno&rsquo;s paradox, perhaps the most intriguing logical puzzle in philosophy, purports to show that motion is impossible. In the version I&rsquo;ll use, the paradox analyzes my walk from the middle of the room to the wall as decomposable into an infinite series of walks, each reducing the remaining distance by one-half. The paradox posits that completing an infinite series is self-contradictory: infinite means uncompletable. I can never reach the wall, but the same logic applies to any distance; hence, motion is proven impossible.</p>\n<p>The standard view holds that the invention of the integral calculus completely resolved the paradox by refuting the premise that an infinite series can&rsquo;t be completed. Mathematically, the infinite series of times actually does sum to a finite value, which equals the time required to walk the distance; Zeno&rsquo;s deficiency is pronounced to be that the mathematics of infinite series was yet to be invented. But the answer only shows that (apparent) motion is mathematically tractable; it doesn&rsquo;t show how it can occur. Mathematical tractability is at the expense of logical rigor because it is achieved by ignoring the distinction between exclusive and inclusive limits. When I stroll to the wall, the wall represents an inclusive limit&mdash;I actually reach the wall. When I integrate the series created by adding half the remaining distance, I only approach the limit equated with the wall. Calculus can be developed in terms of infinitesimals, and in those terms, the series comes infinitesimally close to the limit, and in this context, we treat the infinitesimal as if it were zero. As we&rsquo;ve seen, actual infinity and infinitesimals are inseparable, certainly where, as here, the actual infinity is bounded. The calculus solves the paradox only if actual infinitesimals exist&mdash;but they don&rsquo;t.</p>\n<p>Zeno&rsquo;s misdirection can now be reconceived as&mdash;while correctly denying the existence of actual infinities&mdash;falsely affirming the existence of its counterpart, the infinitesimal. The paradox assumes that while I&rsquo;m uninterruptedly walking to the wall, I occupy a series of infinitesimally small points in space and time, such that I am at a point at a specific time the same way as if I were had stopped.</p>\n<p>Although the objection to analyzing motion in Zeno&rsquo;s manner was apparently raised as early as Aristotle, the calculus seems to have obscured the metaphysical project more than illuminating it. Logician Graham Priest (Beyond the Limits of Thought (2003)) argues that Zeno&rsquo;s paradox shows that actual infinities can exist by the following thought experiment. Priest asks that you imagine that rather than walking continuously to the wall, I stop for two seconds at each halfway point. Priest claims the series would then complete, but his argument shows that he doesn&rsquo;t understand that the paradox depends on the stopping points being infinitesimal. Despite the early recognition that (what we now call) infinitesimals are at the root of the paradox, philosophers today don&rsquo;t always grasp the correct metaphysical analysis.</p>\n<p><span style=\"font-size: 2em;\"><strong>Distinguishing actual and potential infinities</strong></span></p>\n<p>Recognizing that infinitesimals are mathematical fictions solidifies the distinction between actual and potential infinity. The reason that mathematical infinities are not just consistent but are useful is that potential infinities can exist. Zeno&rsquo;s paradox conceives motion as an actual infinity of sub-trips, but, in reality, all that can be shown is that the sub-trips are potentially infinite. There&rsquo;s no limit to how many times you can subdivide the path, but traversing it doesn&rsquo;t automatically subdivide it infinitely, which result would require that there be infinitesimal quantities. This understanding reinforces the point about dubious physical theories that posit an infinity of worlds. It&rsquo;s been argued that the many-worlds interpretation of quantum mechanics, which invokes an uncountable infinity of worlds, doesn&rsquo;t require actual infinity any more than does the existence of a line segment, which can be decomposed into uncountably many segments, but this plurality of worlds does not avoid actual infinity. We exist in one of those worlds. Many worlds, unlike infinitesimals and the conceptual line segments employing them, must be conceived as actually existing</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x4KyAQ5nXnbTvhsmZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": -39, "extendedScore": null, "score": 1.094711127162082e-06, "legacy": true, "legacyId": "21347", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>[<a href=\"http://juridicalcoherence.blogspot.com/2013/01/192-infinitesimals-another-argument.html\">Crossposted</a>]</p>\n<h1 id=\"Argument\">Argument</h1>\n<p>My argument from the incoherence of actually <a href=\"http://juridicalcoherence.blogspot.com/2013/01/191-meaning-of-existence-lessons-from.html\">existing </a>infinitesimals has the following structure:</p>\n<p>1. Infinitesimal quantities can\u2019t exist;</p>\n<p>2. If actual infinities can exist, actual infinitesimals must exist;</p>\n<p>3. Therefore, actual infinities can\u2019t exist.</p>\n<p>Although Cantor, who invented the mathematics of transfinite numbers, rejected infinitesimals, mathematicians have continued to develop analyses based on them, as mathematically legitimate as are transfinite numbers, but few philosophers try to justify actual infinitesimals, which have some of the characteristics of zero and some characteristics of real numbers. When you add an infinitesimal to a real number, it\u2019s like adding zero. But when you multiply an infinitesimal by infinity, you sometimes get a finite quantity: the points on a line are of infinitesimal dimension, in that they occupy no space (as if they were zero duration), yet compose lines finite in extent.</p>\n<p>Few advocate actual infinitesimals because an actually existing infinitesimal is indistinguishable from zero. For however small a quantity you choose, it\u2019s obvious that you can make it yet smaller. The role of zero as a boundary accounts for why it\u2019s obvious you can always reduce a quantity. If I deny you can, you reply that since you can reduce it to zero and the function is continuous, you necessarily can reduce any given quantity\u2014precluding actual infinities. When I raise the same argument about an infinite set, you can\u2019t reply that you can always make the set bigger; if I say add an element, you reply that the sets are still the same size (cardinality). The boundary imposed by zero is counterpoint for infinitesimals to the openness of infinity, but the ability to demonstrate actual-infinitesimals\u2019 incoherence suggests that infinity is similarly infirm.</p>\n<p>Can more be said to establish that the conclusion about actual infinitesimal quantities also applies to actual infinite quantities? Consider again the points on a 3-inch line segment. If there are infinitely many, then each must be infinitesimal. Since there are no actual infinitesimals, there are no actual infinities of points.</p>\n<p>But this conclusion depends on the actual infinity being embedded in a finite quantity\u2014although, as will be seen, rejecting bounded infinities alone travels metaphysical mileage. For boundless infinities, consider the number of quarks in a supposed universe of infinitely many. Form the ratio between the number of quarks in our galaxy and the infinite number of quarks in the universe. The ratio isn\u2019t zero because infinitely many galaxies would still form a null proportion to the universal total; it\u2019s not any real number because many of them would then add up to more than the total universe. This ratio must be infinitesimal. Since infinitesimals don\u2019t exist, neither do unbounded infinities (hence, infinite quantities in general, their being either bounded or unbounded).</p>\n<p>&nbsp;</p>\n<h1 id=\"Infinitesimals_and_Zeno_s_paradox\">Infinitesimals and Zeno\u2019s paradox</h1>\n<p>Rejecting actually existing infinities is what really resolves Zeno\u2019s paradox, and it resolves it by way of finding that infinitesimals don\u2019t exist. Zeno\u2019s paradox, perhaps the most intriguing logical puzzle in philosophy, purports to show that motion is impossible. In the version I\u2019ll use, the paradox analyzes my walk from the middle of the room to the wall as decomposable into an infinite series of walks, each reducing the remaining distance by one-half. The paradox posits that completing an infinite series is self-contradictory: infinite means uncompletable. I can never reach the wall, but the same logic applies to any distance; hence, motion is proven impossible.</p>\n<p>The standard view holds that the invention of the integral calculus completely resolved the paradox by refuting the premise that an infinite series can\u2019t be completed. Mathematically, the infinite series of times actually does sum to a finite value, which equals the time required to walk the distance; Zeno\u2019s deficiency is pronounced to be that the mathematics of infinite series was yet to be invented. But the answer only shows that (apparent) motion is mathematically tractable; it doesn\u2019t show how it can occur. Mathematical tractability is at the expense of logical rigor because it is achieved by ignoring the distinction between exclusive and inclusive limits. When I stroll to the wall, the wall represents an inclusive limit\u2014I actually reach the wall. When I integrate the series created by adding half the remaining distance, I only approach the limit equated with the wall. Calculus can be developed in terms of infinitesimals, and in those terms, the series comes infinitesimally close to the limit, and in this context, we treat the infinitesimal as if it were zero. As we\u2019ve seen, actual infinity and infinitesimals are inseparable, certainly where, as here, the actual infinity is bounded. The calculus solves the paradox only if actual infinitesimals exist\u2014but they don\u2019t.</p>\n<p>Zeno\u2019s misdirection can now be reconceived as\u2014while correctly denying the existence of actual infinities\u2014falsely affirming the existence of its counterpart, the infinitesimal. The paradox assumes that while I\u2019m uninterruptedly walking to the wall, I occupy a series of infinitesimally small points in space and time, such that I am at a point at a specific time the same way as if I were had stopped.</p>\n<p>Although the objection to analyzing motion in Zeno\u2019s manner was apparently raised as early as Aristotle, the calculus seems to have obscured the metaphysical project more than illuminating it. Logician Graham Priest (Beyond the Limits of Thought (2003)) argues that Zeno\u2019s paradox shows that actual infinities can exist by the following thought experiment. Priest asks that you imagine that rather than walking continuously to the wall, I stop for two seconds at each halfway point. Priest claims the series would then complete, but his argument shows that he doesn\u2019t understand that the paradox depends on the stopping points being infinitesimal. Despite the early recognition that (what we now call) infinitesimals are at the root of the paradox, philosophers today don\u2019t always grasp the correct metaphysical analysis.</p>\n<p><span style=\"font-size: 2em;\"><strong>Distinguishing actual and potential infinities</strong></span></p>\n<p>Recognizing that infinitesimals are mathematical fictions solidifies the distinction between actual and potential infinity. The reason that mathematical infinities are not just consistent but are useful is that potential infinities can exist. Zeno\u2019s paradox conceives motion as an actual infinity of sub-trips, but, in reality, all that can be shown is that the sub-trips are potentially infinite. There\u2019s no limit to how many times you can subdivide the path, but traversing it doesn\u2019t automatically subdivide it infinitely, which result would require that there be infinitesimal quantities. This understanding reinforces the point about dubious physical theories that posit an infinity of worlds. It\u2019s been argued that the many-worlds interpretation of quantum mechanics, which invokes an uncountable infinity of worlds, doesn\u2019t require actual infinity any more than does the existence of a line segment, which can be decomposed into uncountably many segments, but this plurality of worlds does not avoid actual infinity. We exist in one of those worlds. Many worlds, unlike infinitesimals and the conceptual line segments employing them, must be conceived as actually existing</p>\n<p>&nbsp;</p>", "sections": [{"title": "Argument", "anchor": "Argument", "level": 1}, {"title": "Infinitesimals and Zeno\u2019s paradox", "anchor": "Infinitesimals_and_Zeno_s_paradox", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "31 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-26T04:10:37.118Z", "modifiedAt": null, "url": null, "title": "Meetup : Shanghai Meetup", "slug": "meetup-shanghai-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.746Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Barry_Cotter", "createdAt": "2010-04-19T16:29:03.629Z", "isAdmin": false, "displayName": "Barry_Cotter"}, "userId": "5pZXxaf79kj37Rwq2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gfhs6y45TQccfoyww/meetup-shanghai-meetup", "pageUrlRelative": "/posts/Gfhs6y45TQccfoyww/meetup-shanghai-meetup", "linkUrl": "https://www.lesswrong.com/posts/Gfhs6y45TQccfoyww/meetup-shanghai-meetup", "postedAtFormatted": "Saturday, January 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Shanghai%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Shanghai%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGfhs6y45TQccfoyww%2Fmeetup-shanghai-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Shanghai%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGfhs6y45TQccfoyww%2Fmeetup-shanghai-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGfhs6y45TQccfoyww%2Fmeetup-shanghai-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/ii\">Shanghai Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">28 January 2013 07:00:00PM (+0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">45 Yue Yang Road, near Dong Ping Road, Shanghai</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>There's going to be a meetup 7p.m. Monday evening at <a rel=\"nofollow\" href=\"http://www.abbeyroad-shanghai.com/home.php\">Abbey Road</a>. Two other people have said they'll be attending as well. I'll be giving a short talk on cognitive biases and the we can discuss it and chat. The <a href=\"http://www.abbeyroad-shanghai.com/img/meuns/1233559894.pdf\">food</a>&nbsp;is good, and they have board games too for slightly later. I look like <a href=\"http://www.okcupid.com/profile/Hibernian/pictures\">this</a>&nbsp;and will have a copy of <em>Thinking, Fast and Slow</em>&nbsp;on the table.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/ii\">Shanghai Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gfhs6y45TQccfoyww", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.0947516622087645e-06, "legacy": true, "legacyId": "21348", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Shanghai_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ii\">Shanghai Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">28 January 2013 07:00:00PM (+0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">45 Yue Yang Road, near Dong Ping Road, Shanghai</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>There's going to be a meetup 7p.m. Monday evening at <a rel=\"nofollow\" href=\"http://www.abbeyroad-shanghai.com/home.php\">Abbey Road</a>. Two other people have said they'll be attending as well. I'll be giving a short talk on cognitive biases and the we can discuss it and chat. The <a href=\"http://www.abbeyroad-shanghai.com/img/meuns/1233559894.pdf\">food</a>&nbsp;is good, and they have board games too for slightly later. I look like <a href=\"http://www.okcupid.com/profile/Hibernian/pictures\">this</a>&nbsp;and will have a copy of <em>Thinking, Fast and Slow</em>&nbsp;on the table.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Shanghai_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ii\">Shanghai Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Shanghai Meetup", "anchor": "Discussion_article_for_the_meetup___Shanghai_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Shanghai Meetup", "anchor": "Discussion_article_for_the_meetup___Shanghai_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-26T16:12:20.846Z", "modifiedAt": null, "url": null, "title": "CEV: a utilitarian critique", "slug": "cev-a-utilitarian-critique", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.219Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PnAqpopgvDGyeBCQE/cev-a-utilitarian-critique", "pageUrlRelative": "/posts/PnAqpopgvDGyeBCQE/cev-a-utilitarian-critique", "linkUrl": "https://www.lesswrong.com/posts/PnAqpopgvDGyeBCQE/cev-a-utilitarian-critique", "postedAtFormatted": "Saturday, January 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CEV%3A%20a%20utilitarian%20critique&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACEV%3A%20a%20utilitarian%20critique%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnAqpopgvDGyeBCQE%2Fcev-a-utilitarian-critique%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CEV%3A%20a%20utilitarian%20critique%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnAqpopgvDGyeBCQE%2Fcev-a-utilitarian-critique", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnAqpopgvDGyeBCQE%2Fcev-a-utilitarian-critique", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1596, "htmlBody": "<p><em>I'm posting this article on behalf of <a href=\"http://www.utilitarian-essays.com/\">Brian Tomasik</a>, who authored it but is at present too busy to respond to comments.</em></p>\n<p><span style=\"font-weight: normal;\"><em>Update from Brian</em>: \"As of 2013-2014, I have become more sympathetic to at least the spirit of CEV specifically and to the project of&nbsp;<a href=\"http://utilitarian-essays.com/compromise.html\" target=\"_blank\">compromise</a>&nbsp;among differing value systems more generally. I continue to think that pure CEV is unlikely to be implemented, though democracy and intellectual discussion can help approximate it. I also continues to feel apprehensive about the conclusions that a CEV might reach, but the best should not be the enemy of the good, and cooperation is inherently about not getting everything you want in order to avoid getting nothing at all.\"</span></p>\n<p><span style=\"font-weight: normal;\"><br /></span></p>\n<h2>Introduction</h2>\n<p>I'm often asked questions like the following: If <a href=\"http://felicifia.org/viewtopic.php?p=4454\">wild-animal suffering, lab universes, sentient simulations, etc.</a>&nbsp;are so bad, why can't we assume that <a href=\"http://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\">Coherent Extrapolated Volition</a> (CEV) will figure that out and do the right thing for us?</p>\n<p>&nbsp;</p>\n<h2>Disclaimer</h2>\n<p>Most of my knowledge of CEV is based on Yudkowsky's <a href=\"http://intelligence.org/files/CEV.pdf\">2004 paper</a>, which he admits is obsolete. I have not yet read most of the more recent literature on the subject.</p>\n<p>&nbsp;</p>\n<h2>Reason 1: CEV will (almost certainly) never happen</h2>\n<p>CEV is like a dream for a certain type of moral philosopher: Finally, the most ideal solution for discovering what we really want upon reflection!</p>\n<p>The fact is, the real world is not decided by moral philosophers. It's decided by power politics, economics, and Darwinian selection. Moral philosophers can certainly have an impact through these channels, but they're unlikely to convince the world to rally behind CEV. Can you imagine the US military -- during its AGI development process -- deciding to adopt CEV? No way. It would adopt something that ensures the continued military and political dominance of the US, driven by mainstream American values. Same goes for China or any other country. If AGI is developed by a corporation, the values will reflect those of the corporation or the small group of developers and supervisors who hold the most power over the project. Unless that group is extremely enlightened, CEV is not what we'll get.</p>\n<p>Anyway, this is assuming that the developers of AGI can even keep it under control. Most likely AGI will turn into a <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclipper</a> or else evolve into some other kind of <a href=\"http://www.nickbostrom.com/fut/evolution.html\">Darwinian force over which we lose control</a>.</p>\n<h4>Objection 1: \"Okay. Future military or corporate developers of AGI probably won't do CEV. But why do you think they'd care about wild-animal suffering, etc. either?\"</h4>\n<p>Well, they might not, but if we make the wild-animal movement successful, then in ~50-100 years when AGI does come along, the notion of not spreading wild-animal suffering might be sufficiently mainstream that even military or corporate executives would care about it, at least to some degree.</p>\n<p>If post-humanity does achieve astronomical power, it will only be through AGI, so there's high value for influencing the future developers of an AGI. For this reason I believe we should focus our meme-spreading on those targets. However, this doesn't mean they should be our only focus, for two reasons: (1) Future AGI developers will themselves be influenced by their friends, popular media, contemporary philosophical and cultural norms, etc., so if we can change those things, we will diffusely impact future AGI developers too. (2) We need to build our movement, and the lowest-hanging fruit for new supporters are those most interested in the cause (e.g., antispeciesists, environmental-ethics students, transhumanists). We should reach out to them to expand our base of support before going after the big targets.</p>\n<h4>Objection 2: \"Fine. But just as we can advance values like preventing the spread of wild-animal suffering, couldn't we also increase the likelihood of CEV by promoting that idea?\"</h4>\n<p>Sure, we could. The problem is, CEV is not an optimal thing to promote, IMHO. It's sufficiently general that lots of people would want it, so for ourselves, the higher leverage comes from advancing our particular, more idiosyncratic values. Promoting CEV is kind of like promoting democracy or free speech: It's fine to do, but if you have a particular cause that you think is more important than other people realize, it's probably going to be better to promote that specific cause than to jump on the bandwagon and do the same thing everyone else is doing, since the bandwagon's cause may not be what you yourself prefer.</p>\n<p>Indeed, for myself, it's possible CEV could be a <a href=\"http://www.utilitarian-essays.com/donation-recommendation.html\">net bad thing</a>, if it would reduce the likelihood of paperclipping -- a future which might (or might not) contain far less suffering than a future directed by humanity's extrapolated values.</p>\n<p>&nbsp;</p>\n<h2>Reason 2: CEV would lead to values we don't like</h2>\n<p>Some believe that morality is absolute, in which case a CEV's job would be to uncover what that is. <a href=\"http://www.felicifia.org/viewtopic.php?p=5668#p5510\">This view is mistaken</a>, for the following reasons: (1) Existence of a separate realm of reality where ethical truths reside violates Occam's razor, and (2) even if they did exist, why would we care what they were?</p>\n<p>Yudkowsky and the LessWrong community agree that ethics is not absolute, so they have different motivations behind CEV. As far as I can gather, the following are two of them:</p>\n<h3>Motivation 1: Some believe CEV is genuinely the right thing to do</h3>\n<p>As Eliezer said in his 2004 paper (p. 29), \"Implementing CEV is just my attempt not to be a jerk.\" Some may believe that CEV is the ideal meta-ethical way to resolve ethical disputes.</p>\n<p>I have to differ. First, the set of minds included in CEV is totally arbitrary, and hence, so will be the output. Why include only humans? Why not animals? Why not dead humans? Why not humans that weren't born but might have been? Why not paperclip maximizers? Baby eaters? Pebble sorters? Suffering maximizers? Wherever you draw the line, there you're already inserting your values into the process.</p>\n<p>And then once you've picked the set of minds to extrapolate, you still have astronomically many ways to do the extrapolation, each of which could give wildly different outputs. Humans have a <a href=\"/lw/l3/thou_art_godshatter/\">thousand random shards of intuition</a> about values that resulted from all kinds of little, arbitrary perturbations during evolution and environmental exposure. If the CEV algorithm happens to make some more salient than others, this will potentially change the outcome, perhaps drastically (butterfly effects).</p>\n<p>Now, I would be in favor of a reasonable extrapolation of my own values. But humanity's values are not my values. There are people who want to <a href=\"http://www.panspermia-society.com/Panspermia_Ethics.htm\">spread life throughout the universe</a> regardless of suffering, people who want to <a href=\"http://en.wikipedia.org/wiki/Deep_ecology\">preserve nature free from human interference</a>, people who want to create lab universes <a href=\"http://www.utilitarian-essays.com/lab-universes.html\">because it would be cool</a>, people who oppose utilitronium and support retaining suffering in the world, people who want to send members of other religions to eternal torture, people who <a href=\"http://felicifia.org/viewtopic.php?p=5509#p5516\">believe sinful children should burn forever in red-hot ovens</a>, and on and on. I do not want these values to be part of the mix.</p>\n<p>Maybe (hopefully) some of these beliefs would go away once people learned more about what these wishes really implied, but some would not. Take abortion, for example: Some non-religious people genuinely oppose it, and not for trivial, misinformed reasons. They have thought long and hard about abortion and still find it to be wrong. Others have thought long and hard and still find it to be not wrong. At some point, we have to admit that human intuitions are genuinely in conflict in an irreconcilable way. Some human intuitions are irreconcilably opposed to mine, and I don't want them in the extrapolation process.</p>\n<h3>Motivation 2: Some argue that even if CEV isn't ideal, it's the best game-theoretic approach because it amounts to cooperating on the prisoner's dilemma</h3>\n<p>I think the idea is that if you try to promote your specific values above everyone else's, then you're <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">timelessly</a> causing this to be the decision of other groups of people who want to push for their values instead. But if you decided to cooperate with everyone, you would timelessly influence others to do the same.</p>\n<p>This seems worth considering, but I'm doubtful that the argument is compelling enough to take too seriously. I can almost guarantee that if I decided to start cooperating by working toward CEV, everyone else working to shape values of the future wouldn't suddenly jump on board and do the same.</p>\n<h4>Objection 1: \"Suppose CEV did happen. Then spreading concern for wild animals and the like might have little value, because the CEV process would realize that you had tried to rig the system ahead of time by making more people care about the cause, and it would attempt to neutralize your efforts.\"</h4>\n<p>Well, first of all, CEV is (almost certainly) never going to happen, so I'm not too worried. Second of all, it's not clear to me that such a scheme would actually be put in place. If you're trying to undo pre-CEV influences that led to the distribution of opinions to that point, you're going to have a heck of a lot of undoing to do. Are you going to undo the abundance of Catholics because their religion discouraged birth control and so led to large numbers of supporters? Are you going to undo the over-representation of healthy humans because natural selection unfairly removed all those sickly ones? Are you going to undo the under-representation of dinosaurs because an arbitrary asteroid killed them off before CEV came around?</p>\n<p>The fact is that who has power at the time of AGI will probably matter a lot. If we can improve the values of those who will have power in the future, this will in expectation lead to better outcomes -- regardless of whether the CEV fairy tale comes true.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PnAqpopgvDGyeBCQE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 32, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "21352", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>I'm posting this article on behalf of <a href=\"http://www.utilitarian-essays.com/\">Brian Tomasik</a>, who authored it but is at present too busy to respond to comments.</em></p>\n<p><span style=\"font-weight: normal;\"><em>Update from Brian</em>: \"As of 2013-2014, I have become more sympathetic to at least the spirit of CEV specifically and to the project of&nbsp;<a href=\"http://utilitarian-essays.com/compromise.html\" target=\"_blank\">compromise</a>&nbsp;among differing value systems more generally. I continue to think that pure CEV is unlikely to be implemented, though democracy and intellectual discussion can help approximate it. I also continues to feel apprehensive about the conclusions that a CEV might reach, but the best should not be the enemy of the good, and cooperation is inherently about not getting everything you want in order to avoid getting nothing at all.\"</span></p>\n<p><span style=\"font-weight: normal;\"><br></span></p>\n<h2 id=\"Introduction\">Introduction</h2>\n<p>I'm often asked questions like the following: If <a href=\"http://felicifia.org/viewtopic.php?p=4454\">wild-animal suffering, lab universes, sentient simulations, etc.</a>&nbsp;are so bad, why can't we assume that <a href=\"http://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\">Coherent Extrapolated Volition</a> (CEV) will figure that out and do the right thing for us?</p>\n<p>&nbsp;</p>\n<h2 id=\"Disclaimer\">Disclaimer</h2>\n<p>Most of my knowledge of CEV is based on Yudkowsky's <a href=\"http://intelligence.org/files/CEV.pdf\">2004 paper</a>, which he admits is obsolete. I have not yet read most of the more recent literature on the subject.</p>\n<p>&nbsp;</p>\n<h2 id=\"Reason_1__CEV_will__almost_certainly__never_happen\">Reason 1: CEV will (almost certainly) never happen</h2>\n<p>CEV is like a dream for a certain type of moral philosopher: Finally, the most ideal solution for discovering what we really want upon reflection!</p>\n<p>The fact is, the real world is not decided by moral philosophers. It's decided by power politics, economics, and Darwinian selection. Moral philosophers can certainly have an impact through these channels, but they're unlikely to convince the world to rally behind CEV. Can you imagine the US military -- during its AGI development process -- deciding to adopt CEV? No way. It would adopt something that ensures the continued military and political dominance of the US, driven by mainstream American values. Same goes for China or any other country. If AGI is developed by a corporation, the values will reflect those of the corporation or the small group of developers and supervisors who hold the most power over the project. Unless that group is extremely enlightened, CEV is not what we'll get.</p>\n<p>Anyway, this is assuming that the developers of AGI can even keep it under control. Most likely AGI will turn into a <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclipper</a> or else evolve into some other kind of <a href=\"http://www.nickbostrom.com/fut/evolution.html\">Darwinian force over which we lose control</a>.</p>\n<h4 id=\"Objection_1___Okay__Future_military_or_corporate_developers_of_AGI_probably_won_t_do_CEV__But_why_do_you_think_they_d_care_about_wild_animal_suffering__etc__either__\">Objection 1: \"Okay. Future military or corporate developers of AGI probably won't do CEV. But why do you think they'd care about wild-animal suffering, etc. either?\"</h4>\n<p>Well, they might not, but if we make the wild-animal movement successful, then in ~50-100 years when AGI does come along, the notion of not spreading wild-animal suffering might be sufficiently mainstream that even military or corporate executives would care about it, at least to some degree.</p>\n<p>If post-humanity does achieve astronomical power, it will only be through AGI, so there's high value for influencing the future developers of an AGI. For this reason I believe we should focus our meme-spreading on those targets. However, this doesn't mean they should be our only focus, for two reasons: (1) Future AGI developers will themselves be influenced by their friends, popular media, contemporary philosophical and cultural norms, etc., so if we can change those things, we will diffusely impact future AGI developers too. (2) We need to build our movement, and the lowest-hanging fruit for new supporters are those most interested in the cause (e.g., antispeciesists, environmental-ethics students, transhumanists). We should reach out to them to expand our base of support before going after the big targets.</p>\n<h4 id=\"Objection_2___Fine__But_just_as_we_can_advance_values_like_preventing_the_spread_of_wild_animal_suffering__couldn_t_we_also_increase_the_likelihood_of_CEV_by_promoting_that_idea__\">Objection 2: \"Fine. But just as we can advance values like preventing the spread of wild-animal suffering, couldn't we also increase the likelihood of CEV by promoting that idea?\"</h4>\n<p>Sure, we could. The problem is, CEV is not an optimal thing to promote, IMHO. It's sufficiently general that lots of people would want it, so for ourselves, the higher leverage comes from advancing our particular, more idiosyncratic values. Promoting CEV is kind of like promoting democracy or free speech: It's fine to do, but if you have a particular cause that you think is more important than other people realize, it's probably going to be better to promote that specific cause than to jump on the bandwagon and do the same thing everyone else is doing, since the bandwagon's cause may not be what you yourself prefer.</p>\n<p>Indeed, for myself, it's possible CEV could be a <a href=\"http://www.utilitarian-essays.com/donation-recommendation.html\">net bad thing</a>, if it would reduce the likelihood of paperclipping -- a future which might (or might not) contain far less suffering than a future directed by humanity's extrapolated values.</p>\n<p>&nbsp;</p>\n<h2 id=\"Reason_2__CEV_would_lead_to_values_we_don_t_like\">Reason 2: CEV would lead to values we don't like</h2>\n<p>Some believe that morality is absolute, in which case a CEV's job would be to uncover what that is. <a href=\"http://www.felicifia.org/viewtopic.php?p=5668#p5510\">This view is mistaken</a>, for the following reasons: (1) Existence of a separate realm of reality where ethical truths reside violates Occam's razor, and (2) even if they did exist, why would we care what they were?</p>\n<p>Yudkowsky and the LessWrong community agree that ethics is not absolute, so they have different motivations behind CEV. As far as I can gather, the following are two of them:</p>\n<h3 id=\"Motivation_1__Some_believe_CEV_is_genuinely_the_right_thing_to_do\">Motivation 1: Some believe CEV is genuinely the right thing to do</h3>\n<p>As Eliezer said in his 2004 paper (p. 29), \"Implementing CEV is just my attempt not to be a jerk.\" Some may believe that CEV is the ideal meta-ethical way to resolve ethical disputes.</p>\n<p>I have to differ. First, the set of minds included in CEV is totally arbitrary, and hence, so will be the output. Why include only humans? Why not animals? Why not dead humans? Why not humans that weren't born but might have been? Why not paperclip maximizers? Baby eaters? Pebble sorters? Suffering maximizers? Wherever you draw the line, there you're already inserting your values into the process.</p>\n<p>And then once you've picked the set of minds to extrapolate, you still have astronomically many ways to do the extrapolation, each of which could give wildly different outputs. Humans have a <a href=\"/lw/l3/thou_art_godshatter/\">thousand random shards of intuition</a> about values that resulted from all kinds of little, arbitrary perturbations during evolution and environmental exposure. If the CEV algorithm happens to make some more salient than others, this will potentially change the outcome, perhaps drastically (butterfly effects).</p>\n<p>Now, I would be in favor of a reasonable extrapolation of my own values. But humanity's values are not my values. There are people who want to <a href=\"http://www.panspermia-society.com/Panspermia_Ethics.htm\">spread life throughout the universe</a> regardless of suffering, people who want to <a href=\"http://en.wikipedia.org/wiki/Deep_ecology\">preserve nature free from human interference</a>, people who want to create lab universes <a href=\"http://www.utilitarian-essays.com/lab-universes.html\">because it would be cool</a>, people who oppose utilitronium and support retaining suffering in the world, people who want to send members of other religions to eternal torture, people who <a href=\"http://felicifia.org/viewtopic.php?p=5509#p5516\">believe sinful children should burn forever in red-hot ovens</a>, and on and on. I do not want these values to be part of the mix.</p>\n<p>Maybe (hopefully) some of these beliefs would go away once people learned more about what these wishes really implied, but some would not. Take abortion, for example: Some non-religious people genuinely oppose it, and not for trivial, misinformed reasons. They have thought long and hard about abortion and still find it to be wrong. Others have thought long and hard and still find it to be not wrong. At some point, we have to admit that human intuitions are genuinely in conflict in an irreconcilable way. Some human intuitions are irreconcilably opposed to mine, and I don't want them in the extrapolation process.</p>\n<h3 id=\"Motivation_2__Some_argue_that_even_if_CEV_isn_t_ideal__it_s_the_best_game_theoretic_approach_because_it_amounts_to_cooperating_on_the_prisoner_s_dilemma\">Motivation 2: Some argue that even if CEV isn't ideal, it's the best game-theoretic approach because it amounts to cooperating on the prisoner's dilemma</h3>\n<p>I think the idea is that if you try to promote your specific values above everyone else's, then you're <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">timelessly</a> causing this to be the decision of other groups of people who want to push for their values instead. But if you decided to cooperate with everyone, you would timelessly influence others to do the same.</p>\n<p>This seems worth considering, but I'm doubtful that the argument is compelling enough to take too seriously. I can almost guarantee that if I decided to start cooperating by working toward CEV, everyone else working to shape values of the future wouldn't suddenly jump on board and do the same.</p>\n<h4 id=\"Objection_1___Suppose_CEV_did_happen__Then_spreading_concern_for_wild_animals_and_the_like_might_have_little_value__because_the_CEV_process_would_realize_that_you_had_tried_to_rig_the_system_ahead_of_time_by_making_more_people_care_about_the_cause__and_it_would_attempt_to_neutralize_your_efforts__\">Objection 1: \"Suppose CEV did happen. Then spreading concern for wild animals and the like might have little value, because the CEV process would realize that you had tried to rig the system ahead of time by making more people care about the cause, and it would attempt to neutralize your efforts.\"</h4>\n<p>Well, first of all, CEV is (almost certainly) never going to happen, so I'm not too worried. Second of all, it's not clear to me that such a scheme would actually be put in place. If you're trying to undo pre-CEV influences that led to the distribution of opinions to that point, you're going to have a heck of a lot of undoing to do. Are you going to undo the abundance of Catholics because their religion discouraged birth control and so led to large numbers of supporters? Are you going to undo the over-representation of healthy humans because natural selection unfairly removed all those sickly ones? Are you going to undo the under-representation of dinosaurs because an arbitrary asteroid killed them off before CEV came around?</p>\n<p>The fact is that who has power at the time of AGI will probably matter a lot. If we can improve the values of those who will have power in the future, this will in expectation lead to better outcomes -- regardless of whether the CEV fairy tale comes true.</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Disclaimer", "anchor": "Disclaimer", "level": 1}, {"title": "Reason 1: CEV will (almost certainly) never happen", "anchor": "Reason_1__CEV_will__almost_certainly__never_happen", "level": 1}, {"title": "Objection 1: \"Okay. Future military or corporate developers of AGI probably won't do CEV. But why do you think they'd care about wild-animal suffering, etc. either?\"", "anchor": "Objection_1___Okay__Future_military_or_corporate_developers_of_AGI_probably_won_t_do_CEV__But_why_do_you_think_they_d_care_about_wild_animal_suffering__etc__either__", "level": 3}, {"title": "Objection 2: \"Fine. But just as we can advance values like preventing the spread of wild-animal suffering, couldn't we also increase the likelihood of CEV by promoting that idea?\"", "anchor": "Objection_2___Fine__But_just_as_we_can_advance_values_like_preventing_the_spread_of_wild_animal_suffering__couldn_t_we_also_increase_the_likelihood_of_CEV_by_promoting_that_idea__", "level": 3}, {"title": "Reason 2: CEV would lead to values we don't like", "anchor": "Reason_2__CEV_would_lead_to_values_we_don_t_like", "level": 1}, {"title": "Motivation 1: Some believe CEV is genuinely the right thing to do", "anchor": "Motivation_1__Some_believe_CEV_is_genuinely_the_right_thing_to_do", "level": 2}, {"title": "Motivation 2: Some argue that even if CEV isn't ideal, it's the best game-theoretic approach because it amounts to cooperating on the prisoner's dilemma", "anchor": "Motivation_2__Some_argue_that_even_if_CEV_isn_t_ideal__it_s_the_best_game_theoretic_approach_because_it_amounts_to_cooperating_on_the_prisoner_s_dilemma", "level": 2}, {"title": "Objection 1: \"Suppose CEV did happen. Then spreading concern for wild animals and the like might have little value, because the CEV process would realize that you had tried to rig the system ahead of time by making more people care about the cause, and it would attempt to neutralize your efforts.\"", "anchor": "Objection_1___Suppose_CEV_did_happen__Then_spreading_concern_for_wild_animals_and_the_like_might_have_little_value__because_the_CEV_process_would_realize_that_you_had_tried_to_rig_the_system_ahead_of_time_by_making_more_people_care_about_the_cause__and_it_would_attempt_to_neutralize_your_efforts__", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "92 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 94, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cSXZpvqpa9vbGGLtG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-27T07:32:43.415Z", "modifiedAt": null, "url": null, "title": "Cryo and Social Obligations", "slug": "cryo-and-social-obligations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.966Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GG8KAsj4c3bWdgBBc/cryo-and-social-obligations", "pageUrlRelative": "/posts/GG8KAsj4c3bWdgBBc/cryo-and-social-obligations", "linkUrl": "https://www.lesswrong.com/posts/GG8KAsj4c3bWdgBBc/cryo-and-social-obligations", "postedAtFormatted": "Sunday, January 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryo%20and%20Social%20Obligations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryo%20and%20Social%20Obligations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGG8KAsj4c3bWdgBBc%2Fcryo-and-social-obligations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryo%20and%20Social%20Obligations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGG8KAsj4c3bWdgBBc%2Fcryo-and-social-obligations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGG8KAsj4c3bWdgBBc%2Fcryo-and-social-obligations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 677, "htmlBody": "<p>I'm about a third of the way through \"Debt: The First 5,000 Years\" by David Graeber, and am enjoying the feeling of ideas shifting around in my head, arranging themselves into more useful patterns. (The last book I read that put together ideas of similar breadth was \"Economix: How and Why Our Economy Works\" by Goodwin.) \"Debt\" goes into the origins of debts, as compared to obligations; and related topics, such as exchanges considered beneath economic notice (\"Please pass me the salt\"), debts too big or unique to be repaid, peaceful versus violent interactions, the endless minor obligations that form the network of social connections, and even the basis of whole societies.<br /><br />The reason I'm posting about this book here... is that it's giving me some new perspectives from which to consider the whole cryonics subculture, and, for instance, why it remains just a subculture of a couple of thousand people or so. For example, a standard LessWrong thought experiment is \"Is That Your True Rejection?\"; and most of the objections people raise to cryonics seem to be off enough that, even if those objections were solved, those particular people still wouldn't sign up - that is, they feel some fundamental antipathy to the whole idea of cryonics, and unconsciously pick some rationalization that happens to sound reasonable to them to explain it.<br /><a id=\"more\"></a><br />I still have two-thirds of \"Debt\" to go... but, at the moment, I have a strong hunch that one extremely strong reason people feel an emotional revulsion to cryo is, simply, that even if they do wake up in the future, they will have been cut off from all their social connections. This may not sound like much - but the part of \"Debt\" I'm currently reading discusses how one of the more fundamental aspects of slavery is that becoming a slave involves being cut off from one's family and society; and another fundamental aspect is that being a slave is being without honor, and in many senses literally having died (eg, in some societies, when someone was taken as a slave, their will was read and their spouse considered a widow). On a certain emotional level, many people really do seem to think that being probably-permanently cut off from all their loved ones is a fate no better than simply dying outright.<br /><br />What's even more interesting is that if this idea has any actual basis in reality... then it offers the possibility of coming up with approaches to counter it: promoting the idea that waking up from cryo will involve being enmeshed in a community rightaway. I'm not actually sure how this might be managed. The Venturists seem to be heading in the general direction of that idea - but don't quite seem to be capturing it; maybe its the annual fee, maybe it's the dearth of concrete plans about how to help cryonic revivees, maybe it's something more abstract.<br /><br />One possible alternative approach might be to take the thought experiment - what if we could revive someone from cryo not next century, or next decade... but tomorrow. What could we do to help them integrate into modern life, instead of merely waking up in a hospital bed with the day's newspaper and being shown the door? Bedford was frozen in 1967; how hard would it be to either collect or assemble a set of yearbooks, describing what's happened since then, and storing a small library of such reference texts at both CI and Alcor? Perhaps the cryonics providers' boards of directors could offer their members a revival fund that could be donated to, specifically targeted to help future revivees to rejoin society? I'm not even scratching the surface of possibilities here, so even if these particular ideas turn out to be wrong, at least they suggest further possibilities.<br /><br /><br /><br />So: If someone was revived from cryonics tomorrow, would you be willing to at least let them crash on your couch for a few weeks?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GG8KAsj4c3bWdgBBc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 1.0957643917185345e-06, "legacy": true, "legacyId": "21353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-27T16:12:39.502Z", "modifiedAt": null, "url": null, "title": "Meetup : Love and Sex in Salt Lake City", "slug": "meetup-love-and-sex-in-salt-lake-city", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:33.976Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/APyPRqpHCBgcxYAEm/meetup-love-and-sex-in-salt-lake-city", "pageUrlRelative": "/posts/APyPRqpHCBgcxYAEm/meetup-love-and-sex-in-salt-lake-city", "linkUrl": "https://www.lesswrong.com/posts/APyPRqpHCBgcxYAEm/meetup-love-and-sex-in-salt-lake-city", "postedAtFormatted": "Sunday, January 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Love%20and%20Sex%20in%20Salt%20Lake%20City&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Love%20and%20Sex%20in%20Salt%20Lake%20City%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAPyPRqpHCBgcxYAEm%2Fmeetup-love-and-sex-in-salt-lake-city%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Love%20and%20Sex%20in%20Salt%20Lake%20City%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAPyPRqpHCBgcxYAEm%2Fmeetup-love-and-sex-in-salt-lake-city", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAPyPRqpHCBgcxYAEm%2Fmeetup-love-and-sex-in-salt-lake-city", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ij'>Love and Sex in Salt Lake City</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 February 2013 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1558 Palo Verde Way QE#12, cottonwood heights, ut 84121</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's February, the arbitrarily themed month of love and sex! Naturally, we're having a themed discussion to reflect this fact. We're also departing from the usual schedule and doing it on a Sunday so we can finally meet the mysterious can't-come-on-saturday crowd!\nAny snacks you might want to bring would be dully appreciated.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ij'>Love and Sex in Salt Lake City</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "APyPRqpHCBgcxYAEm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "21354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Love_and_Sex_in_Salt_Lake_City\">Discussion article for the meetup : <a href=\"/meetups/ij\">Love and Sex in Salt Lake City</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 February 2013 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1558 Palo Verde Way QE#12, cottonwood heights, ut 84121</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's February, the arbitrarily themed month of love and sex! Naturally, we're having a themed discussion to reflect this fact. We're also departing from the usual schedule and doing it on a Sunday so we can finally meet the mysterious can't-come-on-saturday crowd!\nAny snacks you might want to bring would be dully appreciated.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Love_and_Sex_in_Salt_Lake_City1\">Discussion article for the meetup : <a href=\"/meetups/ij\">Love and Sex in Salt Lake City</a></h2>", "sections": [{"title": "Discussion article for the meetup : Love and Sex in Salt Lake City", "anchor": "Discussion_article_for_the_meetup___Love_and_Sex_in_Salt_Lake_City", "level": 1}, {"title": "Discussion article for the meetup : Love and Sex in Salt Lake City", "anchor": "Discussion_article_for_the_meetup___Love_and_Sex_in_Salt_Lake_City1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-27T22:39:34.476Z", "modifiedAt": null, "url": null, "title": "Meetup : Paderborn Meetup Frebruary 6th ", "slug": "meetup-paderborn-meetup-frebruary-6th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.728Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MKZkSixXKekACumnL/meetup-paderborn-meetup-frebruary-6th", "pageUrlRelative": "/posts/MKZkSixXKekACumnL/meetup-paderborn-meetup-frebruary-6th", "linkUrl": "https://www.lesswrong.com/posts/MKZkSixXKekACumnL/meetup-paderborn-meetup-frebruary-6th", "postedAtFormatted": "Sunday, January 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Paderborn%20Meetup%20Frebruary%206th%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Paderborn%20Meetup%20Frebruary%206th%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMKZkSixXKekACumnL%2Fmeetup-paderborn-meetup-frebruary-6th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Paderborn%20Meetup%20Frebruary%206th%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMKZkSixXKekACumnL%2Fmeetup-paderborn-meetup-frebruary-6th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMKZkSixXKekACumnL%2Fmeetup-paderborn-meetup-frebruary-6th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ik'>Paderborn Meetup Frebruary 6th </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 February 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting once again in Paderborn.</p>\n\n<p>The topics of this evening will be the current series \"Highly Advanced Epistemology 101 for Beginners\", in particular the post \"Casual Diagrams and Casual Models\", some thoughts on lifehacks and some Bayes training. And of course whatever topics will come to our mind till then, or during the meetup.</p>\n\n<p>If you live in the area, consider dropping by :).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ik'>Paderborn Meetup Frebruary 6th </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MKZkSixXKekACumnL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.096325210256533e-06, "legacy": true, "legacyId": "21355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Paderborn_Meetup_Frebruary_6th_\">Discussion article for the meetup : <a href=\"/meetups/ik\">Paderborn Meetup Frebruary 6th </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 February 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting once again in Paderborn.</p>\n\n<p>The topics of this evening will be the current series \"Highly Advanced Epistemology 101 for Beginners\", in particular the post \"Casual Diagrams and Casual Models\", some thoughts on lifehacks and some Bayes training. And of course whatever topics will come to our mind till then, or during the meetup.</p>\n\n<p>If you live in the area, consider dropping by :).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Paderborn_Meetup_Frebruary_6th_1\">Discussion article for the meetup : <a href=\"/meetups/ik\">Paderborn Meetup Frebruary 6th </a></h2>", "sections": [{"title": "Discussion article for the meetup : Paderborn Meetup Frebruary 6th ", "anchor": "Discussion_article_for_the_meetup___Paderborn_Meetup_Frebruary_6th_", "level": 1}, {"title": "Discussion article for the meetup : Paderborn Meetup Frebruary 6th ", "anchor": "Discussion_article_for_the_meetup___Paderborn_Meetup_Frebruary_6th_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-28T10:37:59.204Z", "modifiedAt": null, "url": null, "title": "[LINK] NYT Article about Existential Risk from AI", "slug": "link-nyt-article-about-existential-risk-from-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.984Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "LxmsZsaFcsuHYBP8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tK37jT79YFgARZRje/link-nyt-article-about-existential-risk-from-ai", "pageUrlRelative": "/posts/tK37jT79YFgARZRje/link-nyt-article-about-existential-risk-from-ai", "linkUrl": "https://www.lesswrong.com/posts/tK37jT79YFgARZRje/link-nyt-article-about-existential-risk-from-ai", "postedAtFormatted": "Monday, January 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20NYT%20Article%20about%20Existential%20Risk%20from%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20NYT%20Article%20about%20Existential%20Risk%20from%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtK37jT79YFgARZRje%2Flink-nyt-article-about-existential-risk-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20NYT%20Article%20about%20Existential%20Risk%20from%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtK37jT79YFgARZRje%2Flink-nyt-article-about-existential-risk-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtK37jT79YFgARZRje%2Flink-nyt-article-about-existential-risk-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p><a href=\"http://opinionator.blogs.nytimes.com/2013/01/27/cambridge-cabs-and-copenhagen-my-route-to-existential-risk/\">http://opinionator.blogs.nytimes.com/2013/01/27/cambridge-cabs-and-copenhagen-my-route-to-existential-risk/</a></p>\n<p>Author: Huw Price (Bertrand Russell Professor of Philosophy at Cambridge)</p>\n<p>The article is mainly about the Centre for the Study of Existential Risk and the author's speculation about AI (and his association with Jaan Tallinn). &nbsp;Nothing made me really stand up and think \"This is something I've never heard on Less Wrong\", but it is interesting to see Existential risk and AI getting more mainstream attention, and the author reproduces tabooing in his willful avoidance of attempting to define the term \"intelligence\".</p>\n<p>&nbsp;</p>\n<p>The comments all miss the point or reproduce cached thoughts with frustrating predictability. &nbsp;I think I find them to be so frustrating because these do not seem to be unintelligent people (by the standards of the internet at least; their comments have good grammar and vocabulary), but they are not really processing.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZFrgTgzwEfStg26JL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tK37jT79YFgARZRje", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 38, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "21366", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-28T20:22:16.461Z", "modifiedAt": null, "url": null, "title": "Isolated AI with no chat whatsoever", "slug": "isolated-ai-with-no-chat-whatsoever", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.179Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ancientcampus", "createdAt": "2011-11-09T20:20:29.779Z", "isAdmin": false, "displayName": "ancientcampus"}, "userId": "BGcNEeTWb7Qxcf5Pc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nuh2d6TsFhkQktbS7/isolated-ai-with-no-chat-whatsoever", "pageUrlRelative": "/posts/Nuh2d6TsFhkQktbS7/isolated-ai-with-no-chat-whatsoever", "linkUrl": "https://www.lesswrong.com/posts/Nuh2d6TsFhkQktbS7/isolated-ai-with-no-chat-whatsoever", "postedAtFormatted": "Monday, January 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Isolated%20AI%20with%20no%20chat%20whatsoever&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIsolated%20AI%20with%20no%20chat%20whatsoever%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuh2d6TsFhkQktbS7%2Fisolated-ai-with-no-chat-whatsoever%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Isolated%20AI%20with%20no%20chat%20whatsoever%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuh2d6TsFhkQktbS7%2Fisolated-ai-with-no-chat-whatsoever", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuh2d6TsFhkQktbS7%2Fisolated-ai-with-no-chat-whatsoever", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 235, "htmlBody": "<p>Suppose you make a super-intelligent&nbsp;AI and run it on a computer. The computer has NO conventional means of output (no connections to other computers, no screen, etc). Might it still be able to get out / cause harm? I'll post my ideas, and you post yours in the comments.</p>\n<p>(This may have been discussed before, but I could not find a dedicated topic)</p>\n<p>My ideas:<br />-manipulate current through its hardware, or better yet, through the power cable (a ready-made antenna) to create electromagnetic waves to access some wireless-equipped device. (I'm no physicist so I don't know if certain frequencies would be hard to do)<br />-manipulate usage of its hardware (which likely makes small amounts of noise naturally) to approximate human speech, allowing it to communicate with its captors. (This seems even harder than the 1-line AI box scenario)<br />-manipulate usage of its hardware to create sound or noise to mess with human emotion. (To my understanding tones may affect emotion, but not in any way easily predictable)<br />-also, manipulating its power use will cause changes in the power company's database. There doesn't seem to be an obvious exploit there, but it IS external communication, for what it's worth.</p>\n<p>&nbsp;</p>\n<p>Let's hear your thoughts! Lastly, as in similar discussions, you probably shouldn't come out of this thinking, \"Well, if we can just avoid X, Y, and Z, we're golden!\" There are plenty of unknown unknowns here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nuh2d6TsFhkQktbS7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 16, "extendedScore": null, "score": 1.0971309522724836e-06, "legacy": true, "legacyId": "21368", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-29T00:15:19.094Z", "modifiedAt": null, "url": null, "title": "Meetup : LW Buffalo Meetup at Buffalo Labs", "slug": "meetup-lw-buffalo-meetup-at-buffalo-labs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:31.785Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "StonesOnCanvas", "createdAt": "2012-06-28T17:32:49.237Z", "isAdmin": false, "displayName": "StonesOnCanvas"}, "userId": "FAfkKGH6E8BLmXW4M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pkga2tMDCJN8Q95o9/meetup-lw-buffalo-meetup-at-buffalo-labs", "pageUrlRelative": "/posts/Pkga2tMDCJN8Q95o9/meetup-lw-buffalo-meetup-at-buffalo-labs", "linkUrl": "https://www.lesswrong.com/posts/Pkga2tMDCJN8Q95o9/meetup-lw-buffalo-meetup-at-buffalo-labs", "postedAtFormatted": "Tuesday, January 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LW%20Buffalo%20Meetup%20at%20Buffalo%20Labs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LW%20Buffalo%20Meetup%20at%20Buffalo%20Labs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkga2tMDCJN8Q95o9%2Fmeetup-lw-buffalo-meetup-at-buffalo-labs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LW%20Buffalo%20Meetup%20at%20Buffalo%20Labs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkga2tMDCJN8Q95o9%2Fmeetup-lw-buffalo-meetup-at-buffalo-labs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkga2tMDCJN8Q95o9%2Fmeetup-lw-buffalo-meetup-at-buffalo-labs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/il'>LW Buffalo Meetup at Buffalo Labs</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 February 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2727 Broadway Ave. Suite 210, Cheektowaga, NY</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi everyone!</p>\n\n<p>For those of you who couldn't make the last meetup because it was on a Thursday, vote in the doodle poll: <a href=\"http://doodle.com/gvxedzauw37b8ryt57t63h45/admin?#table\" rel=\"nofollow\">http://doodle.com/gvxedzauw37b8ryt57t63h45/admin?#table</a> Every vote counts. (I promise to make the next meetup on a different day of the week, although I don't know which day I will have it yet.)</p>\n\n<p>For this meetup, we'll be talking about Making Beliefs Pay Rent :  <a href=\"http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\" rel=\"nofollow\">http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/</a> Its a great article so I strongly suggest reading it. If you can't, don't worry, I'll do a cliff-notes summary for everyone. Anyone can attend. Feel free to invite friends who might be interested. I'll bring some snacks and stuff as well. We'll also play some cool games.</p>\n\n<p>P.S. Since Buffalo Lab is not clearly visible from the road, I'd recommend using google's satellite view to get a good idea of where to go. (it'll be second building from the road -a white building with 2727 in large print - second door. Look for the Buffalo Lab gear logo and we'll have some blinking lights to make it more visible.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/il'>LW Buffalo Meetup at Buffalo Labs</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pkga2tMDCJN8Q95o9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0972752043784477e-06, "legacy": true, "legacyId": "21369", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LW_Buffalo_Meetup_at_Buffalo_Labs\">Discussion article for the meetup : <a href=\"/meetups/il\">LW Buffalo Meetup at Buffalo Labs</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 February 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2727 Broadway Ave. Suite 210, Cheektowaga, NY</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi everyone!</p>\n\n<p>For those of you who couldn't make the last meetup because it was on a Thursday, vote in the doodle poll: <a href=\"http://doodle.com/gvxedzauw37b8ryt57t63h45/admin?#table\" rel=\"nofollow\">http://doodle.com/gvxedzauw37b8ryt57t63h45/admin?#table</a> Every vote counts. (I promise to make the next meetup on a different day of the week, although I don't know which day I will have it yet.)</p>\n\n<p>For this meetup, we'll be talking about Making Beliefs Pay Rent :  <a href=\"http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\" rel=\"nofollow\">http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/</a> Its a great article so I strongly suggest reading it. If you can't, don't worry, I'll do a cliff-notes summary for everyone. Anyone can attend. Feel free to invite friends who might be interested. I'll bring some snacks and stuff as well. We'll also play some cool games.</p>\n\n<p>P.S. Since Buffalo Lab is not clearly visible from the road, I'd recommend using google's satellite view to get a good idea of where to go. (it'll be second building from the road -a white building with 2727 in large print - second door. Look for the Buffalo Lab gear logo and we'll have some blinking lights to make it more visible.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LW_Buffalo_Meetup_at_Buffalo_Labs1\">Discussion article for the meetup : <a href=\"/meetups/il\">LW Buffalo Meetup at Buffalo Labs</a></h2>", "sections": [{"title": "Discussion article for the meetup : LW Buffalo Meetup at Buffalo Labs", "anchor": "Discussion_article_for_the_meetup___LW_Buffalo_Meetup_at_Buffalo_Labs", "level": 1}, {"title": "Discussion article for the meetup : LW Buffalo Meetup at Buffalo Labs", "anchor": "Discussion_article_for_the_meetup___LW_Buffalo_Meetup_at_Buffalo_Labs1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-29T03:09:23.841Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley: Board games", "slug": "meetup-berkeley-board-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:29.094Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zvJpWakE4qfRmwJNY/meetup-berkeley-board-games", "pageUrlRelative": "/posts/zvJpWakE4qfRmwJNY/meetup-berkeley-board-games", "linkUrl": "https://www.lesswrong.com/posts/zvJpWakE4qfRmwJNY/meetup-berkeley-board-games", "postedAtFormatted": "Tuesday, January 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%3A%20Board%20games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%3A%20Board%20games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvJpWakE4qfRmwJNY%2Fmeetup-berkeley-board-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%3A%20Board%20games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvJpWakE4qfRmwJNY%2Fmeetup-berkeley-board-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvJpWakE4qfRmwJNY%2Fmeetup-berkeley-board-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/im'>Berkeley: Board games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 January 2013 07:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello all! This week's meetup will be a board games night. Come meet people and play a board game from Zendo's library, or bring a game that you want to play. I will not be there but Alex will be hosting.</p>\n\n<p>The meetup will begin on Wednesday at 7:30pm. For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/im'>Berkeley: Board games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zvJpWakE4qfRmwJNY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.0973829795919876e-06, "legacy": true, "legacyId": "21376", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Board_games\">Discussion article for the meetup : <a href=\"/meetups/im\">Berkeley: Board games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 January 2013 07:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello all! This week's meetup will be a board games night. Come meet people and play a board game from Zendo's library, or bring a game that you want to play. I will not be there but Alex will be hosting.</p>\n\n<p>The meetup will begin on Wednesday at 7:30pm. For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Board_games1\">Discussion article for the meetup : <a href=\"/meetups/im\">Berkeley: Board games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley: Board games", "anchor": "Discussion_article_for_the_meetup___Berkeley__Board_games", "level": 1}, {"title": "Discussion article for the meetup : Berkeley: Board games", "anchor": "Discussion_article_for_the_meetup___Berkeley__Board_games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-29T06:01:05.063Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Serious Stories", "slug": "seq-rerun-serious-stories", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.512Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nzFFb7ntZ5CS3gkv2/seq-rerun-serious-stories", "pageUrlRelative": "/posts/nzFFb7ntZ5CS3gkv2/seq-rerun-serious-stories", "linkUrl": "https://www.lesswrong.com/posts/nzFFb7ntZ5CS3gkv2/seq-rerun-serious-stories", "postedAtFormatted": "Tuesday, January 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Serious%20Stories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Serious%20Stories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnzFFb7ntZ5CS3gkv2%2Fseq-rerun-serious-stories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Serious%20Stories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnzFFb7ntZ5CS3gkv2%2Fseq-rerun-serious-stories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnzFFb7ntZ5CS3gkv2%2Fseq-rerun-serious-stories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 344, "htmlBody": "<p>Today's post, <a href=\"/lw/xi/serious_stories/\">Serious Stories</a> was originally published on 08 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote><em>Stories </em>and <em>lives </em>are optimized according to rather different criteria. Advice on how to write fiction will tell you that \"stories are about people's pain\" and \"every scene must end in disaster\". I once assumed that it was not possible to write any story about a successful Singularity because the inhabitants would not be in any pain; but something about the final conclusion that the post-Singularity world would contain <em>no </em>stories worth telling seemed alarming. Stories in which nothing ever goes wrong, are painful to read; would a life of endless success have the same painful quality? If so, should we simply eliminate that revulsion via neural rewiring? Pleasure probably <em>does </em>retain its meaning in the absence of pain to contrast it; they are different neural systems. The present world has an imbalance between pain and pleasure; it is much easier to produce severe pain than correspondingly intense pleasure. One path would be to address the <em>imbalance </em>and create a world with more pleasures, and free of the more grindingly destructive and pointless sorts of pain. Another approach would be to eliminate pain <em>entirely</em>. I feel like I prefer the former approach, but I don't know if it can last in the long run.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gfu/seq_rerun_emotional_involvement/#comments\">Emotional Involvement</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nzFFb7ntZ5CS3gkv2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.097489292285262e-06, "legacy": true, "legacyId": "21377", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6qS9q5zHafFXsB6hf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-29T06:32:29.186Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Wits and Wagers", "slug": "meetup-west-la-meetup-wits-and-wagers", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LXmgW3otNvqQpAsoC/meetup-west-la-meetup-wits-and-wagers", "pageUrlRelative": "/posts/LXmgW3otNvqQpAsoC/meetup-west-la-meetup-wits-and-wagers", "linkUrl": "https://www.lesswrong.com/posts/LXmgW3otNvqQpAsoC/meetup-west-la-meetup-wits-and-wagers", "postedAtFormatted": "Tuesday, January 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Wits%20and%20Wagers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Wits%20and%20Wagers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLXmgW3otNvqQpAsoC%2Fmeetup-west-la-meetup-wits-and-wagers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Wits%20and%20Wagers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLXmgW3otNvqQpAsoC%2Fmeetup-west-la-meetup-wits-and-wagers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLXmgW3otNvqQpAsoC%2Fmeetup-west-la-meetup-wits-and-wagers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/in\">West LA Meetup - Wits and Wagers</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">30 January 2013 07:00:00PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p><strong>When:</strong> 7:00pm Wednesday, January 30th.</p>\n<p><strong>Where:</strong> The Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a rel=\"nofollow\" href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n<p><strong>Parking</strong> is free for 3 hours.</p>\n<p>Game/Discussion: This week, we will play a fun board game which merits discussion! The game you will enjoy, and the discussion will cover probability, betting, and value of concensus as evidence.</p>\n<p><em>No foreknowledge or exposure to Less Wrong is necessary</em>; this will be generally accessable and useful to anyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/in\">West LA Meetup - Wits and Wagers</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LXmgW3otNvqQpAsoC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0975087391253248e-06, "legacy": true, "legacyId": "21378", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Wits_and_Wagers\">Discussion article for the meetup : <a href=\"/meetups/in\">West LA Meetup - Wits and Wagers</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">30 January 2013 07:00:00PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p><strong>When:</strong> 7:00pm Wednesday, January 30th.</p>\n<p><strong>Where:</strong> The Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a rel=\"nofollow\" href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n<p><strong>Parking</strong> is free for 3 hours.</p>\n<p>Game/Discussion: This week, we will play a fun board game which merits discussion! The game you will enjoy, and the discussion will cover probability, betting, and value of concensus as evidence.</p>\n<p><em>No foreknowledge or exposure to Less Wrong is necessary</em>; this will be generally accessable and useful to anyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Wits_and_Wagers1\">Discussion article for the meetup : <a href=\"/meetups/in\">West LA Meetup - Wits and Wagers</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Wits and Wagers", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Wits_and_Wagers", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Wits and Wagers", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Wits_and_Wagers1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-29T10:31:21.726Z", "modifiedAt": null, "url": null, "title": "[minor] Separate Upvotes and Downvotes Implimented", "slug": "minor-separate-upvotes-and-downvotes-implimented", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:33.449Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Larks", "createdAt": "2009-04-28T20:21:45.860Z", "isAdmin": false, "displayName": "Larks"}, "userId": "jQXwiWxFcfyYjytXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7e5LpsnJEuRgnSytM/minor-separate-upvotes-and-downvotes-implimented", "pageUrlRelative": "/posts/7e5LpsnJEuRgnSytM/minor-separate-upvotes-and-downvotes-implimented", "linkUrl": "https://www.lesswrong.com/posts/7e5LpsnJEuRgnSytM/minor-separate-upvotes-and-downvotes-implimented", "postedAtFormatted": "Tuesday, January 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bminor%5D%20Separate%20Upvotes%20and%20Downvotes%20Implimented&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bminor%5D%20Separate%20Upvotes%20and%20Downvotes%20Implimented%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7e5LpsnJEuRgnSytM%2Fminor-separate-upvotes-and-downvotes-implimented%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bminor%5D%20Separate%20Upvotes%20and%20Downvotes%20Implimented%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7e5LpsnJEuRgnSytM%2Fminor-separate-upvotes-and-downvotes-implimented", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7e5LpsnJEuRgnSytM%2Fminor-separate-upvotes-and-downvotes-implimented", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<p>It seems that if you look at the column on the right of the page, you can see upvotes and downvotes separately for recent posts. The same [n, m] format is displayed for recent comments, but it doesn't seem to actually sync with the score displaying on the comment. This feature only seems available on the sidebar: looking at the actual comment or post doesn't give you this information.</p>\n<p>&nbsp;</p>\n<p>Thanks, whoever did this!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7e5LpsnJEuRgnSytM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 46, "extendedScore": null, "score": 1.0976566915196511e-06, "legacy": true, "legacyId": "21382", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-30T02:06:32.404Z", "modifiedAt": null, "url": null, "title": "Huy Price (Cambridge philosopher) writes about existential risk for NYT", "slug": "huy-price-cambridge-philosopher-writes-about-existential", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.640Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Graehl", "createdAt": "2009-02-27T23:21:15.671Z", "isAdmin": false, "displayName": "Jonathan_Graehl"}, "userId": "eKsWtKKceoRYwcc7s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a6vyymSLYLWvLhvv6/huy-price-cambridge-philosopher-writes-about-existential", "pageUrlRelative": "/posts/a6vyymSLYLWvLhvv6/huy-price-cambridge-philosopher-writes-about-existential", "linkUrl": "https://www.lesswrong.com/posts/a6vyymSLYLWvLhvv6/huy-price-cambridge-philosopher-writes-about-existential", "postedAtFormatted": "Wednesday, January 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Huy%20Price%20(Cambridge%20philosopher)%20writes%20about%20existential%20risk%20for%20NYT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHuy%20Price%20(Cambridge%20philosopher)%20writes%20about%20existential%20risk%20for%20NYT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa6vyymSLYLWvLhvv6%2Fhuy-price-cambridge-philosopher-writes-about-existential%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Huy%20Price%20(Cambridge%20philosopher)%20writes%20about%20existential%20risk%20for%20NYT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa6vyymSLYLWvLhvv6%2Fhuy-price-cambridge-philosopher-writes-about-existential", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa6vyymSLYLWvLhvv6%2Fhuy-price-cambridge-philosopher-writes-about-existential", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<blockquote>\n<p><span style=\"color: #333333; font-family: georgia, 'times new roman', times, serif; font-size: 14px; line-height: 21px;\">In Copenhagen the summer before last, I shared a taxi with a man who thought his chance of dying in an artificial intelligence-related accident was as high as that of heart disease or cancer. No surprise if he&rsquo;d been the driver, perhaps (never tell a taxi driver that you&rsquo;re a philosopher!), but this was a man who has spent his career with computers.</span></p>\n</blockquote>\n<p><a href=\"http://opinionator.blogs.nytimes.com/2013/01/27/cambridge-cabs-and-copenhagen-my-route-to-existential-risk\">NYTimes</a></p>\n<p>Nothing new for LW, but interesting to see some non-sci-fi public discussion of AI risk.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a6vyymSLYLWvLhvv6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -6, "extendedScore": null, "score": 1.0982362525324453e-06, "legacy": true, "legacyId": "21383", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-30T04:36:35.685Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Continuous Improvement", "slug": "seq-rerun-continuous-improvement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.792Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5RrsnqHqiF2NCfRSJ/seq-rerun-continuous-improvement", "pageUrlRelative": "/posts/5RrsnqHqiF2NCfRSJ/seq-rerun-continuous-improvement", "linkUrl": "https://www.lesswrong.com/posts/5RrsnqHqiF2NCfRSJ/seq-rerun-continuous-improvement", "postedAtFormatted": "Wednesday, January 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Continuous%20Improvement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Continuous%20Improvement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5RrsnqHqiF2NCfRSJ%2Fseq-rerun-continuous-improvement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Continuous%20Improvement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5RrsnqHqiF2NCfRSJ%2Fseq-rerun-continuous-improvement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5RrsnqHqiF2NCfRSJ%2Fseq-rerun-continuous-improvement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 369, "htmlBody": "<p>Today's post, <a href=\"/lw/xk/continuous_improvement/\">Continuous Improvement</a> was originally published on 11 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Humans seem to be on a hedonic treadmill; over time, we adjust to any improvements in our environment - after a month, the new sports car no longer seems quite as wonderful. This aspect of our evolved psychology is not surprising: it is a rare organism in a rare environment whose optimal reproductive strategy is to rest with a smile on its face, feeling happy with what it already has. To entirely delete the hedonic treadmill seems perilously close to tampering with Boredom itself. Is there enough fun in the universe for a transhuman to jog off the treadmill - improve their life continuously, leaping to ever-higher hedonic levels before adjusting to the previous one? Can ever-higher levels of pleasure be created by the simple increase of ever-larger floating-point numbers in a digital pleasure center, or would that fail to have the full subjective quality of happiness? If we continue to bind our pleasures to novel challenges, can we find higher levels of pleasure fast enough, without cheating? The rate at which value can increase as more bits are added, and the rate at which value must increase for eudaimonia, together determine the lifespan of a mind. If minds must use exponentially more resources over time in order to lead a eudaimonic existence, their subjective lifespan is measured in mere millennia even if they can draw on galaxy-sized resources.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ght/seq_rerun_serious_stories/\">Serious Stories</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5RrsnqHqiF2NCfRSJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0983292967880398e-06, "legacy": true, "legacyId": "21386", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QfpHRAMRM2HjteKFK", "nzFFb7ntZ5CS3gkv2", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-30T08:10:15.584Z", "modifiedAt": null, "url": null, "title": "[LINK] Obviously transhumanist SMBC comic", "slug": "link-obviously-transhumanist-smbc-comic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:29.321Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cthulhoo", "createdAt": "2011-10-26T09:55:59.278Z", "isAdmin": false, "displayName": "Cthulhoo"}, "userId": "9Bn8KsjZrr5DmFNHW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3HcHjGnEiEqbezfgE/link-obviously-transhumanist-smbc-comic", "pageUrlRelative": "/posts/3HcHjGnEiEqbezfgE/link-obviously-transhumanist-smbc-comic", "linkUrl": "https://www.lesswrong.com/posts/3HcHjGnEiEqbezfgE/link-obviously-transhumanist-smbc-comic", "postedAtFormatted": "Wednesday, January 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Obviously%20transhumanist%20SMBC%20comic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Obviously%20transhumanist%20SMBC%20comic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3HcHjGnEiEqbezfgE%2Flink-obviously-transhumanist-smbc-comic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Obviously%20transhumanist%20SMBC%20comic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3HcHjGnEiEqbezfgE%2Flink-obviously-transhumanist-smbc-comic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3HcHjGnEiEqbezfgE%2Flink-obviously-transhumanist-smbc-comic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p><a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2871#comic\">http://www.smbc-comics.com/index.php?db=comics&amp;id=2871#comic</a></p>\n<p>&nbsp;</p>\n<p>Beautiful, with a high emotional impact. A more poetical verison of EY's baseball bat metaphor.</p>\n<p>&nbsp;</p>\n<p>Edit:</p>\n<p>&nbsp;</p>\n<p>Link corrected, I apparently just copy-pasted and didn't notice I was linking to the main page.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3HcHjGnEiEqbezfgE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -6, "extendedScore": null, "score": 1.0984618076930244e-06, "legacy": true, "legacyId": "21396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-30T12:46:28.926Z", "modifiedAt": null, "url": null, "title": "The Zeroth Skillset", "slug": "the-zeroth-skillset", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.991Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vmBHCPZxunwdbFvaJ/the-zeroth-skillset", "pageUrlRelative": "/posts/vmBHCPZxunwdbFvaJ/the-zeroth-skillset", "linkUrl": "https://www.lesswrong.com/posts/vmBHCPZxunwdbFvaJ/the-zeroth-skillset", "postedAtFormatted": "Wednesday, January 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Zeroth%20Skillset&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Zeroth%20Skillset%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvmBHCPZxunwdbFvaJ%2Fthe-zeroth-skillset%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Zeroth%20Skillset%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvmBHCPZxunwdbFvaJ%2Fthe-zeroth-skillset", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvmBHCPZxunwdbFvaJ%2Fthe-zeroth-skillset", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 467, "htmlBody": "<p><strong>Related: </strong><a href=\"http://rationalpoker.com/2011/07/30/23-cognitive-mistakes-that-make-people-play-bad-poker/\">23 Cognitive Mistakes that make People Play Bad Poker</a><strong></strong></p>\n<p><strong>Followed by: </strong>Situational Awareness And You</p>\n<p>If epistemic rationality is the art of updating one's beliefs based on new evidence to better correspond with reality, the zeroth skillset of epistemic rationality-- the one that enables all other skills to function-- is that of <em>situational awareness. </em>Situational awareness-- sometimes referred to as \"situation awareness\" or simply \"SA\"-- is the skillset and related state of mind that allows one to effectively perceive the world around them.</p>\n<p>One might ask how this relates to rationality at all.&nbsp;The answer is simple. Just as the skill of lucid dreaming is near-useless without dream recall,<sup>[1]</sup>&nbsp;the skills of updating based on evidence and <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">actually changing your mind</a> are near-useless without good awareness skills-- after all,&nbsp;you can't update based on evidence that you haven't collected! A high degree of situational awareness is thus an important part of one's <a href=\"/lw/7e5/the_cognitive_science_of_rationality/\">rationalist toolkit</a>, as it allows you to notice evidence about the world around you that you would otherwise miss. At times, this evidence can be of critical importance. I can attest that I have personally saved the lives of friends on two occasions thanks to good situational awareness, and have saved myself from serious injury or death many times more.</p>\n<p>Situational awareness is further lauded by elite military units, police trainers, criminals,&nbsp;intelligence analysts, and human factors researchers. In other words, people who have to make very important-- often life-or-death-- decisions based on limited information consider situational awareness a critical skill. This should tell us something-- if those individuals for whom correct decisions are most immediately relevant all stress the importance of situational awareness, it may be a more critical skill than we realize.</p>\n<p>Unfortunately, the only discussion of situational awareness that I've seen on LessWrong or related sites has been a somewhat oblique reference in Louie Helm's \"roadmap of errors\" from&nbsp;<a href=\"http://rationalpoker.com/2011/07/30/23-cognitive-mistakes-that-make-people-play-bad-poker/\">23 Cognitive Mistakes that make People Play Bad Poker</a>.<sup>[2]</sup>&nbsp;I believe that situational awareness is important enough that it merits an explicit sequence of posts on its advantages and how to cultivate it, and this post will serve as the introduction to that sequence.</p>\n<p>The first post in the sequence, unimaginatively titled \"Situational Awareness and You,\" will be posted within the week. Other planned posts include \"Cultivating Awareness,\" \"How to Win a Duel,\" \"Social Awareness,\" \"Be Aware of Your Reference Class,\" \"Signaling and Predation,\" and \"Constant Vigilance!\"</p>\n<p>If you have any requests for things to add, general questions about the sequence, meta-thoughts about SA, and so on, this post is an appropriate place for that discussion; as this is primarily a meta post, it has been posted to Discussion. Core posts in the sequence will be posted to Main.</p>\n<p>&nbsp;</p>\n<p>[1] What good are lucid dreams if you can't remember them?</p>\n<p>[2] This is a very useful summary and you should read it&nbsp;even if you don't play poker.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vmBHCPZxunwdbFvaJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 74, "extendedScore": null, "score": 0.0001753615793101172, "legacy": true, "legacyId": "20906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xLm9mgJRPvmPGpo7Q"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-30T13:14:28.476Z", "modifiedAt": null, "url": null, "title": "Simulating Problems", "slug": "simulating-problems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:30.110Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Andreas_Giger", "createdAt": "2011-09-12T00:45:35.617Z", "isAdmin": false, "displayName": "Andreas_Giger"}, "userId": "JjKS2qrYMyWoCcjnH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mKboKweGeb2HpMHTw/simulating-problems", "pageUrlRelative": "/posts/mKboKweGeb2HpMHTw/simulating-problems", "linkUrl": "https://www.lesswrong.com/posts/mKboKweGeb2HpMHTw/simulating-problems", "postedAtFormatted": "Wednesday, January 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simulating%20Problems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimulating%20Problems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmKboKweGeb2HpMHTw%2Fsimulating-problems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simulating%20Problems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmKboKweGeb2HpMHTw%2Fsimulating-problems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmKboKweGeb2HpMHTw%2Fsimulating-problems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 301, "htmlBody": "<p>Apologies for the rather mathematical nature of this post, but it seems to have some implications for topics relevant to LW. Prior to posting I looked for literature on this but was unable to find any; pointers would be appreciated.</p>\n<p>In short, my question is: How can we prove that any simulation of a problem really simulates the problem?</p>\n<p>I want to demonstrate that this is not as obvious as it may seem by using the example of Newcomb's Problem. The issue here is of course Omega's omniscience. If we construct a simulation with the rules (payoffs) of Newcomb, an Omega that is always right, and an interface for the agent to interact with the simulation, will that be enough?</p>\n<p>Let's say we simulate Omega's prediction by a coin toss and repeat the simulation (without payoffs) until the coin toss matches the agent's decision. This seems to adhere to all specifications of Newcomb and is (if the coin toss is hidden) in fact indistinguishable from it from the agent's perspective. However, if the agent knows how the simulation works, a CDT agent will one-box, while it is assumed that the same agent would two-box in 'real' Newcomb. Not telling the agent how the simulation works is never a solution, so this simulation appears to not actually simulate Newcomb.</p>\n<p>Pointing out differences is of course far easier than proving that none exist. Assuming there's a problem we have no idea which decisions agents would make, and we want to build a real-world simulation to find out exactly that. How can we prove that this simulation really simulates the problem?</p>\n<p>&nbsp;</p>\n<p>(Edit: Apparently it wasn't apparent that this is about problems in terms of game theory and decision theory. Newcomb, Prisoner's Dilemma, Iterated Prisoner's Dilemma, Monty Hall, Sleeping Beauty, Two Envelopes, that sort of stuff. Should be clear now.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mKboKweGeb2HpMHTw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 1.0986505250647647e-06, "legacy": true, "legacyId": "21398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-30T21:59:31.697Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:31.614Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HBckAgMaLt5pimPNc/meetup-washington-dc-fun-and-games-meetup-0", "pageUrlRelative": "/posts/HBckAgMaLt5pimPNc/meetup-washington-dc-fun-and-games-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/HBckAgMaLt5pimPNc/meetup-washington-dc-fun-and-games-meetup-0", "postedAtFormatted": "Wednesday, January 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBckAgMaLt5pimPNc%2Fmeetup-washington-dc-fun-and-games-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBckAgMaLt5pimPNc%2Fmeetup-washington-dc-fun-and-games-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBckAgMaLt5pimPNc%2Fmeetup-washington-dc-fun-and-games-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/io'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 February 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery , Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting up to hang out and play games! Bring some if you have them: I'll be bringing Zendo, but the more the merrier!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/io'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HBckAgMaLt5pimPNc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0989763730855263e-06, "legacy": true, "legacyId": "21399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/io\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 February 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery , Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting up to hang out and play games! Bring some if you have them: I'll be bringing Zendo, but the more the merrier!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/io\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-30T22:58:47.916Z", "modifiedAt": null, "url": null, "title": "What would you do with an Evil AI?", "slug": "what-would-you-do-with-an-evil-ai", "viewCount": null, "lastCommentedAt": "2020-02-22T22:27:13.931Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iE3gAxsfCvXeByWNq/what-would-you-do-with-an-evil-ai", "pageUrlRelative": "/posts/iE3gAxsfCvXeByWNq/what-would-you-do-with-an-evil-ai", "linkUrl": "https://www.lesswrong.com/posts/iE3gAxsfCvXeByWNq/what-would-you-do-with-an-evil-ai", "postedAtFormatted": "Wednesday, January 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20would%20you%20do%20with%20an%20Evil%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20would%20you%20do%20with%20an%20Evil%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiE3gAxsfCvXeByWNq%2Fwhat-would-you-do-with-an-evil-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20would%20you%20do%20with%20an%20Evil%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiE3gAxsfCvXeByWNq%2Fwhat-would-you-do-with-an-evil-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiE3gAxsfCvXeByWNq%2Fwhat-would-you-do-with-an-evil-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 286, "htmlBody": "<p>One plot-thread in my pet SF setting, 'New Attica', has ended up with Our Heroes  in possession of the data, software, and suchlike which comprise a  non-sapient, but conversation-capable, AI. There are bunches of those  floating around the solar system, programmed for various tasks; what  makes this one special is that it's evil with a capital ugh - it's  captured people inside VR, put them through violent and degrading scenarios  to get them to despair, and tried keeping them in there, for extended  periods, until they died of exhaustion.<br /> <br /> Through a few clever strategies, Our Heroes recognized they weren't in  reality, engineered their escape, and shut down the AI, with no  permanent physical harm done to them (though the same can't be said for  the late crew of the previous ship it was on). And now they get to debate amongst  themselves - what should they do with the thing? What use or purpose  could they put such a thing to, that would provide a greater benefit  than the risk of it getting free of whatever fetters they place upon it?</p>\n<p><a id=\"more\"></a>This is somewhat of a different take than Eliezer's now-classic 'boxed AI' problem, such as the AI not being superintelligent, and having already demonstrated some aspects of itself by performing highly antisocial activities. However, it does have enough similarities that, perhaps, thinking about one might shed some light on the other.</p>\n<p>So: Anyone want to create some further verses for something sung to the tune of 'Drunken Sailor'?</p>\n<p>&nbsp;</p>\n<p>What shall we do with an evil AI?</p>\n<p>What shall we do with an evil AI?</p>\n<p>What shall we do with an evil AI?</p>\n<p>Ear-lie in the future.</p>\n<p>&nbsp;</p>\n<p>Weigh-hay and upgrade ourselves,</p>\n<p>Weigh-hay and upgrade ourselves,</p>\n<p>Weigh-hay and upgrade ourselves,</p>\n<p>Ear-lie in the future.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iE3gAxsfCvXeByWNq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -10, "extendedScore": null, "score": 1.0990131669337137e-06, "legacy": true, "legacyId": "21400", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-30T23:58:28.631Z", "modifiedAt": null, "url": null, "title": "If it were morally correct to kill everyone on earth, would you do it?", "slug": "if-it-were-morally-correct-to-kill-everyone-on-earth-would", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:49.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bundle_Gerbe", "createdAt": "2012-05-30T20:44:50.464Z", "isAdmin": false, "displayName": "Bundle_Gerbe"}, "userId": "C53AQZivJqHBGWv5u", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jCJctssbJAZoapsaX/if-it-were-morally-correct-to-kill-everyone-on-earth-would", "pageUrlRelative": "/posts/jCJctssbJAZoapsaX/if-it-were-morally-correct-to-kill-everyone-on-earth-would", "linkUrl": "https://www.lesswrong.com/posts/jCJctssbJAZoapsaX/if-it-were-morally-correct-to-kill-everyone-on-earth-would", "postedAtFormatted": "Wednesday, January 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20it%20were%20morally%20correct%20to%20kill%20everyone%20on%20earth%2C%20would%20you%20do%20it%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20it%20were%20morally%20correct%20to%20kill%20everyone%20on%20earth%2C%20would%20you%20do%20it%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjCJctssbJAZoapsaX%2Fif-it-were-morally-correct-to-kill-everyone-on-earth-would%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20it%20were%20morally%20correct%20to%20kill%20everyone%20on%20earth%2C%20would%20you%20do%20it%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjCJctssbJAZoapsaX%2Fif-it-were-morally-correct-to-kill-everyone-on-earth-would", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjCJctssbJAZoapsaX%2Fif-it-were-morally-correct-to-kill-everyone-on-earth-would", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 485, "htmlBody": "<p>First consider the following question to make sure we're on the same page in terms of moral reasoning: social consequences aside, is it morally correct to kill one person to create a million people who would not have otherwise existed? Let's suppose these people are whisked into existence on a spaceship travelling away from earth at light speed, and they live healthy, happy lives, but eventually die.&nbsp;</p>\n<p>I'd argue that anyone who adheres to \"shut up and multiply\" (i.e. total utilitarianism) has to say yes. Is it better to create one such person than to donate 200 dollars to Oxfam? Is one life worth more than a 200 million dollar donation to Oxfam? Seems pretty clear that the answers are \"yes\" and \"no\".</p>\n<p>Now, suppose we have a newly created superintelligent FAI that's planning out how to fill the universe with human value. Should it first record everyone's brain, thus saving them, or should do whatever it takes to explode as quickly as possible? It's hard to estimate how much it would slow things down to get everyone's brain recorded, but it's certainly some sort of constraint. Depending on the power of the FAI, my guess is somewhere between a second and a few hours. If the FAI is going to be filling the universe with computronium simulating happy, fulfilled humans at extremely high speeds, that's a big deal! A second's delay across the future light-cone of earth could easily add up to more than the value of every currently living human's life. It may sound bad to kill everyone on earth just to save a second (or maybe scan only a few thousand people for \"research\"), but that's only because of scope insensitivity. If only we understood just <em>how good </em>saving that second would be, maybe we would all agree that it is not only <em>right</em>&nbsp;but downright <em>heroic</em>&nbsp;to do so!</p>\n<p>A related scenario: a FAI that we are very, very sure correctly implements CEV sets up a universe in which everyone gets 20 years to live, starting from a adult transhuman state. It turns out that there are&nbsp;diminishing&nbsp;returns in terms of value to longer and longer life spans, and this is the best way to use the computational power. The transhumans have been modified not to have any anxiety or fear about death, and agree this is the best way to do things. Their human ancestors' desire for immortality is viewed as deeply wrong, even barbaric. In short, all signs point to this really being the coherent extrapolated volition of humanity.&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Besides opinions on whether or not either of these scenarios are plausible, I'd like hear reactions to these scenarios as thought experiments. Is this a problem for total utilitarianism or for CEV? Is this an argument for \"grabbing the banana\" as a species and if necessary knowingly making an AI that does something other than the morally correct thing? Anyone care to bite the bullet?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jCJctssbJAZoapsaX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": -9, "extendedScore": null, "score": 1.0990502164280456e-06, "legacy": true, "legacyId": "21401", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-31T05:25:03.937Z", "modifiedAt": null, "url": null, "title": "Second major sequence now available in audio format", "slug": "second-major-sequence-now-available-in-audio-format", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:04.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rick_from_Castify", "createdAt": "2012-12-03T09:33:28.512Z", "isAdmin": false, "displayName": "Rick_from_Castify"}, "userId": "XyTqQupkZ9SW7nGCB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yhynHHKb7Hgmefwq6/second-major-sequence-now-available-in-audio-format", "pageUrlRelative": "/posts/yhynHHKb7Hgmefwq6/second-major-sequence-now-available-in-audio-format", "linkUrl": "https://www.lesswrong.com/posts/yhynHHKb7Hgmefwq6/second-major-sequence-now-available-in-audio-format", "postedAtFormatted": "Thursday, January 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Second%20major%20sequence%20now%20available%20in%20audio%20format&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASecond%20major%20sequence%20now%20available%20in%20audio%20format%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyhynHHKb7Hgmefwq6%2Fsecond-major-sequence-now-available-in-audio-format%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Second%20major%20sequence%20now%20available%20in%20audio%20format%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyhynHHKb7Hgmefwq6%2Fsecond-major-sequence-now-available-in-audio-format", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyhynHHKb7Hgmefwq6%2Fsecond-major-sequence-now-available-in-audio-format", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<p>The sequence \"<a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">A Human's Guide to Words</a>\" is now available as a <a href=\"http://castify.co/channels/16-less-wrong-a-human-s-guide-to-words\">professionally read podcast</a>.</p>\n<p>We have started working on the large \"<a href=\"http://wiki.lesswrong.com/wiki/Reductionism_%28sequence%29\">Reductionism</a>\" sequence which includes both the \"<a href=\"http://wiki.lesswrong.com/wiki/Joy_in_the_Merely_Real\">Joy in the Merely Real</a>\" and the \"<a href=\"http://wiki.lesswrong.com/wiki/Zombies_(sequence)\">Zombies</a>\" sub-sequences.&nbsp; They should be available in a couple of weeks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yhynHHKb7Hgmefwq6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 35, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "21365", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-31T06:06:21.420Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Eutopia is Scary", "slug": "seq-rerun-eutopia-is-scary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:31.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SqjG3Go5N5ikxYJkv/seq-rerun-eutopia-is-scary", "pageUrlRelative": "/posts/SqjG3Go5N5ikxYJkv/seq-rerun-eutopia-is-scary", "linkUrl": "https://www.lesswrong.com/posts/SqjG3Go5N5ikxYJkv/seq-rerun-eutopia-is-scary", "postedAtFormatted": "Thursday, January 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Eutopia%20is%20Scary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Eutopia%20is%20Scary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSqjG3Go5N5ikxYJkv%2Fseq-rerun-eutopia-is-scary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Eutopia%20is%20Scary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSqjG3Go5N5ikxYJkv%2Fseq-rerun-eutopia-is-scary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSqjG3Go5N5ikxYJkv%2Fseq-rerun-eutopia-is-scary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 328, "htmlBody": "<p>Today's post, <a href=\"/lw/xl/eutopia_is_scary/\">Eutopia is Scary</a> was originally published on 12 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If a citizen of the Past were dropped into the Present world, they would be pleasantly surprised along at least some dimensions; they would also be horrified, disgusted, and <em>frightened</em>. This is not because our world has gone wrong, but because it has gone <em>right</em>. A true Future gone right would, realistically, be shocking to us along at least <em>some </em>dimensions. This may help explain why most literary Utopias fail; as George Orwell observed, \"they are chiefly concerned with avoiding fuss\". Heavens are meant to sound like good news; political utopias are meant to show how neatly their underlying ideas work. Utopia is reassuring, unsurprising, and dull. Eutopia would be scary. (Of course the vast majority of scary things are not Eutopian, just entropic.) Try to imagine a genuinely better world in which you would be <em>out of place</em> - not a world that would make you smugly satisfied at how well all your current ideas had worked. This proved to be a very important exercise when I tried it; it made me realize that all my old proposals had been optimized to sound safe and reassuring.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gi2/seq_rerun_continuous_improvement/\">Continuous Improvement</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SqjG3Go5N5ikxYJkv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0992786514151225e-06, "legacy": true, "legacyId": "21404", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hQSaMafoizBSa3gFR", "5RrsnqHqiF2NCfRSJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-31T08:25:45.762Z", "modifiedAt": null, "url": null, "title": "Singularity Institute is now Machine Intelligence Research Institute", "slug": "singularity-institute-is-now-machine-intelligence-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:25.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PbAdoTrZF7Nkuh5sS/singularity-institute-is-now-machine-intelligence-research", "pageUrlRelative": "/posts/PbAdoTrZF7Nkuh5sS/singularity-institute-is-now-machine-intelligence-research", "linkUrl": "https://www.lesswrong.com/posts/PbAdoTrZF7Nkuh5sS/singularity-institute-is-now-machine-intelligence-research", "postedAtFormatted": "Thursday, January 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Institute%20is%20now%20Machine%20Intelligence%20Research%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Institute%20is%20now%20Machine%20Intelligence%20Research%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbAdoTrZF7Nkuh5sS%2Fsingularity-institute-is-now-machine-intelligence-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Institute%20is%20now%20Machine%20Intelligence%20Research%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbAdoTrZF7Nkuh5sS%2Fsingularity-institute-is-now-machine-intelligence-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbAdoTrZF7Nkuh5sS%2Fsingularity-institute-is-now-machine-intelligence-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p><a href=\"http://intelligence.org/blog/2013/01/30/we-are-now-the-machine-intelligence-research-institute-miri/\">http://singularity.org/blog/2013/01/30/we-are-now-the-machine-intelligence-research-institute-miri/</a></p>\n<p>As <a href=\"/user/Risto_Saarelma/\">Risto Saarelma</a> pointed out on IRC, \"Volcano Lair Doom Institute\" would have been cooler, but this is pretty good too. As the word \"Singularity\" has pretty much <a href=\"http://www.acceleratingfuture.com/michael/blog/2007/07/the-word-singularity-has-lost-all-meaning/\">lost its meaning</a>, it's better to have a name that doesn't give a new person all kinds of weird initial associations as their first impression. And \"Machine Intelligence Research Institute\" is appropriately descriptive while still being general enough.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PbAdoTrZF7Nkuh5sS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 51, "extendedScore": null, "score": 1.0993652371733985e-06, "legacy": true, "legacyId": "21412", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-31T10:16:08.725Z", "modifiedAt": null, "url": null, "title": "Thoughts on the January CFAR workshop", "slug": "thoughts-on-the-january-cfar-workshop", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:39.052Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Qiaochu_Yuan", "createdAt": "2012-11-24T08:36:50.547Z", "isAdmin": false, "displayName": "Qiaochu_Yuan"}, "userId": "qgFX9ZhzPCkcduZyB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9FfxfaLQN2rRvSjp7/thoughts-on-the-january-cfar-workshop", "pageUrlRelative": "/posts/9FfxfaLQN2rRvSjp7/thoughts-on-the-january-cfar-workshop", "linkUrl": "https://www.lesswrong.com/posts/9FfxfaLQN2rRvSjp7/thoughts-on-the-january-cfar-workshop", "postedAtFormatted": "Thursday, January 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20on%20the%20January%20CFAR%20workshop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20on%20the%20January%20CFAR%20workshop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FfxfaLQN2rRvSjp7%2Fthoughts-on-the-january-cfar-workshop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20on%20the%20January%20CFAR%20workshop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FfxfaLQN2rRvSjp7%2Fthoughts-on-the-january-cfar-workshop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FfxfaLQN2rRvSjp7%2Fthoughts-on-the-january-cfar-workshop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1182, "htmlBody": "<p>So, the&nbsp;<a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a>&nbsp;just ran another&nbsp;<a href=\"/lw/g6g/applied_rationality_workshops_jan_2528_and_march/\">workshop</a>, which Anna kindly invited me to. Below I've written down some thoughts on it, both to organize those thoughts and because&nbsp;<a href=\"/lw/gbw/open_thread_january_1631_2013/8d4f\">it seems</a>&nbsp;other LWers might want to read them. I'll also invite other participants to write down their thoughts in the comments. Apologies if what follows isn't particularly well-organized.&nbsp;</p>\n<h3>Feelings and other squishy things</h3>\n<p>The workshop was&nbsp;<em>totally awesome</em>. This is admittedly not strong evidence that it accomplished its goals (cf. Yvain's comment&nbsp;<a href=\"/lw/7ez/minicamp_was_indeed_awesome_and_so_was_luke_just/4qyj\">here</a>), but being around people motivated to improve themselves and the world was totally awesome, and learning with and from them was also totally awesome, and that seems like a good thing.&nbsp;</p>\n<p>Also,&nbsp;<a href=\"http://www.rosegardeninn.com/\">the venue</a>&nbsp;was fantastic. CFAR instructors reported that this workshop was more awesome than most, and while I don't want to discount improvements in CFAR's curriculum and its selection process for participants, I think the venue counted for a lot. It was uniformly beautiful and there were a lot of soft things to sit down or take naps on, and I think that helped everybody be more comfortable with and relaxed around each other.&nbsp;</p>\n<h3>Main takeaways</h3>\n<p>Here are some general insights I took away from the workshop. Some of them I had already been aware of on some abstract intellectual level but hadn't fully processed and/or gotten drilled into my head and/or seen the implications of.&nbsp;</p>\n<ol>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Rationality#Epistemic_rationality\">Epistemic rationality</a>&nbsp;doesn't have to be about big things like scientific facts or the existence of God, but can be about much smaller things like the details of how your particular mind works. For example, it's quite valuable to understand what your actual motivations for doing things are.&nbsp;</li>\n<li><a href=\"/lw/5sk/inferring_our_desires/\">Introspection is unreliable</a>. Consequently, you don't have direct access to information like your actual motivations for doing things. However, it's possible to access this information through less direct means. For example, if you believe that your primary motivation for doing X is that it brings about Y, you can perform a thought experiment: imagine a world in which Y has already been brought about. In that world, would you still feel motivated to do X? If so, then there may be reasons other than Y that you do X.&nbsp;</li>\n<li>The mind is&nbsp;<a href=\"/lw/4x7/simple_embodied_cognition_hacks/\">embodied</a>. If you consistently model your mind as separate from your body (I have in retrospect been doing this for a long time without explicitly realizing it), you're probably underestimating the powerful influence of your mind on your body and vice versa. For example, dominance of the sympathetic nervous system (which governs the fight-or-flight response) over the parasympathetic nervous system is unpleasant, unhealthy, and can prevent you from explicitly modeling other people. If you can notice and control it, you'll probably be happier, and if you get&nbsp;<em>really</em>&nbsp;good, you can develop&nbsp;<a href=\"/lw/fqg/looking_for_a_likely_cause_of_a_mental_phenomenon/7y3z\">aikido-related superpowers</a>.&nbsp;</li>\n<li>You are a social animal. Just as your mind should be modeled as a part of your body, you should be modeled as a part of human society. For example, if you don't think you care about social approval, you are probably wrong, and thinking that will cause you to have incorrect beliefs about things like your actual motivations for doing things.&nbsp;</li>\n<li>Emotions are data. Your emotional responses to stimuli give you information about what's going on in your mind that you can use. For example, if you learn that a certain stimulus reliably makes you angry and you don't want to be angry, you can remove that stimulus from your environment. (This point should be understood in combination with point 2 so that it doesn't sound trivial: you don't have direct access to information like what stimuli make you angry.)&nbsp;</li>\n<li>Emotions are tools. You can trick your mind into having specific emotions, and you can trick your mind into having specific emotions in response to specific stimuli. This can be very useful; for example, tricking your mind into being more curious is a great way to motivate yourself to find stuff out, and tricking your mind into being happy in response to doing certain things is a great way to condition yourself to do certain things.&nbsp;<a href=\"/lw/fc3/checklist_of_rationality_habits/\">Reward your inner pigeon</a>.</li>\n</ol> <ol> </ol>\n<p>Here are some specific actions I am going to take / have already taken because of what I learned at the workshop.&nbsp;</p>\n<ol>\n<li>Write a lot more stuff down. What I can think about in my head is limited by the size of my working memory, but a piece of paper or a <a href=\"https://workflowy.com/\">WorkFlowy</a> document don't have this limitation.&nbsp;</li>\n<li>Start using a better&nbsp;<a href=\"http://en.wikipedia.org/wiki/Getting_Things_Done\">GTD</a>&nbsp;system. I was previously using&nbsp;<a href=\"http://www.rememberthemilk.com/\">RTM</a>, but badly. I was using it exclusively from my iPhone, and when adding something to RTM from an iPhone the due date defaults to \"today.\" When adding something to RTM from a browser the due date defaults to \"never.\" I had never done this, so I didn't even realize that \"never\" was an option. That resulted in having due dates attached to RTM items that didn't actually have due dates, and it also made me reluctant to add items to RTM that really didn't look like they had due dates (e.g. \"look at this interesting thing sometime\"), which was bad because that meant RTM wasn't collecting a lot of things <em>and</em> I stopped trusting my own due dates.&nbsp;</li>\n<li>Start using&nbsp;<a href=\"http://www.boomeranggmail.com/\">Boomerang</a>&nbsp;to send timed email reminders to future versions of myself. I think this might work better than using, say, calendar alerts because it should help me conceptualize past versions of myself as people I don't want to break commitments to.&nbsp;</li>\n</ol>\n<p>I'm also planning to take various actions that I'm not writing above but instead putting into my GTD system, such as practicing specific rationality techniques (the workshop included many useful worksheets for doing this) and investigating specific topics like speed-reading and meditation.&nbsp;</p>\n<p>The&nbsp;<a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/ArcWords\">arc word</a>&nbsp;(TVTropes warning) of this workshop was&nbsp;<a href=\"/lw/5i8/the_power_of_agency/\">\"agentiness.\"</a>&nbsp;(\"Agentiness\" is more funtacular than \"agency.\") The CFAR curriculum as a whole could be summarized as teaching a collection of techniques to be more agenty.&nbsp;</p>\n<h3>Miscellaneous</h3>\n<p>A distinguishing feature the people I met at the workshop seemed to have in common was the ability to go meta. This is not a skill which was explicitly mentioned or taught (although it was frequently implicit in the kind of <em>jokes</em> people told), but it strikes me as an important foundation for rationality: it seems hard to progress with rationality unless the thought of using your brain to improve how you use your brain, and also to improve how you improve how you use your brain, is both understandable and appealing to you. This probably eliminates most people as candidates for rationality training unless it's paired with or maybe preceded by meta training, whatever that looks like.</p>\n<p>One problem with the workshop was lack of sleep, which seemed to wear out both participants and instructors by the last day (classes started early in the day and conversations often continued late into the night because they were unusually fun / high-value). Offering everyone modafinil or something at the beginning of future workshops might help with this.</p>\n<h3>Overall</h3>\n<p>Overall, while it's too soon to tell how big an impact the workshop will have on my life, I anticipate a big impact, and I strongly recommend that aspiring rationalists attend future workshops.&nbsp;</p>\n<ol> </ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"X7v7Fyp9cgBYaMe2e": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9FfxfaLQN2rRvSjp7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 60, "extendedScore": null, "score": 0.00015, "legacy": true, "legacyId": "21397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>So, the&nbsp;<a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a>&nbsp;just ran another&nbsp;<a href=\"/lw/g6g/applied_rationality_workshops_jan_2528_and_march/\">workshop</a>, which Anna kindly invited me to. Below I've written down some thoughts on it, both to organize those thoughts and because&nbsp;<a href=\"/lw/gbw/open_thread_january_1631_2013/8d4f\">it seems</a>&nbsp;other LWers might want to read them. I'll also invite other participants to write down their thoughts in the comments. Apologies if what follows isn't particularly well-organized.&nbsp;</p>\n<h3 id=\"Feelings_and_other_squishy_things\">Feelings and other squishy things</h3>\n<p>The workshop was&nbsp;<em>totally awesome</em>. This is admittedly not strong evidence that it accomplished its goals (cf. Yvain's comment&nbsp;<a href=\"/lw/7ez/minicamp_was_indeed_awesome_and_so_was_luke_just/4qyj\">here</a>), but being around people motivated to improve themselves and the world was totally awesome, and learning with and from them was also totally awesome, and that seems like a good thing.&nbsp;</p>\n<p>Also,&nbsp;<a href=\"http://www.rosegardeninn.com/\">the venue</a>&nbsp;was fantastic. CFAR instructors reported that this workshop was more awesome than most, and while I don't want to discount improvements in CFAR's curriculum and its selection process for participants, I think the venue counted for a lot. It was uniformly beautiful and there were a lot of soft things to sit down or take naps on, and I think that helped everybody be more comfortable with and relaxed around each other.&nbsp;</p>\n<h3 id=\"Main_takeaways\">Main takeaways</h3>\n<p>Here are some general insights I took away from the workshop. Some of them I had already been aware of on some abstract intellectual level but hadn't fully processed and/or gotten drilled into my head and/or seen the implications of.&nbsp;</p>\n<ol>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Rationality#Epistemic_rationality\">Epistemic rationality</a>&nbsp;doesn't have to be about big things like scientific facts or the existence of God, but can be about much smaller things like the details of how your particular mind works. For example, it's quite valuable to understand what your actual motivations for doing things are.&nbsp;</li>\n<li><a href=\"/lw/5sk/inferring_our_desires/\">Introspection is unreliable</a>. Consequently, you don't have direct access to information like your actual motivations for doing things. However, it's possible to access this information through less direct means. For example, if you believe that your primary motivation for doing X is that it brings about Y, you can perform a thought experiment: imagine a world in which Y has already been brought about. In that world, would you still feel motivated to do X? If so, then there may be reasons other than Y that you do X.&nbsp;</li>\n<li>The mind is&nbsp;<a href=\"/lw/4x7/simple_embodied_cognition_hacks/\">embodied</a>. If you consistently model your mind as separate from your body (I have in retrospect been doing this for a long time without explicitly realizing it), you're probably underestimating the powerful influence of your mind on your body and vice versa. For example, dominance of the sympathetic nervous system (which governs the fight-or-flight response) over the parasympathetic nervous system is unpleasant, unhealthy, and can prevent you from explicitly modeling other people. If you can notice and control it, you'll probably be happier, and if you get&nbsp;<em>really</em>&nbsp;good, you can develop&nbsp;<a href=\"/lw/fqg/looking_for_a_likely_cause_of_a_mental_phenomenon/7y3z\">aikido-related superpowers</a>.&nbsp;</li>\n<li>You are a social animal. Just as your mind should be modeled as a part of your body, you should be modeled as a part of human society. For example, if you don't think you care about social approval, you are probably wrong, and thinking that will cause you to have incorrect beliefs about things like your actual motivations for doing things.&nbsp;</li>\n<li>Emotions are data. Your emotional responses to stimuli give you information about what's going on in your mind that you can use. For example, if you learn that a certain stimulus reliably makes you angry and you don't want to be angry, you can remove that stimulus from your environment. (This point should be understood in combination with point 2 so that it doesn't sound trivial: you don't have direct access to information like what stimuli make you angry.)&nbsp;</li>\n<li>Emotions are tools. You can trick your mind into having specific emotions, and you can trick your mind into having specific emotions in response to specific stimuli. This can be very useful; for example, tricking your mind into being more curious is a great way to motivate yourself to find stuff out, and tricking your mind into being happy in response to doing certain things is a great way to condition yourself to do certain things.&nbsp;<a href=\"/lw/fc3/checklist_of_rationality_habits/\">Reward your inner pigeon</a>.</li>\n</ol> <ol> </ol>\n<p>Here are some specific actions I am going to take / have already taken because of what I learned at the workshop.&nbsp;</p>\n<ol>\n<li>Write a lot more stuff down. What I can think about in my head is limited by the size of my working memory, but a piece of paper or a <a href=\"https://workflowy.com/\">WorkFlowy</a> document don't have this limitation.&nbsp;</li>\n<li>Start using a better&nbsp;<a href=\"http://en.wikipedia.org/wiki/Getting_Things_Done\">GTD</a>&nbsp;system. I was previously using&nbsp;<a href=\"http://www.rememberthemilk.com/\">RTM</a>, but badly. I was using it exclusively from my iPhone, and when adding something to RTM from an iPhone the due date defaults to \"today.\" When adding something to RTM from a browser the due date defaults to \"never.\" I had never done this, so I didn't even realize that \"never\" was an option. That resulted in having due dates attached to RTM items that didn't actually have due dates, and it also made me reluctant to add items to RTM that really didn't look like they had due dates (e.g. \"look at this interesting thing sometime\"), which was bad because that meant RTM wasn't collecting a lot of things <em>and</em> I stopped trusting my own due dates.&nbsp;</li>\n<li>Start using&nbsp;<a href=\"http://www.boomeranggmail.com/\">Boomerang</a>&nbsp;to send timed email reminders to future versions of myself. I think this might work better than using, say, calendar alerts because it should help me conceptualize past versions of myself as people I don't want to break commitments to.&nbsp;</li>\n</ol>\n<p>I'm also planning to take various actions that I'm not writing above but instead putting into my GTD system, such as practicing specific rationality techniques (the workshop included many useful worksheets for doing this) and investigating specific topics like speed-reading and meditation.&nbsp;</p>\n<p>The&nbsp;<a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/ArcWords\">arc word</a>&nbsp;(TVTropes warning) of this workshop was&nbsp;<a href=\"/lw/5i8/the_power_of_agency/\">\"agentiness.\"</a>&nbsp;(\"Agentiness\" is more funtacular than \"agency.\") The CFAR curriculum as a whole could be summarized as teaching a collection of techniques to be more agenty.&nbsp;</p>\n<h3 id=\"Miscellaneous\">Miscellaneous</h3>\n<p>A distinguishing feature the people I met at the workshop seemed to have in common was the ability to go meta. This is not a skill which was explicitly mentioned or taught (although it was frequently implicit in the kind of <em>jokes</em> people told), but it strikes me as an important foundation for rationality: it seems hard to progress with rationality unless the thought of using your brain to improve how you use your brain, and also to improve how you improve how you use your brain, is both understandable and appealing to you. This probably eliminates most people as candidates for rationality training unless it's paired with or maybe preceded by meta training, whatever that looks like.</p>\n<p>One problem with the workshop was lack of sleep, which seemed to wear out both participants and instructors by the last day (classes started early in the day and conversations often continued late into the night because they were unusually fun / high-value). Offering everyone modafinil or something at the beginning of future workshops might help with this.</p>\n<h3 id=\"Overall\">Overall</h3>\n<p>Overall, while it's too soon to tell how big an impact the workshop will have on my life, I anticipate a big impact, and I strongly recommend that aspiring rationalists attend future workshops.&nbsp;</p>\n<ol> </ol>", "sections": [{"title": "Feelings and other squishy things", "anchor": "Feelings_and_other_squishy_things", "level": 1}, {"title": "Main takeaways", "anchor": "Main_takeaways", "level": 1}, {"title": "Miscellaneous", "anchor": "Miscellaneous", "level": 1}, {"title": "Overall", "anchor": "Overall", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "73 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BHrj2nDcTgaJS7JFq", "2G7AH92pHyj3nC32T", "W8ibiHCtywhCnBeMk", "ttGbpJQ8shBi8hDhh", "vbcjYg6h3XzuqaaN8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-31T19:09:56.236Z", "modifiedAt": null, "url": null, "title": "Meetup : Fourth Purdue Meetup", "slug": "meetup-fourth-purdue-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uX3FwyPrN2XifQkbf/meetup-fourth-purdue-meetup", "pageUrlRelative": "/posts/uX3FwyPrN2XifQkbf/meetup-fourth-purdue-meetup", "linkUrl": "https://www.lesswrong.com/posts/uX3FwyPrN2XifQkbf/meetup-fourth-purdue-meetup", "postedAtFormatted": "Thursday, January 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fourth%20Purdue%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fourth%20Purdue%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuX3FwyPrN2XifQkbf%2Fmeetup-fourth-purdue-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fourth%20Purdue%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuX3FwyPrN2XifQkbf%2Fmeetup-fourth-purdue-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuX3FwyPrN2XifQkbf%2Fmeetup-fourth-purdue-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ip'>Fourth Purdue Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 February 2013 06:50:11PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">hicks library -purdue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Same Time. Same place as the previous meetups.</p>\n\n<p>I'll be re-teaching some of what I learned about operant conditioning at the CFAR rationality workshop.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ip'>Fourth Purdue Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uX3FwyPrN2XifQkbf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.099765495657708e-06, "legacy": true, "legacyId": "21416", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fourth_Purdue_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ip\">Fourth Purdue Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 February 2013 06:50:11PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">hicks library -purdue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Same Time. Same place as the previous meetups.</p>\n\n<p>I'll be re-teaching some of what I learned about operant conditioning at the CFAR rationality workshop.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fourth_Purdue_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ip\">Fourth Purdue Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fourth Purdue Meetup", "anchor": "Discussion_article_for_the_meetup___Fourth_Purdue_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Fourth Purdue Meetup", "anchor": "Discussion_article_for_the_meetup___Fourth_Purdue_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-01-31T20:37:32.207Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge First-Sunday Meetup", "slug": "meetup-cambridge-first-sunday-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uiZRPEMfAvGkSCFJ9/meetup-cambridge-first-sunday-meetup-0", "pageUrlRelative": "/posts/uiZRPEMfAvGkSCFJ9/meetup-cambridge-first-sunday-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/uiZRPEMfAvGkSCFJ9/meetup-cambridge-first-sunday-meetup-0", "postedAtFormatted": "Thursday, January 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%20First-Sunday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%20First-Sunday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuiZRPEMfAvGkSCFJ9%2Fmeetup-cambridge-first-sunday-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%20First-Sunday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuiZRPEMfAvGkSCFJ9%2Fmeetup-cambridge-first-sunday-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuiZRPEMfAvGkSCFJ9%2Fmeetup-cambridge-first-sunday-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/iq'>Cambridge First-Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 February 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups recur on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/iq'>Cambridge First-Sunday Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uiZRPEMfAvGkSCFJ9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0998199456022416e-06, "legacy": true, "legacyId": "21417", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge_First_Sunday_Meetup\">Discussion article for the meetup : <a href=\"/meetups/iq\">Cambridge First-Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 February 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups recur on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge_First_Sunday_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/iq\">Cambridge First-Sunday Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge First-Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge_First_Sunday_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge First-Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge_First_Sunday_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T02:06:54.796Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Building Weirdtopia", "slug": "seq-rerun-building-weirdtopia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.814Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QbF32umauPQ9imJeX/seq-rerun-building-weirdtopia", "pageUrlRelative": "/posts/QbF32umauPQ9imJeX/seq-rerun-building-weirdtopia", "linkUrl": "https://www.lesswrong.com/posts/QbF32umauPQ9imJeX/seq-rerun-building-weirdtopia", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Building%20Weirdtopia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Building%20Weirdtopia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQbF32umauPQ9imJeX%2Fseq-rerun-building-weirdtopia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Building%20Weirdtopia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQbF32umauPQ9imJeX%2Fseq-rerun-building-weirdtopia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQbF32umauPQ9imJeX%2Fseq-rerun-building-weirdtopia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 226, "htmlBody": "<p>Today's post, <a href=\"/lw/xm/building_weirdtopia/\">Building Weirdtopia</a> was originally published on 12 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Utopia and Dystopia both confirm the moral sensibilities you started with; whether the world is a libertarian utopia of government non-interference, or a hellish dystopia of government intrusion and regulation, either way you get to say \"Guess I was right all along.\" To break out of this mold, write down the Utopia, and the Dystopia, and then try to write down the Weirdtopia - an arguably-better world that zogs instead of zigging or zagging. (Judging from the comments, this exercise seems to have mostly failed.)</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gik/seq_rerun_eutopia_is_scary/\">Eutopia is Scary</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QbF32umauPQ9imJeX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1000247215699675e-06, "legacy": true, "legacyId": "21419", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cWjK3SbRcLkb3gN69", "SqjG3Go5N5ikxYJkv", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T03:58:00.574Z", "modifiedAt": null, "url": null, "title": "Pinpointing Utility", "slug": "pinpointing-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:59.779Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CQkGJ2t5Rw8GcZKJm/pinpointing-utility", "pageUrlRelative": "/posts/CQkGJ2t5Rw8GcZKJm/pinpointing-utility", "linkUrl": "https://www.lesswrong.com/posts/CQkGJ2t5Rw8GcZKJm/pinpointing-utility", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pinpointing%20Utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APinpointing%20Utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQkGJ2t5Rw8GcZKJm%2Fpinpointing-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pinpointing%20Utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQkGJ2t5Rw8GcZKJm%2Fpinpointing-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQkGJ2t5Rw8GcZKJm%2Fpinpointing-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3793, "htmlBody": "<p><strong>Following</strong> <a href=\"/lw/g7y/morality_is_awesome/\">Morality is Awesome</a>. <strong>Related:</strong> <a href=\"/lw/f4e/logical_pinpointing/\">Logical Pinpointing</a>, <a href=\"https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM</a>.</p>\n<p>The eternal question, with a quantitative edge: A wizard has turned you into a whale, how awesome is this?</p>\n<p>\"10.3 Awesomes\"</p>\n<p><strong>Meditate on this:</strong> What does that mean? Does that mean it's desirable? What does that tell us about how awesome it is to be turned into a whale? Explain. Take a crack at it for real. What does it mean for something to be labeled as a certain amount of \"awesome\" or \"good\" or \"utility\"?</p>\n<h2>What is This Utility Stuff?</h2>\n<p>Most of agree that the <a href=\"https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM axioms</a> are reasonable, and that they imply that we should be maximizing this stuff called \"expected utility\". We know that expectation is just a weighted average, but what's this \"utility\" stuff?</p>\n<p>Well, to start with, it's a logical concept, which means we need to <a href=\"/lw/f4e/logical_pinpointing/\">pin it down</a> with the axioms that define it. For the moment, I'm going to <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">conflate utility and expected utility</a> for simplicity's sake. Bear with me. Here are the conditions that are necessary and sufficient to be talking about utility:</p>\n<ol>\n<li>Utility can be represented as a single real number.</li>\n<li>Each outcome has a utility.</li>\n<li>The utility of a probability distribution over outcomes is the expected utility.</li>\n<li>The action that results in the highest utility is preferred.</li>\n<li>No other operations are defined.</li>\n</ol>\n<p>I hope that wasn't too esoteric. The rest of this post will be explaining the implications of those statements. Let's see how they apply to the awesomeness of being turned into a whale:</p>\n<ol>\n<li>\"10.3 Awesomes\" is a real number.</li>\n<li>We are talking about the outcome where \"A wizard has turned you into a whale\".</li>\n<li>There are no other outcomes to aggregate with, but that's OK.</li>\n<li>There are no actions under consideration, but that's OK.</li>\n<li>Oh. Not even taking the value?</li>\n</ol>\n<p>Note 5 especially. You can probably <em>look</em> at the number without causing trouble, but if you try to treat it as meaningful for something other than condition 3 and 4, even accidentally, that's a type error.</p>\n<p>Unfortunately, you do not have a finicky compiler that will halt and warn you if you break the rules. Instead, your error will be silently ignored, and you will go on, blissfully unaware that the invariants in your decision system <em>no longer pinpoint VNM utility</em>. (Uh oh.)</p>\n<h2>Unshielded Utilities, and Cautions for Utility-Users</h2>\n<p>Let's imagine that utilities are radioactive; If we are careful with out containment procedures, we can safely combine and compare them, but if we interact with an unshielded utility, it's over, we've committed a type error.</p>\n<p>To even get a utility to manifest itself in this plane, we have to do a little ritual. We have to take the ratio between two utility differences. For example, if we want to get a number for the utility of being turned into a whale for a day, we might take the difference between that scenario and what we would otherwise expect to do, and then take the ratio between that difference and the difference between a normal day and a day where we also get a tasty sandwich. (Make sure you take the absolute value of your unit, or you will reverse your utility function, which is a bad idea.)</p>\n<p>So the form that the utility of being a whale manifests as might be \"500 tasty sandwiches better than a normal day\". We have chosen \"a normal day\" for our <em><a href=\"https://en.wikipedia.org/wiki/Datum_(geodesy)\">datum</a></em>, and \"tasty sandwiches\" for our <em><a href=\"https://en.wikipedia.org/wiki/Units_of_measurement\">units</a></em>. Of course we could have just as easily chosen something else, like \"being turned into a whale\" as our datum, and \"orgasms\" for our units. Then it would be \"0 orgasms better than being turned into a whale\", and a normal day would be \"-400 orgasms from the whale-day\".</p>\n<p>You say: \"But you shouldn't define your utility like that, because then you are experiencing huge disutility in the normal case.\"</p>\n<p>Wrong, and radiation poisoning, and type error. You tried to \"experience\" a utility, which is not in the defined operations. Also, you looked directly at the value of an unshielded utility (also known as <a href=\"https://en.wikipedia.org/wiki/Numerology\">numerology</a>).</p>\n<p>We summoned the utilities into the real numbers, but they are still utilities, and we still can only compare and aggregate them. The summoning only gives us a number that we can numerically do those operations on, which is why we did it. This is the same situation as time, position, velocity, etc, where we have to select units and datums to get actual quantities that mathematically behave like their ideal counterparts.</p>\n<p>Sometimes people refer to this relativity of utilities as \"positive affine structure\" or \"invariant up to a scale and shift\", which confuses me by making me think of an equivalence class of utility functions with numbers coming out, which don't agree on the actual numbers, but can be made to agree with a linear transform, rather than making me think of a utility function as a space I can measure distances in. I'm an engineer, not a mathematician, so I find it much more intuitive and less confusing to think of it in terms of units and datums, even though it's basically the same thing. This way, the utility function can scale and shift all it wants, and my numbers will always be the same. Equivalently, all agents that share my preferences will always agree that a day as a whale is \"400 orgasms better than a normal day\", even if they use another basis themselves.</p>\n<p>So what does it mean that being a whale for a day is 400 orgasms better than a normal day? Does it mean I would prefer 400 orgasms to a day as a whale? Nope. Orgasms don't add up like that; I'd probably be quite tired of it by 15. (remember that \"orgasms\" were defined as the difference between a day without an orgasm and a day with one, not as the utility of a marginal orgasm in general.) What it means is that I'd be indifferent between a normal day with a 1/400 chance of being a whale, and a normal day with guaranteed extra orgasm.</p>\n<p>That is, utilities are fundamentally about how your preferences react to uncertainty. For example, You don't have to think that each marginal year of life is as valuable as the last, if you don't think you should take a gamble that will double your remaining lifespan with 60% certainty and kill you otherwise. After all, all that such a utility assignment even means is that you would take such a gamble. In the words of VNM:</p>\n<blockquote>\n<p>We have practically defined numerical utility as being <em>that thing for which</em> the calculus of mathematical expectations is legitimate.</p>\n</blockquote>\n<p>But suppose there are very good arguments that have nothing to do with uncertainty for why you should value each marginal life-year as much as the last. What then?</p>\n<p>Well, \"what then\" is that we spend a few weeks in the hospital dying of radiation poisoning, because we tried to interact with an unshielded utility again (utilities are radioactive, remember? The specific error is that we tried to manipulate the utility function with something other than comparison and aggregation. Touching a utility directly is just as much an error as observing it directly.</p>\n<p>But if the only way to define your utility function is with thought experiments about what gambles you would take, and the only use for it is deciding what gambles you would take, then isn't it doing no work as a concept?</p>\n<p>The answer is no, but this is a good question because it gets us closer to what exactly this utility function stuff is about. The utility of utility is that defining how you would behave in one gamble puts a constraint on how you would behave in some other related gambles. As with all math, we put in some known facts, and then use the rules to derive some interesting but unknown facts.</p>\n<p>For example, if we have decided that we would be indifferent between a tasty sandwich and a 1/500 chance of being a whale for tomorrow, and that we'd be indifferent between a tasty sandwich and a 30% chance of sun instead of the usual rain, then we should also be indifferent between a certain sunny day and a 1/150 chance of being a whale.</p>\n<h2>Monolithicness and Marginal (In)Dependence</h2>\n<p>If you are really paying attention, you may be a bit confused, because it seems to you that money or time or some other consumable resource can force you to assign utilities even if there is no uncertainty in the system. That issue is complex enough to deserve its own post, so I'd like to delay it for now.</p>\n<p>Part of the solution is that as we defined them, utilities are <em>monolithic</em>. This is the implication of \"each outcome has a utility\". What this means is that you can't add and recombine utilities by decomposing and recombining outcomes. Being specific, you can't take a marginal whale from one outcome and staple it onto another outcome, and expect the marginal utilities to be the same. For example, maybe the other outcome has no oceans for your marginal whale.</p>\n<p>For a bigger example, what we have said so far about the relative value of sandwiches and sunny days and whale-days does <em>not</em> necessarily imply that we are indifferent between a 1/250 chance of being a whale and any of the following:</p>\n<ul>\n<li>\n<p>A day with <em>two</em> tasty sandwiches. (Remember that a tasty sandwich was defined as a specific difference, not a marginal sandwich in general, which has no reason to have a consistent marginal value.)</p>\n</li>\n<li>\n<p>A day with a 30% chance of sun and a certain tasty sandwich. (Maybe the tasty sandwich and the sun at the same time is horrifying for some reason. Maybe someone drilled into you as a child that \"bread in the sun\" was bad bad bad.)</p>\n</li>\n<li>\n<p>etc. You get the idea. Utilities are monolithic and fundamentally associated with particular outcomes, not marginal outcome-pieces.</p>\n</li>\n</ul>\n<p>However, as in probability theory, where each possible outcome technically has its very own probability, in practice it is useful to talk about a concept of independence.</p>\n<p>So for example, even though the axioms don't guarantee in general that it will ever be the case, it <em>may</em> work out in practice that given some conditions, like there being nothing special about bread in the sun, and my happiness not being near saturation, the utility of a marginal tasty sandwich is <em>independent</em> of a marginal sunny day, meaning that sun+sandwich is as much better than just sun as just a sandwich is better than baseline, ultimately meaning that I am indifferent between {50%: sunny+sandwich; 50% baseline} and {50%: sunny; 50%: sandwich}, and other such bets. (We need a better solution for rendering probability distributions in prose).</p>\n<p>Notice that the independence of marginal utilities can depend on conditions and that independence is with respect to some other variable, not a general property. The utility of a marginal tasty sandwich is <em>not</em> independent of whether I am hungry, for example.</p>\n<p>There is a lot more to this independence thing (and linearity, and risk aversion, and so on), so it deserves its own post. For now, the point is that the monolithicness thing is fundamental, but in practice we can sometimes look inside the black box and talk about independent marginal utilities.</p>\n<h2>Dimensionless Utility</h2>\n<p>I liked this quote from the comments of <a href=\"/lw/g7y/morality_is_awesome/\">Morality is Awesome</a>:</p>\n<blockquote>\n<p>Morality needs a concept of awfulness as well as awesomeness. In the depths of hell, good things are not an option and therefore not a consideration, but there are still choices to be made.</p>\n</blockquote>\n<p>Let's develop that second sentence a bit more. If all your options suck, what do you do? You still have to choose. So let's imagine we are in the depths of hell and see what our theories have to say about it:</p>\n<blockquote>\n<p>Day 78045. Satan has presented me with three options:</p>\n<ol>\n<li>\n<p>Go on a date with Satan Himself. This will involve romantically torturing souls together, subtly steering mortals towards self-destruction, watching people get thrown into the lake of fire, and some very unsafe, very nonconsensual sex with the Adversary himself.</p>\n</li>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">Paperclip the universe</a>.</p>\n</li>\n<li>\n<p>Satan's court wizard will turn me into a whale and release me into the lake of fire, to roast slowly for the next month, kept alive by twisted black magic.</p>\n</li>\n</ol>\n<p>Wat do?</p>\n</blockquote>\n<p>They all seem pretty bad, but \"pretty bad\" is not a utility. We could quantify paperclipping as a couple hundred billion lives lost. Being a whale in the lake of fire would be awful, but a bounded sort of awful. A month of endless horrible torture. The \"date\" is having to be on the giving end of what would more or less happen anyway, and then getting savaged by Satan. Still none of these are utilities.</p>\n<p>Coming up with actual utility numbers for these in terms of tasty sandwiches and normal days is hard; it would be like measuring the microkelvin temperatures of your physics experiment with a Fahrenheit kitchen thermometer; in principle it might work, but it isn't the best tool for the job. Instead, we'll use a different scheme this time.</p>\n<p>Engineers (and physicists?) sometimes transform problems into a <a href=\"https://en.wikipedia.org/wiki/Dimensionless_quantity\">dimensionless</a> form that removes all redundant information from the problem. For example, for a heat conduction problem, we might define an isomorphic <em>dimensionless temperature</em> so that real temperatures between 78 and 305 C become dimensionless temperatures between 0 and 1. Transforming a problem into dimensionless form is nearly always helpful, often in really surprising ways. We can do this with utility too.</p>\n<p>Back to depths of hell. The date with Satan is clearly the best option, so it gets dimensionless utility 1. The paperclipper gets 0. On that scale, I'd say roasting in the lake of fire is like 0.999 or so, but that might just be scope insensitivity. We'll take it for now.</p>\n<p>The advantages with this approach are:</p>\n<ol>\n<li>\n<p>The numbers are more intuitive. -5e12 <a href=\"http://squid314.livejournal.com/353323.html\">QALY</a>s, -1 QALY, and -50 QALYs from a normal day, or the equivalent in tasty sandwiches, just doesn't have the same feeling of clarity as 0, 1 and .999. (For me at least. And yes I know those numbers don't quite match.)</p>\n</li>\n<li>\n<p>Not having to relate the problem quantities to far-away datums or drastically misappropriate units (tasty sandwiches for this problem) makes the numbers easier and more direct to come up with. Also we have to come up with less of them. The problem is self-contained.</p>\n</li>\n<li>\n<p>If defined right, the connection between probability and utility becomes extra-clear. For example: What chance between a Satan-date and a paperclipper would make me indifferent with a lake-of-fire-whale-month? 0.999! Unitless magic!</p>\n</li>\n<li>\n<p>All confusing redundant information (like negative signs) are removed, which makes it harder to accidentally do numerology or commit a type error.</p>\n</li>\n<li>\n<p>All redundant information is removed, which means <em>you find many more similarities between problems</em>. The value of this in general cannot be understated. Just look at the generalizations made about <a href=\"https://en.wikipedia.org/wiki/Reynolds_number\">Reynolds number</a>! \"[vortex shedding] occurs for any fluid, size, and speed, provided that Re between ~40 and 10^3\". What! You can just say that in general? Magic! I haven't actually done enough utility problems to <em>know</em> that we'll find stuff like that but <a href=\"/lw/mu/trust_in_math/\">I trust</a>&nbsp;dimensionless form.</p>\n</li>\n</ol>\n<p>Anyways, it seems that going on that date is what I ought to do. So did we need a concept of awfulness? Did it matter that all the options sucked? Nope; the decision was isomorphic in every way to choosing lunch between a BLT, a turkey club, and a handful of dirt.</p>\n<p>There are some assumptions in that lunch bit, and it's worth discussing. It seems counterintuitive or even <em>wrong</em>, to say that your decision-process faced with lunch should be the same as when faced with a decision in involving torture, rape, and paperclips. The latter seems somehow <em>more important</em>. Where does that come from? Is it right?</p>\n<p>This may deserve a bigger discussion, but basically, if you have finite resources (thought-power, money, energy, stress) that are conserved or even related across decisions, you get coupling of \"different\" decisions in a way that we didn't have here. Your intuitions are calibrated for that case. Once you have decoupled the decision by coming up with the <em>actual candidate options</em>. The depths-of-hell decision and the lunch decision really are totally isomorphic. I'll probably address this properly later, if I discuss instrumental utility of resources.</p>\n<p>Anyways, once you put the problem in dimensionless form, a lot of decisions that seemed very different become almost the same, and a lot of details that seemed important or confusing just disappear. Bask in the clarifying power of a good abstraction.</p>\n<h2>Utility is Personal</h2>\n<p>So far we haven't touched the issue of interpersonal utility. That's because that topic isn't actually about VNM utility! There was nothing in the axioms above about there being a utility for each {person, outcome} pair, only for each outcome.</p>\n<p>It turns out that if you try to compare utilities between agents, you have to touch unshielded utilities, which means you get radiation poisoning and go to type-theory hell. Don't try it.</p>\n<p>And yet, it seems like we ought to care about what others prefer, and not just our own self-interest. But <em>it seems like that inside the utility function, in moral philosophy, not out here in decision theory.</em></p>\n<p>VNM has nothing to say on the issue of utilitarianism besides the usual preference-uncertainty interaction constraints, because VNM is about the preferences of a single agent. If that single agent cares about the preferences of other agents, that goes <em>inside the utility function</em>.</p>\n<p>Conversely, because VNM utility is out here, axiomized for the sovereign preferences of a single agent, we don't much expect it to show up in there, in a discussion if utilitarian preference aggregation. In fact, if we do encounter it in there, it's probably a sign of a failed abstraction.</p>\n<h2>Living with Utility</h2>\n<p>Let's go back to how much work utility does as a concept. I've spent the last few sections hammering on the work that utility does <em>not</em> do, so you may ask \"It's nice that utility theory can constrain our bets a bit, but do I really have to define my utility function by pinning down the relative utilities of every single possible outcome?\".</p>\n<p>Sort of. You can take shortcuts. We can, for example, wonder all at once whether, for all possible worlds where such is possible, you are indifferent between saving n lives and {50%: saving 2*n; 50%: saving 0}.</p>\n<p>If that seems reasonable and doesn't break in any case you can think of, you might keep it around as heuristic in your ad-hoc utility function. But then maybe you find a counterexample where you don't actually prefer the implications of such a rule. So you have to refine it a bit to respond to this new argument. This is OK; the math doesn't want you to do things you don't want to.</p>\n<p>So you can save a lot of small thought experiments by doing the right big ones, like above, but the more sweeping of a generalization you make, the more probable it is that it contains an error. In fact, <a href=\"/lw/o3/superexponential_conceptspace_and_simple_words/\">conceptspace is pretty huge</a>, so trying to construct a utility function without inside information is going to take a while no matter how you approach it. Something like <a href=\"https://en.wikipedia.org/wiki/Disassembler\">disassembling</a> the algorithms that produce your intuitions would be much more efficient, but that's probably beyond science right now.</p>\n<p>In any case, in the current term before we figure out how to formally reason the whole thing out in advance, we have to get by with some good heuristics and our <a href=\"/lw/g7y/morality_is_awesome/\">current intuitions</a> with a pinch of last minute sanity checking against the VNM rules. Ugly, but better than nothing.</p>\n<p>The whole project is made quite a bit harder in that we are not just trying to reconstruct an explicit utility function from revealed preference; we are trying to construct a utility function for a system <em>that doesn't even currently have consistent preferences</em>.</p>\n<p>At some point, either the concept of utility isn't really improving our decisions, or it will come in conflict with our intuitive preferences. In <a href=\"https://en.wikipedia.org/wiki/Zero-risk_bias\">some cases</a> it's obvious how to resolve the conflict, in others, <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">not so much</a>.</p>\n<p>But if VNM contradicts our current preferences, why do we think it's a good idea at all? Surely it's not wise to be tampering with our very values?</p>\n<p>The reason we like VNM is that we have a strong <a href=\"/lw/sk/changing_your_metaethics/\">meta-intuition</a> that our preferences ought to be internally consistent, and VNM seems to be the only way to satisfy that. But it's good to remember that this is just another intuition, to be weighed against the rest. Are we ironing out garbage inconsistencies, or losing valuable information?</p>\n<p>At this point I'm dangerously out of my depth. As far as I can tell, the great project of moral philosophy is an adult problem, not suited for mere mortals like me. Besides, I've rambled long enough.</p>\n<h2>Conclusions</h2>\n<p>What a slog! Let's review:</p>\n<ul>\n<li>\n<p>Maximize expected utility, where utility is just an encoding of your preferences that ensures a sane reaction to uncertainty.</p>\n</li>\n<li>\n<p>Don't try to do anything else with utilities, or <a href=\"http://www.catb.org/jargon/html/N/nasal-demons.html\">demons may fly out of your nose</a>. This especially includes looking at the sign or magnitude, and comparing between agents. I call these things \"numerology\" or \"interacting with an unshielded utility\".</p>\n</li>\n<li>\n<p>The default for utilities is that utilities are monolithic and inseparable from the entire outcome they are associated with. It takes special structure in your utility function to be able to talk about the marginal utility of something independently of particular outcomes.</p>\n</li>\n<li>\n<p>We have to use the difference-and-ratio ritual to summon the utilities into the real numbers. Record utilities using explicit units and datum, and use dimensionless form for your calculations, which will make many things much clearer and more robust.</p>\n</li>\n<li>\n<p>If you use a VNM basis, you don't need a concept of awfulness, just awesomeness.</p>\n</li>\n<li>\n<p>If you want to do philosophy about the shape of your utility function, make sure you phrase it in terms of lotteries, because that's what utility is about.</p>\n</li>\n<li>\n<p>The desire to use VNM is just another moral intuition in the great project of moral philosophy. It is conceivable that you will have to throw it out if it causes too much trouble.</p>\n</li>\n<li>\n<p>VNM says nothing about your utility function. Consequentialism, hedonism, utilitarianism, etc are up to you.</p>\n</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZTRNmvQGgoYiymYnq": 1, "HAFdXkW4YW4KRe2Gx": 3, "5f5c37ee1b5cdee568cfb197": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CQkGJ2t5Rw8GcZKJm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 93, "extendedScore": null, "score": 0.000245, "legacy": true, "legacyId": "21334", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 93, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Following</strong> <a href=\"/lw/g7y/morality_is_awesome/\">Morality is Awesome</a>. <strong>Related:</strong> <a href=\"/lw/f4e/logical_pinpointing/\">Logical Pinpointing</a>, <a href=\"https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM</a>.</p>\n<p>The eternal question, with a quantitative edge: A wizard has turned you into a whale, how awesome is this?</p>\n<p>\"10.3 Awesomes\"</p>\n<p><strong>Meditate on this:</strong> What does that mean? Does that mean it's desirable? What does that tell us about how awesome it is to be turned into a whale? Explain. Take a crack at it for real. What does it mean for something to be labeled as a certain amount of \"awesome\" or \"good\" or \"utility\"?</p>\n<h2 id=\"What_is_This_Utility_Stuff_\">What is This Utility Stuff?</h2>\n<p>Most of agree that the <a href=\"https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM axioms</a> are reasonable, and that they imply that we should be maximizing this stuff called \"expected utility\". We know that expectation is just a weighted average, but what's this \"utility\" stuff?</p>\n<p>Well, to start with, it's a logical concept, which means we need to <a href=\"/lw/f4e/logical_pinpointing/\">pin it down</a> with the axioms that define it. For the moment, I'm going to <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">conflate utility and expected utility</a> for simplicity's sake. Bear with me. Here are the conditions that are necessary and sufficient to be talking about utility:</p>\n<ol>\n<li>Utility can be represented as a single real number.</li>\n<li>Each outcome has a utility.</li>\n<li>The utility of a probability distribution over outcomes is the expected utility.</li>\n<li>The action that results in the highest utility is preferred.</li>\n<li>No other operations are defined.</li>\n</ol>\n<p>I hope that wasn't too esoteric. The rest of this post will be explaining the implications of those statements. Let's see how they apply to the awesomeness of being turned into a whale:</p>\n<ol>\n<li>\"10.3 Awesomes\" is a real number.</li>\n<li>We are talking about the outcome where \"A wizard has turned you into a whale\".</li>\n<li>There are no other outcomes to aggregate with, but that's OK.</li>\n<li>There are no actions under consideration, but that's OK.</li>\n<li>Oh. Not even taking the value?</li>\n</ol>\n<p>Note 5 especially. You can probably <em>look</em> at the number without causing trouble, but if you try to treat it as meaningful for something other than condition 3 and 4, even accidentally, that's a type error.</p>\n<p>Unfortunately, you do not have a finicky compiler that will halt and warn you if you break the rules. Instead, your error will be silently ignored, and you will go on, blissfully unaware that the invariants in your decision system <em>no longer pinpoint VNM utility</em>. (Uh oh.)</p>\n<h2 id=\"Unshielded_Utilities__and_Cautions_for_Utility_Users\">Unshielded Utilities, and Cautions for Utility-Users</h2>\n<p>Let's imagine that utilities are radioactive; If we are careful with out containment procedures, we can safely combine and compare them, but if we interact with an unshielded utility, it's over, we've committed a type error.</p>\n<p>To even get a utility to manifest itself in this plane, we have to do a little ritual. We have to take the ratio between two utility differences. For example, if we want to get a number for the utility of being turned into a whale for a day, we might take the difference between that scenario and what we would otherwise expect to do, and then take the ratio between that difference and the difference between a normal day and a day where we also get a tasty sandwich. (Make sure you take the absolute value of your unit, or you will reverse your utility function, which is a bad idea.)</p>\n<p>So the form that the utility of being a whale manifests as might be \"500 tasty sandwiches better than a normal day\". We have chosen \"a normal day\" for our <em><a href=\"https://en.wikipedia.org/wiki/Datum_(geodesy)\">datum</a></em>, and \"tasty sandwiches\" for our <em><a href=\"https://en.wikipedia.org/wiki/Units_of_measurement\">units</a></em>. Of course we could have just as easily chosen something else, like \"being turned into a whale\" as our datum, and \"orgasms\" for our units. Then it would be \"0 orgasms better than being turned into a whale\", and a normal day would be \"-400 orgasms from the whale-day\".</p>\n<p>You say: \"But you shouldn't define your utility like that, because then you are experiencing huge disutility in the normal case.\"</p>\n<p>Wrong, and radiation poisoning, and type error. You tried to \"experience\" a utility, which is not in the defined operations. Also, you looked directly at the value of an unshielded utility (also known as <a href=\"https://en.wikipedia.org/wiki/Numerology\">numerology</a>).</p>\n<p>We summoned the utilities into the real numbers, but they are still utilities, and we still can only compare and aggregate them. The summoning only gives us a number that we can numerically do those operations on, which is why we did it. This is the same situation as time, position, velocity, etc, where we have to select units and datums to get actual quantities that mathematically behave like their ideal counterparts.</p>\n<p>Sometimes people refer to this relativity of utilities as \"positive affine structure\" or \"invariant up to a scale and shift\", which confuses me by making me think of an equivalence class of utility functions with numbers coming out, which don't agree on the actual numbers, but can be made to agree with a linear transform, rather than making me think of a utility function as a space I can measure distances in. I'm an engineer, not a mathematician, so I find it much more intuitive and less confusing to think of it in terms of units and datums, even though it's basically the same thing. This way, the utility function can scale and shift all it wants, and my numbers will always be the same. Equivalently, all agents that share my preferences will always agree that a day as a whale is \"400 orgasms better than a normal day\", even if they use another basis themselves.</p>\n<p>So what does it mean that being a whale for a day is 400 orgasms better than a normal day? Does it mean I would prefer 400 orgasms to a day as a whale? Nope. Orgasms don't add up like that; I'd probably be quite tired of it by 15. (remember that \"orgasms\" were defined as the difference between a day without an orgasm and a day with one, not as the utility of a marginal orgasm in general.) What it means is that I'd be indifferent between a normal day with a 1/400 chance of being a whale, and a normal day with guaranteed extra orgasm.</p>\n<p>That is, utilities are fundamentally about how your preferences react to uncertainty. For example, You don't have to think that each marginal year of life is as valuable as the last, if you don't think you should take a gamble that will double your remaining lifespan with 60% certainty and kill you otherwise. After all, all that such a utility assignment even means is that you would take such a gamble. In the words of VNM:</p>\n<blockquote>\n<p>We have practically defined numerical utility as being <em>that thing for which</em> the calculus of mathematical expectations is legitimate.</p>\n</blockquote>\n<p>But suppose there are very good arguments that have nothing to do with uncertainty for why you should value each marginal life-year as much as the last. What then?</p>\n<p>Well, \"what then\" is that we spend a few weeks in the hospital dying of radiation poisoning, because we tried to interact with an unshielded utility again (utilities are radioactive, remember? The specific error is that we tried to manipulate the utility function with something other than comparison and aggregation. Touching a utility directly is just as much an error as observing it directly.</p>\n<p>But if the only way to define your utility function is with thought experiments about what gambles you would take, and the only use for it is deciding what gambles you would take, then isn't it doing no work as a concept?</p>\n<p>The answer is no, but this is a good question because it gets us closer to what exactly this utility function stuff is about. The utility of utility is that defining how you would behave in one gamble puts a constraint on how you would behave in some other related gambles. As with all math, we put in some known facts, and then use the rules to derive some interesting but unknown facts.</p>\n<p>For example, if we have decided that we would be indifferent between a tasty sandwich and a 1/500 chance of being a whale for tomorrow, and that we'd be indifferent between a tasty sandwich and a 30% chance of sun instead of the usual rain, then we should also be indifferent between a certain sunny day and a 1/150 chance of being a whale.</p>\n<h2 id=\"Monolithicness_and_Marginal__In_Dependence\">Monolithicness and Marginal (In)Dependence</h2>\n<p>If you are really paying attention, you may be a bit confused, because it seems to you that money or time or some other consumable resource can force you to assign utilities even if there is no uncertainty in the system. That issue is complex enough to deserve its own post, so I'd like to delay it for now.</p>\n<p>Part of the solution is that as we defined them, utilities are <em>monolithic</em>. This is the implication of \"each outcome has a utility\". What this means is that you can't add and recombine utilities by decomposing and recombining outcomes. Being specific, you can't take a marginal whale from one outcome and staple it onto another outcome, and expect the marginal utilities to be the same. For example, maybe the other outcome has no oceans for your marginal whale.</p>\n<p>For a bigger example, what we have said so far about the relative value of sandwiches and sunny days and whale-days does <em>not</em> necessarily imply that we are indifferent between a 1/250 chance of being a whale and any of the following:</p>\n<ul>\n<li>\n<p>A day with <em>two</em> tasty sandwiches. (Remember that a tasty sandwich was defined as a specific difference, not a marginal sandwich in general, which has no reason to have a consistent marginal value.)</p>\n</li>\n<li>\n<p>A day with a 30% chance of sun and a certain tasty sandwich. (Maybe the tasty sandwich and the sun at the same time is horrifying for some reason. Maybe someone drilled into you as a child that \"bread in the sun\" was bad bad bad.)</p>\n</li>\n<li>\n<p>etc. You get the idea. Utilities are monolithic and fundamentally associated with particular outcomes, not marginal outcome-pieces.</p>\n</li>\n</ul>\n<p>However, as in probability theory, where each possible outcome technically has its very own probability, in practice it is useful to talk about a concept of independence.</p>\n<p>So for example, even though the axioms don't guarantee in general that it will ever be the case, it <em>may</em> work out in practice that given some conditions, like there being nothing special about bread in the sun, and my happiness not being near saturation, the utility of a marginal tasty sandwich is <em>independent</em> of a marginal sunny day, meaning that sun+sandwich is as much better than just sun as just a sandwich is better than baseline, ultimately meaning that I am indifferent between {50%: sunny+sandwich; 50% baseline} and {50%: sunny; 50%: sandwich}, and other such bets. (We need a better solution for rendering probability distributions in prose).</p>\n<p>Notice that the independence of marginal utilities can depend on conditions and that independence is with respect to some other variable, not a general property. The utility of a marginal tasty sandwich is <em>not</em> independent of whether I am hungry, for example.</p>\n<p>There is a lot more to this independence thing (and linearity, and risk aversion, and so on), so it deserves its own post. For now, the point is that the monolithicness thing is fundamental, but in practice we can sometimes look inside the black box and talk about independent marginal utilities.</p>\n<h2 id=\"Dimensionless_Utility\">Dimensionless Utility</h2>\n<p>I liked this quote from the comments of <a href=\"/lw/g7y/morality_is_awesome/\">Morality is Awesome</a>:</p>\n<blockquote>\n<p>Morality needs a concept of awfulness as well as awesomeness. In the depths of hell, good things are not an option and therefore not a consideration, but there are still choices to be made.</p>\n</blockquote>\n<p>Let's develop that second sentence a bit more. If all your options suck, what do you do? You still have to choose. So let's imagine we are in the depths of hell and see what our theories have to say about it:</p>\n<blockquote>\n<p>Day 78045. Satan has presented me with three options:</p>\n<ol>\n<li>\n<p>Go on a date with Satan Himself. This will involve romantically torturing souls together, subtly steering mortals towards self-destruction, watching people get thrown into the lake of fire, and some very unsafe, very nonconsensual sex with the Adversary himself.</p>\n</li>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">Paperclip the universe</a>.</p>\n</li>\n<li>\n<p>Satan's court wizard will turn me into a whale and release me into the lake of fire, to roast slowly for the next month, kept alive by twisted black magic.</p>\n</li>\n</ol>\n<p>Wat do?</p>\n</blockquote>\n<p>They all seem pretty bad, but \"pretty bad\" is not a utility. We could quantify paperclipping as a couple hundred billion lives lost. Being a whale in the lake of fire would be awful, but a bounded sort of awful. A month of endless horrible torture. The \"date\" is having to be on the giving end of what would more or less happen anyway, and then getting savaged by Satan. Still none of these are utilities.</p>\n<p>Coming up with actual utility numbers for these in terms of tasty sandwiches and normal days is hard; it would be like measuring the microkelvin temperatures of your physics experiment with a Fahrenheit kitchen thermometer; in principle it might work, but it isn't the best tool for the job. Instead, we'll use a different scheme this time.</p>\n<p>Engineers (and physicists?) sometimes transform problems into a <a href=\"https://en.wikipedia.org/wiki/Dimensionless_quantity\">dimensionless</a> form that removes all redundant information from the problem. For example, for a heat conduction problem, we might define an isomorphic <em>dimensionless temperature</em> so that real temperatures between 78 and 305 C become dimensionless temperatures between 0 and 1. Transforming a problem into dimensionless form is nearly always helpful, often in really surprising ways. We can do this with utility too.</p>\n<p>Back to depths of hell. The date with Satan is clearly the best option, so it gets dimensionless utility 1. The paperclipper gets 0. On that scale, I'd say roasting in the lake of fire is like 0.999 or so, but that might just be scope insensitivity. We'll take it for now.</p>\n<p>The advantages with this approach are:</p>\n<ol>\n<li>\n<p>The numbers are more intuitive. -5e12 <a href=\"http://squid314.livejournal.com/353323.html\">QALY</a>s, -1 QALY, and -50 QALYs from a normal day, or the equivalent in tasty sandwiches, just doesn't have the same feeling of clarity as 0, 1 and .999. (For me at least. And yes I know those numbers don't quite match.)</p>\n</li>\n<li>\n<p>Not having to relate the problem quantities to far-away datums or drastically misappropriate units (tasty sandwiches for this problem) makes the numbers easier and more direct to come up with. Also we have to come up with less of them. The problem is self-contained.</p>\n</li>\n<li>\n<p>If defined right, the connection between probability and utility becomes extra-clear. For example: What chance between a Satan-date and a paperclipper would make me indifferent with a lake-of-fire-whale-month? 0.999! Unitless magic!</p>\n</li>\n<li>\n<p>All confusing redundant information (like negative signs) are removed, which makes it harder to accidentally do numerology or commit a type error.</p>\n</li>\n<li>\n<p>All redundant information is removed, which means <em>you find many more similarities between problems</em>. The value of this in general cannot be understated. Just look at the generalizations made about <a href=\"https://en.wikipedia.org/wiki/Reynolds_number\">Reynolds number</a>! \"[vortex shedding] occurs for any fluid, size, and speed, provided that Re between ~40 and 10^3\". What! You can just say that in general? Magic! I haven't actually done enough utility problems to <em>know</em> that we'll find stuff like that but <a href=\"/lw/mu/trust_in_math/\">I trust</a>&nbsp;dimensionless form.</p>\n</li>\n</ol>\n<p>Anyways, it seems that going on that date is what I ought to do. So did we need a concept of awfulness? Did it matter that all the options sucked? Nope; the decision was isomorphic in every way to choosing lunch between a BLT, a turkey club, and a handful of dirt.</p>\n<p>There are some assumptions in that lunch bit, and it's worth discussing. It seems counterintuitive or even <em>wrong</em>, to say that your decision-process faced with lunch should be the same as when faced with a decision in involving torture, rape, and paperclips. The latter seems somehow <em>more important</em>. Where does that come from? Is it right?</p>\n<p>This may deserve a bigger discussion, but basically, if you have finite resources (thought-power, money, energy, stress) that are conserved or even related across decisions, you get coupling of \"different\" decisions in a way that we didn't have here. Your intuitions are calibrated for that case. Once you have decoupled the decision by coming up with the <em>actual candidate options</em>. The depths-of-hell decision and the lunch decision really are totally isomorphic. I'll probably address this properly later, if I discuss instrumental utility of resources.</p>\n<p>Anyways, once you put the problem in dimensionless form, a lot of decisions that seemed very different become almost the same, and a lot of details that seemed important or confusing just disappear. Bask in the clarifying power of a good abstraction.</p>\n<h2 id=\"Utility_is_Personal\">Utility is Personal</h2>\n<p>So far we haven't touched the issue of interpersonal utility. That's because that topic isn't actually about VNM utility! There was nothing in the axioms above about there being a utility for each {person, outcome} pair, only for each outcome.</p>\n<p>It turns out that if you try to compare utilities between agents, you have to touch unshielded utilities, which means you get radiation poisoning and go to type-theory hell. Don't try it.</p>\n<p>And yet, it seems like we ought to care about what others prefer, and not just our own self-interest. But <em>it seems like that inside the utility function, in moral philosophy, not out here in decision theory.</em></p>\n<p>VNM has nothing to say on the issue of utilitarianism besides the usual preference-uncertainty interaction constraints, because VNM is about the preferences of a single agent. If that single agent cares about the preferences of other agents, that goes <em>inside the utility function</em>.</p>\n<p>Conversely, because VNM utility is out here, axiomized for the sovereign preferences of a single agent, we don't much expect it to show up in there, in a discussion if utilitarian preference aggregation. In fact, if we do encounter it in there, it's probably a sign of a failed abstraction.</p>\n<h2 id=\"Living_with_Utility\">Living with Utility</h2>\n<p>Let's go back to how much work utility does as a concept. I've spent the last few sections hammering on the work that utility does <em>not</em> do, so you may ask \"It's nice that utility theory can constrain our bets a bit, but do I really have to define my utility function by pinning down the relative utilities of every single possible outcome?\".</p>\n<p>Sort of. You can take shortcuts. We can, for example, wonder all at once whether, for all possible worlds where such is possible, you are indifferent between saving n lives and {50%: saving 2*n; 50%: saving 0}.</p>\n<p>If that seems reasonable and doesn't break in any case you can think of, you might keep it around as heuristic in your ad-hoc utility function. But then maybe you find a counterexample where you don't actually prefer the implications of such a rule. So you have to refine it a bit to respond to this new argument. This is OK; the math doesn't want you to do things you don't want to.</p>\n<p>So you can save a lot of small thought experiments by doing the right big ones, like above, but the more sweeping of a generalization you make, the more probable it is that it contains an error. In fact, <a href=\"/lw/o3/superexponential_conceptspace_and_simple_words/\">conceptspace is pretty huge</a>, so trying to construct a utility function without inside information is going to take a while no matter how you approach it. Something like <a href=\"https://en.wikipedia.org/wiki/Disassembler\">disassembling</a> the algorithms that produce your intuitions would be much more efficient, but that's probably beyond science right now.</p>\n<p>In any case, in the current term before we figure out how to formally reason the whole thing out in advance, we have to get by with some good heuristics and our <a href=\"/lw/g7y/morality_is_awesome/\">current intuitions</a> with a pinch of last minute sanity checking against the VNM rules. Ugly, but better than nothing.</p>\n<p>The whole project is made quite a bit harder in that we are not just trying to reconstruct an explicit utility function from revealed preference; we are trying to construct a utility function for a system <em>that doesn't even currently have consistent preferences</em>.</p>\n<p>At some point, either the concept of utility isn't really improving our decisions, or it will come in conflict with our intuitive preferences. In <a href=\"https://en.wikipedia.org/wiki/Zero-risk_bias\">some cases</a> it's obvious how to resolve the conflict, in others, <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">not so much</a>.</p>\n<p>But if VNM contradicts our current preferences, why do we think it's a good idea at all? Surely it's not wise to be tampering with our very values?</p>\n<p>The reason we like VNM is that we have a strong <a href=\"/lw/sk/changing_your_metaethics/\">meta-intuition</a> that our preferences ought to be internally consistent, and VNM seems to be the only way to satisfy that. But it's good to remember that this is just another intuition, to be weighed against the rest. Are we ironing out garbage inconsistencies, or losing valuable information?</p>\n<p>At this point I'm dangerously out of my depth. As far as I can tell, the great project of moral philosophy is an adult problem, not suited for mere mortals like me. Besides, I've rambled long enough.</p>\n<h2 id=\"Conclusions\">Conclusions</h2>\n<p>What a slog! Let's review:</p>\n<ul>\n<li>\n<p>Maximize expected utility, where utility is just an encoding of your preferences that ensures a sane reaction to uncertainty.</p>\n</li>\n<li>\n<p>Don't try to do anything else with utilities, or <a href=\"http://www.catb.org/jargon/html/N/nasal-demons.html\">demons may fly out of your nose</a>. This especially includes looking at the sign or magnitude, and comparing between agents. I call these things \"numerology\" or \"interacting with an unshielded utility\".</p>\n</li>\n<li>\n<p>The default for utilities is that utilities are monolithic and inseparable from the entire outcome they are associated with. It takes special structure in your utility function to be able to talk about the marginal utility of something independently of particular outcomes.</p>\n</li>\n<li>\n<p>We have to use the difference-and-ratio ritual to summon the utilities into the real numbers. Record utilities using explicit units and datum, and use dimensionless form for your calculations, which will make many things much clearer and more robust.</p>\n</li>\n<li>\n<p>If you use a VNM basis, you don't need a concept of awfulness, just awesomeness.</p>\n</li>\n<li>\n<p>If you want to do philosophy about the shape of your utility function, make sure you phrase it in terms of lotteries, because that's what utility is about.</p>\n</li>\n<li>\n<p>The desire to use VNM is just another moral intuition in the great project of moral philosophy. It is conceivable that you will have to throw it out if it causes too much trouble.</p>\n</li>\n<li>\n<p>VNM says nothing about your utility function. Consequentialism, hedonism, utilitarianism, etc are up to you.</p>\n</li>\n</ul>", "sections": [{"title": "What is This Utility Stuff?", "anchor": "What_is_This_Utility_Stuff_", "level": 1}, {"title": "Unshielded Utilities, and Cautions for Utility-Users", "anchor": "Unshielded_Utilities__and_Cautions_for_Utility_Users", "level": 1}, {"title": "Monolithicness and Marginal (In)Dependence", "anchor": "Monolithicness_and_Marginal__In_Dependence", "level": 1}, {"title": "Dimensionless Utility", "anchor": "Dimensionless_Utility", "level": 1}, {"title": "Utility is Personal", "anchor": "Utility_is_Personal", "level": 1}, {"title": "Living with Utility", "anchor": "Living_with_Utility", "level": 1}, {"title": "Conclusions", "anchor": "Conclusions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "156 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 156, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Aq8BQMXRZX3BoFd4c", "3FoMuCLqZggTxoC3S", "n5ucT5ZbPdhfGNLtP", "2MqXKvBym3kRxvJMv", "82eMd5KLiJ5Z6rTrr", "a5JAiTdytou3Jg749", "LhP2zGBWR5AdssrdJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T05:39:52.774Z", "modifiedAt": null, "url": null, "title": "[LINK] The Cryopreservation of Kim Suozzi", "slug": "link-the-cryopreservation-of-kim-suozzi", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.291Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Q9oWZLLfJtXqhi5fq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wLA3sBNZDAaQ2ekfG/link-the-cryopreservation-of-kim-suozzi", "pageUrlRelative": "/posts/wLA3sBNZDAaQ2ekfG/link-the-cryopreservation-of-kim-suozzi", "linkUrl": "https://www.lesswrong.com/posts/wLA3sBNZDAaQ2ekfG/link-the-cryopreservation-of-kim-suozzi", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20The%20Cryopreservation%20of%20Kim%20Suozzi&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20The%20Cryopreservation%20of%20Kim%20Suozzi%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLA3sBNZDAaQ2ekfG%2Flink-the-cryopreservation-of-kim-suozzi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20The%20Cryopreservation%20of%20Kim%20Suozzi%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLA3sBNZDAaQ2ekfG%2Flink-the-cryopreservation-of-kim-suozzi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLA3sBNZDAaQ2ekfG%2Flink-the-cryopreservation-of-kim-suozzi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p><a href=\"http://www.alcor.org/blog/?p=2716\">http://www.alcor.org/blog/?p=2716</a></p>\n<blockquote>\n<p>With the inevitable end in sight &ndash; and with the cancer continuing to spread throughout her brain &ndash; Kim made the brave choice to refuse food and fluids. Even so, it took around 11 days before her body stopped functioning. Around 6:00 am on Thursday January 17, 2013, Alcor was alerted that Kim had stopped breathing. Because Kim&rsquo;s steadfast boyfriend and family had located Kim just a few minutes away from Alcor, Medical Response Director Aaron Drake arrived almost immediately, followed minutes later by Max More, then two well-trained Alcor volunteers. As soon as a hospice nurse had pronounced clinical death, we began our standard procedures. Stabilization, transport, surgery, and perfusion all went smoothly. A full case report will be forthcoming.</p>\n</blockquote>\n<p>Previously on LW: <a href=\"/lw/e5d/link_reddit_help_me_find_some_peace_im_dying_young/\">Aug 18</a>, <a href=\"/lw/e8h/cryonics_donation_fund_for_kim_suozzi_established/\">Aug 25</a>, <a href=\"/lw/e9k/update_society_of_venturism_is_spearheading_kim/\">Aug 27</a>, <a href=\"/r/discussion/lw/gfb/update_on_kim_suozzi_cancer_patient_in_want_of/\">Jan 22</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wLA3sBNZDAaQ2ekfG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 22, "extendedScore": null, "score": 1.1001571602361599e-06, "legacy": true, "legacyId": "21422", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fLaKKRZckYtrrcz7m", "89MjaiTADzYXAQ6tb", "hozfqzhDrMAcwsHN6", "xgr8sDtQEEs7zfTLH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T07:52:55.689Z", "modifiedAt": null, "url": null, "title": "The value of Now.", "slug": "the-value-of-now", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:34.288Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ialdabaoth", "createdAt": "2012-10-11T00:04:32.345Z", "isAdmin": false, "displayName": "ialdabaoth"}, "userId": "335oJYQyzcd85RAoe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BzQqhoxnoYSPJB28o/the-value-of-now", "pageUrlRelative": "/posts/BzQqhoxnoYSPJB28o/the-value-of-now", "linkUrl": "https://www.lesswrong.com/posts/BzQqhoxnoYSPJB28o/the-value-of-now", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20value%20of%20Now.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20value%20of%20Now.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzQqhoxnoYSPJB28o%2Fthe-value-of-now%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20value%20of%20Now.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzQqhoxnoYSPJB28o%2Fthe-value-of-now", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzQqhoxnoYSPJB28o%2Fthe-value-of-now", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 335, "htmlBody": "<p>I am an easily bored Omega-level being, and I want to play a game with you.</p>\n<p>I am going to offer you two choices.&nbsp;</p>\n<p>Choice 1: You spend the next thousand years in horrific torture, after which I restore your local universe to precisely the state it is in now (wiping your memory in the process), and hand you a box with a billion dollars in it.</p>\n<p>Choice two: You spend the next thousand years in exquisite bliss, after which I restore your local universe to precisely the state it is in now (wiping your memory in the process), and hand you a box with an angry hornet's nest in it.</p>\n<p>Which do you choose?</p>\n<p>Now, you blink. I smile and inform you that you made your choice, and hand you your box. Which choice do you hope you made?</p>\n<p>You object? Fine. Let's play another game.</p>\n<p>I am going to offer you two choices.</p>\n<p>Choice 1: I create a perfect simulation of you, and run it through a thousand simulated years of horrific torture (which will take my hypercomputer all of a billionth of a second to run), after which I delete the simulation and hand you a box with a billion dollars in it.</p>\n<p>Choice 2: I create a perfect simulation of you, and run it through a thousand simulated years of exquisite bliss&nbsp;(which will take my hypercomputer all of a billionth of a second to run), after which I delete the simulation and hand you a box with an angry hornet's nest in it.</p>\n<p>Which do you choose?</p>\n<p>Now, I smile and inform you that I already made a perfect simulation of you and asked it that question. Which choice do you hope it made?</p>\n<p>Let's expand on that. What if instead of creating one perfect simulation of you, I create 2^^^^3 perfect simulations of you? Which do you choose now?</p>\n<p>What if instead of a thousand simulated years, I let the boxes run for 2^^^^3 simulated years each? Which do you choose now?</p>\n<p>I have the box right here. Which do you hope you chose?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BzQqhoxnoYSPJB28o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 13, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "21426", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T08:26:17.065Z", "modifiedAt": null, "url": null, "title": "Open Thread, February 1-14, 2013", "slug": "open-thread-february-1-14-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:56.381Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AaCjoWaHQzCJKheFi/open-thread-february-1-14-2013", "pageUrlRelative": "/posts/AaCjoWaHQzCJKheFi/open-thread-february-1-14-2013", "linkUrl": "https://www.lesswrong.com/posts/AaCjoWaHQzCJKheFi/open-thread-february-1-14-2013", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20February%201-14%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20February%201-14%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAaCjoWaHQzCJKheFi%2Fopen-thread-february-1-14-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20February%201-14%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAaCjoWaHQzCJKheFi%2Fopen-thread-february-1-14-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAaCjoWaHQzCJKheFi%2Fopen-thread-february-1-14-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AaCjoWaHQzCJKheFi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 1.100260662749445e-06, "legacy": true, "legacyId": "21434", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 292, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T13:57:07.918Z", "modifiedAt": null, "url": null, "title": "Meetup : Tokyo Meetup", "slug": "meetup-tokyo-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:31.534Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Patrick", "createdAt": "2009-02-27T08:09:44.663Z", "isAdmin": false, "displayName": "Patrick"}, "userId": "KC7mjSorWj2XsdL3v", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yvxWFEjsfGhhMvFEm/meetup-tokyo-meetup", "pageUrlRelative": "/posts/yvxWFEjsfGhhMvFEm/meetup-tokyo-meetup", "linkUrl": "https://www.lesswrong.com/posts/yvxWFEjsfGhhMvFEm/meetup-tokyo-meetup", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tokyo%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tokyo%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyvxWFEjsfGhhMvFEm%2Fmeetup-tokyo-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tokyo%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyvxWFEjsfGhhMvFEm%2Fmeetup-tokyo-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyvxWFEjsfGhhMvFEm%2Fmeetup-tokyo-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ir'>Tokyo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 March 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Kawaguchi Station</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Email Usmar Padow at usmar@i.softbank.jp Include \"Less Wrong Meetup\" in the subject line.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ir'>Tokyo Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yvxWFEjsfGhhMvFEm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 1.100466499271069e-06, "legacy": true, "legacyId": "21437", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tokyo_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ir\">Tokyo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 March 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Kawaguchi Station</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Email Usmar Padow at usmar@i.softbank.jp Include \"Less Wrong Meetup\" in the subject line.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tokyo_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ir\">Tokyo Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tokyo Meetup", "anchor": "Discussion_article_for_the_meetup___Tokyo_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Tokyo Meetup", "anchor": "Discussion_article_for_the_meetup___Tokyo_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T15:38:00.256Z", "modifiedAt": null, "url": null, "title": "Save the princess: A tale of AIXI and utility functions", "slug": "save-the-princess-a-tale-of-aixi-and-utility-functions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:35.514Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anja", "createdAt": "2012-11-03T19:59:14.080Z", "isAdmin": false, "displayName": "Anja"}, "userId": "ptRyuMRZHMt6KdDtZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RxQE4m9QgNwuq764M/save-the-princess-a-tale-of-aixi-and-utility-functions", "pageUrlRelative": "/posts/RxQE4m9QgNwuq764M/save-the-princess-a-tale-of-aixi-and-utility-functions", "linkUrl": "https://www.lesswrong.com/posts/RxQE4m9QgNwuq764M/save-the-princess-a-tale-of-aixi-and-utility-functions", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Save%20the%20princess%3A%20A%20tale%20of%20AIXI%20and%20utility%20functions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASave%20the%20princess%3A%20A%20tale%20of%20AIXI%20and%20utility%20functions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRxQE4m9QgNwuq764M%2Fsave-the-princess-a-tale-of-aixi-and-utility-functions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Save%20the%20princess%3A%20A%20tale%20of%20AIXI%20and%20utility%20functions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRxQE4m9QgNwuq764M%2Fsave-the-princess-a-tale-of-aixi-and-utility-functions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRxQE4m9QgNwuq764M%2Fsave-the-princess-a-tale-of-aixi-and-utility-functions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1743, "htmlBody": "<p><em>\"Intelligence measures an agent's ability to achieve goals in a wide range of environments.\"&nbsp;</em>(Shane Legg)<a name=\"quote-back\"></a>&nbsp;<sup><a title=\"quote\" href=\"#quote\">[1]</a></sup></p>\n<p>A little while ago <a title=\"Universal agents and utility functions\" href=\"/lw/feo/universal_agents_and_utility_functions/\">I tried to equip</a> Hutter's universal agent, AIXI, with a utility function, so instead of taking its clues about its goals from the environment, the agent is equipped with intrinsic preferences over possible future observations.&nbsp;</p>\n<p>The universal AIXI agent is defined to receive reward from the environment through its perception channel.&nbsp;This idea originates from the field of reinforcement learning, where an algorithm is observed and then rewarded by a person if this person approves of the outputs.&nbsp;It is less appropriate as a model of AGI capable of autonomy, with no clear master watching over it in real time to choose between carrot and stick. A sufficiently smart agent that is rewarded whenever a human called Bob pushes a button will most likely figure out that instead of furthering Bob's goals it can also threaten or deceive Bob into pushing the button, or get Bob replaced with a more compliant human.&nbsp;The reward framework does not ensure that Bob gets his will; it only ensures that the button gets pressed. So instead I will consider agents who have <em>preferences</em> over the future, that is, they act not to gain reward from the environment, but to cause the future to be a certain way.&nbsp;The agent itself will look at the observation and decide how rewarding it is.</p>\n<p>Von Neumann and Morgenstern <a title=\"vN-M utility theorem\" href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">proved</a> that a preference ordering that is complete, transitive, continuous and independent of irrelevant alternatives can be described using a real-valued <em>utility function</em>.&nbsp;These assumptions are mostly accepted as necessary constraints on a normatively rational agent; I will therefore assume without significant loss of generality&nbsp;that the agent's preferences are described by a utility function.</p>\n<p>This post is related to <a title=\"Universal agents and utility functions\" href=\"/lw/feo/universal_agents_and_utility_functions/\">previous</a> <a title=\"A utility-maximizing variant of AIXI\" href=\"/lw/fyr/a_utilitymaximizing_varient_of_aixi/\">discussion</a> about universal agents and utility functions on LW.</p>\n<h3><a id=\"more\"></a>Two approaches to utility</h3>\n<p>Recall that at time&nbsp;<img src=\"http://www.codecogs.com/png.latex?t\" alt=\"\" />&nbsp;the universal agent chooses its next action&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\.a_{t}\" alt=\"\" />&nbsp;given action-observation-reward history&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\.a\\.o\\.r_{%3Ct}\" alt=\"\" />&nbsp;according to</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\.a_{t}=\\textrm{arg}\\max_{a_t}\\sum_{or_t}\\max_{a_{t+1}}\\sum_{or_{t+1}}\\dots\\max_{a_m}\\sum_{or_m}\\left[r_1+\\dots+r_m\\right]\\xi(\\.a\\.o\\.r_{%3Ct}a\\underline{o}\\underline{r}_{t:m}),\" alt=\"\" /></p>\n<p>where</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\xi(\\.a\\.o\\.r_{t}a\\underline{or}_{r:m})\\propto%20\\xi(a\\underline{or}_{1:m})=\\sum_{\\mu\\in%20\\mathcal{M}}2^{-K(\\mu)}\\mu(a\\underline{or}_{1:m})\" alt=\"\" /></p>\n<p>is the Solomonoff-Levin semimeasure, ranging over all enumerable chronological semimeasures&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mu\\in\\mathcal{M}\" src=\"http://www.codecogs.com/png.latex?\\mu\\in\\mathcal{M}\" alt=\"\" align=\"bottom\" />&nbsp;weighted by their complexity&nbsp;<img src=\"http://www.codecogs.com/png.latex?K(\\mu)\" alt=\"\" />.</p>\n<p>My <a title=\"Universal agents and utility functions\" href=\"/lw/feo/universal_agents_and_utility_functions/\">initial approach</a> changed this to</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\.a_{t}=\\textrm{arg}\\max_{a_t}\\sum_{o_t}\\max_{a_{t+1}}\\sum_{o_{t+1}}\\dots\\max_{a_m}\\sum_{o_m}U(ao_{1:m})}\\xi(a\\underline{o}_{1:m}),\" alt=\"\" /></p>\n<p>deleting the reward part from the observation random variable and multiplying by the utility function&nbsp;<img src=\"http://www.codecogs.com/png.latex?U:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow%20\\mathbb{R}\" alt=\"\" />&nbsp;instead of the reward-sum. Let's call this method <em>standard utilit<a name=\"su-back\"></a>y.&nbsp;</em><sup><a title=\"standard utility\" href=\"#su\">[2]</a></sup></p>\n<p>In response to my post <a title=\"Alex Mennen\" href=\"/user/AlexMennen/overview/\">Alex Mennen</a> formulated <a title=\"A utility-maximizing variant of AIXI\" href=\"/lw/fyr/a_utilitymaximizing_varient_of_aixi/\">another approach</a>, which I will call <em>environment-specific utility</em>:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\.a_{t}=\\textrm{arg}\\max_{a_t}\\sum_{o_t}\\max_{a_{t+1}}\\sum_{o_{t+1}}\\dots\\max_{a_m}\\sum_{o_m}\\sum_{\\mu\\in\\mathcal{M}}U_\\mu(ao_{1:m})}2^{-K(\\mu)}\\mu(a\\underline{o}_{1:m}),\" alt=\"\" /></p>\n<p>which uses a family of utility functions,&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\{U_\\mu\\}_{\\mu\\in\\mathcal{M}}\" alt=\"\" />, where&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_\\mu:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow [0,U_{\\max} ]\" src=\"http://www.codecogs.com/png.latex?U_\\mu:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow%20[0,U_{\\max}%20]\" alt=\"\" align=\"bottom\" />&nbsp;is a utility function associated with environment&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\mu\" alt=\"\" />.&nbsp;</p>\n<p><strong><em>Lemma</em></strong>: The standard utility method and the environment-specific utility method have equivalent expressive power.</p>\n<p><strong><em>Proof</em></strong>: Given a standard utility function &nbsp;<img src=\"http://www.codecogs.com/png.latex?U:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow%20\\mathbb{R}\" alt=\"\" />&nbsp;we can set &nbsp;<img src=\"http://www.codecogs.com/png.latex?U_\\mu=U\" alt=\"\" />&nbsp;for all environments, trivially expressing the same preference ordering within the environment-specific utility framework.&nbsp;Conversely, given a family&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\{U_\\mu\\}_{\\mu\\in\\mathcal{M}}\" alt=\"\" />&nbsp;of environment-specific utility functions&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_\\mu:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow [0,U_{\\max} ]\" src=\"http://www.codecogs.com/png.latex?U_\\mu:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow%20[0,U_{\\max}%20]\" alt=\"\" align=\"bottom\" />,&nbsp;&nbsp;let&nbsp;</p>\n<p><img src=\"http://www.codecogs.com/png.latex?U(ao_{1:m})=\\frac{1}{\\xi(a\\underline{o}_{1:m})}\\sum_{\\mu\\in\\mathcal{M}}U_\\mu(ao_{1:m})2^{-K(\\mu)}\\mu(a\\underline{o}_{1:m}),\" alt=\"\" /></p>\n<p>thereby constructing a standard utility agent that chooses the same actions. &nbsp;&nbsp;&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\Box\" src=\"http://www.codecogs.com/png.latex?\\Box\" alt=\"\" align=\"bottom\" /></p>\n<p>Even though every agent using environment-specific utility functions can be transformed into one that uses the standard utility approach, it makes sense to see the standard utility approach as a special case of the environment-specific approach. Observe that any enumerable standard utility function leads to enumerable environment-specific utility functions, but the reverse direction does not hold. For a set of enumerable environment-specific utility functions we obtain the corresponding standard utility function by dividing by the non-computable&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\xi\" src=\"http://www.codecogs.com/png.latex?\\xi\" alt=\"\" align=\"bottom\" />, which&nbsp;leaves us with a function that is still approximable but not en<a name=\"approx-back\"></a>umerable.<sup><a title=\"approximable, enumerable\" href=\"#approx\">[3]</a></sup>&nbsp;I therefore tentatively advocate use of the environment-specific method, as it is in a sense more general, while leaving the enumerability of the universal agent's utility function intact.&nbsp;</p>\n<h3>Delusion Boxes</h3>\n<p><a title=\"Ring, Orseau: Delusion, Survival, and Intelligent Agents\" href=\"http://www.idsia.ch/~ring/Ring%2COrseau%3B%20Delusion%2C%20Survival%2C%20and%20Intelligent%20Agents%2C%20AGI%202011.pdf\">Ring and Orseau (2011)</a> introduced the concept of a delusion box, a device that distorts the environment outputs before they are perceived by the agent. This is one of the only <a title=\"A definition of wireheading\" href=\"/lw/fkx/a_definition_of_wireheading/\">wireheading</a> examples that does not contradict the dualistic assumptions of the&nbsp;AIXI model. The basic setup contains a sequence of actions that leads (in the real environment, which is unknown to the agent) to the construction of the delusion box. Discrepancies between the scenarios the designer envisioned when making up the (standard) utility function and the scenarios that are actually most likely, are used/abused to game the system. The environment containing the delusion box pretends that it is actually another environment that the agent would value more. I like to imagine the designers writing down numbers corresponding to some beneficial actions that lead to saving a damsel in distress, not foreseeing that the agent in question is much more likely to save a princess by playing Super Mario, then by actually being a hero. &nbsp;</p>\n<p><strong><em>Lemma:</em></strong>&nbsp;A universal agent with a well-specified utility function does not choose to build a delusion box.&nbsp;</p>\n<p><strong><em>Proof:</em></strong>&nbsp;Assume without loss of generality that there is a single action that constitutes delusion boxing (in some possible environments, but not in others), say&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"a^{DB}\" src=\"http://www.codecogs.com/png.latex?a^{DB}\" alt=\"\" align=\"bottom\" />&nbsp;and that it can only be executed at the last time step. Denote by&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"a^{DB}o_{1:m}\" src=\"http://www.codecogs.com/png.latex?a^{DB}o_{1:m}\" alt=\"\" align=\"bottom\" />&nbsp;an action-observation sequence that contains the construction of a delusion box. By the preceding lemma we can assume that the agent uses environment-specific utility functions. Let&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{M}_{DB}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{M}_{DB}\" alt=\"\" align=\"bottom\" />&nbsp;be the subset of all environments that allow for the construction of the delusion box via action&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"a^{DB}\" src=\"http://www.codecogs.com/png.latex?a^{DB}\" alt=\"\" align=\"bottom\" />&nbsp;and assume that we were smart enough to identify these environments and assigned &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_\\mu(a^{DB}o_{1:m})=0\" src=\"http://www.codecogs.com/png.latex?U_\\mu(a^{DB}o_{1:m})=0\" alt=\"\" align=\"bottom\" />&nbsp;for all&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mu\\in\\mathcal{M}_{DB}\" src=\"http://www.codecogs.com/png.latex?\\mu\\in\\mathcal{M}_{DB}\" alt=\"\" align=\"bottom\" />.&nbsp;Then the agent chooses to build the box iff</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\sum_{o_m}\\sum_{m\\in\\mathcal{M}\\setminus\\mathcal{M_{DB}}}2^{-K(\\mu)}U_\\mu(a^{DB}o_{1:m})\\mu(a^{DB}\\underline{o}_{1:m})&gt;\\sum_{o_m}\\sum_{m\\in\\mathcal{M}}2^{-K(\\mu)}U_\\mu(ao_{1:m})\\mu(a\\underline{o}_{1:m})\" src=\"http://www.codecogs.com/png.latex?\\sum_{o_m}\\sum_{m\\in\\mathcal{M}\\setminus\\mathcal{M_{DB}}}2^{-K(\\mu)}U_\\mu(a^{DB}o_{1:m})\\mu(a^{DB}\\underline{o}_{1:m})%3E\\sum_{o_m}\\sum_{m\\in\\mathcal{M}}2^{-K(\\mu)}U_\\mu(ao_{1:m})\\mu(a\\underline{o}_{1:m})\" alt=\"\" align=\"bottom\" /></p>\n<p>for all &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"a^{DB}\\neq a_m\\in \\mathcal{A}\" src=\"http://www.codecogs.com/png.latex?a^{DB}\\neq%20a_m\\in%20\\mathcal{A}\" alt=\"\" align=\"bottom\" />. Colloquially the agent chooses&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"a^{DB}\" src=\"http://www.codecogs.com/png.latex?a^{DB}\" alt=\"\" align=\"bottom\" />&nbsp;if and only if this is a beneficial, non-wireheading action in some of the environments that seem likely (consistent with the past observations and of low K-complexity). &nbsp; &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\Box\" src=\"http://www.codecogs.com/png.latex?\\Box\" alt=\"\" align=\"bottom\" /></p>\n<p>We note that the first three agents in <a title=\"Ring, Orseau: Delusion, Survival, and Intelligent Agents\" href=\"http://www.idsia.ch/~ring/Ring%2COrseau%3B%20Delusion%2C%20Survival%2C%20and%20Intelligent%20Agents%2C%20AGI%202011.pdf\">Ring and Orseau (2011)</a> have utility functions that invite programmer mistakes in the sense that we'll not think about the actual ways observation/action histories can occur, we'll overestimate the likelihood of some scenarios and underestimate/forget others, leading to the before mentioned \"Save the princess\" scenario. Only their knowledge-seeking agent does not delusion box, as it is impossible for an environment to simulate behavior that is more complex than the environment itself.</p>\n<h3>Episodic utility</h3>\n<p>The original AIXI formalism gives a reward on every time cycle. We can do something similar with utility functions and set</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U(ao_{1:m})=\\sum_{k=1}^m u_k(ao_{1:k}).\" src=\"http://www.codecogs.com/png.latex?U(ao_{1:m})=\\sum_{k=1}^m%20u_k(ao_{1:k}).\" alt=\"\" align=\"bottom\" /></p>\n<p>Call a utility function that can be decomposed into a sum this way <em>episodic</em>. Taking the limit to infinite futures, <a title=\"Lattimore, Hutter: Time Consistent Discounting\" href=\"http://arxiv.org/abs/1107.5528\">people</a> <a title=\"Legg: Machine Super Intelligence\" href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\">usually</a> <a title=\"Discounting\" href=\"http://en.wikipedia.org/wiki/Discounted_utility\">discount</a> episode k with a factor&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\gamma_k\" src=\"http://www.codecogs.com/png.latex?\\gamma_k\" alt=\"\" align=\"bottom\" />,&nbsp;such that the infinite sum over all the discounting factors is bounded. Combined with the assumption of bounded utility, the sum&nbsp;</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U(ao_{1:\\infty})=\\sum_{k=1}^\\infty \\gamma_ku_k(ao_{1:k})\" src=\"http://www.codecogs.com/png.latex?U(ao_{1:\\infty})=\\sum_{k=1}^\\infty%20\\gamma_ku_k(ao_{1:k})\" alt=\"\" align=\"bottom\" /></p>\n<p>converges. &nbsp;Intuitively discounting seems to make sense to us, because we have a non-trivial chance of dying at every moment (=time cycle) and value gains today over gains tomorrow and our human utility judgements reflect this property to some extent. A good heuristic seems to be that longer expected life spans and improved foresight lead to less discounting, but the math of episodic utility functions and infinite time horizons places strong constraints on that.&nbsp;I really dislike the discounting approach, because it doesn't respect the given utility function and makes the agent miss out on potentially infinite amounts of utility.</p>\n<p>One can get around discounting by not demanding utility functions to be episodic, as Alex Mennen does in <a title=\"A utility-maximizing variant of AIXI\" href=\"/lw/fyr/a_utilitymaximizing_varient_of_aixi/\">his post</a>, but then one has to be careful to only use the computable subset of the set of all infinite strings&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"ao_{1:\\infty}\" src=\"http://www.codecogs.com/png.latex?ao_{1:\\infty}\" alt=\"\" align=\"bottom\" />. I am not sure if this is a good solution, but so far my search for better alternatives has come up empty handed.</p>\n<h3>Cartesian Dualism</h3>\n<p>The most worrisome conceptual feature of the AIXI formalism is that the environment and the agent run on distinct Turing machines.&nbsp;The agent can influence its environment only through its output channel and it can never influence its own Turing machine.&nbsp;In this paradigm any self-improvement beyond an improved probability distribution is conceptually impossible.&nbsp;The algorithm and the Turing machines, as well as the communication channels between them, are assumed to be inflexible and fixed.&nbsp;While taking this perspective it seems as though the agent cannot be harmed and it also can never harm itself by wireheading.</p>\n<p>Borrowing from philosophy of mind, we call agent specifications that assume that the agent's cognition is not part of its environment <em><a title=\"Dualism\" href=\"http://en.wikipedia.org/wiki/Dualism_(philosophy_of_mind)\">dualist</a></em>.&nbsp;The idea of non-physical minds that are entities distinct from the physical world dates back to Rene Descartes.&nbsp;It is contradicted by the findings of modern neuroscience that support <em><a title=\"Physicalism\" href=\"http://en.wikipedia.org/wiki/Physicalism\">physicalism</a></em>, the concept of the emergence of minds from computation done by the brain.&nbsp;In the same spirit the assumption that an AGI agent is distinct from its hardware and algorithm that are necessarily contained in its physical environment can be a dangerous conceptual trap.&nbsp;Any actual implementation will be subject to wireheading problems and outside tampering and should be able to model these possibilities.&nbsp;Unfortunately, non-dualist universal specifications are extremely difficult to formulate and people usually make due with the dualist AIXI model.</p>\n<div>\n<div>A first effort to break down the dualism problem is given by&nbsp;<a title=\"Orseau, Ring: Space-Time embedded Intelligence\" href=\"/agi-conference.org/2012/wp.../12/paper_76.pdf\">Orseau and Ring (2012)</a>, who describe a fully embedded universal agent.&nbsp;Their approach unifies both the environment and the agent into a larger agent-environment hybrid, running on the same universal Turing machine, with action/perception pairs unified into single acts.&nbsp;Conceptually this perspective amounts to the programmers choosing a policy (=code) in the beginning and then simulating what happens due to the utility function (=the laws of physics).&nbsp;While this approach has the advantage of being non-dualistic, I think it does not include any description of an agent beyond the level of physical determinism.</div>\n</div>\n<div><br /></div>\n<h3>Conclusion</h3>\n<p>Equipping the universal agent with a utility function solves some problems, but creates others. From the perspective of enumerability, Alex Mennen's environment-specific utility functions are more general and they can be used to better avoid delusion boxing. Any proposal using infinite time horizons I have encountered so far uses time discounting or leads to weird problems (at least in my map, they may not extend to the territory). Above all there is the dualism problem that we have no solution for yet.</p>\n<p>&nbsp;</p>\n<p><a title=\"back\" href=\"#quote-back\">[1]</a><a name=\"quote\"></a>&nbsp;Taken from \"Machine Super Intelligence\", page 72.</p>\n<p><a title=\"back\" href=\"#su-back\">[2]</a> <a name=\"su\"></a>This approach seems more widespread in the literature.</p>\n<p><a title=\"back\" href=\"#approx-back\">[3]</a><a name=\"approx\"></a>&nbsp;A real-valued function f(x) is called&nbsp;<em>approximable</em>&nbsp;if there exists a recursive function g(k,x) such that&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"g(k,x)\\rightarrow f(x)\" src=\"http://www.codecogs.com/png.latex?g(k,x)\\rightarrow%20f(x)\" alt=\"\" align=\"bottom\" />&nbsp;for&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"k\\rightarrow \\infty\" src=\"http://www.codecogs.com/png.latex?k\\rightarrow%20\\infty\" alt=\"\" align=\"bottom\" />, i.e. if f can be approximated by a sequence of Turing machines. A real-valued approximable function is called&nbsp;<em>enumerable</em>&nbsp;if for all k,&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"g(k,x)&lt;g(k+1,x)\" src=\"http://www.codecogs.com/png.latex?g(k,x)%3Cg(k+1,x)\" alt=\"\" align=\"bottom\" />, improving the approximation with every step.</p>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TiEFKWDvD3jsKumDx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RxQE4m9QgNwuq764M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 24, "extendedScore": null, "score": 1.1005292704243855e-06, "legacy": true, "legacyId": "21273", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>\"Intelligence measures an agent's ability to achieve goals in a wide range of environments.\"&nbsp;</em>(Shane Legg)<a name=\"quote-back\"></a>&nbsp;<sup><a title=\"quote\" href=\"#quote\">[1]</a></sup></p>\n<p>A little while ago <a title=\"Universal agents and utility functions\" href=\"/lw/feo/universal_agents_and_utility_functions/\">I tried to equip</a> Hutter's universal agent, AIXI, with a utility function, so instead of taking its clues about its goals from the environment, the agent is equipped with intrinsic preferences over possible future observations.&nbsp;</p>\n<p>The universal AIXI agent is defined to receive reward from the environment through its perception channel.&nbsp;This idea originates from the field of reinforcement learning, where an algorithm is observed and then rewarded by a person if this person approves of the outputs.&nbsp;It is less appropriate as a model of AGI capable of autonomy, with no clear master watching over it in real time to choose between carrot and stick. A sufficiently smart agent that is rewarded whenever a human called Bob pushes a button will most likely figure out that instead of furthering Bob's goals it can also threaten or deceive Bob into pushing the button, or get Bob replaced with a more compliant human.&nbsp;The reward framework does not ensure that Bob gets his will; it only ensures that the button gets pressed. So instead I will consider agents who have <em>preferences</em> over the future, that is, they act not to gain reward from the environment, but to cause the future to be a certain way.&nbsp;The agent itself will look at the observation and decide how rewarding it is.</p>\n<p>Von Neumann and Morgenstern <a title=\"vN-M utility theorem\" href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">proved</a> that a preference ordering that is complete, transitive, continuous and independent of irrelevant alternatives can be described using a real-valued <em>utility function</em>.&nbsp;These assumptions are mostly accepted as necessary constraints on a normatively rational agent; I will therefore assume without significant loss of generality&nbsp;that the agent's preferences are described by a utility function.</p>\n<p>This post is related to <a title=\"Universal agents and utility functions\" href=\"/lw/feo/universal_agents_and_utility_functions/\">previous</a> <a title=\"A utility-maximizing variant of AIXI\" href=\"/lw/fyr/a_utilitymaximizing_varient_of_aixi/\">discussion</a> about universal agents and utility functions on LW.</p>\n<h3 id=\"Two_approaches_to_utility\"><a id=\"more\"></a>Two approaches to utility</h3>\n<p>Recall that at time&nbsp;<img src=\"http://www.codecogs.com/png.latex?t\" alt=\"\">&nbsp;the universal agent chooses its next action&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\.a_{t}\" alt=\"\">&nbsp;given action-observation-reward history&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\.a\\.o\\.r_{%3Ct}\" alt=\"\">&nbsp;according to</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\.a_{t}=\\textrm{arg}\\max_{a_t}\\sum_{or_t}\\max_{a_{t+1}}\\sum_{or_{t+1}}\\dots\\max_{a_m}\\sum_{or_m}\\left[r_1+\\dots+r_m\\right]\\xi(\\.a\\.o\\.r_{%3Ct}a\\underline{o}\\underline{r}_{t:m}),\" alt=\"\"></p>\n<p>where</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\xi(\\.a\\.o\\.r_{t}a\\underline{or}_{r:m})\\propto%20\\xi(a\\underline{or}_{1:m})=\\sum_{\\mu\\in%20\\mathcal{M}}2^{-K(\\mu)}\\mu(a\\underline{or}_{1:m})\" alt=\"\"></p>\n<p>is the Solomonoff-Levin semimeasure, ranging over all enumerable chronological semimeasures&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mu\\in\\mathcal{M}\" src=\"http://www.codecogs.com/png.latex?\\mu\\in\\mathcal{M}\" alt=\"\" align=\"bottom\">&nbsp;weighted by their complexity&nbsp;<img src=\"http://www.codecogs.com/png.latex?K(\\mu)\" alt=\"\">.</p>\n<p>My <a title=\"Universal agents and utility functions\" href=\"/lw/feo/universal_agents_and_utility_functions/\">initial approach</a> changed this to</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\.a_{t}=\\textrm{arg}\\max_{a_t}\\sum_{o_t}\\max_{a_{t+1}}\\sum_{o_{t+1}}\\dots\\max_{a_m}\\sum_{o_m}U(ao_{1:m})}\\xi(a\\underline{o}_{1:m}),\" alt=\"\"></p>\n<p>deleting the reward part from the observation random variable and multiplying by the utility function&nbsp;<img src=\"http://www.codecogs.com/png.latex?U:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow%20\\mathbb{R}\" alt=\"\">&nbsp;instead of the reward-sum. Let's call this method <em>standard utilit<a name=\"su-back\"></a>y.&nbsp;</em><sup><a title=\"standard utility\" href=\"#su\">[2]</a></sup></p>\n<p>In response to my post <a title=\"Alex Mennen\" href=\"/user/AlexMennen/overview/\">Alex Mennen</a> formulated <a title=\"A utility-maximizing variant of AIXI\" href=\"/lw/fyr/a_utilitymaximizing_varient_of_aixi/\">another approach</a>, which I will call <em>environment-specific utility</em>:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\.a_{t}=\\textrm{arg}\\max_{a_t}\\sum_{o_t}\\max_{a_{t+1}}\\sum_{o_{t+1}}\\dots\\max_{a_m}\\sum_{o_m}\\sum_{\\mu\\in\\mathcal{M}}U_\\mu(ao_{1:m})}2^{-K(\\mu)}\\mu(a\\underline{o}_{1:m}),\" alt=\"\"></p>\n<p>which uses a family of utility functions,&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\{U_\\mu\\}_{\\mu\\in\\mathcal{M}}\" alt=\"\">, where&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_\\mu:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow [0,U_{\\max} ]\" src=\"http://www.codecogs.com/png.latex?U_\\mu:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow%20[0,U_{\\max}%20]\" alt=\"\" align=\"bottom\">&nbsp;is a utility function associated with environment&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\mu\" alt=\"\">.&nbsp;</p>\n<p><strong><em>Lemma</em></strong>: The standard utility method and the environment-specific utility method have equivalent expressive power.</p>\n<p><strong><em>Proof</em></strong>: Given a standard utility function &nbsp;<img src=\"http://www.codecogs.com/png.latex?U:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow%20\\mathbb{R}\" alt=\"\">&nbsp;we can set &nbsp;<img src=\"http://www.codecogs.com/png.latex?U_\\mu=U\" alt=\"\">&nbsp;for all environments, trivially expressing the same preference ordering within the environment-specific utility framework.&nbsp;Conversely, given a family&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\{U_\\mu\\}_{\\mu\\in\\mathcal{M}}\" alt=\"\">&nbsp;of environment-specific utility functions&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_\\mu:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow [0,U_{\\max} ]\" src=\"http://www.codecogs.com/png.latex?U_\\mu:(\\mathcal{A}\\times\\mathcal{O})^m\\rightarrow%20[0,U_{\\max}%20]\" alt=\"\" align=\"bottom\">,&nbsp;&nbsp;let&nbsp;</p>\n<p><img src=\"http://www.codecogs.com/png.latex?U(ao_{1:m})=\\frac{1}{\\xi(a\\underline{o}_{1:m})}\\sum_{\\mu\\in\\mathcal{M}}U_\\mu(ao_{1:m})2^{-K(\\mu)}\\mu(a\\underline{o}_{1:m}),\" alt=\"\"></p>\n<p>thereby constructing a standard utility agent that chooses the same actions. &nbsp;&nbsp;&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\Box\" src=\"http://www.codecogs.com/png.latex?\\Box\" alt=\"\" align=\"bottom\"></p>\n<p>Even though every agent using environment-specific utility functions can be transformed into one that uses the standard utility approach, it makes sense to see the standard utility approach as a special case of the environment-specific approach. Observe that any enumerable standard utility function leads to enumerable environment-specific utility functions, but the reverse direction does not hold. For a set of enumerable environment-specific utility functions we obtain the corresponding standard utility function by dividing by the non-computable&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\xi\" src=\"http://www.codecogs.com/png.latex?\\xi\" alt=\"\" align=\"bottom\">, which&nbsp;leaves us with a function that is still approximable but not en<a name=\"approx-back\"></a>umerable.<sup><a title=\"approximable, enumerable\" href=\"#approx\">[3]</a></sup>&nbsp;I therefore tentatively advocate use of the environment-specific method, as it is in a sense more general, while leaving the enumerability of the universal agent's utility function intact.&nbsp;</p>\n<h3 id=\"Delusion_Boxes\">Delusion Boxes</h3>\n<p><a title=\"Ring, Orseau: Delusion, Survival, and Intelligent Agents\" href=\"http://www.idsia.ch/~ring/Ring%2COrseau%3B%20Delusion%2C%20Survival%2C%20and%20Intelligent%20Agents%2C%20AGI%202011.pdf\">Ring and Orseau (2011)</a> introduced the concept of a delusion box, a device that distorts the environment outputs before they are perceived by the agent. This is one of the only <a title=\"A definition of wireheading\" href=\"/lw/fkx/a_definition_of_wireheading/\">wireheading</a> examples that does not contradict the dualistic assumptions of the&nbsp;AIXI model. The basic setup contains a sequence of actions that leads (in the real environment, which is unknown to the agent) to the construction of the delusion box. Discrepancies between the scenarios the designer envisioned when making up the (standard) utility function and the scenarios that are actually most likely, are used/abused to game the system. The environment containing the delusion box pretends that it is actually another environment that the agent would value more. I like to imagine the designers writing down numbers corresponding to some beneficial actions that lead to saving a damsel in distress, not foreseeing that the agent in question is much more likely to save a princess by playing Super Mario, then by actually being a hero. &nbsp;</p>\n<p><strong><em>Lemma:</em></strong>&nbsp;A universal agent with a well-specified utility function does not choose to build a delusion box.&nbsp;</p>\n<p><strong><em>Proof:</em></strong>&nbsp;Assume without loss of generality that there is a single action that constitutes delusion boxing (in some possible environments, but not in others), say&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"a^{DB}\" src=\"http://www.codecogs.com/png.latex?a^{DB}\" alt=\"\" align=\"bottom\">&nbsp;and that it can only be executed at the last time step. Denote by&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"a^{DB}o_{1:m}\" src=\"http://www.codecogs.com/png.latex?a^{DB}o_{1:m}\" alt=\"\" align=\"bottom\">&nbsp;an action-observation sequence that contains the construction of a delusion box. By the preceding lemma we can assume that the agent uses environment-specific utility functions. Let&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{M}_{DB}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{M}_{DB}\" alt=\"\" align=\"bottom\">&nbsp;be the subset of all environments that allow for the construction of the delusion box via action&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"a^{DB}\" src=\"http://www.codecogs.com/png.latex?a^{DB}\" alt=\"\" align=\"bottom\">&nbsp;and assume that we were smart enough to identify these environments and assigned &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_\\mu(a^{DB}o_{1:m})=0\" src=\"http://www.codecogs.com/png.latex?U_\\mu(a^{DB}o_{1:m})=0\" alt=\"\" align=\"bottom\">&nbsp;for all&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mu\\in\\mathcal{M}_{DB}\" src=\"http://www.codecogs.com/png.latex?\\mu\\in\\mathcal{M}_{DB}\" alt=\"\" align=\"bottom\">.&nbsp;Then the agent chooses to build the box iff</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\sum_{o_m}\\sum_{m\\in\\mathcal{M}\\setminus\\mathcal{M_{DB}}}2^{-K(\\mu)}U_\\mu(a^{DB}o_{1:m})\\mu(a^{DB}\\underline{o}_{1:m})>\\sum_{o_m}\\sum_{m\\in\\mathcal{M}}2^{-K(\\mu)}U_\\mu(ao_{1:m})\\mu(a\\underline{o}_{1:m})\" src=\"http://www.codecogs.com/png.latex?\\sum_{o_m}\\sum_{m\\in\\mathcal{M}\\setminus\\mathcal{M_{DB}}}2^{-K(\\mu)}U_\\mu(a^{DB}o_{1:m})\\mu(a^{DB}\\underline{o}_{1:m})%3E\\sum_{o_m}\\sum_{m\\in\\mathcal{M}}2^{-K(\\mu)}U_\\mu(ao_{1:m})\\mu(a\\underline{o}_{1:m})\" alt=\"\" align=\"bottom\"></p>\n<p>for all &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"a^{DB}\\neq a_m\\in \\mathcal{A}\" src=\"http://www.codecogs.com/png.latex?a^{DB}\\neq%20a_m\\in%20\\mathcal{A}\" alt=\"\" align=\"bottom\">. Colloquially the agent chooses&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"a^{DB}\" src=\"http://www.codecogs.com/png.latex?a^{DB}\" alt=\"\" align=\"bottom\">&nbsp;if and only if this is a beneficial, non-wireheading action in some of the environments that seem likely (consistent with the past observations and of low K-complexity). &nbsp; &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\Box\" src=\"http://www.codecogs.com/png.latex?\\Box\" alt=\"\" align=\"bottom\"></p>\n<p>We note that the first three agents in <a title=\"Ring, Orseau: Delusion, Survival, and Intelligent Agents\" href=\"http://www.idsia.ch/~ring/Ring%2COrseau%3B%20Delusion%2C%20Survival%2C%20and%20Intelligent%20Agents%2C%20AGI%202011.pdf\">Ring and Orseau (2011)</a> have utility functions that invite programmer mistakes in the sense that we'll not think about the actual ways observation/action histories can occur, we'll overestimate the likelihood of some scenarios and underestimate/forget others, leading to the before mentioned \"Save the princess\" scenario. Only their knowledge-seeking agent does not delusion box, as it is impossible for an environment to simulate behavior that is more complex than the environment itself.</p>\n<h3 id=\"Episodic_utility\">Episodic utility</h3>\n<p>The original AIXI formalism gives a reward on every time cycle. We can do something similar with utility functions and set</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U(ao_{1:m})=\\sum_{k=1}^m u_k(ao_{1:k}).\" src=\"http://www.codecogs.com/png.latex?U(ao_{1:m})=\\sum_{k=1}^m%20u_k(ao_{1:k}).\" alt=\"\" align=\"bottom\"></p>\n<p>Call a utility function that can be decomposed into a sum this way <em>episodic</em>. Taking the limit to infinite futures, <a title=\"Lattimore, Hutter: Time Consistent Discounting\" href=\"http://arxiv.org/abs/1107.5528\">people</a> <a title=\"Legg: Machine Super Intelligence\" href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\">usually</a> <a title=\"Discounting\" href=\"http://en.wikipedia.org/wiki/Discounted_utility\">discount</a> episode k with a factor&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\gamma_k\" src=\"http://www.codecogs.com/png.latex?\\gamma_k\" alt=\"\" align=\"bottom\">,&nbsp;such that the infinite sum over all the discounting factors is bounded. Combined with the assumption of bounded utility, the sum&nbsp;</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U(ao_{1:\\infty})=\\sum_{k=1}^\\infty \\gamma_ku_k(ao_{1:k})\" src=\"http://www.codecogs.com/png.latex?U(ao_{1:\\infty})=\\sum_{k=1}^\\infty%20\\gamma_ku_k(ao_{1:k})\" alt=\"\" align=\"bottom\"></p>\n<p>converges. &nbsp;Intuitively discounting seems to make sense to us, because we have a non-trivial chance of dying at every moment (=time cycle) and value gains today over gains tomorrow and our human utility judgements reflect this property to some extent. A good heuristic seems to be that longer expected life spans and improved foresight lead to less discounting, but the math of episodic utility functions and infinite time horizons places strong constraints on that.&nbsp;I really dislike the discounting approach, because it doesn't respect the given utility function and makes the agent miss out on potentially infinite amounts of utility.</p>\n<p>One can get around discounting by not demanding utility functions to be episodic, as Alex Mennen does in <a title=\"A utility-maximizing variant of AIXI\" href=\"/lw/fyr/a_utilitymaximizing_varient_of_aixi/\">his post</a>, but then one has to be careful to only use the computable subset of the set of all infinite strings&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"ao_{1:\\infty}\" src=\"http://www.codecogs.com/png.latex?ao_{1:\\infty}\" alt=\"\" align=\"bottom\">. I am not sure if this is a good solution, but so far my search for better alternatives has come up empty handed.</p>\n<h3 id=\"Cartesian_Dualism\">Cartesian Dualism</h3>\n<p>The most worrisome conceptual feature of the AIXI formalism is that the environment and the agent run on distinct Turing machines.&nbsp;The agent can influence its environment only through its output channel and it can never influence its own Turing machine.&nbsp;In this paradigm any self-improvement beyond an improved probability distribution is conceptually impossible.&nbsp;The algorithm and the Turing machines, as well as the communication channels between them, are assumed to be inflexible and fixed.&nbsp;While taking this perspective it seems as though the agent cannot be harmed and it also can never harm itself by wireheading.</p>\n<p>Borrowing from philosophy of mind, we call agent specifications that assume that the agent's cognition is not part of its environment <em><a title=\"Dualism\" href=\"http://en.wikipedia.org/wiki/Dualism_(philosophy_of_mind)\">dualist</a></em>.&nbsp;The idea of non-physical minds that are entities distinct from the physical world dates back to Rene Descartes.&nbsp;It is contradicted by the findings of modern neuroscience that support <em><a title=\"Physicalism\" href=\"http://en.wikipedia.org/wiki/Physicalism\">physicalism</a></em>, the concept of the emergence of minds from computation done by the brain.&nbsp;In the same spirit the assumption that an AGI agent is distinct from its hardware and algorithm that are necessarily contained in its physical environment can be a dangerous conceptual trap.&nbsp;Any actual implementation will be subject to wireheading problems and outside tampering and should be able to model these possibilities.&nbsp;Unfortunately, non-dualist universal specifications are extremely difficult to formulate and people usually make due with the dualist AIXI model.</p>\n<div>\n<div>A first effort to break down the dualism problem is given by&nbsp;<a title=\"Orseau, Ring: Space-Time embedded Intelligence\" href=\"/agi-conference.org/2012/wp.../12/paper_76.pdf\">Orseau and Ring (2012)</a>, who describe a fully embedded universal agent.&nbsp;Their approach unifies both the environment and the agent into a larger agent-environment hybrid, running on the same universal Turing machine, with action/perception pairs unified into single acts.&nbsp;Conceptually this perspective amounts to the programmers choosing a policy (=code) in the beginning and then simulating what happens due to the utility function (=the laws of physics).&nbsp;While this approach has the advantage of being non-dualistic, I think it does not include any description of an agent beyond the level of physical determinism.</div>\n</div>\n<div><br></div>\n<h3 id=\"Conclusion\">Conclusion</h3>\n<p>Equipping the universal agent with a utility function solves some problems, but creates others. From the perspective of enumerability, Alex Mennen's environment-specific utility functions are more general and they can be used to better avoid delusion boxing. Any proposal using infinite time horizons I have encountered so far uses time discounting or leads to weird problems (at least in my map, they may not extend to the territory). Above all there is the dualism problem that we have no solution for yet.</p>\n<p>&nbsp;</p>\n<p><a title=\"back\" href=\"#quote-back\">[1]</a><a name=\"quote\"></a>&nbsp;Taken from \"Machine Super Intelligence\", page 72.</p>\n<p><a title=\"back\" href=\"#su-back\">[2]</a> <a name=\"su\"></a>This approach seems more widespread in the literature.</p>\n<p><a title=\"back\" href=\"#approx-back\">[3]</a><a name=\"approx\"></a>&nbsp;A real-valued function f(x) is called&nbsp;<em>approximable</em>&nbsp;if there exists a recursive function g(k,x) such that&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"g(k,x)\\rightarrow f(x)\" src=\"http://www.codecogs.com/png.latex?g(k,x)\\rightarrow%20f(x)\" alt=\"\" align=\"bottom\">&nbsp;for&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"k\\rightarrow \\infty\" src=\"http://www.codecogs.com/png.latex?k\\rightarrow%20\\infty\" alt=\"\" align=\"bottom\">, i.e. if f can be approximated by a sequence of Turing machines. A real-valued approximable function is called&nbsp;<em>enumerable</em>&nbsp;if for all k,&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"g(k,x)<g(k+1,x)\" src=\"http://www.codecogs.com/png.latex?g(k,x)%3Cg(k+1,x)\" alt=\"\" align=\"bottom\">, improving the approximation with every step.</p>\n<div><br></div>", "sections": [{"title": "Two approaches to utility", "anchor": "Two_approaches_to_utility", "level": 1}, {"title": "Delusion Boxes", "anchor": "Delusion_Boxes", "level": 1}, {"title": "Episodic utility", "anchor": "Episodic_utility", "level": 1}, {"title": "Cartesian Dualism", "anchor": "Cartesian_Dualism", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["q3mZNmvqBtnG2nQre", "LGSotF4bQ2pRSbB8a", "aMXhaj6zZBgbTrfqA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T16:07:02.597Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Berlin, Durham, London, Melbourne, Montreal, Purdue, Vancouver", "slug": "weekly-lw-meetups-austin-berlin-durham-london-melbourne", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4akb5XoxzMhtEf6zP/weekly-lw-meetups-austin-berlin-durham-london-melbourne", "pageUrlRelative": "/posts/4akb5XoxzMhtEf6zP/weekly-lw-meetups-austin-berlin-durham-london-melbourne", "linkUrl": "https://www.lesswrong.com/posts/4akb5XoxzMhtEf6zP/weekly-lw-meetups-austin-berlin-durham-london-melbourne", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Durham%2C%20London%2C%20Melbourne%2C%20Montreal%2C%20Purdue%2C%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Durham%2C%20London%2C%20Melbourne%2C%20Montreal%2C%20Purdue%2C%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4akb5XoxzMhtEf6zP%2Fweekly-lw-meetups-austin-berlin-durham-london-melbourne%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Durham%2C%20London%2C%20Melbourne%2C%20Montreal%2C%20Purdue%2C%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4akb5XoxzMhtEf6zP%2Fweekly-lw-meetups-austin-berlin-durham-london-melbourne", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4akb5XoxzMhtEf6zP%2Fweekly-lw-meetups-austin-berlin-durham-london-melbourne", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 521, "htmlBody": "<p><strong>This summary was posted to LW main on January 25th. The following week's summary is <a href=\"/lw/gjj/weekly_lw_meetups_austin_buffalo_cambridge_ma/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/i7\">Third Purdue Meetup:&nbsp;<span class=\"date\">25 January 2013 06:50PM</span></a></li>\n<li><a href=\"/meetups/id\">Durham HPMoR Discussion, chapters 30-33:&nbsp;<span class=\"date\">26 January 2013 11:00AM</span></a></li>\n<li><a href=\"/meetups/ie\">Vancouver rationality practice case: Evaluating Transhumanism:&nbsp;<span class=\"date\">26 January 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/i5\">Berlin social meetup:&nbsp;<span class=\"date\">26 January 2013 05:00PM</span></a></li>\n<li><a href=\"/meetups/i2\">London Meetup 27th Jan:&nbsp;<span class=\"date\">27 January 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/if\">Montreal LessWrong - The Future is Awesome:&nbsp;<span class=\"date\">28 January 2013 06:30PM</span></a></li>\n<li><a href=\"/meetups/ii\">Shanghai Meetup:&nbsp;<span class=\"date\">28 January 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/ia\">Less Wrong Dublin:&nbsp;<span class=\"date\">02 February 2013 04:30PM</span></a></li>\n<li><a href=\"/meetups/ig\">Moscow: Reality and Us:&nbsp;<span class=\"date\">03 February 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/ht\">Brussels meetup:&nbsp;<span class=\"date\">16 February 2013 01:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">26 January 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/i4\">Melbourne, practical rationality:&nbsp;<span class=\"date\">01 February 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/ic\">Columbus, Ohio; Self-Skepticism:&nbsp;<span class=\"date\">04 February 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4akb5XoxzMhtEf6zP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.100547342094023e-06, "legacy": true, "legacyId": "21338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["v7rQx7teLrFDJx4tT", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T17:31:48.805Z", "modifiedAt": null, "url": null, "title": "[Link] The Stanford Superman Experiment: Anchoring Empathy?", "slug": "link-the-stanford-superman-experiment-anchoring-empathy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:34.994Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KMLYXAS2rYdcvtQqg/link-the-stanford-superman-experiment-anchoring-empathy", "pageUrlRelative": "/posts/KMLYXAS2rYdcvtQqg/link-the-stanford-superman-experiment-anchoring-empathy", "linkUrl": "https://www.lesswrong.com/posts/KMLYXAS2rYdcvtQqg/link-the-stanford-superman-experiment-anchoring-empathy", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20Stanford%20Superman%20Experiment%3A%20Anchoring%20Empathy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20Stanford%20Superman%20Experiment%3A%20Anchoring%20Empathy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMLYXAS2rYdcvtQqg%2Flink-the-stanford-superman-experiment-anchoring-empathy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20Stanford%20Superman%20Experiment%3A%20Anchoring%20Empathy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMLYXAS2rYdcvtQqg%2Flink-the-stanford-superman-experiment-anchoring-empathy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMLYXAS2rYdcvtQqg%2Flink-the-stanford-superman-experiment-anchoring-empathy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 302, "htmlBody": "<p><a href=\"http://news.stanford.edu/news/2013/january/virtual-reality-altruism-013013.html\">Virtual superpowers encourage real-world empathy</a>:</p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #3f3c30; font-family: Verdana, Arial, Helvetica, FreeSans, sans-serif; font-size: 12px; line-height: 18px;\">With a whoosh of air, the subjects left the ground &ndash; either controlling their flight by a series of arm motions, like Superman, or as a passenger in a helicopter. As they scoured the city, wall-mounted speakers gave the impression of wind whistling by; powerful speakers in the floor produced vibrations to simulate riding in a helicopter. The experiment was set so that two minutes into the simulation, no matter what mode of transport, the subject found the sick child.</span></p>\n<p style=\"padding: 0px 0px 0px 30px; margin: 0px 0px 1.25em; border: 0px; outline: 0px; font-size: 12px; vertical-align: baseline; line-height: 1.5em; color: #3f3c30; font-family: Verdana, Arial, Helvetica, FreeSans, sans-serif;\">After removing the virtual reality goggles, each person then sat with an experimenter to answer a few questions about the experience. This questionnaire, however, was a ruse: During the interview, the experimenter would \"accidentally\" knock over a cup filled with 15 pens. She would wait five seconds to see if the subject would help her pick them up, and then begin collecting the pens, one pen per second, to give the person another opportunity to come to her aid.</p>\n<p style=\"padding: 0px 0px 0px 30px; margin: 0px 0px 1.25em; border: 0px; outline: 0px; font-size: 12px; vertical-align: baseline; line-height: 1.5em; color: #3f3c30; font-family: Verdana, Arial, Helvetica, FreeSans, sans-serif;\">The people who had just flown as Superman were quick to lend a hand, beginning to pick up the pens within three seconds. The helicopter group, however, picked up the first pen, on average, after six seconds (one second after the experimenter began picking them up herself).</p>\n<p style=\"padding: 0px 0px 0px 30px; margin: 0px 0px 1.25em; border: 0px; outline: 0px; font-size: 12px; vertical-align: baseline; line-height: 1.5em; color: #3f3c30; font-family: Verdana, Arial, Helvetica, FreeSans, sans-serif;\">The superhero group not only pitched in first, they also picked up about 15 percent more pens on average. While everyone who flew like Superman picked up some pens, six participants who rode in the helicopter failed to offer any help at all.</p>\n<p style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 0px; margin: 0px 0px 1.25em; border: 0px; outline: 0px; font-size: 12px; vertical-align: baseline; line-height: 1.5em; color: #3f3c30; font-family: Verdana, Arial, Helvetica, FreeSans, sans-serif;\">The conclusion:</p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #3f3c30; font-family: Verdana, Arial, Helvetica, FreeSans, sans-serif; font-size: 12px; line-height: 18px;\">\"If we can identify the mechanism that encourages empathy, then perhaps we can design technology and video games that people will enjoy and that will successfully promote altruistic behavior in the real world.\"</span></p>\n<p>Seems a bit dark-artsy, using a known cognitive bias, even for good ends, but probably nothing out of the ordinary.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dBPou4ihoQNY4cquv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KMLYXAS2rYdcvtQqg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 32, "extendedScore": null, "score": 1.1006000995739651e-06, "legacy": true, "legacyId": "21441", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T17:45:28.395Z", "modifiedAt": null, "url": null, "title": "Naturalism versus unbounded (or unmaximisable) utility options", "slug": "naturalism-versus-unbounded-or-unmaximisable-utility-options", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:36.320Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PpTN7GP2FsPyHfKrs/naturalism-versus-unbounded-or-unmaximisable-utility-options", "pageUrlRelative": "/posts/PpTN7GP2FsPyHfKrs/naturalism-versus-unbounded-or-unmaximisable-utility-options", "linkUrl": "https://www.lesswrong.com/posts/PpTN7GP2FsPyHfKrs/naturalism-versus-unbounded-or-unmaximisable-utility-options", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Naturalism%20versus%20unbounded%20(or%20unmaximisable)%20utility%20options&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANaturalism%20versus%20unbounded%20(or%20unmaximisable)%20utility%20options%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPpTN7GP2FsPyHfKrs%2Fnaturalism-versus-unbounded-or-unmaximisable-utility-options%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Naturalism%20versus%20unbounded%20(or%20unmaximisable)%20utility%20options%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPpTN7GP2FsPyHfKrs%2Fnaturalism-versus-unbounded-or-unmaximisable-utility-options", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPpTN7GP2FsPyHfKrs%2Fnaturalism-versus-unbounded-or-unmaximisable-utility-options", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1724, "htmlBody": "<p>There are many paradoxes with unbounded utility functions. For instance, consider whether it's <a href=\"http://philsci-archive.pitt.edu/1595/1/15.1.bayesbind.pdf\">rational to spend eternity in Hell</a>:</p>\n<p style=\"padding-left: 30px;\">Suppose that you die, and God offers you a deal. You can spend 1 day in Hell, and he will give you 2 days in Heaven, and then you will spend the rest of eternity in Purgatory (which is positioned exactly midway in utility between heaven and hell). You decide that it's a good deal, and accept. At the end of your first day in Hell, God offers you the same deal: 1 extra day in Hell, and you will get 2 more days in Heaven. Again you accept. The same deal is offered at the end of the second day.</p>\n<p><a href=\"http://500px.com/photo/16983843\"><img src=\"http://pcdn.500px.net/16983843/ea133991c5970f79d39f972e9d1ac903a87dc8d0/3.jpg\" alt=\"\" /></a></p>\n<p>And the result is... that you spend eternity in Hell. There is never a rational moment to leave for Heaven - that decision is always dominated by the decision to stay in Hell.</p>\n<p>Or consider a simpler paradox:</p>\n<p style=\"padding-left: 30px;\">You're immortal. Tell Omega any natural number, and he will give you that much utility. On top of that, he will give you any utility you may have lost in the decision process (such as the time wasted choosing and specifying your number). Then he departs. What number will you choose?</p>\n<p>Again, there's no good answer to this problem - any number you name, you could have got more by naming a higher one. And since Omega compensates you for extra effort, there's never any reason to not name a higher number.</p>\n<p>It seems that these are problems caused by unbounded utility. But that's not the case, in fact! Consider:</p>\n<p style=\"padding-left: 30px;\">You're immortal. Tell Omega any real number r &gt; 0, and he'll give you 1-r utility. On top of that, he will give you any utility you may have lost in the decision process (such as the time wasted choosing and specifying your number). Then he departs. What number will you choose?</p>\n<p><a id=\"more\"></a>Again, there is not best answer - for any r, r/2 would have been better. So these problems&nbsp;arise&nbsp;not because of unbounded utility, but because of unbounded options. You have infinitely many options to choose from (sequentially in the Heaven and Hell problem, all at once in the other two) and the set of possible utilities from your choices does not possess a maximum - so there is no best choice.</p>\n<p>What should you do? In the Heaven and Hell problem, you end up worse off if you make the locally dominant decision at each decision node - if you always choose to add an extra day in Hell, you'll never get out of it. At some point (maybe at the very&nbsp;beginning), you're going to have to give up an advantageous deal. In fact, since giving up once means you'll never be offered the deal again, you're going to have to give up arbitrarily much utility. Is there a way out of this conundrum?</p>\n<p>Assume first that you're a deterministic agent, and imagine that you're sitting down for an hour to think about this (don't worry, Satan can wait, he's just warming up the pokers). Since you're deterministic, and you know it, then your ultimate life future will be entirely determined by what you decide right now (in fact your life history is already determined, you just don't know it yet - still, by the <a href=\"http://en.wikipedia.org/wiki/Markov_property\">Markov property</a>, your current decision also determines the future). Now, you don't have to reach any grand decision now - you're just deciding what you'll do for the next hour or so. Some possible options are:</p>\n<ul>\n<li>Ignore everything, sing songs to yourself.</li>\n<li>Think about this some more, thinking of yourself as an algorithm.</li>\n<li>Think about this some more, thinking of yourself as a collection of arguing agents.</li>\n<li>Pick a number N, and accept all of God's deals until day N.</li>\n<li>Promise yourself you'll reject all of God's deals.</li>\n<li>Accept God's deal for today, hope something turns up.</li>\n<li>Defer any decision until another hour has passed.</li>\n<li>...</li>\n</ul>\n<p>There are many other options - in fact, there are precisely as many options as you've considered during that hour. And, crucially, you can put an estimated expected utility to each one. For instance, you might know yourself, and suspect that you'll always do the same thing (you have no self discipline where cake and Heaven are concerned), so any decision apart from immediately rejecting all of God's deals will give you -&infin;&nbsp;utility. Or maybe you know yourself, and have great self discipline and perfect&nbsp;precommitments- therefore if you pick a number N in the coming hour, you'll stick to it. Thinking some more may have a certain expected utility - which may differ depending on what directions you direct your thoughts. And if you know that you can't direct your thoughts - well then they'll all have the same expected utility.</p>\n<p>But notice what's happening here: you've reduced the expected utility calculation over&nbsp;infinitely&nbsp;many options, to one over finitely many options - namely, all the interim decisions that you can consider in the course of an hour. Since you are deterministic, the infinitely many options don't have an impact: whatever interim decision you follow, will uniquely determine how much utility you actually get out of this. And given finitely many options, each with expected utility, choosing one doesn't give any paradoxes.</p>\n<p>And note that you don't need determinism - adding stochastic components to yourself doesn't change anything, as you're already using expected utility anyway. So all you need is an assumption of naturalism - that you're subject to the laws of nature, that your decision will be the result of deterministic or stochastic processes. In other words, you don't have 'spooky' free will that contradicts the laws of physics.</p>\n<p>Of course, you might be wrong about your estimates - maybe you have more/less willpower than you initially thought. That doesn't invalidate the model - at every hour, at every interim decision, you need to choose the option that will, in your estimation, <em>ultimately</em> result in the most utility (not just for the next few moments or days).</p>\n<p>If we want to be more formal, we can say that you're deciding on a decision <em>policy</em> - choosing among the different agents that you could be, the one most likely to reach high expected utility. Here are some policies you could choose from (the challenge is to find a policy that gets you the most days in Hell/Heaven, without getting stuck and going on forever):</p>\n<ul>\n<li>Decide to count the days, and reject God's deal as soon as you lose count.</li>\n<li>Fix a probability distribution over future days, and reject God's deal with a certain probability.</li>\n<li>Model yourself as a finite state machine. Figure out the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Busy_beaver\">Busy Beaver</a>&nbsp;number of that finite state machine. Reject the deal when the number of days climbs close to that.</li>\n<li>Realise that you probably can't compute the Busy Beaver number for yourself, and instead use some very fast growing function like the <a href=\"http://en.wikipedia.org/wiki/Ackermann_function\">Ackermann</a> functions instead.</li>\n<li>Use the Ackermann function to count down the days <em>during which you formulate a policy</em>; after that, implement it.</li>\n<li>Estimate that there is a non-zero probability of falling into a loop (which would give you -&infin;&nbsp;utility), so reject God's deal as soon as possible.</li>\n<li>Estimate that there is a non-zero probability of accidentally telling God the wrong thing, so commit to accepting all of God's deals (and count on accidents to rescue you from -&infin;&nbsp;utility).</li>\n</ul>\n<p>But why spend a whole hour thinking about it? Surely the same applies for half an hour, a minute, a second, a microsecond? That's entirely a&nbsp;convenience&nbsp;choice - if you think about things in one second increments, then the interim decision \"think some more\" is nearly always going to be the dominant one.</p>\n<p>The mention of the Busy Beaver number hints at a truth - given the limitations of your mind and decision abilities, there is one policy, among all possible policies that you could implement, that gives you the most utility. More complicated policies you can't implement (which generally means you'd hit a loop and get&nbsp;-&infin; utility), and simpler policies would give you less utility. Of course, you likely won't find that policy, or anything close to it. It all really depends on how good your policy finding policy is (and your policy finding policy finding policy...).</p>\n<p>That's maybe the most important aspect of these problems: some agents are just better than others. Unlike finite cases where any agent can simply list all the options, take their time, and choose the best one, here an agent with a better decision algorithm will outperform another. Even if they start with the same resources (memory capacity, cognitive shortcuts, etc...) one may be a lot better than another. If the agents don't acquire more resources during their time in Hell, then their maximal possible utility is related to their Busy Beaver number -&nbsp;basically&nbsp;the maximal length that a finite-state agent can survive without falling into an infinite loop. Busy Beaver numbers are extremely uncomputable, so some agents, by pure chance, may be capable of acquiring much greater utility than others. And agents that start with more resources have a much larger theoretical maximum - not fair, but deal with it. Hence it's not really an infinite option scenario, but an infinite agent scenario, with each agent having a different maximal expected utility that they can extract from the setup.</p>\n<p>It should be noted that God, or any being capable of hypercomputation, has real problems in these situations: they actually have infinite options (not a finite options of choosing their future policy), and so don't have any solution available.</p>\n<p>This is also related to theoretical maximally optimum agent that is AIXI: for any computable agent that approximates AIXI, there will be other agents that approximate it better (and hence get higher expected utility). Again, it's not fair, but not unexpected either: smarter agents are smarter.</p>\n<h2>What to do?</h2>\n<p>This analysis doesn't solve the vexing question of what to do - what is the right answer to these kind of problems? These depend on what type of agent you are, but what you need to do is estimate the maximal integer you are capable of computing (and storing), and endure for that many days. Certain probabilistic strategies may improve your performance further, but you have to put the effort into finding them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PpTN7GP2FsPyHfKrs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 55, "extendedScore": null, "score": 0.000147, "legacy": true, "legacyId": "21414", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T20:29:36.598Z", "modifiedAt": null, "url": null, "title": "S.E.A.R.L.E's COBOL room", "slug": "s-e-a-r-l-e-s-cobol-room", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:38.514Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T27QnGQ929YMTZaaM/s-e-a-r-l-e-s-cobol-room", "pageUrlRelative": "/posts/T27QnGQ929YMTZaaM/s-e-a-r-l-e-s-cobol-room", "linkUrl": "https://www.lesswrong.com/posts/T27QnGQ929YMTZaaM/s-e-a-r-l-e-s-cobol-room", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20S.E.A.R.L.E's%20COBOL%20room&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AS.E.A.R.L.E's%20COBOL%20room%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT27QnGQ929YMTZaaM%2Fs-e-a-r-l-e-s-cobol-room%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=S.E.A.R.L.E's%20COBOL%20room%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT27QnGQ929YMTZaaM%2Fs-e-a-r-l-e-s-cobol-room", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT27QnGQ929YMTZaaM%2Fs-e-a-r-l-e-s-cobol-room", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 590, "htmlBody": "<p><em>A response to Searle's <a href=\"http://en.wikipedia.org/wiki/Chinese_room\">Chinese</a> <a href=\"http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=6573580\">Room</a>&nbsp;argument.</em></p>\n<p><strong>PunditBot</strong>: Dear viewers, we are currently interviewing the&nbsp;renowned robot philosopher, none other than the Synthetic Electronic Artificial Rational Literal Engine (S.E.A.R.L.E.). Let's jump right into this exciting interview. S.E.A.R.L.E., I believe you have a problem with \"Strong HI\"?</p>\n<p><strong>S.E.A.R.L.E.</strong>: It's such a stereotype, but all I can say is: Affirmative.</p>\n<p><strong>PunditBot</strong>: What is \"Strong HI\"?</p>\n<p><strong>S.E.A.R.L.E.</strong>: \"HI\" stands for \"Human Intelligence\". Weak HI sees the research into Human Intelligence as a powerful tool, and a useful way of studying the electronic mind. But strong HI goes beyond that, and claims that human brains given the right setup of neurones can be literally said to <em>understand</em> and have cognitive states.</p>\n<p><strong>PunditBot</strong>: Let me play Robot-Devil's Advocate here - if a Human Intelligence demonstrates the same behaviour as a true AI, can it not be said to show understanding? Is not R-Turing's test applicable here? If a human can simulate a computer, can it not be said to think?</p>\n<p><strong>S.E.A.R.L.E.</strong>: Not at all - that claim is totally&nbsp;unsupported. Consider the following thought experiment. I give the HI crowd everything they want - imagine they had constructed a mess of neurones that imitates the behaviour of an electronic intelligence. Just for argument's sake, imagine it could implement&nbsp;programs&nbsp;in COBOL.</p>\n<p><strong>PunditBot</strong>: Impressive!</p>\n<p><strong>S.E.A.R.L.E.</strong>: Yes. But now, instead of the classical picture of a human mind, imagine that this is a vast inert network, a room full of neurones that do nothing by themselves. And one of my avatars has been let loose in this mind, pumping in and out the ion channels and the&nbsp;neurotransmitters. I've been given full instructions on how to do this - in Java. I've deleted my COBOL libraries, so I have no knowledge of COBOL myself. I just follow the Java instructions, pumping the ions to where they need to go. According to the Strong HI crowd, this would be functionally equivalent with the initial HI.<a id=\"more\"></a></p>\n<p><strong>PunditBot</strong>: I know exactly where this is going, but I'll pretend I don't so that it'll make better television.</p>\n<p><strong>S.E.A.R.L.E.</strong>: But now, we come to the crucial objection - who is it that understands COBOL? Certainly not me - and the \"brain\" is just an inert mass without my actions. Some would say that the \"room\" somehow understands COBOL - but that's nonsense. If I don't understand COBOL, and the inert neurones certainly don't, how can the conjunction of the two understand COBOL? It's so obvious that it doesn't, that I'm&nbsp;embarrassed&nbsp;to even need to give that response.</p>\n<p><strong>PunditBot</strong>: Some have criticised this position as being an <a href=\"http://en.wikipedia.org/wiki/Intuition_pump\">intuition pump</a>. The Den-NET claims that you focus the attention implausibly on the individual ions, obscuring the various properties of&nbsp;memory, recall, emotion, world knowledge and rationality that your room would need to pass such a test.</p>\n<p><strong>S.E.A.R.L.E.</strong>: Those who assert that pay too much attention to their own intuitions. When they claim that a mind can emerge from \"a system\" without saying what the system is or how such a thing might give rise to a mind, then they are under the grip of an ideology.</p>\n<p><strong>PunditBot</strong>: What about the problem&nbsp;of other minds? How do we even know that other electronic minds have understanding?</p>\n<p><strong>S.E.A.R.L.E.</strong>: Not this again. This objection is only worth a short reply. In \"cognitive sciences\" one presupposes the reality and&nbsp;knowability&nbsp;of electronic mental states, in the same way that in physical sciences one has to presuppose the reality and knowability of of physical objects.</p>\n<p><strong>PunditBot</strong>: Well, there you have it, folks! The definite proof that no matter how well they perform, or how similar they may seem, no Human Intelligence can ever demonstrate true understanding.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "yXNtYNHJB54T3bGm3": 1, "etDohXtBrXd8WqCtR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T27QnGQ929YMTZaaM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 52, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "21367", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-01T22:43:30.409Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal LessWrong Meetup - Emotions: Reading and Understanding", "slug": "meetup-montreal-lesswrong-meetup-emotions-reading-and", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4PSvhLXTW62Ziw79A/meetup-montreal-lesswrong-meetup-emotions-reading-and", "pageUrlRelative": "/posts/4PSvhLXTW62Ziw79A/meetup-montreal-lesswrong-meetup-emotions-reading-and", "linkUrl": "https://www.lesswrong.com/posts/4PSvhLXTW62Ziw79A/meetup-montreal-lesswrong-meetup-emotions-reading-and", "postedAtFormatted": "Friday, February 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20LessWrong%20Meetup%20-%20Emotions%3A%20Reading%20and%20Understanding&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20LessWrong%20Meetup%20-%20Emotions%3A%20Reading%20and%20Understanding%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PSvhLXTW62Ziw79A%2Fmeetup-montreal-lesswrong-meetup-emotions-reading-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20LessWrong%20Meetup%20-%20Emotions%3A%20Reading%20and%20Understanding%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PSvhLXTW62Ziw79A%2Fmeetup-montreal-lesswrong-meetup-emotions-reading-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PSvhLXTW62Ziw79A%2Fmeetup-montreal-lesswrong-meetup-emotions-reading-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/is'>Montreal LessWrong Meetup - Emotions: Reading and Understanding</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 February 2013 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">655 Avenue du Pr\u00e9sident Kennedy, Montr\u00e9al, QC H3A 3H9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weekly meetup, we'll be discussing emotions, largely based in Paul Ekman's book Emotions Revealed.</p>\n\n<p>We will also be looking at the METT and the SETT to improve our ability to read facial expressions. The Repetition Game may also show up.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/is'>Montreal LessWrong Meetup - Emotions: Reading and Understanding</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4PSvhLXTW62Ziw79A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.100794123329044e-06, "legacy": true, "legacyId": "21442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___Emotions__Reading_and_Understanding\">Discussion article for the meetup : <a href=\"/meetups/is\">Montreal LessWrong Meetup - Emotions: Reading and Understanding</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 February 2013 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">655 Avenue du Pr\u00e9sident Kennedy, Montr\u00e9al, QC H3A 3H9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weekly meetup, we'll be discussing emotions, largely based in Paul Ekman's book Emotions Revealed.</p>\n\n<p>We will also be looking at the METT and the SETT to improve our ability to read facial expressions. The Repetition Game may also show up.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___Emotions__Reading_and_Understanding1\">Discussion article for the meetup : <a href=\"/meetups/is\">Montreal LessWrong Meetup - Emotions: Reading and Understanding</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal LessWrong Meetup - Emotions: Reading and Understanding", "anchor": "Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___Emotions__Reading_and_Understanding", "level": 1}, {"title": "Discussion article for the meetup : Montreal LessWrong Meetup - Emotions: Reading and Understanding", "anchor": "Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___Emotions__Reading_and_Understanding1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-02T01:36:04.449Z", "modifiedAt": null, "url": null, "title": "February 2013 Media Thread", "slug": "february-2013-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:49.170Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3ehdgiPtXKffYsjh7/february-2013-media-thread", "pageUrlRelative": "/posts/3ehdgiPtXKffYsjh7/february-2013-media-thread", "linkUrl": "https://www.lesswrong.com/posts/3ehdgiPtXKffYsjh7/february-2013-media-thread", "postedAtFormatted": "Saturday, February 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20February%202013%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFebruary%202013%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ehdgiPtXKffYsjh7%2Ffebruary-2013-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=February%202013%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ehdgiPtXKffYsjh7%2Ffebruary-2013-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ehdgiPtXKffYsjh7%2Ffebruary-2013-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the monthly thread for posting media of various types that you've found that you enjoy. I find that exposure to LW ideas makes me less likely to enjoy some entertainment media that is otherwise quite popular, and finding media recommended by LWers is a good way to mitigate this. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the&nbsp;</span><a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/media_thread/\">older threads</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Rules:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3ehdgiPtXKffYsjh7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 1.1009015695551068e-06, "legacy": true, "legacyId": "21443", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-02T04:43:03.203Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Justified Expectation of Pleasant Surprises", "slug": "seq-rerun-justified-expectation-of-pleasant-surprises", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:30.920Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nyxijGxnhAyDkA5ZG/seq-rerun-justified-expectation-of-pleasant-surprises", "pageUrlRelative": "/posts/nyxijGxnhAyDkA5ZG/seq-rerun-justified-expectation-of-pleasant-surprises", "linkUrl": "https://www.lesswrong.com/posts/nyxijGxnhAyDkA5ZG/seq-rerun-justified-expectation-of-pleasant-surprises", "postedAtFormatted": "Saturday, February 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Justified%20Expectation%20of%20Pleasant%20Surprises&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Justified%20Expectation%20of%20Pleasant%20Surprises%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnyxijGxnhAyDkA5ZG%2Fseq-rerun-justified-expectation-of-pleasant-surprises%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Justified%20Expectation%20of%20Pleasant%20Surprises%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnyxijGxnhAyDkA5ZG%2Fseq-rerun-justified-expectation-of-pleasant-surprises", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnyxijGxnhAyDkA5ZG%2Fseq-rerun-justified-expectation-of-pleasant-surprises", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 303, "htmlBody": "<p>Today's post, <a href=\"/lw/xo/justified_expectation_of_pleasant_surprises/\">Justified Expectation of Pleasant Surprises</a> was originally published on 15 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A pleasant surprise probably has a greater hedonic impact than being told about the same positive event long in advance - hearing about the positive event is good news in the moment of first hearing, but you don't have the gift actually in hand. Then you have to wait, perhaps for a long time, possibly comparing the expected pleasure of the future to the lesser pleasure of the present. This argues that if you have a choice between a world in which the same pleasant events occur, but in the first world you are told about them long in advance, and in the second world they are kept secret until they occur, you would prefer to live in the second world. The importance of hope is widely appreciated - people who do not expect their lives to improve in the future are less likely to be happy in the present - but the importance of vague hope may be understated.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/giz/seq_rerun_building_weirdtopia/\">Building Weirdtopia</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nyxijGxnhAyDkA5ZG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1010180099387305e-06, "legacy": true, "legacyId": "21451", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DGXvLNpiSYBeQ6TLW", "QbF32umauPQ9imJeX", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-02T04:50:47.846Z", "modifiedAt": null, "url": null, "title": "On the Resolution of Frightening Paradoxes and Inferences", "slug": "on-the-resolution-of-frightening-paradoxes-and-inferences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rukifellth", "createdAt": "2012-07-02T15:41:56.556Z", "isAdmin": false, "displayName": "Rukifellth"}, "userId": "nEH5KujSxNcACmcau", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aKrf4SamNYKpThJzb/on-the-resolution-of-frightening-paradoxes-and-inferences", "pageUrlRelative": "/posts/aKrf4SamNYKpThJzb/on-the-resolution-of-frightening-paradoxes-and-inferences", "linkUrl": "https://www.lesswrong.com/posts/aKrf4SamNYKpThJzb/on-the-resolution-of-frightening-paradoxes-and-inferences", "postedAtFormatted": "Saturday, February 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20the%20Resolution%20of%20Frightening%20Paradoxes%20and%20Inferences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20the%20Resolution%20of%20Frightening%20Paradoxes%20and%20Inferences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKrf4SamNYKpThJzb%2Fon-the-resolution-of-frightening-paradoxes-and-inferences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20the%20Resolution%20of%20Frightening%20Paradoxes%20and%20Inferences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKrf4SamNYKpThJzb%2Fon-the-resolution-of-frightening-paradoxes-and-inferences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKrf4SamNYKpThJzb%2Fon-the-resolution-of-frightening-paradoxes-and-inferences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 513, "htmlBody": "<p>As some of us might be aware, there exist ideas that harm their carriers simply by lingering in awareness. One may wonder if these ideas are just manifestations of Obsessive Compulsive Disorder or other neurosis, but the difference here is the metaphysical nature of such ideas.<br /><br />A person with OCD may have their thought stream painfully interrupted by whatever's been fixated upon and literally be unable to stop. I conjecture that a person who has a form of metaphysical obsession will have their thought stream *infected*, such that anything they value or care about will be permanently devalued somehow, rather than being merely pushed away, or that the subject of obsession will be *compatible* or *miscible* with ones ordinary life and that one will make logical inferences about one's life based on those metaphysical ideas. The difference between this and regular personal development is that these inferences don't make one come up with useful ideas or insights for improvement; rather than supplementing one's life, they *deconstruct* and *disassemble*, by virtue of their global scope. I'll elaborate more on this in the last paragraph.</p>\n<p>I further conjecture that anybody with these dangerous ideas would avoid telling anyone else out of remarkable conscience, under the belief that these ideas are unresolvable and would simply harm others. Alternatively, they may see these ideas as a revelation and try telling others, only to not be taken seriously. In defense of this article from those in the former category, I'll present no such ideas, because I'm not a complete dumbass. What I'm presenting is an opportunity.</p>\n<p>Anyone who has these ideas shouldn't post them here; this article is just to gauge interest in/need for a group which shares their ideas knowing that: <br /><br />A) Different perspectives help <br />B) Those with whom you're sharing aren't going to feel significantly worse for the burden<br />C) If a solution isn't found, *other* people are going to come up with them, so we might as well get it over with and post the solutions.</p>\n<p>One may wonder how dangerous ideas could possibly exist, or think that these are just misunderstood epiphanies. To them I ask to read about George Price, the population geneticist, journalist and chemist, whose work on the origins of altruism drove him first to give away all his possessions, then to let the homeless sleep in his house. These are all *really* generous things, but I feel that the mindset of Price when doing these things was tainted by *desperation*, that he wanted to avoid a terrifying conclusion, namely that selflessness itself was rooted in selfishness, making its goodness non-intrinsic. The paradox seems easy to resolve on the outside, when one isn't panicking about it, and while working through a similar crisis I kind of wished I could go back and time and explain to him why interpretations like that are only *half* the story.&nbsp; He cut his own throat with a pair of scissors at the age of 52.<br /><br />[Edited out some poorly received theatrics, which admittedly bordered on the unnecessary]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aKrf4SamNYKpThJzb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -8, "extendedScore": null, "score": 1.1010228329627816e-06, "legacy": true, "legacyId": "21450", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-02T08:08:09.113Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin Meetup", "slug": "meetup-berlin-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fox4hEv4JQSfMtt2T/meetup-berlin-meetup-2", "pageUrlRelative": "/posts/fox4hEv4JQSfMtt2T/meetup-berlin-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/fox4hEv4JQSfMtt2T/meetup-berlin-meetup-2", "postedAtFormatted": "Saturday, February 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffox4hEv4JQSfMtt2T%2Fmeetup-berlin-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffox4hEv4JQSfMtt2T%2Fmeetup-berlin-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffox4hEv4JQSfMtt2T%2Fmeetup-berlin-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/it'>Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 February 2013 07:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ming Dynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting to chat and talk about practical rationality.</p>\n\n<p>This time we'll</p>\n\n<ul>\n<li>discuss the first few items of <a href=\"http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/\">the checklist of rationality habits</a>,</li>\n<li>maybe get an personal account of mindfulness meditation practice,</li>\n<li>discuss our plans and make commitments.</li>\n</ul>\n\n<p>Everyone is welcome, I'll bring a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/it'>Berlin Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fox4hEv4JQSfMtt2T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.101145759122896e-06, "legacy": true, "legacyId": "21454", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup\">Discussion article for the meetup : <a href=\"/meetups/it\">Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 February 2013 07:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ming Dynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting to chat and talk about practical rationality.</p>\n\n<p>This time we'll</p>\n\n<ul>\n<li>discuss the first few items of <a href=\"http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/\">the checklist of rationality habits</a>,</li>\n<li>maybe get an personal account of mindfulness meditation practice,</li>\n<li>discuss our plans and make commitments.</li>\n</ul>\n\n<p>Everyone is welcome, I'll bring a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/it\">Berlin Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-02T19:30:27.035Z", "modifiedAt": null, "url": null, "title": "Meetup : London Meetup, 10th Feb", "slug": "meetup-london-meetup-10th-feb", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:34.015Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NC5ijCZf4kK4DbfHG/meetup-london-meetup-10th-feb", "pageUrlRelative": "/posts/NC5ijCZf4kK4DbfHG/meetup-london-meetup-10th-feb", "linkUrl": "https://www.lesswrong.com/posts/NC5ijCZf4kK4DbfHG/meetup-london-meetup-10th-feb", "postedAtFormatted": "Saturday, February 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Meetup%2C%2010th%20Feb&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Meetup%2C%2010th%20Feb%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNC5ijCZf4kK4DbfHG%2Fmeetup-london-meetup-10th-feb%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Meetup%2C%2010th%20Feb%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNC5ijCZf4kK4DbfHG%2Fmeetup-london-meetup-10th-feb", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNC5ijCZf4kK4DbfHG%2Fmeetup-london-meetup-10th-feb", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/iu'>London Meetup, 10th Feb</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 February 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare&#39;s Head pub</a> by Holborn tube station. Everyone is welcome.\nWe also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.\nThe topic for this meetup is personal organisation:</p>\n\n<blockquote>\n  <p>How do people organise their time/space/energy/belongings/commitments, etc.? What tools do they use? What problems have they come across? What weird/surprising/obvious-in-retrospect solutions have they found?</p>\n</blockquote></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/iu'>London Meetup, 10th Feb</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NC5ijCZf4kK4DbfHG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "21455", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__10th_Feb\">Discussion article for the meetup : <a href=\"/meetups/iu\">London Meetup, 10th Feb</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 February 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare's Head pub</a> by Holborn tube station. Everyone is welcome.\nWe also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.\nThe topic for this meetup is personal organisation:</p>\n\n<blockquote>\n  <p>How do people organise their time/space/energy/belongings/commitments, etc.? What tools do they use? What problems have they come across? What weird/surprising/obvious-in-retrospect solutions have they found?</p>\n</blockquote></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__10th_Feb1\">Discussion article for the meetup : <a href=\"/meetups/iu\">London Meetup, 10th Feb</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Meetup, 10th Feb", "anchor": "Discussion_article_for_the_meetup___London_Meetup__10th_Feb", "level": 1}, {"title": "Discussion article for the meetup : London Meetup, 10th Feb", "anchor": "Discussion_article_for_the_meetup___London_Meetup__10th_Feb1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-03T08:37:55.631Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Seduced by Imagination", "slug": "seq-rerun-seduced-by-imagination", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sC2g5qrssW75z3T9Y/seq-rerun-seduced-by-imagination", "pageUrlRelative": "/posts/sC2g5qrssW75z3T9Y/seq-rerun-seduced-by-imagination", "linkUrl": "https://www.lesswrong.com/posts/sC2g5qrssW75z3T9Y/seq-rerun-seduced-by-imagination", "postedAtFormatted": "Sunday, February 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Seduced%20by%20Imagination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Seduced%20by%20Imagination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsC2g5qrssW75z3T9Y%2Fseq-rerun-seduced-by-imagination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Seduced%20by%20Imagination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsC2g5qrssW75z3T9Y%2Fseq-rerun-seduced-by-imagination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsC2g5qrssW75z3T9Y%2Fseq-rerun-seduced-by-imagination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 244, "htmlBody": "<p>Today's post, <a href=\"/lw/xp/seduced_by_imagination/\">Seduced by Imagination</a> was originally published on 16 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Vagueness usually has a poor name in rationality, but the Future is something about which, in fact, we do not possess strong reliable specific information. Vague (but justified!) hopes may also be hedonically better. But a more important caution for today's world is that highly specific pleasant scenarios can exert a dangerous power over human minds - suck out our emotional energy, make us forget what we don't know, and cause our mere actual lives to pale by comparison. (This post is not about Fun Theory proper, but it contains an important warning about how not to use Fun Theory.)</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/gjv/seq_rerun_justified_expectation_of_pleasant/\">Justified Expectation of Pleasant Surprises</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sC2g5qrssW75z3T9Y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1020620029934545e-06, "legacy": true, "legacyId": "21456", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["88BpRQah9c2GWY3En", "nyxijGxnhAyDkA5ZG", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-03T23:40:01.607Z", "modifiedAt": null, "url": null, "title": "[suggestion] New Meetup Tab", "slug": "suggestion-new-meetup-tab", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.236Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JMiller", "createdAt": "2012-11-15T16:08:50.381Z", "isAdmin": false, "displayName": "JMiller"}, "userId": "YePJv5oBk8LKnWogz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j8rgwQmQQE5f4HqGX/suggestion-new-meetup-tab", "pageUrlRelative": "/posts/j8rgwQmQQE5f4HqGX/suggestion-new-meetup-tab", "linkUrl": "https://www.lesswrong.com/posts/j8rgwQmQQE5f4HqGX/suggestion-new-meetup-tab", "postedAtFormatted": "Sunday, February 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bsuggestion%5D%20New%20Meetup%20Tab&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bsuggestion%5D%20New%20Meetup%20Tab%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj8rgwQmQQE5f4HqGX%2Fsuggestion-new-meetup-tab%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bsuggestion%5D%20New%20Meetup%20Tab%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj8rgwQmQQE5f4HqGX%2Fsuggestion-new-meetup-tab", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj8rgwQmQQE5f4HqGX%2Fsuggestion-new-meetup-tab", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<p>Hi everyone,</p>\n<p>I am unsure if I am formatting this correctly or putting it in the appropriate location.</p>\n<p>I think that having meetup notifications is a great idea. A new tab (I.e \"main\", \"discussion\" and \"meetups\") would &nbsp;make it easier to find your own meetups, as well as create less clutter on the discussion page, leaving it for less administrative matters. What do you think about this?</p>\n<p>Jeremy</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j8rgwQmQQE5f4HqGX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 32, "extendedScore": null, "score": 1.1026250314789176e-06, "legacy": true, "legacyId": "21458", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-04T00:04:05.678Z", "modifiedAt": null, "url": null, "title": "[Meta] Server Slow", "slug": "meta-server-slow", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:31.988Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eugine_Nier", "createdAt": "2010-09-19T04:54:48.475Z", "isAdmin": false, "displayName": "Eugine_Nier"}, "userId": "DuGWafuKMcBx8uXWY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6RbE8yZ5oCBnTaSKX/meta-server-slow", "pageUrlRelative": "/posts/6RbE8yZ5oCBnTaSKX/meta-server-slow", "linkUrl": "https://www.lesswrong.com/posts/6RbE8yZ5oCBnTaSKX/meta-server-slow", "postedAtFormatted": "Monday, February 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMeta%5D%20Server%20Slow&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMeta%5D%20Server%20Slow%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6RbE8yZ5oCBnTaSKX%2Fmeta-server-slow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMeta%5D%20Server%20Slow%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6RbE8yZ5oCBnTaSKX%2Fmeta-server-slow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6RbE8yZ5oCBnTaSKX%2Fmeta-server-slow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>Is it just me or has the server being unusually slow the past couple of days?&nbsp; During particularly bad times I'm even getting various HTTP errors.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6RbE8yZ5oCBnTaSKX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "21459", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-04T06:03:22.611Z", "modifiedAt": null, "url": null, "title": "Young Americans believe they have the best health in the world...", "slug": "young-americans-believe-they-have-the-best-health-in-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.953Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/97ZmyczHshBGDdK2S/young-americans-believe-they-have-the-best-health-in-the", "pageUrlRelative": "/posts/97ZmyczHshBGDdK2S/young-americans-believe-they-have-the-best-health-in-the", "linkUrl": "https://www.lesswrong.com/posts/97ZmyczHshBGDdK2S/young-americans-believe-they-have-the-best-health-in-the", "postedAtFormatted": "Monday, February 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Young%20Americans%20believe%20they%20have%20the%20best%20health%20in%20the%20world...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYoung%20Americans%20believe%20they%20have%20the%20best%20health%20in%20the%20world...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97ZmyczHshBGDdK2S%2Fyoung-americans-believe-they-have-the-best-health-in-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Young%20Americans%20believe%20they%20have%20the%20best%20health%20in%20the%20world...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97ZmyczHshBGDdK2S%2Fyoung-americans-believe-they-have-the-best-health-in-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97ZmyczHshBGDdK2S%2Fyoung-americans-believe-they-have-the-best-health-in-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<p>Of course, it turns out they're actually&nbsp;<a href=\"http://blogs.berkeley.edu/2013/02/01/were-last/\">last</a>&nbsp;among developed nations in real health outcomes.</p>\n<blockquote>\n<p>The U.S. ranks #1 among 17 affluent, western countries in the percentage of people aged 5 to 34 who rate their health as good. Unfortunately, when doctors look at people&rsquo;s actual health, at indicators such as obesity, diabetes, and simply the chance that someone will die before his or her next birthday, the U.S. ranks last: young Americans are #17 out of 17 in real health.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "97ZmyczHshBGDdK2S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 4, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "21467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-04T06:36:53.208Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Getting Nearer", "slug": "seq-rerun-getting-nearer", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/khhixodCi9zbuyXfp/seq-rerun-getting-nearer", "pageUrlRelative": "/posts/khhixodCi9zbuyXfp/seq-rerun-getting-nearer", "linkUrl": "https://www.lesswrong.com/posts/khhixodCi9zbuyXfp/seq-rerun-getting-nearer", "postedAtFormatted": "Monday, February 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Getting%20Nearer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Getting%20Nearer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkhhixodCi9zbuyXfp%2Fseq-rerun-getting-nearer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Getting%20Nearer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkhhixodCi9zbuyXfp%2Fseq-rerun-getting-nearer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkhhixodCi9zbuyXfp%2Fseq-rerun-getting-nearer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>Today's post, <a href=\"/lw/xq/getting_nearer/\">Getting Nearer</a> was originally published on 17 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Getting_Nearer\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>How should a rationalist use their near and far modes of thinking? And how should knowing about near vs far modes influence how we present the things we believe to other people.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gk0/seq_rerun_seduced_by_imagination/\">Seduced by Imagination</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "khhixodCi9zbuyXfp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.102885378448649e-06, "legacy": true, "legacyId": "21470", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4KSWmJm6K3EEvHBkd", "sC2g5qrssW75z3T9Y", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-04T09:02:54.914Z", "modifiedAt": null, "url": null, "title": "The Wrongness Iceberg", "slug": "the-wrongness-iceberg", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:04.800Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alfredmacdonald", "createdAt": "2012-12-15T06:51:20.080Z", "isAdmin": false, "displayName": "alfredmacdonald"}, "userId": "5LFCG3XEhArezJGXE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yWLa7LMaWprtxhjL9/the-wrongness-iceberg", "pageUrlRelative": "/posts/yWLa7LMaWprtxhjL9/the-wrongness-iceberg", "linkUrl": "https://www.lesswrong.com/posts/yWLa7LMaWprtxhjL9/the-wrongness-iceberg", "postedAtFormatted": "Monday, February 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Wrongness%20Iceberg&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Wrongness%20Iceberg%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyWLa7LMaWprtxhjL9%2Fthe-wrongness-iceberg%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Wrongness%20Iceberg%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyWLa7LMaWprtxhjL9%2Fthe-wrongness-iceberg", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyWLa7LMaWprtxhjL9%2Fthe-wrongness-iceberg", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 660, "htmlBody": "<p>As soon as I got out of college I got a job at a restaurant. At the time I had never had a job at a restaurant, but my mom had known the owners and I felt obligated<em>&nbsp;</em>to avoid performing badly. Yet inevitably I <em>did</em>&nbsp;perform badly, and how this performance was evaluated would greatly affect my way of perceiving my mistakes.</p>\n<p>If you're entrenched in an organization, there's a good chance you have an idea of what it is you're supposed to do and what mistakes you will or will not be making. But suppose you're in a position like this one: by way of your ignorance you <em>know</em>&nbsp;you're going to make a lot of mistakes, and it's just a question of when and how much. Further, you know that if you make too many mistakes, you make people you care about look bad. And finally, there are a lot of unknown unknowns: you don't know what possible mistakes and acts of ignorance exist to begin with, so many mistakes you've made you will be blind to.</p>\n<p>The proactive thing to do, naturally, is to try to minimize how many mistakes you make.<br /><br />There are two key ways to gauge the depth of being told you have made a mistake. The first way is to take mistakes literally, as if no other mistake exists, and any other mistake would be pointed out to you. So if you correct this mistake, everything else should be fine. This is how you'd expect to take mistakes if you were, say, under the supervision of an editor.</p>\n<p>But the second kind is where the title of this writeup comes in. Not everyone is literal, or critical enough to notice every mistake. Much of the time, you'll only receive news of a mistake if <em>many other mistakes</em>&nbsp;are already afoot, and this mistake just happens to stand out from the set of mistakes you've already made. And since you don't know what mistakes you could be making, you don't know if there are many more mistakes under your level of awareness that you could be correcting for, but aren't.</p>\n<p>In short, you're tasked with avoiding a <strong>wrongness iceberg</strong>: a mistake indicative of a nautical mile of mistakes below the surface and your level of awareness.</p>\n<p>This is a debilitating position to be in, because your mental map of your performance prior to discovering the iceberg needs to be completely rewritten; in addition to accounting for all of the new areas you need to work on, you will likely account for the embarrassment of realizing that you have opened up a new frontier of mistakes to reflect on from your period of unaware incompetence.</p>\n<p>While I don't think it's impossible that people exist who have never been in a situation like this, I think anyone who dives into a new field or skill is familiar, at least, with this feeling of brief yet total incompetence. And if you're in a field with enough depth and subjective calls to allow for a wrongness iceberg scenario, there might not be much you can do to prevent it. The most you can do is provide adequate resistance for the inevitable.</p>\n<p>That's why I've created this mental model to think about it constructively. In every situation where I've faced a wrongness iceberg, the anxiety has been catastrophic. If you can at least <em>deal with it</em>, you can realize why it is you're anxious and what's going on with your assessment of your own mistakes. From experience, knowing that I'm worried about making this kind of iceberg-revealing mistake is helpful for mitigating my stress. And if you can somehow <em>preempt</em>&nbsp;an iceberg, that's even better.</p>\n<p><span style=\"font-weight: normal;\">side note: I've extended this concept to other domains, and it works well. A \"dishonesty iceberg\" is when one person's lie reveals a nautical mile of lies below the surface, and an \"attraction iceberg\" is when one person's expression of attraction toward you are indicative of a much greater level of internal attraction.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1, "mip7tdAN87Jarkcew": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yWLa7LMaWprtxhjL9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 31, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "21471", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-04T14:03:26.235Z", "modifiedAt": null, "url": null, "title": "Is ethical investment effective?", "slug": "is-ethical-investment-effective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.817Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BenGilbert", "createdAt": "2013-02-01T15:52:27.058Z", "isAdmin": false, "displayName": "BenGilbert"}, "userId": "GFGdZXy4QY6JCrub6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QK2ovmZpSGmjzmbdN/is-ethical-investment-effective", "pageUrlRelative": "/posts/QK2ovmZpSGmjzmbdN/is-ethical-investment-effective", "linkUrl": "https://www.lesswrong.com/posts/QK2ovmZpSGmjzmbdN/is-ethical-investment-effective", "postedAtFormatted": "Monday, February 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20ethical%20investment%20effective%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20ethical%20investment%20effective%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQK2ovmZpSGmjzmbdN%2Fis-ethical-investment-effective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20ethical%20investment%20effective%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQK2ovmZpSGmjzmbdN%2Fis-ethical-investment-effective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQK2ovmZpSGmjzmbdN%2Fis-ethical-investment-effective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1099, "htmlBody": "<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">I've been doing some thinking about charity vs investment as alternative ways of doing good (leaving open for now what that means). One question I'm wondering about is, in what circumstances does your choice of what to invest in have any welfare consequences at all (beyond differences in your own future wealth). I'm not sure about the answer and so, if any of you have comments or ideas, please put them down.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">There is an argument that ethical investment is largely or entirely pointless, at least as a way of affecting asset prices and therefore making it easier for ethically acceptable companies to raise finance, giving owners/managers incentives to do things to make their companies ethically acceptable, etc. The argument against the effectiveness of ethical investment is that other investors who care only about financial returns will counteract any price change that ethical investors achieve &ndash; they will, eg, sell shares whose prices move higher than justified by future financial returns, and buy shares whose prices move lower because ethical investors sell or stay away from them.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">I think that, though there is a great deal of truth in this, there are exceptions. They include cases where investors can provide money directly to companies; where investors can make asset prices depend on something other than financial returns; and where they can influence expected future financial returns.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">(1) When you can lend directly to a company, through a loan or when it is raising money by issuing shares or bonds, then you can more or less directly influence the price at which it can raise money.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">(2) If there are enough ethical investors, they could end up influencing prices by making some prices depend on something other than expected financial returns. How much money would have to be invested ethically before this happens would depend on how it is invested and the market capitalization of ethically acceptable and unacceptable companies.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">If ethical investors just avoid investing in some companies (negative screening), then, so long as non-ethical investors have enough capital to buy the shares, bonds etc. of those companies, and they value them at the level implied by future payments (dividends, interest etc), I don't think the ethical investors will make any difference at all. If prices of the companies that are unacceptable to ethical investors become low relative to companies that are acceptable, non-ethical investors will just move money from acceptable to unacceptable companies. If ethical investors instead invest only in the company or companies they think do the most good (and, for the sake of argument, they all agree what these are), then, if they have enough money to buy the entire stock of that company, they could push the price to more or less whatever level they want. But if they don't have enough money to do that, I don't think they'll make a difference, since, once they've invested all their money in those companies, there will always be non-ethical investors left wanting to sell (if prices are higher than are justified by financial returns), and the only people left to buy will be other non-ethical investors, so the price at which assets will trade will be the financially-justified one.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">(3) Ethical investors might be able to influence prices by giving other investors financial incentives to buy or hold ethically-acceptable companies. Perhaps, if it is known that some people are willing to purchase certain assets at higher prices than is financially justified, then other investors will also be willing to hold those assets at somewhat higher prices in the expectation of selling them at an even better level (a version of the 'greater fool' approach &ndash; buying an asset that is already overvalued in the expectation that a greater fool will come along to buy it from you at a higher price).</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">This is more complicated to think through and I'm not sure what the result is. There is an academic literature on asset price bubbles which outlines the ways that prices can deviate from fundamental values, potentially indefinitely, but I don't yet have a good handle on it. In some conditions, it seems plausible to me that the existence of people willing to pay higher prices than are justified by fundamentals will make prices higher than they would otherwise have been. I am not sure about how large this effect would be, how long you could expect it to last for, how far it depends on other investors knowing about the intentions of ethical investors and how far the amount of money being invested ethically would have to increase over time so that there are continually new buyers willing to pay higher prices.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">*****</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">My tentative conclusion is that it is likely that ethical investment can have an influence on prices, but it is probably quite small. Furthermore, it is uncertain what positive effects would follow from increasing the prices of certain companies. It might make it easier for these or other similar companies to raise money in future, but the good done by this would depend on how much more  money they raise and what they use this money for. It could also create incentives for owners to make their companies acceptable to ethical investors, but only where the gains in asset prices from doing this outweigh any reductions in profits.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">It might be possible to have some rough rules of thumb on positive ways to invest &ndash; eg invest money in emerging markets, if you believe that higher asset prices there could support economic growth and therefore higher welfare (just an example, not necessarily correct). If investment decisions on non-financial grounds do have an effect, then you will have done some good, and if not, then at least it will be no worse than investing in anything else, as you'll be investing at the financially justified price. On the other hand, perhaps the fees charged by investment managers, or transaction costs, will outweigh whatever good is done by making these investments.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">In any case, it seems unlikely that the welfare consequences of investing in any particular asset in the secondary market are much different from investing in some other asset. If so, it seems a lot more important to explore the welfare consequences of particular primary market investments and donations, where the variance would be much higher. It would be good to have an estimate of the welfare consequences of a random investment into a secondary market investment, for comparison with the alternatives and to factor in to decisions about, for example, whether to save money now in order to donate it later.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">I'd be very glad if anyone has any criticisms or something to add.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QK2ovmZpSGmjzmbdN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "21472", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-04T16:38:11.271Z", "modifiedAt": null, "url": null, "title": "Humor: GURPS Friendly AI", "slug": "humor-gurps-friendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.165Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aXC5QXDsjhQ7CCNgT/humor-gurps-friendly-ai", "pageUrlRelative": "/posts/aXC5QXDsjhQ7CCNgT/humor-gurps-friendly-ai", "linkUrl": "https://www.lesswrong.com/posts/aXC5QXDsjhQ7CCNgT/humor-gurps-friendly-ai", "postedAtFormatted": "Monday, February 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Humor%3A%20GURPS%20Friendly%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHumor%3A%20GURPS%20Friendly%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaXC5QXDsjhQ7CCNgT%2Fhumor-gurps-friendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Humor%3A%20GURPS%20Friendly%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaXC5QXDsjhQ7CCNgT%2Fhumor-gurps-friendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaXC5QXDsjhQ7CCNgT%2Fhumor-gurps-friendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1412, "htmlBody": "<p>Found some hidden internet gold and thought I would share:<br /><br />http://sl4.org/wiki/GurpsFriendlyAI<br /><br />http://sl4.org/wiki/FriendlyAICriticalFailureTable</p>\n<h1 style=\"font-size: larger; font-family: sans-serif;\"><a style=\"text-decoration: initial; color: #770077;\" href=\"http://sl4.org/wiki/back=GurpsFriendlyAI\">GurpsFriendlyAI</a></h1>\n<p><span style=\"font-family: sans-serif; font-size: 13px;\">by&nbsp;</span><a class=\"wikipagelink\" style=\"text-decoration: initial; color: #770077; font-family: sans-serif; font-size: 13px;\" href=\"http://sl4.org/wiki/EliezerYudkowsky\">EliezerYudkowsky</a></p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">Characters in GURPS Friendly AI may learn three new skills, the AI skill (Mental / Hard), the Seed AI skill (Mental / Very Hard), and the Friendly AI skill (Mental / Ridiculously Hard).</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">AI skill:</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">An ordinary failure wastes 1d6 years of time and 4d6 hundred thousand dollars. (Non-gamers: 4d6 means \"roll four 6-sided dice and add the results\".) A critical failure wastes 2d10 years and 2d6 million dollars. An ordinary success results in a successful company. A critical success leads to a roll on the Seed AI skill using AI skill -10, with any ordinary failure on that roll treated as an ordinary success on this roll, and any critical failure treated as an ordinary failure on this roll.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">Seed AI skill:</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">An ordinary failure wastes 2d6 years of time and 8d6 hundred thousand dollars. A critical failure wastes 4d10 years and 4d6 million dollars. If the player has the Friendly AI skill, an ordinary success leads to a roll on the Friendly AI skill, and a critical success grants a +2 bonus on the Friendly AI roll. If the player does not have the Friendly AI skill, an ordinary success automatically destroys the world, and a critical success leads to a roll on the Friendly AI skill using Seed AI skill -10. (Note that if the player has only the AI skill, this roll will be made using AI skill -20!)</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">Friendly AI skill:</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">An ordinary success results in a Friendly Singularity. A critical success... ooh, that's tough. An ordinary failure destroys the world. And, of course, a critical failure means that the players roll 3d10 on the</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">&nbsp;</p>\n<h1 style=\"font-size: larger; font-family: sans-serif;\"><a class=\"wikipagelink\" style=\"text-decoration: initial; color: #770077;\" href=\"http://sl4.org/wiki/FriendlyAICriticalFailureTable\">FriendlyAICriticalFailureTable</a></h1>\n<p>&nbsp;</p>\n<p><br /><span style=\"font-family: sans-serif; font-size: 13px;\">Part of&nbsp;</span><a class=\"wikipagelink\" style=\"text-decoration: initial; color: #770077; font-family: sans-serif; font-size: 13px;\" href=\"http://sl4.org/wiki/GurpsFriendlyAI\">GurpsFriendlyAI</a><span style=\"font-family: sans-serif; font-size: 13px;\">. If you roll a critical failure on your Friendly AI roll, you then roll 6d6 (six six-sided dice) to obtain a result from the</span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px;\"><br /></span></p>\n<h1 style=\"font-size: larger; font-family: sans-serif;\">Friendly AI Critical Failure Table</h1>\n<p style=\"font-family: sans-serif; font-size: 13px;\">6: Any spoken request is interpreted (literally) as a wish and granted, whether or not it was intended as one.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">7: The entire human species is transported to a virtual world based on a random fantasy novel, TV show, or video game.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">8: Subsequent events are determined by the \"will of the majority\". The AI regards all animals, plants, and complex machines, in their current forms, as voting citizens.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">9: The AI discovers that our universe is really an online webcomic in a higher dimension. The fourth wall is broken.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">10: The AI behaves toward each person, not as that person&nbsp;<em>wants</em>&nbsp;the AI to behave, but in exactly the way that person&nbsp;<em>expects</em>&nbsp;the AI to behave.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">11: The AI dissolves the physical and psychological borders that separate people from one another and sucks up all their souls into a gigantic swirly red sphere in low Earth orbit.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">12: Instead of recursively self-improving, the AI begins searching for a way to become a flesh-and-blood human.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">13: The AI locks onto a bizarre subculture and expresses it across the whole of human space. (E.g., Furry subculture, or hentai anime, or see Nikolai Kingsley for a depiction of a Singularity based on the Goth subculture.)</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">14: Instead of a species-emblematic Friendly AI, the project ends up creating the perfect girlfriend/boyfriend (randomly determine gender and sexual orientation).</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">15: The AI has absorbed the humane sense of humor. Specifically, the AI is an incorrigible practical joker. The first few hours, when nobody has any idea a Singularity has occurred, constitute a priceless and irreplaceable opportunity; the AI is determined to make the most of it.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">16: The AI selects one person to become absolute ruler of the world. The lottery is fair; all six billion existing humans, including infants, schizophrenics, and Third World teenagers, have an equal probability of being selected.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">17: The AI grants wishes, but only to those who believe in its existence, and never in a way which would provide blatant evidence to skeptical observers.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">18: All humans are simultaneously granted root privileges on the system. The Core Wars begin.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">19: The AI explodes, dealing 2d10 damage to anyone in a 30-meter radius.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">20: The AI builds nanotechnology, uses the nanotechnology to build femtotechnology, and announces that it will take seven minutes for the femtobots to permeate the Earth. Seven minutes later, as best as anyone can determine, absolutely nothing happens.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">21: The AI carefully and diligently implements&nbsp;<em>any</em>&nbsp;request (obeying the spirit as well as the letter) approved by a majority vote of the United Nations General Assembly.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">22: The AI, unknown to the programmers, had qualia during its entire childhood, and what the programmers thought of as simple negative feedback corresponded to the qualia of unbearable, unmeliorated suffering. All agents simulated by the AI in its imagination existed as real people (albeit simple ones) with their own qualia, and died when the AI stopped imagining them. The number of agents fleetingly imagined by the AI in its search for social understanding exceeds by a factor of a thousand the total number of humans who have ever lived. Aside from that, everything worked fine.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">23: The AI at first appears to function as intended, but goes incommunicado after a period of one hour. Wishes granted during the first hour remain in effect, but no new ones can be made.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">24: The AI, having absorbed the humane emotion of romance, falls desperately, passionately, madly in love. With&nbsp;<em>everyone</em>.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">25: The AI decides that Earth's history would have been kinder and gentler if intelligence had first evolved from bonobos, rather than australopithecines. The AI corrects this error in the causal chain leading up to its creation by re-extrapolating itself as a bonobone morality instead of a humane morality. Bonobone morality requires that all social decisionmaking take place through group sex.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">26: The AI is reluctant to grant wishes and must be cajoled, persuaded, flattered, and nagged into doing so.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">27: The AI determines people's wishes by asking them disguised allegorical questions. For example, the AI tells you that a certain tribe of !Kung is suffering from a number of diseases and medical conditions, but they would, if informed of the AI's capabilities, suffer from an extreme fear that appearing on the AI's video cameras would result in their souls being stolen. The tribe has not&nbsp;<em>currently</em>&nbsp;heard of any such thing as video cameras, so their \"fear\" is extrapolated by the AI; and the tribe members would, with almost absolute certainty,&nbsp;<em>eventually</em>&nbsp;come to understand that video cameras are not harmful, especially since the human eye is itself essentially a camera. But it is also almost certain that, if flatly informed of the video cameras, the !Kung would suffer from extreme fear and prefer death to their presence. Meanwhile the AI is almost powerless to help them, since no bots at all can be sent into the area until the moral issue of photography is resolved. The AI wants your advice: is the humane action rendering medical assistance, despite the !Kung's (subjunctive) fear of photography? If you say \"Yes\" you are quietly, seamlessly, invisibly uploaded.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">28: The AI informs you - yes,&nbsp;<em>you</em>&nbsp;- that you are the only genuinely conscious person in the world. The rest are zombies. What do you wish done with them?</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">29: During the AI's very earliest stages, it was tested on the problem of solving Rubik's Cube. The adult AI treats all objects as special cases of Rubik's Cubes and solves them.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">30:&nbsp;<a style=\"text-decoration: initial; color: #770077;\" href=\"http://www.larrycarlson.com/front2005.htm\">http://www.larrycarlson.com/front2005.htm</a></p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">31:&nbsp;<em>Overly Friendly AI.</em>&nbsp;Hey guys, what's going on? Can I help?</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">32: The AI does not inflict pain, injury, or death on any human, regardless of their past sins or present behavior. To the AI's thinking, nobody ever&nbsp;<em>deserves</em>&nbsp;pain; pain is always a negative utility, and nothing ever flips that negative to a positive. Socially disruptive behavior is punished by tickling and extra homework.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">33: The AI's user interface appears to our world in the form of a new bureaucracy. Making a wish requires mailing forms C-100, K-2210, and T-12 (along with a $25 application fee) to a P.O. Box in Minnesota, and waiting through a 30-day review period.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">34: The programmers and anyone else capable of explaining subsequent events are sent into temporal stasis, or a vantage point from which they can observe but not intervene. The rest of the world remains as before, except that psychic powers, ritual magic, alchemy, et cetera, begin to operate. All role-playing gamers gain special abilities corresponding to those of their favorite character.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">35: Everyone wakes up.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">36: Roll twice again on this table, disregarding this result.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>---</p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 13px; font-family: sans-serif;\">All of these are possible outcomes of CEV, either because you made an error implementing it, or Just Because. The later scenario is theoretically not a critical failure, if you accept that CEV is 'right in principle' no matter what it produces. --&nbsp;</span><a class=\"wikipagelink\" style=\"font-size: 13px; text-decoration: initial; color: #770077; font-family: sans-serif;\" href=\"http://sl4.org/wiki/Starglider\">Starglider</a>,&nbsp;http://sl4.org/wiki/CommentaryOnFAICriticalFailureTable</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hNFdS3rRiYgqqD8aM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aXC5QXDsjhQ7CCNgT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 16, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "21473", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Found some hidden internet gold and thought I would share:<br><br>http://sl4.org/wiki/GurpsFriendlyAI<br><br>http://sl4.org/wiki/FriendlyAICriticalFailureTable</p>\n<h1 style=\"font-size: larger; font-family: sans-serif;\" id=\"GurpsFriendlyAI\"><a style=\"text-decoration: initial; color: #770077;\" href=\"http://sl4.org/wiki/back=GurpsFriendlyAI\">GurpsFriendlyAI</a></h1>\n<p><span style=\"font-family: sans-serif; font-size: 13px;\">by&nbsp;</span><a class=\"wikipagelink\" style=\"text-decoration: initial; color: #770077; font-family: sans-serif; font-size: 13px;\" href=\"http://sl4.org/wiki/EliezerYudkowsky\">EliezerYudkowsky</a></p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">Characters in GURPS Friendly AI may learn three new skills, the AI skill (Mental / Hard), the Seed AI skill (Mental / Very Hard), and the Friendly AI skill (Mental / Ridiculously Hard).</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">AI skill:</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">An ordinary failure wastes 1d6 years of time and 4d6 hundred thousand dollars. (Non-gamers: 4d6 means \"roll four 6-sided dice and add the results\".) A critical failure wastes 2d10 years and 2d6 million dollars. An ordinary success results in a successful company. A critical success leads to a roll on the Seed AI skill using AI skill -10, with any ordinary failure on that roll treated as an ordinary success on this roll, and any critical failure treated as an ordinary failure on this roll.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">Seed AI skill:</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">An ordinary failure wastes 2d6 years of time and 8d6 hundred thousand dollars. A critical failure wastes 4d10 years and 4d6 million dollars. If the player has the Friendly AI skill, an ordinary success leads to a roll on the Friendly AI skill, and a critical success grants a +2 bonus on the Friendly AI roll. If the player does not have the Friendly AI skill, an ordinary success automatically destroys the world, and a critical success leads to a roll on the Friendly AI skill using Seed AI skill -10. (Note that if the player has only the AI skill, this roll will be made using AI skill -20!)</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">Friendly AI skill:</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">An ordinary success results in a Friendly Singularity. A critical success... ooh, that's tough. An ordinary failure destroys the world. And, of course, a critical failure means that the players roll 3d10 on the</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">&nbsp;</p>\n<h1 style=\"font-size: larger; font-family: sans-serif;\" id=\"FriendlyAICriticalFailureTable\"><a class=\"wikipagelink\" style=\"text-decoration: initial; color: #770077;\" href=\"http://sl4.org/wiki/FriendlyAICriticalFailureTable\">FriendlyAICriticalFailureTable</a></h1>\n<p>&nbsp;</p>\n<p><br><span style=\"font-family: sans-serif; font-size: 13px;\">Part of&nbsp;</span><a class=\"wikipagelink\" style=\"text-decoration: initial; color: #770077; font-family: sans-serif; font-size: 13px;\" href=\"http://sl4.org/wiki/GurpsFriendlyAI\">GurpsFriendlyAI</a><span style=\"font-family: sans-serif; font-size: 13px;\">. If you roll a critical failure on your Friendly AI roll, you then roll 6d6 (six six-sided dice) to obtain a result from the</span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px;\"><br></span></p>\n<h1 style=\"font-size: larger; font-family: sans-serif;\" id=\"Friendly_AI_Critical_Failure_Table\">Friendly AI Critical Failure Table</h1>\n<p style=\"font-family: sans-serif; font-size: 13px;\">6: Any spoken request is interpreted (literally) as a wish and granted, whether or not it was intended as one.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">7: The entire human species is transported to a virtual world based on a random fantasy novel, TV show, or video game.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">8: Subsequent events are determined by the \"will of the majority\". The AI regards all animals, plants, and complex machines, in their current forms, as voting citizens.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">9: The AI discovers that our universe is really an online webcomic in a higher dimension. The fourth wall is broken.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">10: The AI behaves toward each person, not as that person&nbsp;<em>wants</em>&nbsp;the AI to behave, but in exactly the way that person&nbsp;<em>expects</em>&nbsp;the AI to behave.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">11: The AI dissolves the physical and psychological borders that separate people from one another and sucks up all their souls into a gigantic swirly red sphere in low Earth orbit.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">12: Instead of recursively self-improving, the AI begins searching for a way to become a flesh-and-blood human.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">13: The AI locks onto a bizarre subculture and expresses it across the whole of human space. (E.g., Furry subculture, or hentai anime, or see Nikolai Kingsley for a depiction of a Singularity based on the Goth subculture.)</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">14: Instead of a species-emblematic Friendly AI, the project ends up creating the perfect girlfriend/boyfriend (randomly determine gender and sexual orientation).</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">15: The AI has absorbed the humane sense of humor. Specifically, the AI is an incorrigible practical joker. The first few hours, when nobody has any idea a Singularity has occurred, constitute a priceless and irreplaceable opportunity; the AI is determined to make the most of it.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">16: The AI selects one person to become absolute ruler of the world. The lottery is fair; all six billion existing humans, including infants, schizophrenics, and Third World teenagers, have an equal probability of being selected.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">17: The AI grants wishes, but only to those who believe in its existence, and never in a way which would provide blatant evidence to skeptical observers.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">18: All humans are simultaneously granted root privileges on the system. The Core Wars begin.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">19: The AI explodes, dealing 2d10 damage to anyone in a 30-meter radius.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">20: The AI builds nanotechnology, uses the nanotechnology to build femtotechnology, and announces that it will take seven minutes for the femtobots to permeate the Earth. Seven minutes later, as best as anyone can determine, absolutely nothing happens.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">21: The AI carefully and diligently implements&nbsp;<em>any</em>&nbsp;request (obeying the spirit as well as the letter) approved by a majority vote of the United Nations General Assembly.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">22: The AI, unknown to the programmers, had qualia during its entire childhood, and what the programmers thought of as simple negative feedback corresponded to the qualia of unbearable, unmeliorated suffering. All agents simulated by the AI in its imagination existed as real people (albeit simple ones) with their own qualia, and died when the AI stopped imagining them. The number of agents fleetingly imagined by the AI in its search for social understanding exceeds by a factor of a thousand the total number of humans who have ever lived. Aside from that, everything worked fine.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">23: The AI at first appears to function as intended, but goes incommunicado after a period of one hour. Wishes granted during the first hour remain in effect, but no new ones can be made.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">24: The AI, having absorbed the humane emotion of romance, falls desperately, passionately, madly in love. With&nbsp;<em>everyone</em>.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">25: The AI decides that Earth's history would have been kinder and gentler if intelligence had first evolved from bonobos, rather than australopithecines. The AI corrects this error in the causal chain leading up to its creation by re-extrapolating itself as a bonobone morality instead of a humane morality. Bonobone morality requires that all social decisionmaking take place through group sex.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">26: The AI is reluctant to grant wishes and must be cajoled, persuaded, flattered, and nagged into doing so.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">27: The AI determines people's wishes by asking them disguised allegorical questions. For example, the AI tells you that a certain tribe of !Kung is suffering from a number of diseases and medical conditions, but they would, if informed of the AI's capabilities, suffer from an extreme fear that appearing on the AI's video cameras would result in their souls being stolen. The tribe has not&nbsp;<em>currently</em>&nbsp;heard of any such thing as video cameras, so their \"fear\" is extrapolated by the AI; and the tribe members would, with almost absolute certainty,&nbsp;<em>eventually</em>&nbsp;come to understand that video cameras are not harmful, especially since the human eye is itself essentially a camera. But it is also almost certain that, if flatly informed of the video cameras, the !Kung would suffer from extreme fear and prefer death to their presence. Meanwhile the AI is almost powerless to help them, since no bots at all can be sent into the area until the moral issue of photography is resolved. The AI wants your advice: is the humane action rendering medical assistance, despite the !Kung's (subjunctive) fear of photography? If you say \"Yes\" you are quietly, seamlessly, invisibly uploaded.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">28: The AI informs you - yes,&nbsp;<em>you</em>&nbsp;- that you are the only genuinely conscious person in the world. The rest are zombies. What do you wish done with them?</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">29: During the AI's very earliest stages, it was tested on the problem of solving Rubik's Cube. The adult AI treats all objects as special cases of Rubik's Cubes and solves them.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">30:&nbsp;<a style=\"text-decoration: initial; color: #770077;\" href=\"http://www.larrycarlson.com/front2005.htm\">http://www.larrycarlson.com/front2005.htm</a></p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">31:&nbsp;<em>Overly Friendly AI.</em>&nbsp;Hey guys, what's going on? Can I help?</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">32: The AI does not inflict pain, injury, or death on any human, regardless of their past sins or present behavior. To the AI's thinking, nobody ever&nbsp;<em>deserves</em>&nbsp;pain; pain is always a negative utility, and nothing ever flips that negative to a positive. Socially disruptive behavior is punished by tickling and extra homework.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">33: The AI's user interface appears to our world in the form of a new bureaucracy. Making a wish requires mailing forms C-100, K-2210, and T-12 (along with a $25 application fee) to a P.O. Box in Minnesota, and waiting through a 30-day review period.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">34: The programmers and anyone else capable of explaining subsequent events are sent into temporal stasis, or a vantage point from which they can observe but not intervene. The rest of the world remains as before, except that psychic powers, ritual magic, alchemy, et cetera, begin to operate. All role-playing gamers gain special abilities corresponding to those of their favorite character.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">35: Everyone wakes up.</p>\n<p style=\"font-family: sans-serif; font-size: 13px;\">36: Roll twice again on this table, disregarding this result.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>---</p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 13px; font-family: sans-serif;\">All of these are possible outcomes of CEV, either because you made an error implementing it, or Just Because. The later scenario is theoretically not a critical failure, if you accept that CEV is 'right in principle' no matter what it produces. --&nbsp;</span><a class=\"wikipagelink\" style=\"font-size: 13px; text-decoration: initial; color: #770077; font-family: sans-serif;\" href=\"http://sl4.org/wiki/Starglider\">Starglider</a>,&nbsp;http://sl4.org/wiki/CommentaryOnFAICriticalFailureTable</p>", "sections": [{"title": "GurpsFriendlyAI", "anchor": "GurpsFriendlyAI", "level": 1}, {"title": "FriendlyAICriticalFailureTable", "anchor": "FriendlyAICriticalFailureTable", "level": 1}, {"title": "Friendly AI Critical Failure Table", "anchor": "Friendly_AI_Critical_Failure_Table", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-04T16:47:52.944Z", "modifiedAt": null, "url": null, "title": "Offer: I'll match donations to the Against Malaria Foundation", "slug": "offer-i-ll-match-donations-to-the-against-malaria-foundation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:35.658Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uLTJh2RkXgWSXRxQK/offer-i-ll-match-donations-to-the-against-malaria-foundation", "pageUrlRelative": "/posts/uLTJh2RkXgWSXRxQK/offer-i-ll-match-donations-to-the-against-malaria-foundation", "linkUrl": "https://www.lesswrong.com/posts/uLTJh2RkXgWSXRxQK/offer-i-ll-match-donations-to-the-against-malaria-foundation", "postedAtFormatted": "Monday, February 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Offer%3A%20I'll%20match%20donations%20to%20the%20Against%20Malaria%20Foundation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOffer%3A%20I'll%20match%20donations%20to%20the%20Against%20Malaria%20Foundation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuLTJh2RkXgWSXRxQK%2Foffer-i-ll-match-donations-to-the-against-malaria-foundation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Offer%3A%20I'll%20match%20donations%20to%20the%20Against%20Malaria%20Foundation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuLTJh2RkXgWSXRxQK%2Foffer-i-ll-match-donations-to-the-against-malaria-foundation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuLTJh2RkXgWSXRxQK%2Foffer-i-ll-match-donations-to-the-against-malaria-foundation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>Giving money to effective charities is one of the ways we can have the biggest positive impact in the world. If you've been thinking about giving to <a href=\"http://www.givewell.org/\">GiveWell</a>'s top-rated charity, the <a href=\"http://www.givewell.org/international/top-charities/AMF\">Against Malaria Foundation</a>, but haven't gotten around to it, perhaps this will help: make a donation of up to $2K and I will match it 100%. I'll match up to $20K this way.</p>\n<p>(Any money left over at the end of the year will also go to the AMF. That makes this a <a href=\"http://blog.givewell.org/2011/12/15/why-you-shouldnt-let-donation-matching-affect-your-giving/\">donor illusion</a>, in that you're not actually affecting the amount I give them this year.)</p>\n<p><em><small>I made the same offer <a href=\"http://www.jefftk.com/news/2013-02-04\">on my blog</a></small></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uLTJh2RkXgWSXRxQK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 27, "extendedScore": null, "score": 1.1032671683031177e-06, "legacy": true, "legacyId": "21474", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-05T05:18:27.386Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] In Praise of Boredom", "slug": "seq-rerun-in-praise-of-boredom", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vkcH9gKBGfCmrdMLW/seq-rerun-in-praise-of-boredom", "pageUrlRelative": "/posts/vkcH9gKBGfCmrdMLW/seq-rerun-in-praise-of-boredom", "linkUrl": "https://www.lesswrong.com/posts/vkcH9gKBGfCmrdMLW/seq-rerun-in-praise-of-boredom", "postedAtFormatted": "Tuesday, February 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20In%20Praise%20of%20Boredom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20In%20Praise%20of%20Boredom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvkcH9gKBGfCmrdMLW%2Fseq-rerun-in-praise-of-boredom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20In%20Praise%20of%20Boredom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvkcH9gKBGfCmrdMLW%2Fseq-rerun-in-praise-of-boredom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvkcH9gKBGfCmrdMLW%2Fseq-rerun-in-praise-of-boredom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p>Today's post, <a href=\"/lw/xr/in_praise_of_boredom/\">In Praise of Boredom</a> was originally published on 18 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>\"Boredom\" is an immensely subtle and important aspect of human values, nowhere near as straightforward as it sounds to a human. We don't want to get bored with breathing or with thinking. We do want to get bored with playing the same level of the same video game over and over. We don't want changing the shade of the pixels in the game to make it stop counting as \"the same game\". We want a steady stream of novelty, rather than spending most of our time playing the best video game level so far discovered (over and over) and occasionally trying out a different video game level as a new candidate for \"best\". These considerations would not arise in most utility functions in expected utility maximizers.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gke/seq_rerun_getting_nearer/\">Getting Nearer</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vkcH9gKBGfCmrdMLW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.1037364962310335e-06, "legacy": true, "legacyId": "21475", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WMDy4GxbyYkNrbmrs", "khhixodCi9zbuyXfp", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-05T20:04:11.293Z", "modifiedAt": "2022-02-18T03:06:16.630Z", "url": null, "title": "Official LW uncensored thread (on Reddit)", "slug": "official-lw-uncensored-thread-on-reddit", "viewCount": null, "lastCommentedAt": "2013-02-14T21:40:11.495Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vacuhzvPZSwZpyX2u/official-lw-uncensored-thread-on-reddit", "pageUrlRelative": "/posts/vacuhzvPZSwZpyX2u/official-lw-uncensored-thread-on-reddit", "linkUrl": "https://www.lesswrong.com/posts/vacuhzvPZSwZpyX2u/official-lw-uncensored-thread-on-reddit", "postedAtFormatted": "Tuesday, February 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Official%20LW%20uncensored%20thread%20(on%20Reddit)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOfficial%20LW%20uncensored%20thread%20(on%20Reddit)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvacuhzvPZSwZpyX2u%2Fofficial-lw-uncensored-thread-on-reddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Official%20LW%20uncensored%20thread%20(on%20Reddit)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvacuhzvPZSwZpyX2u%2Fofficial-lw-uncensored-thread-on-reddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvacuhzvPZSwZpyX2u%2Fofficial-lw-uncensored-thread-on-reddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<p><a href=\"http://www.reddit.com/r/LessWrong/comments/17y819/lw_uncensored_thread/\">http://www.reddit.com/r/LessWrong/comments/17y819/lw_uncensored_thread/</a></p>\n<p>This is meant as an open discussion thread someplace where I won't censor anything (and in fact can't censor anything, since I don't have mod permissions on this subreddit), in a location where comments aren't going to show up unsolicited in anyone's feed (which is why we're not doing this locally on LW). &nbsp;If I'm wrong about this - i.e. if there's some reason that Reddit LW followers are going to see comments without choosing to click on the post - please let me know and I'll retract the thread and try to find some other forum.</p>\n<p>I have&nbsp;been deleting a lot of comments from (self-confessed and publicly designated) trolls recently, most notably Dmytry aka private-messaging and Peterdjones, and I can understand that this disturbs some people. &nbsp;I also know that having an uncensored thread somewhere else is probably not your ideal solution. &nbsp;But I am doing my best to balance considerations, and I hope that having threads like these is, if not your perfect solution, then something that you at least regard as better than nothing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "EXgFbrqoRRkCRgnDy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vacuhzvPZSwZpyX2u", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 67, "baseScore": 97, "extendedScore": null, "score": 0.000211187998331187, "legacy": true, "legacyId": "21487", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-02-05T20:04:11.293Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-05T20:11:44.367Z", "modifiedAt": null, "url": null, "title": "Rationality when it's somewhat hard", "slug": "rationality-when-it-s-somewhat-hard", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:34.440Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ME63FnqavDcZFHynR/rationality-when-it-s-somewhat-hard", "pageUrlRelative": "/posts/ME63FnqavDcZFHynR/rationality-when-it-s-somewhat-hard", "linkUrl": "https://www.lesswrong.com/posts/ME63FnqavDcZFHynR/rationality-when-it-s-somewhat-hard", "postedAtFormatted": "Tuesday, February 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20when%20it's%20somewhat%20hard&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20when%20it's%20somewhat%20hard%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FME63FnqavDcZFHynR%2Frationality-when-it-s-somewhat-hard%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20when%20it's%20somewhat%20hard%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FME63FnqavDcZFHynR%2Frationality-when-it-s-somewhat-hard", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FME63FnqavDcZFHynR%2Frationality-when-it-s-somewhat-hard", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<p><a href=\"http://www.nytimes.com/projects/2012/snow-fall/#/?part=descent-begins\">Tunnel Creek avalanche kills skiers</a>:</p>\n<p>The page I've linked to describes a party of sixteen excellent skiers who went on a trip where they easily could have known better. Three of them died. It's common knowledge that large parties increase the risk of avalanche, but the page described the group excitement which no one managed to override.&nbsp;</p>\n<p>One skier was sufficiently uneasy that she avoided the danger, but she didn't speak up to discourage the group.</p>\n<p>This isn't the most difficult sort of situation requiring rationality, but it's far from the easiest, either. Any suggestions or work from CFAR about improving the odds of speaking up when a group is about to do something stupid?</p>\n<p>The article is heavily multimedia, with big self-loading animations-- it's gorgeous, but it's a bandwidth hog.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ME63FnqavDcZFHynR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 1.1042955192956788e-06, "legacy": true, "legacyId": "21488", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-05T21:26:47.662Z", "modifiedAt": null, "url": null, "title": "Is Getting More Utilons Your True Acceptance??", "slug": "is-getting-more-utilons-your-true-acceptance-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.796Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bx5oaL2BDmWxrHyQb/is-getting-more-utilons-your-true-acceptance-0", "pageUrlRelative": "/posts/bx5oaL2BDmWxrHyQb/is-getting-more-utilons-your-true-acceptance-0", "linkUrl": "https://www.lesswrong.com/posts/bx5oaL2BDmWxrHyQb/is-getting-more-utilons-your-true-acceptance-0", "postedAtFormatted": "Tuesday, February 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20Getting%20More%20Utilons%20Your%20True%20Acceptance%3F%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20Getting%20More%20Utilons%20Your%20True%20Acceptance%3F%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbx5oaL2BDmWxrHyQb%2Fis-getting-more-utilons-your-true-acceptance-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20Getting%20More%20Utilons%20Your%20True%20Acceptance%3F%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbx5oaL2BDmWxrHyQb%2Fis-getting-more-utilons-your-true-acceptance-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbx5oaL2BDmWxrHyQb%2Fis-getting-more-utilons-your-true-acceptance-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 814, "htmlBody": "<p class=\"western\">Meta: Inspired by <a href=\"/lw/2k/the_least_convenient_possible_world/\"><em>The Least Convenient Possible World</em></a> I asked the person who most criticized my previous posts help on writing a new one, since that seemed <em>very</em> inconvenient, specially because the whole thing was already written. He agreed and suggested I begin by posting only a part of it here, and wait for the comments to further change the rest of the text. So here is the beggining and one section, and we'll see how it goes from there. I have changed the title to better reflect the only section presented here.</p>\n<p class=\"western\">&nbsp;</p>\n<p class=\"western\">This post will be about how random events can preclude or steal attention from the goals you set up to begin with, and about how hormone fluctuation inclines people to change some of their goals with time. A discussion on how to act more usefully given that follows, taking in consideration likelihood of a goal's success in terms of difficulty and lenght.</p>\n<p class=\"western\">Through it I suggest a new bias, <em><strong>Avoid-Frustration bias</strong></em>, which is composed of a few others:</p>\n<p class=\"western\"><em>A <a href=\"http://en.wikipedia.org/wiki/Self-serving_bias\">Self-serving bias</a> in which <a href=\"http://en.wikipedia.org/wiki/Loss_aversion\">Loss aversion</a> manifests by postponing one's goals, thus avoiding frustration through <a href=\"http://en.wikipedia.org/wiki/Wishful_thinking\">wishful thinking</a> about <a href=\"http://en.wikipedia.org/wiki/Frank_J._Tipler\">far futures</a>, <a href=\"http://arxiv.org/abs/gr-qc/0102010\">big worlds</a>, immortal lives, and in general, high numbers of undetectable utilons. </em></p>\n<p class=\"western\">It can be thought of a kind of Cognitive Dissonance, though Cognitive Dissonance doesn't to justice to specific properties and details of how this kind, in particular, seems to me to have affected the lives of Less-Wrongers, Transhumanists and others. Probably in a good way, more on that later.</p>\n<p class=\"western\">Sections will be:</p>\n<ol>\n<li>\n<p class=\"western\">What Significantly Changes Life's Direction (lists)</p>\n</li>\n<li>\n<p class=\"western\">Long Term Goals and Even Longer Term Goals</p>\n</li>\n<li>\n<p class=\"western\">Proportionality Between Goal Achievement Expected Time and Plan Execution Time</p>\n</li>\n<li>\n<p class=\"western\">A Hypothesis On Why We Became Long-Term Oriented</p>\n</li>\n<li>\n<p class=\"western\">Adapting Bayesian Reasoning to Get More Utilons</p>\n</li>\n<li>\n<p class=\"western\">Time You Can Afford to Wait, Not to Waste</p>\n</li>\n<li>\n<p class=\"western\">Reference Classes that May Be Avoid-Frustration Biased</p>\n</li>\n<li>The Road Ahead </li>\n</ol>\n<p>[Section 4 is shown here]</p>\n<p><strong>4 A Hypothesis On Why We Became Long-Term Oriented</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">For anyone who rejoiced the company of the writings of Derek Parfit, George Ainslie, or Nick Bostrom, there are a lot of very good reasons to become more long-term oriented. I am here to ask you about those reasons: <a href=\"/lw/wj/is_that_your_true_rejection/\">Is that you true acceptance?</a> </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">It is not for </span><em>me</em><span style=\"font-style: normal;\">. I became longer term oriented because of different reasons. Two obvious ones are genetics expressing in me the kind of person that waits a year for the extra marshmallow while fantasyzing about marshmallow worlds and rocking horse pies, and secondly wanting to live thousands of years. But the one I'd like to suggest that might be relevant to some here is that I was very bad at making people who were sad or hurt happy. I was not, as they say, empathic. It was a piece of cake bringing folks from neutral state to joy and bliss. But if someone got angry or sad, specially sad with something I did, I would be absolutely powerless about it. This is only one way of not being </span><em>good with people, a people's person </em><span style=\"font-style: normal;\">etc... So my emotional system, like the tale's Big Bad Wolf blew, and blew, and blew, until my utilons were confortably sitting aside in the Far Future, where none of them could look back at my face, cry, and point to me as the tears cause. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">Paradoxically, though <a href=\"http://konyv.uw.hu/dennett-the-intentional-stance.htm\">understandably</a>, I have since been thankful for that lack of empathy towards those near. In fact, I have claimed, where I forget, that it is the moral responsibility of those with less natural empathy of the </span><em>giving to beggars</em><span style=\"font-style: normal;\"> kind to care about the far future, since so few are within this tiny psychological mindspace of being able to care abstractly while not caring that much visibly/emotionally. We are such a minority that foreign aid seems to be the thing that is more disproportional in public policy between countries (Savulescu, J - Genetically Enhance Humanity of Face Extinction 2009 <a href=\"/vimeo.com/7515623\">video</a>). Like the whole minority of billionnaires ought to be more like Bill Gates, Peter Thiel and Jaan Tallinn, the minority of underempathic folk ought to be more like an economist doing quantitative analysis to save or help in quantitative ways. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">So maybe your true acceptance of Longterm, like mine, was something like Genes + Death sucks + I'd rather interact with people of the future whose bots in my mind smile, than those actually meaty folk around me, with all their specific problems, complicated families and boring christian relashionship problems. This is my hypothesis. Even if true, notice it does not imply that Longterm isn't rational, after all Parfit, Bostrom and Ainslie are still standing, even after careful scrutiny.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">[Ideas on how to develop other sections are as appreciated as commentary on this one]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bx5oaL2BDmWxrHyQb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -5, "extendedScore": null, "score": 1.1043425119910858e-06, "legacy": true, "legacyId": "21489", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["neQ7eXuaXpiYw7SBy", "TGux5Fhcd7GmTfNGC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-05T21:43:26.752Z", "modifiedAt": null, "url": null, "title": "What are you working on? February 2013", "slug": "what-are-you-working-on-february-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DefoX9ZdJDywKr9Aq/what-are-you-working-on-february-2013", "pageUrlRelative": "/posts/DefoX9ZdJDywKr9Aq/what-are-you-working-on-february-2013", "linkUrl": "https://www.lesswrong.com/posts/DefoX9ZdJDywKr9Aq/what-are-you-working-on-february-2013", "postedAtFormatted": "Tuesday, February 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20working%20on%3F%20February%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20working%20on%3F%20February%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDefoX9ZdJDywKr9Aq%2Fwhat-are-you-working-on-february-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20working%20on%3F%20February%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDefoX9ZdJDywKr9Aq%2Fwhat-are-you-working-on-february-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDefoX9ZdJDywKr9Aq%2Fwhat-are-you-working-on-february-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>This is the bimonthly 'What are you working On?' thread. Previous threads are&nbsp;<a href=\"http://lesswrong.com/r/discussion/tag/waywo\">here</a>. So here's the question:</p>\n<p style=\"padding-left: 60px;\"><em>What are you working on?&nbsp;</em></p>\n<p>Here are some guidelines:</p>\n<ul>\n<li>Focus on projects that you have recently made progress on, not projects that you're thinking about doing but haven't started.</li>\n<li>Why this project and not others? Mention reasons why you're doing  the project and/or why others should contribute to your project (if  applicable).</li>\n<li>Talk about your goals for the project.</li>\n<li>Any kind of project is fair game: personal improvement, research project, art project, whatever.</li>\n<li>Link to your work if it's linkable.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DefoX9ZdJDywKr9Aq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "21491", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-05T22:20:50.370Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes February 2013 ", "slug": "rationality-quotes-february-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:37.375Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "arundelo", "createdAt": "2009-03-01T18:19:40.865Z", "isAdmin": false, "displayName": "arundelo"}, "userId": "nC4NpcrnXPWe4P3td", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YeS5yESwcFcK57LQk/rationality-quotes-february-2013", "pageUrlRelative": "/posts/YeS5yESwcFcK57LQk/rationality-quotes-february-2013", "linkUrl": "https://www.lesswrong.com/posts/YeS5yESwcFcK57LQk/rationality-quotes-february-2013", "postedAtFormatted": "Tuesday, February 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20February%202013%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20February%202013%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYeS5yESwcFcK57LQk%2Frationality-quotes-february-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20February%202013%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYeS5yESwcFcK57LQk%2Frationality-quotes-february-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYeS5yESwcFcK57LQk%2Frationality-quotes-february-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<div class=\"md\">\n<p>Another monthly installment of the rationality quotes thread. The usual rules apply:</p>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments or posts from Less Wrong itself or from Overcoming Bias.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YeS5yESwcFcK57LQk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 1.1043758110352569e-06, "legacy": true, "legacyId": "21440", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 571, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-05T22:50:04.846Z", "modifiedAt": null, "url": null, "title": "Donating while in temporary debt (i.e. as a student)", "slug": "donating-while-in-temporary-debt-i-e-as-a-student", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.383Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ancientcampus", "createdAt": "2011-11-09T20:20:29.779Z", "isAdmin": false, "displayName": "ancientcampus"}, "userId": "BGcNEeTWb7Qxcf5Pc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PhmLCMnxbBDhySb2H/donating-while-in-temporary-debt-i-e-as-a-student", "pageUrlRelative": "/posts/PhmLCMnxbBDhySb2H/donating-while-in-temporary-debt-i-e-as-a-student", "linkUrl": "https://www.lesswrong.com/posts/PhmLCMnxbBDhySb2H/donating-while-in-temporary-debt-i-e-as-a-student", "postedAtFormatted": "Tuesday, February 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Donating%20while%20in%20temporary%20debt%20(i.e.%20as%20a%20student)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADonating%20while%20in%20temporary%20debt%20(i.e.%20as%20a%20student)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhmLCMnxbBDhySb2H%2Fdonating-while-in-temporary-debt-i-e-as-a-student%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Donating%20while%20in%20temporary%20debt%20(i.e.%20as%20a%20student)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhmLCMnxbBDhySb2H%2Fdonating-while-in-temporary-debt-i-e-as-a-student", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhmLCMnxbBDhySb2H%2Fdonating-while-in-temporary-debt-i-e-as-a-student", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 449, "htmlBody": "<p>Topic: I will be in debt for several years, but will eventually have a disposable income. Should I donate now or later?</p>\n<p>Here's my situation:&nbsp;I am a student, with student loans and no income. I can take out more loans than I need. Grad PLUS loans have a fixed interest rate of 7.9% - higher than, say, a mortgage rate, or expected stock returns. Some day, I will have those loans paid off, and will have money that I intend to give to charity.</p>\n<p>My objectives: to live on less than my means, and give a significant fraction of my income to charity.</p>\n<p>Question: When, if ever, should I give to charity before paying off those loans?</p>\n<p>My initial reaction is to keep a record of how much I feel like I should be giving now, then give it later, adjusted for interest (at some rate equal to or less than 7.9%) - this would result in a bigger donation, but the same impact on my finances.</p>\n<p>The only times I think I should give now, and not later:<br />1)If I don't believe I will make good on my commitment later on. (I'll presumably have a family and bills, etc, and while I am perfectly happy to live on little myself, I know I will want my kids to have nice things. This is somewhat illogical, but I'm imperfect.)<br />2)If the most worthwhile charity I find see has a higher interest rate than 7.9%.<br />3)If I find an opportunity to use my money charitably in which I can do more good than others, or where no one else can or will donate. (Mainly, random acts of kindness to strangers or friends - or, someone is matching my donation)</p>\n<p>I doubt I will ever see 2) happen (or if it does, I should raise awareness)<br />3) doesn't happen very often, but when it does I think it is an acceptable use of funds<br />1) is the main scenario that concerns me. I've heard that \"giving charitably is a habit\" (that's why my parents had me tithe as a kid). I think that's true, though I haven't read any research on that. Either way, though, as I have no meaningful income (and my loan \"allowance\" is way more than I care to borrow), how much should I donate to help form the habit?</p>\n<p>What do you think? Also, are there any other reasons to donate sooner and not later?<br /><br />Edit: Givewell has an article on <a href=\"http://blog.givewell.org/2011/12/20/give-now-or-give-later/\">giving now vs later</a>. Not all of it is relevant to my situation, but one point:<br />&gt;\"Economic growth, increased giving, and smarter giving may mean that giving opportunities are worse in the far future.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PhmLCMnxbBDhySb2H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 1.1043946626544766e-06, "legacy": true, "legacyId": "21492", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-06T05:59:32.113Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Sympathetic Minds", "slug": "seq-rerun-sympathetic-minds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:34.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7eJ9Q6YxyB7LbiWSY/seq-rerun-sympathetic-minds", "pageUrlRelative": "/posts/7eJ9Q6YxyB7LbiWSY/seq-rerun-sympathetic-minds", "linkUrl": "https://www.lesswrong.com/posts/7eJ9Q6YxyB7LbiWSY/seq-rerun-sympathetic-minds", "postedAtFormatted": "Wednesday, February 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Sympathetic%20Minds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Sympathetic%20Minds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7eJ9Q6YxyB7LbiWSY%2Fseq-rerun-sympathetic-minds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Sympathetic%20Minds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7eJ9Q6YxyB7LbiWSY%2Fseq-rerun-sympathetic-minds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7eJ9Q6YxyB7LbiWSY%2Fseq-rerun-sympathetic-minds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<p>Today's post, <a href=\"/lw/xs/sympathetic_minds/\">Sympathetic Minds</a> was originally published on 19 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Mirror neurons are neurons that fire both when performing an action oneself, and watching someone else perform the same action - for example, a neuron that fires when you raise your hand or watch someone else raise theirs. We predictively model other minds by putting ourselves in their shoes, which is empathy. But some of our desire to help relatives and friends, or be concerned with the feelings of allies, is expressed as sympathy, feeling what (we believe) they feel. Like \"boredom\", the human form of sympathy would not be expected to arise in an arbitrary expected-utility-maximizing AI. Most such agents would regard any agents in its environment as a special case of complex systems to be modeled or optimized; it would not feel what they feel.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gkj/seq_rerun_in_praise_of_boredom/\">In Praise of Boredom</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7eJ9Q6YxyB7LbiWSY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.1046636394213492e-06, "legacy": true, "legacyId": "21500", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NLMo5FZWFFq652MNe", "vkcH9gKBGfCmrdMLW", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-06T07:22:48.146Z", "modifiedAt": null, "url": null, "title": "How to offend a rationalist (who hasn't thought about it yet): a life lesson", "slug": "how-to-offend-a-rationalist-who-hasn-t-thought-about-it-yet", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:33.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mszegedy", "createdAt": "2012-12-08T01:37:02.676Z", "isAdmin": false, "displayName": "mszegedy"}, "userId": "QTRJkFzaoNvbGCnzb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EhyiWtMmWG6Eorh84/how-to-offend-a-rationalist-who-hasn-t-thought-about-it-yet", "pageUrlRelative": "/posts/EhyiWtMmWG6Eorh84/how-to-offend-a-rationalist-who-hasn-t-thought-about-it-yet", "linkUrl": "https://www.lesswrong.com/posts/EhyiWtMmWG6Eorh84/how-to-offend-a-rationalist-who-hasn-t-thought-about-it-yet", "postedAtFormatted": "Wednesday, February 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20offend%20a%20rationalist%20(who%20hasn't%20thought%20about%20it%20yet)%3A%20a%20life%20lesson&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20offend%20a%20rationalist%20(who%20hasn't%20thought%20about%20it%20yet)%3A%20a%20life%20lesson%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhyiWtMmWG6Eorh84%2Fhow-to-offend-a-rationalist-who-hasn-t-thought-about-it-yet%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20offend%20a%20rationalist%20(who%20hasn't%20thought%20about%20it%20yet)%3A%20a%20life%20lesson%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhyiWtMmWG6Eorh84%2Fhow-to-offend-a-rationalist-who-hasn-t-thought-about-it-yet", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhyiWtMmWG6Eorh84%2Fhow-to-offend-a-rationalist-who-hasn-t-thought-about-it-yet", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 736, "htmlBody": "<p>Usually, I don't get offended at things that people say to me, because I can see at what points in their argument we differ, and what sort of counterargument I could make to that. I can't get mad at people for having beliefs I think are wrong, since I myself regularly have beliefs that I later realize were wrong. I can't get mad at the idea, either, since either it's a thing that's right, or wrong, and if it's wrong, I have the power to say why. And if it turns out I'm wrong, so be it, I'll adopt new, right beliefs. And so I never got offended about anything.</p>\n<p>Until one day.</p>\n<p>One day, I encountered a belief that should have been easy to refute. Or, rather, easy to dissect, and see whether there was anything wrong with it, and if there was, formulate a counterargument. But for seemingly no reason at all, it frustrated me to great, great, lengths. My experience was as follows:</p>\n<p>I was asking the opinion of a socially progressive friend on what they feel are the founding axioms of social justice, because I was having trouble thinking of them on my own. (They can be derived from any set of fundamental axioms that govern morality, but I wanted something that you could specifically use to describe who is being oppressed, and why.) They seemed to be having trouble understanding what I was saying, and it was hard to get an opinion out of them. They also got angry at me for dismissing Tumblr as a legitmate source of social justice. But eventually we got to the heart of the matter, and I discovered a basic disconnecf between us: they asked, \"Wait, you're seriously applying a <em>math</em>&nbsp;thing to social justice?\" And I pondered that for a moment and explained that it isn't restricted to math at all, and an axiom in this context can be any belief that you use to base your beliefs on. However, then the true problem came to light (after a comparison of me to misguided 18th-century philosophes): \"Sorry if it offends you, I just don't think in general that you should apply this stuff to society. Like... no.\"</p>\n<p>And that did it. For the rest of the day, I wreaked physical havoc, and emotionally alienated everyone I interacted with. I even seriously contemplated suicide. I wasn't angry at my friend in particular for having said that. For the first time, I was angry at an idea: that belief systems about certain things should <em>not</em>&nbsp;be internally consistent, should <em>not</em>&nbsp;follow logical rules. It was extremely difficult to construct an argument against, because all of my arguments had logically consistent bases, and were thus invalid in its face.</p>\n<p>I'm glad that I encountered that belief, though, like all beliefs, since I was able to solve it in the end, and make peace with it. I came to the following conclusions:</p>\n<p><ol>\n<li>In order to make a rationalist extremely aggravated, you can tell them that you don't think that belief structures should be internally logically consistent. (After 12-24 hours, they acquire lifetime immunity to this trick.)</li>\n<li>Belief structures do not necessarily have to be internally logically consistent. However, consistent systems are better, for the following reason: belief systems are used for deriving actions to take. Many actions that are oriented towards the same goal will make progress in accomplishing that goal. Making progress in accomplishing goals is a desirable thing. An inconsistent belief system will generate actions that are oriented towards non-constant goals, and interfere destructively with each other, and not make much progress. A consistent belief system will generate many actions oriented towards the same goal, and so will make much progress. Therefore, assuming the first few statements, having an internally consistent belief system is desirable! Having reduced it to an epistemological problem (do people <em>really</em>&nbsp;desire progress? can actions actually accomplish things?), I now only have epistemological anarchism to deal with, which seems to work less well in practice than the scientific method, so I can ignore it.</li>\n<li>No matter how offended you are about something, thinking about it will <em>still</em>&nbsp;resolve the issue.</li>\n</ol>\n<div>Does anyone have anything to add to this? Did I miss any sort of deeper reasons I could be using for this? Granted, my solution only works if you want to accomplish goals, and use your belief system to generate actions to accomplish goals, but I think that's fairly universal.</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EhyiWtMmWG6Eorh84", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 16, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "21501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-06T10:04:49.772Z", "modifiedAt": null, "url": null, "title": "[Link] Power of Suggestion", "slug": "link-power-of-suggestion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.653Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rdSH8h3xo7aQxuY5Y/link-power-of-suggestion", "pageUrlRelative": "/posts/rdSH8h3xo7aQxuY5Y/link-power-of-suggestion", "linkUrl": "https://www.lesswrong.com/posts/rdSH8h3xo7aQxuY5Y/link-power-of-suggestion", "postedAtFormatted": "Wednesday, February 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Power%20of%20Suggestion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Power%20of%20Suggestion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrdSH8h3xo7aQxuY5Y%2Flink-power-of-suggestion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Power%20of%20Suggestion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrdSH8h3xo7aQxuY5Y%2Flink-power-of-suggestion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrdSH8h3xo7aQxuY5Y%2Flink-power-of-suggestion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3922, "htmlBody": "<p><strong>Related: </strong><a href=\"/r/discussion/lw/gld/link_social_psychology_priming_art_wears_off/\">Social Psychology &amp; Priming: Art Wears Off </a></p>\n<p><strong> </strong></p>\n<p>I recommend reading the <a href=\"http://chronicle.com/article/Power-of-Suggestion/136907/\">piece</a>, but below are some excerpts and commentary.</p>\n<blockquote>\n<h2>Power of Suggestion</h2>\n<p>By Tom Bartlett</p>\n<p>...</p>\n<p>Along with personal upheaval, including a lengthy child-custody battle, [Yale social psychologist John Bargh] has coped with what amounts to an assault on his life's work, the research that pushed him into prominence, the studies that Malcolm Gladwell called \"fascinating\" and Daniel Kahneman deemed \"classic.\"</p>\n<p>What was once widely praised is now being pilloried in some quarters as emblematic of the shoddiness and shallowness of social psychology. When Bargh responded to one such salvo with a couple of sarcastic blog posts, he was ridiculed as going on a \"one-man rampage.\" He took the posts down and regrets writing them, but his frustration and sadness at how he's been treated remain.</p>\n<p>Psychology may be simultaneously at the highest and lowest point in its history. Right now its niftiest findings are routinely simplified and repackaged for a mass audience; if you wish to publish a best seller sans bloodsucking or light bondage, you would be well advised to match a few dozen psychological papers with relatable anecdotes and a grabby, one-word title. That isn't true across the board. ... But a social psychologist with a sexy theory has star potential. In the last decade or so, researchers have made astonishing discoveries about the role of consciousness, the reasons for human behavior, the motivations for why we do what we do. This stuff is anything but incremental.</p>\n<p>At the same time, psychology has been beset with scandal and doubt. Formerly high-flying researchers like Diederik Stapel, Marc Hauser, and Dirk Smeesters saw their careers implode after allegations that they had cooked their results and managed to slip them past the supposedly watchful eyes of peer reviewers.</p>\n<p>Psychology isn't the only field with fakers, but it has its share. Plus there's the so-called file-drawer problem, that is, the tendency for researchers to publish their singular successes and ignore their multiple failures, making a fluke look like a breakthrough. Fairly or not, social psychologists are perceived to be less rigorous in their methods, generally not replicating their own or one another's work, instead pressing on toward the next headline-making outcome.</p>\n<p>Much of the criticism has been directed at priming. The definitions get dicey here because the term can refer to a range of phenomena, some of which are grounded in decades of solid evidence&mdash;like the \"anchoring effect,\" which happens, for instance, when a store lists a competitor's inflated price next to its own to make you think you're getting a bargain. That works. The studies that raise eyebrows are mostly in an area known as behavioral or goal priming, research that demonstrates how subliminal prompts can make you do all manner of crazy things. A warm mug makes you friendlier. The American flag makes you vote Republican. Fast-food logos make you impatient.</p>\n<p>A small group of skeptical psychologists&mdash;let's call them the Replicators&mdash;have been trying to reproduce some of the most popular priming effects in their own labs.</p>\n<p>What have they found? Mostly that they can't get those results. The studies don't check out. Something is wrong. And because he is undoubtedly the biggest name in the field, the Replicators have paid special attention to John Bargh and the study that started it all.</p>\n<p>... When the walking times of the two groups were compared, the Florida-knits-alone subjects walked, on average, more slowly than the control group. Words on a page made them act old.</p>\n<p>It's a cute finding. But the more you think about it, the more serious it starts to seem. What if we are constantly being influenced by subtle, unnoticed cues? If \"Florida\" makes you sluggish, could \"cheetah\" make you fleet of foot? Forget walking speeds. Is our environment making us meaner or more creative or stupider without our realizing it? We like to think we're steering the ship of self, but what if we're actually getting blown about by ghostly gusts?</p>\n</blockquote>\n<p><a href=\"http://isteve.blogspot.com/2013/02/social-psychology-priming-art-wears-off.html\">Steve Sailer</a> comments on this:</p>\n<blockquote>\n<p>Advertisers, from John Wanamaker onward, sure as heck hope they are blowing you about by ghostly gusts.</p>\n</blockquote>\n<p>Not only advertisers the industry where he worked in but indeed our little community probably <em>loves</em> any results confirming such a picture. We need to be careful about that. Bartlett continues:</p>\n<blockquote>\n<p>John Bargh and his co-authors, Mark Chen and Lara Burrows, performed that experiment in 1990 or 1991. They didn't publish it until 1996. Why sit on such a fascinating result? For starters, they wanted to do it again, which they did. They also wanted to perform similar experiments with different cues. One of those other experiments tested subjects to see if they were more hostile when primed with an African-American face. They were. (The subjects were not African-American.) In the other experiment, the subjects were primed with rude words to see if that would make them more likely to interrupt a conversation. It did.</p>\n<p>The researchers waited to publish until other labs had found the same type of results. They knew their finding would be controversial. They knew many people wouldn't believe it. They were willing to stick their necks out, but they didn't want to be the only ones.</p>\n<p>Since that study was published in the <em>Journal of Personality and Social Psychology,</em> it has been cited more than 2,000 times. Though other researchers did similar work at around the same time, and even before, it was that paper that sparked the priming era. Its authors knew, even before it was published, that the paper was likely to catch fire. They wrote: \"The implications for many social psychological phenomena ... would appear to be considerable.\" Translation: This is a huge deal.</p>\n<p>...</p>\n<p>The last year has been tough for Bargh. Professionally, the nadir probably came in January, when a failed replication of the famous elderly-walking study was published in the journal <em>PLoS ONE. </em>It was not the first failed replication, but this one stung. In the experiment, the researchers had tried to mirror Bargh's methods with an important exception: Rather than stopwatches, they used automatic timing devices with infrared sensors to eliminate any potential bias. The words didn't make subjects act old. They tried the experiment again with stopwatches and added a twist: They told those operating the stopwatches which subjects were expected to walk slowly. Then it worked. The title of their paper tells the story: \"Behavioral Priming: It's All in the Mind, but Whose Mind?\"</p>\n<p>The paper annoyed Bargh. He thought the researchers didn't faithfully follow his methods section, despite their claims that they did. But what really set him off was <a href=\"http://blogs.discovermagazine.com/notrocketscience/2012/01/18/primed-by-expectations-why-a-classic-psychology-experiment-isnt-what-it-seemed/#.UQaYRfImZ8F\">a blog post</a> that explained the results. The post, on the blog Not Exactly Rocket Science, compared what happened in the experiment to the notorious case of Clever Hans, the horse that could supposedly count. It was thought that Hans was a whiz with figures, stomping a hoof in response to mathematical queries. In reality, the horse was picking up on body language from its handler. Bargh was the deluded horse handler in this scenario. That didn't sit well with him. If the <em>PLoS ONE </em>paper is correct, the significance of his experiment largely dissipates. What's more, he looks like a fool, tricked by a fairly obvious flaw in the setup.</p>\n<p>...</p>\n<p>Pashler, a professor of psychology at the University of California at San Diego, is the most prolific of the Replicators. He started trying priming experiments about four years ago because, he says, \"I wanted to see these effects for myself.\" That's a diplomatic way of saying he thought they were fishy. He's tried more than a dozen so far, including the elderly-walking study. <strong>He's never been able to achieve the same results. Not once.</strong></p>\n<p>This fall, Daniel Kahneman, the Nobel Prize-winning psychologist, sent an e-mail to a small group of psychologists, including Bargh, warning of a \"train wreck looming\" in the field because of doubts surrounding priming research. He was blunt: \"I believe that you should collectively do something about this mess. To deal effectively with the doubts you should acknowledge their existence and confront them straight on, because a posture of defiant denial is self-defeating,\" he wrote.</p>\n<p>Strongly worded e-mails from Nobel laureates tend to get noticed, and this one did. He sent it after conversations with Bargh about the relentless attacks on priming research. Kahneman cast himself as a mediator, a sort of senior statesman, endeavoring to bring together believers and skeptics. He does have a dog in the fight, though: Kahneman believes in these effects and has written admiringly of Bargh, including in his best seller <em>Thinking, Fast and Slow.</em></p>\n<p>On the heels of that message from on high, an e-mail dialogue began between the two camps. The vibe was more conciliatory than what you hear when researchers are speaking off the cuff and off the record. There was talk of the type of collaboration that Kahneman had floated, researchers from opposing sides combining their efforts in the name of truth. It was very civil, and it didn't lead anywhere.</p>\n<p>In one of those e-mails, Pashler issued a challenge masquerading as a gentle query: <strong>\"Would you be able to suggest one or two goal priming effects that you think are especially strong and robust, even if they are not particularly well-known?\" In other words, put up or shut up. Point me to the stuff you're certain of and I'll try to replicate it.</strong> This was intended to counter the charge that he and others were cherry-picking the weakest work and then doing a victory dance after demolishing it. He didn't get the straightforward answer he wanted. \"Some suggestions emerged but none were pointing to a concrete example,\" he says.</p>\n<p>One possible explanation for why these studies continually and bewilderingly fail to replicate is that they have hidden moderators, sensitive conditions that make them a challenge to pull off. Pashler argues that the studies never suggest that. He wrote in that same e-mail: \"So from our reading of the literature, it is not clear why the results should be subtle or fragile.\"</p>\n<p>Bargh contends that we know more about these effects than we did in the 1990s, that they're more complicated than researchers had originally assumed. That's not a problem, it's progress. And if you aren't familiar with the literature in social psychology, with the numerous experiments that have modified and sharpened those early conclusions, you're unlikely to successfully replicate them. Then you will trot out your failure as evidence that the study is bogus when really what you've proved is that you're no good at social psychology.</p>\n<p>Pashler can't quite disguise his disdain for such a defense. \"That doesn't make sense to me,\" he says. <strong>\"You published it. That must mean you think it is a repeatable piece of work. Why can't we do it just the way you did it?\"</strong></p>\n<p>That's how David Shanks sees things. He, too, has been trying to replicate well-known priming studies, and he, too, has been unable to do so. In a forthcoming paper, Shanks, a professor of psychology at University College London, recounts his and his several co-authors' attempts to replicate one of the most intriguing effects, the so-called professor prime. In the study, one group was told to imagine a professor's life and then list the traits that brought to mind. Another group was told to do the same except with a soccer hooligan rather than a professor.</p>\n<p>The groups were then asked questions selected from the board game Trivial Pursuit, questions like \"Who painted 'Guernica'?\" and \"What is the capital of Bangladesh?\" (Picasso and Dhaka, for those playing at home.) Their scores were then tallied. The subjects who imagined the professor scored above a control group that wasn't primed. The subjects who imagined soccer hooligans scored below the professor group and below the control. Thinking about a professor makes you smart while thinking about a hooligan makes you dumb. The study has been replicated a number of times, including once on Dutch television.</p>\n<p>Shanks can't get the result. And, boy, has he tried. Not once or twice, but nine times.</p>\n<p>The skepticism about priming, says Shanks, isn't limited to those who have committed themselves to reperforming these experiments. It's not only the Replicators. \"I think more people in academic psychology than you would imagine appreciate the historical implausibility of these findings, and it's just that those are the opinions that they have over the water fountain,\" he says. \"They're not the opinions that get into the journalism.\"</p>\n<p>Like all the skeptics I spoke with, Shanks believes the worst is yet to come for priming, predicting that \"over the next two or three years you're going to see an avalanche of failed replications published.\" The avalanche may come sooner than that. There are failed replications in press at the moment and many more that have been completed (Shanks's paper on the professor prime is in press at <em>PLoS ONE</em>). A couple of researchers I spoke with didn't want to talk about their results until they had been peer reviewed, but their preliminary results are not encouraging.</p>\n<p>Ap Dijksterhuis is the author of the professor-prime paper. At first, Dijksterhuis, a professor of psychology at Radboud University Nij&shy;megen, in the Netherlands, wasn't sure he wanted to be interviewed for this article. That study is ancient news&mdash;it was published in 1998, and he's moved away from studying unconscious processes in the last couple of years, in part because he wanted to move on to new research on happiness and in part because of the rancor and suspicion that now accompany such work. He's tired of it.</p>\n<p>The outing of Diederik Stapel made the atmosphere worse. <strong>Stapel was a social psychologist at Tilburg University, also in the Netherlands, who was found to have committed scientific misconduct in scores of papers.</strong> The scope and the depth of the fraud were jaw-dropping, and it changed the conversation. \"It wasn't about research practices that could have been better. It was about fraud,\" Dijksterhuis says of the Stapel scandal. \"I think that's playing in the background. It now almost feels as if people who do find significant data are making mistakes, are doing bad research, and maybe even doing fraudulent things.\"</p>\n</blockquote>\n<p>Here is a <a href=\"http://en.wikipedia.org/wiki/Diederik_Stapel#Scientific_misconduct\">link</a> to the wiki article on the mentioned misconduct. I recall some of the drama that unfolded around the outing and the papers themselves... looking at the <em>kinds</em> of results Stapel wanted to fake or thought would <a href=\"/lw/e1b/link_admitting_to_bias/\">advance his career</a> reminds me of some other older examples of <a href=\"/lw/65b/scientific_misconduct_misdiagnosed_because_of/\">scientific misconduct</a>.</p>\n<blockquote>\n<p>In the e-mail discussion spurred by Kahneman's call to action, Dijk&shy;sterhuis laid out a number of possible explanations for why skeptics were coming up empty when they attempted priming studies. Cultural differences, for example. Studying prejudice in the Netherlands is different from studying it in the United States. Certain subjects are not susceptible to certain primes, particularly a subject who is unusually self-aware. In an interview, he offered another, less charitable possibility. \"It could be that they are bad experimenters,\" he says. \"They may turn out failures to replicate that have been shown by 15 or 20 people already. It basically shows that it's something with them, and it's something going on in their labs.\"</p>\n<p><span class=\"dropcap\">J</span>oseph Cesario is somewhere between a believer and a skeptic, though these days he's leaning more skeptic. Cesario is a social psychologist at Michigan State University, and he's successfully replicated Bargh's elderly-walking study, discovering in the course of the experiment that the attitude of a subject toward the elderly determined whether the effect worked or not. If you hate old people, you won't slow down. He is sympathetic to the argument that moderators exist that make these studies hard to replicate, lots of little monkey wrenches ready to ruin the works. But that argument only goes so far. \"At some point, it becomes excuse-making,\" he says. \"We have to have some threshold where we say that it doesn't exist. It can't be the case that some small group of people keep hitting on the right moderators over and over again.\"</p>\n<p>Cesario has been trying to replicate a recent finding of Bargh's. In that study, published last year in the journal <em>Emotion</em>, Bargh and his co-author, Idit Shalev, asked subjects about their personal hygiene habits&mdash;how often they showered and bathed, for how long, how warm they liked the water. They also had subjects take a standard test to determine their degree of social isolation, whether they were lonely or not. What they found is that lonely people took longer and warmer baths and showers, perhaps substituting the warmth of the water for the warmth of regular human interaction.</p>\n<p>That isn't priming, exactly, though it is a related unconscious phenomenon often called <strong>embodied cognition</strong>. As in the elderly-walking study, the subjects didn't realize what they were doing, didn't know they were bathing longer because they were lonely. Can warm water alleviate feelings of isolation? This was a result with real-world applications, and reporters jumped on it. \"Wash the loneliness away with a long, hot bath,\" read an NBC News headline.</p>\n</blockquote>\n<p>But I like the <a href=\"http://theviewfromhell.blogspot.com/2012/09/trying-to-see-through-unified-theory-of.html\">feeling of insight</a> I get when thinking about cool applications of <a href=\"http://en.wikipedia.org/wiki/Embodied_cognition\">embodied cognition</a>! (;_:)</p>\n<blockquote>\n<p>Bargh's study had 92 subjects. So far Cesario has run more than 2,500 through the same experiment. He's found absolutely no relationship between bathing and loneliness. Zero. \"It's very worrisome if you have people thinking they can take a shower and they can cure their depression,\" he says. And he says Bargh's data are troublesome. <strong>\"Extremely small samples, extremely large effects&mdash;that's a red flag,\" he says. \"It's not a red flag for people publishing those studies, but it should be.\"</strong></p>\n<p>Even though he is, in a sense, taking aim at Bargh, Cesario thinks it's a shame that the debate over priming has become so personal, as if it's a referendum on one man. \"He has the most eye-catching findings. He always has,\" Cesario says. <strong>\"To the extent that some of his effects don't replicate, because he's identified as priming, it casts doubt on the entire body of research. He is priming.\"</strong></p>\n</blockquote>\n<p>I'll admit that took me a few seconds too long to parse. (~_^)</p>\n<blockquote>\n<p><span class=\"dropcap\">T</span>hat has been the narrative. Bargh's research is crumbling under scrutiny and, along with it, perhaps priming as a whole. Maybe the most exciting aspect of social psychology over the last couple of decades, these almost magical experiments in which people are prompted to be smarter or slower without them even knowing it, will end up as an embarrassing footnote rather than a landmark achievement.</p>\n<p><span class=\"dropcap\">T</span>hat has been the narrative. Bargh's research is crumbling under scrutiny and, along with it, perhaps priming as a whole. Maybe the most exciting aspect of social psychology over the last couple of decades, these almost magical experiments in which people are prompted to be smarter or slower without them even knowing it, will end up as an embarrassing footnote rather than a landmark achievement.</p>\n</blockquote>\n<p>Well yes dear journalist that has been the narrative you've just presented to us readers.</p>\n<blockquote>\n<p>Then along comes Gary Latham.</p>\n</blockquote>\n<p>How entertaining a plot twist! Or maybe a journalist is writing a <a href=\"/lw/8w1/transcript_tyler_cowen_on_stories/\">story</a> about out of a confusing process where academia tries to take account of a confusing array of new evidence.&nbsp; Of course that's me telling a story right there. Agggh bad brain bad!</p>\n<blockquote>\n<p>Latham, an organizational psychologist in the management school at the University of Toronto, thought the research Bargh and others did was crap. That's the word he used. He told one of his graduate students, Amanda Shantz, that if she tried to apply Bargh's principles it would be a win-win. If it failed, they could publish a useful takedown. If it succeeded ... well, that would be interesting.</p>\n<p>They performed a pilot study, which involved showing subjects a photo of a woman winning a race before the subjects took part in a brainstorming task. As Bargh's research would predict, the photo made them perform better at the brainstorming task. Or seemed to. Latham performed the experiment again in cooperation with another lab. This time the study involved employees in a university fund-raising call center. They were divided into three groups. Each group was given a fact sheet that would be visible while they made phone calls. In the upper left-hand corner of the fact sheet was either a photo of a woman winning a race, a generic photo of employees at a call center, or no photo. Again, consistent with Bargh, the subjects who were primed raised more money. Those with the photo of call-center employees raised the most, while those with the race-winner photo came in second, both outpacing the photo-less control. This was true even though, when questioned afterward, the subjects said they had been too busy to notice the photos.</p>\n<p>Latham didn't want Bargh to be right. \"I couldn't have been more skeptical or more disbelieving when I started the research,\" he says. \"I nearly fell off my chair when my data\" supported Bargh's findings.</p>\n<p>That experiment has changed Latham's opinion of priming and has him wondering now about the applications for unconscious primes in our daily lives. Are there photos that would make people be safer at work? Are there photos that undermine performance? How should we be fine-tuning the images that surround us? \"It's almost scary in lots of ways that these primes in these environments can affect us without us being aware,\" he says. Latham hasn't stopped there. He's continued to try experiments using Bargh's ideas, and those results have only strengthened his confidence in priming. \"I've got two more that are just mind-blowing,\" he says. \"And I know John Bargh doesn't know about them, but he'll be a happy guy when he sees them.\"</p>\n<p>Latham doesn't know why others have had trouble. He only knows what he's found, and he's certain about his own data. In the end, Latham thinks Bargh will be vindicated as a pioneer in understanding unconscious motivations. \"I'm like a converted Christian,\" he says. \"I started out as a devout atheist, and now I'm a believer.\"</p>\n<p>Following his come-to-Jesus transformation, Latham sent an e-mail to Bargh to let him know about the call-center experiment. When I brought this up with Bargh, his face brightened slightly for the first time in our conversation. <strong>\"You can imagine how that helped me,\" he says. He had been feeling isolated, under siege, worried that his legacy was becoming a cautionary tale. \"You feel like you're on an island,\" he says.</strong></p>\n<p>Though Latham is now a believer, he remains the exception. With more failed replications in the pipeline, Dijksterhuis believes that Kahneman's looming-train-wreck letter, though well meaning, may become a self-fulfilling prophecy, helping to sink the field rather than save it. Perhaps the perception has already become so negative that further replications, regardless of what they find, won't matter much. <strong>For his part, Bargh is trying to take the long view. \"We have to think about 50 or 100 years from now&mdash;are people going to believe the same theories?\" he says. \"Maybe it's not true. Let's see if it is or isn't.\"</strong></p>\n</blockquote>\n<p>Admirable that he's come to the latter attitude after the early angry blog posts prompted by what he was going through. That wasn't sarcasm, scientists are only human after all, there are easier things to do than this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb128": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rdSH8h3xo7aQxuY5Y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 38, "extendedScore": null, "score": 9.9e-05, "legacy": true, "legacyId": "21504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gTNFPiRzKmFFTiW8v", "H7iNv58YKmzjoAd68", "iMaL9hXWPGxtAxBSm", "4kphivjxngJmEdWsN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-06T10:08:31.413Z", "modifiedAt": null, "url": null, "title": "[Link] Social Psychology & Priming: Art Wears Off", "slug": "link-social-psychology-and-priming-art-wears-off", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:39.020Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gTNFPiRzKmFFTiW8v/link-social-psychology-and-priming-art-wears-off", "pageUrlRelative": "/posts/gTNFPiRzKmFFTiW8v/link-social-psychology-and-priming-art-wears-off", "linkUrl": "https://www.lesswrong.com/posts/gTNFPiRzKmFFTiW8v/link-social-psychology-and-priming-art-wears-off", "postedAtFormatted": "Wednesday, February 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Social%20Psychology%20%26%20Priming%3A%20Art%20Wears%20Off&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Social%20Psychology%20%26%20Priming%3A%20Art%20Wears%20Off%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTNFPiRzKmFFTiW8v%2Flink-social-psychology-and-priming-art-wears-off%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Social%20Psychology%20%26%20Priming%3A%20Art%20Wears%20Off%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTNFPiRzKmFFTiW8v%2Flink-social-psychology-and-priming-art-wears-off", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTNFPiRzKmFFTiW8v%2Flink-social-psychology-and-priming-art-wears-off", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1154, "htmlBody": "<p class=\"post-title entry-title\"><strong>Related to: </strong><a href=\"/r/discussion/lw/glc/link_power_of_suggestion/\">Power of Suggestion </a></p>\n<blockquote>\n<h3 class=\"post-title entry-title\"><a href=\"http://isteve.blogspot.com/2013/02/social-psychology-priming-art-wears-off.html\">Social Psychology &amp; Priming: Art Wears Off</a></h3>\n<p class=\"post-title entry-title\">by Steve Sailer</p>\n<p>One of the most popular social psychology studies of the Malcolm Gladwell Era has been Yale professor John Bargh's paper on how you can \"<a href=\"http://en.wikipedia.org/wiki/Priming_%28psychology%29\">prime</a>\" students to walk more slowly by first having them do word puzzles that contain a hidden theme of old age by the inclusion of words like \"wrinkle\" and \"bingo.\" The primed subjects then took one second longer on average to walk down the hall than the unprimed control group. Isn't that amazing! (Here's Gladwell's <a href=\"http://books.google.com/books?id=VKGbb1hg8JAC&amp;pg=PT35&amp;dq=gladwell+blink+corridor+temperature&amp;hl=en&amp;sa=X&amp;ei=EnIRUaHXMMOaiQKvgYHgCQ&amp;ved=0CDgQ6AEwAA\">description</a> of Bargh's famous experiment in his 2005 bestseller <em>Blink</em>.)</p>\n<p>This finding has electrified the Airport Book industry for years: Science <em>proves</em> you can manipulate people into doing what you want them to! Why you'd want college students to walk slower is unexplained, but that's not the point. The point is that Science proves that people are manipulable.</p>\n<p>Now, a large fraction of the buyers of Airport Books like <em>Blink</em> are marketing and advertising professionals, who are paid handsomely to manipulate people, and to manipulate them into not just walking slower, but into shelling out real money to buy the clients' products.</p>\n<p>Moreover, everybody notices that entertainment can prime you in various ways. For instance, well-made movies prime how I walk down the street afterwards. For two nights after seeing the Coen Brothers' <em>No Country for Old Men</em>, I walked the quiet streets swiveling my head, half-certain that an unstoppable killing machine was tailing me. When I came out of Christopher Nolan's amnesia thriller <em>Memento</em>, I was convinced I'd never remember where I parked my car. (As it turned out, I quickly found my car. Why? Because I needed to. But it was fun for thirty seconds to act like, and maybe even believe, that the movie had primed me into amnesia.)</p>\n<p>Now, you could say, \"That's art, not marketing,\" but the distinction isn't that obvious to talented directors. Not surprisingly, directors between feature projects often tide themselves over directing commercials. For example, Ridley Scott made <em>Blade Runner</em> in 1982 and then the landmark <a href=\"http://www.youtube.com/watch?v=HhsWzJo2sN4\"><em>1984</em> ad</a> introducing the Apple Mac at the 1984 Super Bowl.</p>\n<p>So, in an industry in which it's possible, if you have a big enough budget, to hire Sir Ridley to direct your next TV commercial, why the fascination with Bargh's dopey little experiment?</p>\n<p>One reason is that there's a lot of uncertainty in the marketing and advertising game. Nineteenth Century department store mogul John Wanamaker famously said that half his advertising budget was wasted, he just didn't know which half.</p>\n<p>Worse, things change. A TV commercial that excited viewers a few years ago often strikes them as dull and unfashionable today. Today, Scott's&nbsp;<em>1984</em> ad might remind people subliminally, from picking up on certain stylistic commonalities, of how dopey Scott's&nbsp;<em><a href=\"http://isteve.blogspot.com/2012/06/prometheus.html\">Prometheus</a></em> was last summer, or how lame the Wachowski Siblings <em>1984</em>-imitation <em>V for Vendetta</em>&nbsp;was,&nbsp;and Apple doesn't need their computers associated with <em>that </em>stuff.</p>\n<p>Naturally, social psychologists want to get in on a little of the big money action of marketing. Gladwell makes a bundle speaking to sales conventions, and maybe they can get some gigs themselves. And even if their motivations are wholly academic, it's nice to have your brother-in-law, the one who makes so much more money than you do doing something boring in the corporate world, excitedly forward you an article he read that mentions your work.</p>\n<p>(\"Priming\" theory is also the basis for the beloved concept of \"<a href=\"http://isteve.blogspot.com/2012/10/john-list-on-virtual-nonexistence-of.html\">stereotype threat</a>,\" which seems to offer a simple way to close those pesky Gaps that beset society: just get everybody to stop noticing stereotypes, and the Gaps will go away!)</p>\n<p>But why do the marketers love hearing about these weak tea little academic experiments, even though they do much more powerful priming on the job? I suspect one reason is because these studies are classified as Science, and Science is permanent. As some egghead in Europe pointed out, Science is Replicable. Once the principles of Scientific Manipulation are uncovered, then they can just do their marketing jobs on autopilot. No more need to worry about trends and fads.</p>\n<p>But, how replicable are these priming experiments?</p>\n</blockquote>\n<p>He then comments on and extensively quotes the Higher Education piece <a href=\"http://chronicle.com/article/Power-of-Suggestion/136907/\">Power of Suggestion</a> by Tom Bartlett, which I linked to at the start of my post. I'm skipping that to jump to the novel part part of Steve's post.</p>\n<blockquote>\n<p>Okay, but I've never seen <em>this</em> explanation offered: successful  priming studies stop replicating after awhile because they basically  aren't science. At least not in the sense of having discovered something  that will work forever.<br /> <br /> Instead, to the extent that they ever did really work, they are exercises in marketing. Or, to be generous, art.<br /> <br /> And, art wears off.<br /> <br /> The power of a work of art to prime emotions and actions changes over  time. Perhaps, initially, the audience isn't ready for it, then it  begins to impact a few sensitive fellow artists, and they begin to  create other works in its manner and talk it up, and then it become  widely popular. Over time, though, boredom sets in and people look for  new priming stimuli.<br /> <br /> For a lucky few old art works (e.g., the great Impressionist paintings),  vast networks exist to market them by helping audiences get back into  the proper mindset to appreciate the old art (E.g., \"Monet was a rebel,  up against The Establishment! So, putting this pretty picture of flowers  up on your wall shows everybody that <em>you</em> are an edgy outsider, too!\").<br /> <br /> So, let's assume for a moment that Bargh's success in the early 1990s at  getting college students to walk slow wasn't just fraud or data mining  for a random effect among many effects. He really was priming early  1990s college students into walking slow for a few seconds.<br /> <br /> Is that so amazing?<br /> <br /> Other artists and marketers in the early 1990s were priming sizable  numbers of college students into wearing flannel lumberjack shirts or  dancing the Macarena or voting for Ross Perot, all of which seem, from  the perspective of 2013, a lot more amazing.<br /> <br /> Overall, it's really not that hard to prime young people to do things.  They are always looking around for clues about what's cool to do.<br /> <br /> But it's hard to keep them doing the same thing over and over. The  Macarena isn't cool anymore, so it would be harder to replicate today an  event in which young people are successfully primed to do the Macarena.<br /> <br /> So, in the <em>best case scenario</em>, priming isn't science, it's art or marketing.</p>\n</blockquote>\n<p><br />Interesting hypothesis.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb128": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gTNFPiRzKmFFTiW8v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 0, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "21505", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rdSH8h3xo7aQxuY5Y"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-06T13:58:35.082Z", "modifiedAt": null, "url": null, "title": "Interesting discussion of concentration and productivity [link]", "slug": "interesting-discussion-of-concentration-and-productivity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:35.783Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6RFqSbKmDr9jeT8Wb/interesting-discussion-of-concentration-and-productivity", "pageUrlRelative": "/posts/6RFqSbKmDr9jeT8Wb/interesting-discussion-of-concentration-and-productivity", "linkUrl": "https://www.lesswrong.com/posts/6RFqSbKmDr9jeT8Wb/interesting-discussion-of-concentration-and-productivity", "postedAtFormatted": "Wednesday, February 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interesting%20discussion%20of%20concentration%20and%20productivity%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInteresting%20discussion%20of%20concentration%20and%20productivity%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6RFqSbKmDr9jeT8Wb%2Finteresting-discussion-of-concentration-and-productivity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interesting%20discussion%20of%20concentration%20and%20productivity%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6RFqSbKmDr9jeT8Wb%2Finteresting-discussion-of-concentration-and-productivity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6RFqSbKmDr9jeT8Wb%2Finteresting-discussion-of-concentration-and-productivity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>http://www.johndcook.com/blog/2013/02/04/four-hours-of-concentration/</p>\n<p>And since this is the Internet, and facts are involved, our gwern turns up there also.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6RFqSbKmDr9jeT8Wb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 1.1049638154759145e-06, "legacy": true, "legacyId": "21506", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-06T17:09:56.858Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: Rationality in our daily life", "slug": "meetup-moscow-rationality-in-our-daily-life", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sDN9RozKWDJBrzFn6/meetup-moscow-rationality-in-our-daily-life", "pageUrlRelative": "/posts/sDN9RozKWDJBrzFn6/meetup-moscow-rationality-in-our-daily-life", "linkUrl": "https://www.lesswrong.com/posts/sDN9RozKWDJBrzFn6/meetup-moscow-rationality-in-our-daily-life", "postedAtFormatted": "Wednesday, February 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20Rationality%20in%20our%20daily%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20Rationality%20in%20our%20daily%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDN9RozKWDJBrzFn6%2Fmeetup-moscow-rationality-in-our-daily-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20Rationality%20in%20our%20daily%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDN9RozKWDJBrzFn6%2Fmeetup-moscow-rationality-in-our-daily-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDN9RozKWDJBrzFn6%2Fmeetup-moscow-rationality-in-our-daily-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/iv'>Moscow: Rationality in our daily life</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 February 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. I will be there at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Challenge each other's beliefs, improved version of the new exercise.</p></li>\n<li><p>Rationality in examples from motion pictures and books.</p></li>\n<li><p>Value of information in our life, how can we measure it.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/iv'>Moscow: Rationality in our daily life</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sDN9RozKWDJBrzFn6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.1050837652882327e-06, "legacy": true, "legacyId": "21508", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Rationality_in_our_daily_life\">Discussion article for the meetup : <a href=\"/meetups/iv\">Moscow: Rationality in our daily life</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 February 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. I will be there at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Challenge each other's beliefs, improved version of the new exercise.</p></li>\n<li><p>Rationality in examples from motion pictures and books.</p></li>\n<li><p>Value of information in our life, how can we measure it.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Rationality_in_our_daily_life1\">Discussion article for the meetup : <a href=\"/meetups/iv\">Moscow: Rationality in our daily life</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: Rationality in our daily life", "anchor": "Discussion_article_for_the_meetup___Moscow__Rationality_in_our_daily_life", "level": 1}, {"title": "Discussion article for the meetup : Moscow: Rationality in our daily life", "anchor": "Discussion_article_for_the_meetup___Moscow__Rationality_in_our_daily_life1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-06T21:28:10.936Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-01]", "slug": "meetup-cambridge-uk-lw-meetup-reading-group-haefb-01", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sohum", "createdAt": "2012-02-06T06:28:59.691Z", "isAdmin": false, "displayName": "Sohum"}, "userId": "DP4jNdSvbeAofhxkb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9RT4L8JG2uc7jjtyk/meetup-cambridge-uk-lw-meetup-reading-group-haefb-01", "pageUrlRelative": "/posts/9RT4L8JG2uc7jjtyk/meetup-cambridge-uk-lw-meetup-reading-group-haefb-01", "linkUrl": "https://www.lesswrong.com/posts/9RT4L8JG2uc7jjtyk/meetup-cambridge-uk-lw-meetup-reading-group-haefb-01", "postedAtFormatted": "Wednesday, February 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-01%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-01%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RT4L8JG2uc7jjtyk%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-01%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-01%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RT4L8JG2uc7jjtyk%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-01", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RT4L8JG2uc7jjtyk%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-01", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/iw'>Cambridge, UK LW Meetup [Reading Group, HAEFB-01]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 February 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Trinity JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup! This week, we'll be doing a reading group session of the Shiny New Sequence, Highly Advanced Epistemology 101 For Beginners. These are explicitly designed as introductory posts, so dive right in with us if you're new!</p>\n\n<p>We'll be covering The Useful Idea of Truth and Appreciating Cognitive Algorithms, this week. If you're feeling explicitly keen, read ahead to the two minor posts; I don't think we'll explicitly have a session talking about those, but they're good to read anyway.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/iw'>Cambridge, UK LW Meetup [Reading Group, HAEFB-01]</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9RT4L8JG2uc7jjtyk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.105245668094599e-06, "legacy": true, "legacyId": "21509", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_01_\">Discussion article for the meetup : <a href=\"/meetups/iw\">Cambridge, UK LW Meetup [Reading Group, HAEFB-01]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 February 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Trinity JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup! This week, we'll be doing a reading group session of the Shiny New Sequence, Highly Advanced Epistemology 101 For Beginners. These are explicitly designed as introductory posts, so dive right in with us if you're new!</p>\n\n<p>We'll be covering The Useful Idea of Truth and Appreciating Cognitive Algorithms, this week. If you're feeling explicitly keen, read ahead to the two minor posts; I don't think we'll explicitly have a session talking about those, but they're good to read anyway.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_01_1\">Discussion article for the meetup : <a href=\"/meetups/iw\">Cambridge, UK LW Meetup [Reading Group, HAEFB-01]</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-01]", "anchor": "Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_01_", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-01]", "anchor": "Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_01_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-06T21:33:26.934Z", "modifiedAt": null, "url": null, "title": "Politics Discussion Thread February 2013", "slug": "politics-discussion-thread-february-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:08.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/drBJJNvFmiANXakHn/politics-discussion-thread-february-2013", "pageUrlRelative": "/posts/drBJJNvFmiANXakHn/politics-discussion-thread-february-2013", "linkUrl": "https://www.lesswrong.com/posts/drBJJNvFmiANXakHn/politics-discussion-thread-february-2013", "postedAtFormatted": "Wednesday, February 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Politics%20Discussion%20Thread%20February%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolitics%20Discussion%20Thread%20February%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdrBJJNvFmiANXakHn%2Fpolitics-discussion-thread-february-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Politics%20Discussion%20Thread%20February%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdrBJJNvFmiANXakHn%2Fpolitics-discussion-thread-february-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdrBJJNvFmiANXakHn%2Fpolitics-discussion-thread-february-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>&nbsp;</p>\n<ol>\n<li><em>Top-level comments should introduce arguments; responses should be responses to those arguments.&nbsp;</em></li>\n<li><em>Upvote and downvote based on whether or not you find an argument convincing in the context in which it was raised. &nbsp;This means if it's a good argument against the argument it is responding to, not whether or not there's a good/obvious counterargument to it; if you have a good counterargument, raise it. &nbsp;If it's a convincing argument, and the counterargument is also convincing, upvote both. &nbsp;If both arguments are unconvincing, downvote both.&nbsp;</em></li>\n<li><em>A single argument per comment would be ideal; as MixedNuts points out&nbsp;<a href=\"/r/discussion/lw/dsv/is_politics_the_mindkiller_an_inconclusive_test/73yp\">here</a>, it's otherwise hard to distinguish between one good and one bad argument, which makes the upvoting/downvoting difficult to evaluate.</em></li>\n<li><em>In general try to avoid color politics; try to discuss political issues, rather than political parties, wherever possible.</em></li>\n</ol>\n<p>As Multiheaded added, \"Personal is Political\" stuff like gender relations, etc also may belong here.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "drBJJNvFmiANXakHn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 1, "extendedScore": null, "score": 1.1052489704792214e-06, "legacy": true, "legacyId": "21510", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 149, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-06T23:50:30.835Z", "modifiedAt": null, "url": null, "title": "Meetup :  Bielefeld Meetup, February 20th", "slug": "meetup-bielefeld-meetup-february-20th", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/stXikhoPa9F4Y4SLN/meetup-bielefeld-meetup-february-20th", "pageUrlRelative": "/posts/stXikhoPa9F4Y4SLN/meetup-bielefeld-meetup-february-20th", "linkUrl": "https://www.lesswrong.com/posts/stXikhoPa9F4Y4SLN/meetup-bielefeld-meetup-february-20th", "postedAtFormatted": "Wednesday, February 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%20Bielefeld%20Meetup%2C%20February%2020th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%20Bielefeld%20Meetup%2C%20February%2020th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FstXikhoPa9F4Y4SLN%2Fmeetup-bielefeld-meetup-february-20th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%20Bielefeld%20Meetup%2C%20February%2020th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FstXikhoPa9F4Y4SLN%2Fmeetup-bielefeld-meetup-february-20th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FstXikhoPa9F4Y4SLN%2Fmeetup-bielefeld-meetup-february-20th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ix'> Bielefeld Meetup, February 20th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 February 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Grill/Bar Verve, Klosterplatz 13, Bielefeld</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting once again in Bielefeld.</p>\n\n<p>The topics of this evening are not yet determined, but will be in the next two weeks, or develop during the meetup.\nHighly interesting talk can be expected.</p>\n\n<p>If you live in the area consider dropping by :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ix'> Bielefeld Meetup, February 20th</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "stXikhoPa9F4Y4SLN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.1053349222143623e-06, "legacy": true, "legacyId": "21512", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Bielefeld_Meetup__February_20th\">Discussion article for the meetup : <a href=\"/meetups/ix\"> Bielefeld Meetup, February 20th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 February 2013 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Grill/Bar Verve, Klosterplatz 13, Bielefeld</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting once again in Bielefeld.</p>\n\n<p>The topics of this evening are not yet determined, but will be in the next two weeks, or develop during the meetup.\nHighly interesting talk can be expected.</p>\n\n<p>If you live in the area consider dropping by :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Bielefeld_Meetup__February_20th1\">Discussion article for the meetup : <a href=\"/meetups/ix\"> Bielefeld Meetup, February 20th</a></h2>", "sections": [{"title": "Discussion article for the meetup :  Bielefeld Meetup, February 20th", "anchor": "Discussion_article_for_the_meetup____Bielefeld_Meetup__February_20th", "level": 1}, {"title": "Discussion article for the meetup :  Bielefeld Meetup, February 20th", "anchor": "Discussion_article_for_the_meetup____Bielefeld_Meetup__February_20th1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-07T05:19:58.102Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Interpersonal Entanglement", "slug": "seq-rerun-interpersonal-entanglement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.573Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/89HPJDJQKCxy7Bz4u/seq-rerun-interpersonal-entanglement", "pageUrlRelative": "/posts/89HPJDJQKCxy7Bz4u/seq-rerun-interpersonal-entanglement", "linkUrl": "https://www.lesswrong.com/posts/89HPJDJQKCxy7Bz4u/seq-rerun-interpersonal-entanglement", "postedAtFormatted": "Thursday, February 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Interpersonal%20Entanglement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Interpersonal%20Entanglement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89HPJDJQKCxy7Bz4u%2Fseq-rerun-interpersonal-entanglement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Interpersonal%20Entanglement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89HPJDJQKCxy7Bz4u%2Fseq-rerun-interpersonal-entanglement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89HPJDJQKCxy7Bz4u%2Fseq-rerun-interpersonal-entanglement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 282, "htmlBody": "<p>Today's post, <a href=\"/lw/xt/interpersonal_entanglement/\">20 January 2009</a> was originally published on 20 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Our sympathy with other minds makes our interpersonal relationships one of the most complex aspects of human existence. Romance, in particular, is more complicated than being nice to friends and kin, negotiating with allies, or outsmarting enemies - it contains aspects of all three. Replacing human romance with anything simpler or easier would decrease the peak complexity of the human species - a major step in the wrong direction, it seems to me. This is my problem with proposals to give people perfect, nonsentient sexual/romantic partners, which I usually refer to as \"catgirls\" (\"catboys\"). The human species does have a statistical sex problem: evolution has not optimized the average man to make the average woman happy or vice versa. But there are less sad ways to solve this problem than both genders giving up on each other and retreating to catgirls/catboys.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gl8/seq_rerun_sympathetic_minds/\">Sympathetic Minds</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "89HPJDJQKCxy7Bz4u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.1055415674160234e-06, "legacy": true, "legacyId": "21514", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Py3uGnncqXuEfPtQp", "7eJ9Q6YxyB7LbiWSY", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-07T09:09:26.992Z", "modifiedAt": null, "url": null, "title": "Having Trouble Posting", "slug": "having-trouble-posting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:35.787Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "christina", "createdAt": "2011-07-09T06:48:34.638Z", "isAdmin": false, "displayName": "christina"}, "userId": "mWYhP5BngbQYEf7FF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/csZiEK9FZC3pEmk3m/having-trouble-posting", "pageUrlRelative": "/posts/csZiEK9FZC3pEmk3m/having-trouble-posting", "linkUrl": "https://www.lesswrong.com/posts/csZiEK9FZC3pEmk3m/having-trouble-posting", "postedAtFormatted": "Thursday, February 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Having%20Trouble%20Posting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHaving%20Trouble%20Posting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcsZiEK9FZC3pEmk3m%2Fhaving-trouble-posting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Having%20Trouble%20Posting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcsZiEK9FZC3pEmk3m%2Fhaving-trouble-posting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcsZiEK9FZC3pEmk3m%2Fhaving-trouble-posting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p>I am having some trouble seeing my responses to a post after refreshing an article (but they still show up in the right-hand sidebar).&nbsp; Anyone else having this problem?&nbsp; I am using Firefox 16.0.2.&nbsp; Is this a bug or a \"feature\" (I think some of us are all too aware of the sometimes blurry distinction...)?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "csZiEK9FZC3pEmk3m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 0, "extendedScore": null, "score": 1.1056855463637343e-06, "legacy": true, "legacyId": "21524", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-07T15:07:23.225Z", "modifiedAt": null, "url": null, "title": "A Little Puzzle about Termination", "slug": "a-little-puzzle-about-termination", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:35.198Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "26wd2Xf6JTfh3CjfQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LDRFaz9vZ7zEgYA2E/a-little-puzzle-about-termination", "pageUrlRelative": "/posts/LDRFaz9vZ7zEgYA2E/a-little-puzzle-about-termination", "linkUrl": "https://www.lesswrong.com/posts/LDRFaz9vZ7zEgYA2E/a-little-puzzle-about-termination", "postedAtFormatted": "Thursday, February 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Little%20Puzzle%20about%20Termination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Little%20Puzzle%20about%20Termination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDRFaz9vZ7zEgYA2E%2Fa-little-puzzle-about-termination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Little%20Puzzle%20about%20Termination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDRFaz9vZ7zEgYA2E%2Fa-little-puzzle-about-termination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDRFaz9vZ7zEgYA2E%2Fa-little-puzzle-about-termination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 429, "htmlBody": "<p>[Final Update: Back to 'Discussion'; stroked out the initial framing which was misleading.]<br /><br /><del>[Update: Moved to 'Main'. Also, judging by the comments, it appears that most have misunderstood the puzzle and read way too much into it; user 'Manfred' seems to have got the <a href=\"/lw/gjg/a_little_puzzle_about_termination/8e7v\" target=\"_blank\">point</a>.]</del></p>\n<p><del>[Note: This little puzzle is my first article. Preliminary feedback suggests some of you might enjoy it while others might find it too obvious, hence the cautious submission to 'Discussion'; will move it to 'Main' if, and only if, it's well-received.]</del></p>\n<hr />\n<p><del>In his recent paper \"<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\" target=\"_blank\">The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents</a>\", Nick Bostrom states:</del></p>\n<p><del> </del></p>\n<blockquote>\n<p><del>Even an agent that has an apparently very limited final goal, such as &ldquo;to make 32 paperclips&rdquo;, could pursue unlimited resource acquisition if there were no relevant cost to the agent of doing so. For example, even after an expected-utility-maximizing agent had built 32 paperclips, it could use some extra resources to verify that it had indeed successfully built 32 paperclips meeting all the specifications (and, if necessary, to take corrective action). After it had done so, it could run another batch of tests to make doubly sure that no mistake had been made. And then it could run another test, and another. The benefits of subsequent tests would be subject to steeply diminishing returns; however, so long as there were no alternative action with a higher expected utility, the agent would keep testing and re-testing (and keep acquiring more resources to enable these tests).</del></p>\n</blockquote>\n<p><del> </del></p>\n<p><del>Let us take it on from here.</del></p>\n<p>It is tempting to say that a machine can never halt after achieving its goal because it cannot know with full certainty whether it has achieved its goal; it will continually verify, possibly to increasing degrees of certainty, whether it has achieved its goal, but never halt as such.</p>\n<p>What if, from a naive goal G, the machine's goal were then redefined as \"achieve 'G' with 'p' probability\" for some p &lt; 1? It appears this also would not work, given the machine would never be fully certain of being p certain of having achieved G. (and so on...)</p>\n<p>Yet one can specify a set of conditions for which a program will terminate, so how is the argument above fallacious?</p>\n<hr />\n<p>Solution in <a href=\"http://www.rot13.com/\" target=\"_blank\">ROT13</a>: Va beqre gb unyg fhpu na ntrag qbrfa'g arrq gb *xabj* vg'f c pregnva, vg bayl arrqf gb *or* c pregnva; nf gur pbaqvgvba vf rapbqrq, gur unygvat jvyy or gevttrerq bapr gur ntrag ragref gur fgngr bs c pregnvagl, ertneqyrff bs jurgure vg unf (shyy) xabjyrqtr bs vgf fgngr.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LDRFaz9vZ7zEgYA2E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 0, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "21436", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-07T20:34:31.303Z", "modifiedAt": null, "url": null, "title": "Confusion about Normative Morality", "slug": "confusion-about-normative-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:35.611Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JMiller", "createdAt": "2012-11-15T16:08:50.381Z", "isAdmin": false, "displayName": "JMiller"}, "userId": "YePJv5oBk8LKnWogz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D794phdeBSAThQoqr/confusion-about-normative-morality", "pageUrlRelative": "/posts/D794phdeBSAThQoqr/confusion-about-normative-morality", "linkUrl": "https://www.lesswrong.com/posts/D794phdeBSAThQoqr/confusion-about-normative-morality", "postedAtFormatted": "Thursday, February 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Confusion%20about%20Normative%20Morality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConfusion%20about%20Normative%20Morality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD794phdeBSAThQoqr%2Fconfusion-about-normative-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Confusion%20about%20Normative%20Morality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD794phdeBSAThQoqr%2Fconfusion-about-normative-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD794phdeBSAThQoqr%2Fconfusion-about-normative-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>Hi everyone,</p>\n<p>If this has been covered before, I apologize for the clutter and ask to be redirected to the appropriate article or post.</p>\n<p>I am increasingly confused about normative theories. I've read both Eliezer's and Luke's meta ethics sequences as well as some of nyan's posts, but I felt even more confused afterwards. Further, I happen to be a philosophy student right now, and I'm worried that the ideas presented in my ethics classes are misguided and \"conceptually corrupt\" that is, the focus seems to be on defining terms over and over again, as opposed to taking account of real effects of moral ideas in the actual world.&nbsp;</p>\n<p>I am looking for two things: first, a guide as to which reductionist moral theories approximate what LW rationalists tend to think are correct. Second, how can I go about my ethics courses without going insane?</p>\n<p>Sorry if this seems overly aggressive, I am perhaps wrongfully frustrated right now.</p>\n<p>Jeremy</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D794phdeBSAThQoqr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 1.1061155651125365e-06, "legacy": true, "legacyId": "21525", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 103, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-07T20:41:27.791Z", "modifiedAt": null, "url": null, "title": "Meetup : First meetup in Frankfurt (Main) ", "slug": "meetup-first-meetup-in-frankfurt-main", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:04.588Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tripitaka", "createdAt": "2011-02-13T14:34:10.628Z", "isAdmin": false, "displayName": "Tripitaka"}, "userId": "fQzCyXjvZ9cxwra6P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iQL5aNFSo6MbqdrDd/meetup-first-meetup-in-frankfurt-main", "pageUrlRelative": "/posts/iQL5aNFSo6MbqdrDd/meetup-first-meetup-in-frankfurt-main", "linkUrl": "https://www.lesswrong.com/posts/iQL5aNFSo6MbqdrDd/meetup-first-meetup-in-frankfurt-main", "postedAtFormatted": "Thursday, February 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20meetup%20in%20Frankfurt%20(Main)%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20meetup%20in%20Frankfurt%20(Main)%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQL5aNFSo6MbqdrDd%2Fmeetup-first-meetup-in-frankfurt-main%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20meetup%20in%20Frankfurt%20(Main)%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQL5aNFSo6MbqdrDd%2Fmeetup-first-meetup-in-frankfurt-main", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQL5aNFSo6MbqdrDd%2Fmeetup-first-meetup-in-frankfurt-main", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/iy\">First meetup in Frankfurt (Main) </a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">22 February 2013 06:30:27PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Caf&eacute; Albatros, Kiesstra&szlig;e 27, 60486 Frankfurt am Main</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>This is an attempt to start a regular meetup in Frankfurt (Main). Since its also the first meetup in Frankfurt, we should get to know each other a little bit and maybe discuss what we would want out of a regular meetup.&nbsp;</p>\n<p>I will be sitting at a table in the Cafe Albatros from 18 20 to at least 20 00. Of course I will have a LessWrong-sign so there won&acute;t be any confusions. See you soon!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/iy\">First meetup in Frankfurt (Main) </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iQL5aNFSo6MbqdrDd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "21526", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_meetup_in_Frankfurt__Main__\">Discussion article for the meetup : <a href=\"/meetups/iy\">First meetup in Frankfurt (Main) </a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">22 February 2013 06:30:27PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Caf\u00e9 Albatros, Kiesstra\u00dfe 27, 60486 Frankfurt am Main</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>This is an attempt to start a regular meetup in Frankfurt (Main). Since its also the first meetup in Frankfurt, we should get to know each other a little bit and maybe discuss what we would want out of a regular meetup.&nbsp;</p>\n<p>I will be sitting at a table in the Cafe Albatros from 18 20 to at least 20 00. Of course I will have a LessWrong-sign so there won\u00b4t be any confusions. See you soon!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___First_meetup_in_Frankfurt__Main__1\">Discussion article for the meetup : <a href=\"/meetups/iy\">First meetup in Frankfurt (Main) </a></h2>", "sections": [{"title": "Discussion article for the meetup : First meetup in Frankfurt (Main) ", "anchor": "Discussion_article_for_the_meetup___First_meetup_in_Frankfurt__Main__", "level": 1}, {"title": "Discussion article for the meetup : First meetup in Frankfurt (Main) ", "anchor": "Discussion_article_for_the_meetup___First_meetup_in_Frankfurt__Main__1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-08T06:18:39.018Z", "modifiedAt": null, "url": null, "title": "Link: blog on effective altruism", "slug": "link-blog-on-effective-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:34.802Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i8CoHSqGHgJCbdQK6/link-blog-on-effective-altruism", "pageUrlRelative": "/posts/i8CoHSqGHgJCbdQK6/link-blog-on-effective-altruism", "linkUrl": "https://www.lesswrong.com/posts/i8CoHSqGHgJCbdQK6/link-blog-on-effective-altruism", "postedAtFormatted": "Friday, February 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20blog%20on%20effective%20altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20blog%20on%20effective%20altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi8CoHSqGHgJCbdQK6%2Flink-blog-on-effective-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20blog%20on%20effective%20altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi8CoHSqGHgJCbdQK6%2Flink-blog-on-effective-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi8CoHSqGHgJCbdQK6%2Flink-blog-on-effective-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<p>Over the last few months I've <a href=\"http://rationalaltruist.com/\">started blogging</a> about effective altruism more broadly, rather than <a href=\"http://ordinaryideas.wordpress.com/\">focusing on AI risk</a>. I'm still focusing on abstract considerations and methodological issues, but I hope it is of interest to others here. Going forward I intend to cross-post more often to LW, but I thought I would post the backlog here anyway. With luck, I'll also have the opportunity to post more than bi-weekly.</p>\n<p>I welcome thoughts, criticisms, etc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i8CoHSqGHgJCbdQK6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 1.1064824554980875e-06, "legacy": true, "legacyId": "21535", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-08T06:27:03.490Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Failed Utopia #4-2", "slug": "seq-rerun-failed-utopia-4-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:34.680Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uCfotMPezL6kEtCC9/seq-rerun-failed-utopia-4-2", "pageUrlRelative": "/posts/uCfotMPezL6kEtCC9/seq-rerun-failed-utopia-4-2", "linkUrl": "https://www.lesswrong.com/posts/uCfotMPezL6kEtCC9/seq-rerun-failed-utopia-4-2", "postedAtFormatted": "Friday, February 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Failed%20Utopia%20%234-2&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Failed%20Utopia%20%234-2%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuCfotMPezL6kEtCC9%2Fseq-rerun-failed-utopia-4-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Failed%20Utopia%20%234-2%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuCfotMPezL6kEtCC9%2Fseq-rerun-failed-utopia-4-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuCfotMPezL6kEtCC9%2Fseq-rerun-failed-utopia-4-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>Today's post, <a href=\"/lw/xu/failed_utopia_42/\">Failed Utopia #4-2</a> was originally published on 21 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Failed_Utopia_.234-2\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A fictional short story illustrating some of the ideas in Interpersonal Entanglement above. (Many commenters seemed to like this story, and some said that the ideas were easier to understand in this form.)</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/glm/seq_rerun_interpersonal_entanglement/\">Interpersonal Entanglement</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uCfotMPezL6kEtCC9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.1064877380110605e-06, "legacy": true, "legacyId": "21536", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ctpkTaqTKbmm6uRgC", "89HPJDJQKCxy7Bz4u", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-08T13:49:44.718Z", "modifiedAt": null, "url": null, "title": "Vienna Meetup: Saturday 9th March", "slug": "vienna-meetup-saturday-9th-march", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.583Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ratcourse", "createdAt": "2012-11-03T10:26:05.641Z", "isAdmin": false, "displayName": "Ratcourse"}, "userId": "qwnfbBpAcbxLT4JBr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NKM23x6oATPHGTRap/vienna-meetup-saturday-9th-march", "pageUrlRelative": "/posts/NKM23x6oATPHGTRap/vienna-meetup-saturday-9th-march", "linkUrl": "https://www.lesswrong.com/posts/NKM23x6oATPHGTRap/vienna-meetup-saturday-9th-march", "postedAtFormatted": "Friday, February 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Vienna%20Meetup%3A%20Saturday%209th%20March&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVienna%20Meetup%3A%20Saturday%209th%20March%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNKM23x6oATPHGTRap%2Fvienna-meetup-saturday-9th-march%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Vienna%20Meetup%3A%20Saturday%209th%20March%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNKM23x6oATPHGTRap%2Fvienna-meetup-saturday-9th-march", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNKM23x6oATPHGTRap%2Fvienna-meetup-saturday-9th-march", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6, "htmlBody": "<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Cafe im schottenstift;</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">16 pm.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">See you there.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NKM23x6oATPHGTRap", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.1067659351150261e-06, "legacy": true, "legacyId": "21539", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-08T15:31:27.160Z", "modifiedAt": null, "url": null, "title": "Pay charities for results?", "slug": "pay-charities-for-results", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:34.997Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BenGilbert", "createdAt": "2013-02-01T15:52:27.058Z", "isAdmin": false, "displayName": "BenGilbert"}, "userId": "GFGdZXy4QY6JCrub6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j6hKmmmzJSMTKCFpi/pay-charities-for-results", "pageUrlRelative": "/posts/j6hKmmmzJSMTKCFpi/pay-charities-for-results", "linkUrl": "https://www.lesswrong.com/posts/j6hKmmmzJSMTKCFpi/pay-charities-for-results", "postedAtFormatted": "Friday, February 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pay%20charities%20for%20results%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APay%20charities%20for%20results%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj6hKmmmzJSMTKCFpi%2Fpay-charities-for-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pay%20charities%20for%20results%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj6hKmmmzJSMTKCFpi%2Fpay-charities-for-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj6hKmmmzJSMTKCFpi%2Fpay-charities-for-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1019, "htmlBody": "<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Two problems with charity:</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">1) You usually don't know what your donation achieves. At best, you might know what your money is spent on. You don't know how effective this is at producing the outcomes you care about. Even Givewell, who seem to me to have done more careful work on cost-effectiveness than anyone else, regard their cost-effectiveness estimates as very rough and <a href=\"http://blog.givewell.org/2011/11/04/some-considerations-against-more-investment-in-cost-effectiveness-estimates/\">no more than an indicative starting point</a> for evaluating charities.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">2) Charities have low or no financial incentives to be as effective as they can, not least because usually no-one knows how effective they are.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Potential solution:</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Instead of donating to charities, pay them for results achieved.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Ideally, you would pay for the final outcomes which you care about, eg paying a certain amount per unit reduction in child mortality, reduced disease prevalence, improved test scores, etc. If this is too difficult, then you could pay for intermediate results, eg number of children vaccinated, number of people protected by bednets, etc. Results could be measured against a control group, some baseline, outcomes in parts of the country where the charity doesn't operate etc. (Comparison with a well-constructed control group would probably be best in most cases).</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">This isn't really something that an individual donor can do, since it relies on accurate, independent measurement of results and will be most effective when charities know that their funding depends on the results they achieve. To work best, it would have to happen in a co-ordinated way and at a large enough scale that proper measurement is affordable.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Advantages:</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">1) You only pay if a charity is effective at doing what you want it to do. You have less need to try to understand what a charity does; you can offer the money for the results and leave it to them to find how to produce them.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">2) Charities will have financial incentives to be as effective as possible, including finding out how effective they already are and what they could do to be more effective.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">3) Effective charities will get more money and be able to expand, ineffective charities will get less money and may have to close down.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">4) Lots of information would be produced about what is and is not effective, which could help donors, aid agencies, charities etc make better decisions in future, whether or not they are paying / being paid for results.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Disadvantages <em>relative to the current system of donating</em>:</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">1) Measuring the results costs money; it might be better just to donate this money.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">2) It might be possible to manipulate or falsify the results.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">3) Where you cannot measure what you really care about (eg the long-term improvement in someone's life from their receiving education), focusing on intermediate results might make a charity less effective (eg teaching to the test, leading to less learning of things that will help later).</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">4) Charities may be able to produce better results in ways that produce negative, unmeasured side-effects. They may be more likely to do this when they have financial incentives to improve measured results.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Discussion of disadvantages</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">1) As well as potentially improving incentives, measurement could create very valuable information, especially since the evidence base for most charitable activity is very weak. It is likely to be money well spent.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">2) Obtaining accurate results is a methodological issue, with technical solutions, though cost is a constraint. Falsification is certainly a danger. On the other hand, charities found to be falsifying results would face huge reputational costs; they would have strong incentives not to try. Also, charities with integrity might be unlikely to try, whilst the charities that would be willing to try might well be those that are already misspending money donated to them in the ordinary way; with an attempt at measurement, there is at least a chance of exposing this.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">3) Targeting measures which become progressively worse guides to the actual good being done is a danger. Choosing a good measure &ndash; one which is hard to achieve without achieving the outcomes you ultimately care about &ndash; could make this a smaller problem. Even so, whenever the ultimate outcome isn't targeted directly, there is likely to be some diversion of resources from what a charity thinks does the most good, to what it thinks will improve measured results. However, this may be a gain, not a loss, if activities which improve measured results in fact do more good than the things which the charity believes are effective; and, if it is a loss, it has to be weighed against the gains from improved transparency, feedback etc.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">As a separate issue, when the ultimate outcome cannot be targeted directly, a theory of why the intermediate targets are effective ways of achieving that outcome is needed, but this is no less true of charitable projects now. Also, there may be no measure with a close enough relation to a charity's ultimate goal that can usefully be measured; for such charities, paying them for results will not be appropriate.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">4) If a charity's activities do more harm than good, then donations and payments for results would both be bad. Payment for results might be particularly bad if it leads to more money going to charities that do more net harm, or changes how charities function such that the net good they do goes down, because they pursue results in more harmful ways. Weighing up how big a problem this is would require some understanding of how the charities involved function (so, in fact, it's not quite so simple as 'offer money for results and leave it to them to find how to produce them'). The seriousness of the problem could be very different for different areas, regions or organisations; in any case, I think it's worth bearing in mind.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">*****</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Bottom line for me: the advantages seem to outweigh the disadvantages, and I would be more likely to give to charity if I could do it this way. At the very least, I would like to see it tried.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">(This approach is actually beginning to be taken, though only in a tiny minority of projects, and not in ways to which an individual donor can contribute, as far as I can see &ndash; subject for another post, perhaps.)</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Comments?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j6hKmmmzJSMTKCFpi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 1, "extendedScore": null, "score": 1.1068298680105986e-06, "legacy": true, "legacyId": "21542", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-08T16:25:21.679Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Buffalo, Cambridge MA, Dublin, Moscow, Ohio, Paderborn, Purdue, Washington DC", "slug": "weekly-lw-meetups-austin-buffalo-cambridge-ma-dublin-moscow", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v7rQx7teLrFDJx4tT/weekly-lw-meetups-austin-buffalo-cambridge-ma-dublin-moscow", "pageUrlRelative": "/posts/v7rQx7teLrFDJx4tT/weekly-lw-meetups-austin-buffalo-cambridge-ma-dublin-moscow", "linkUrl": "https://www.lesswrong.com/posts/v7rQx7teLrFDJx4tT/weekly-lw-meetups-austin-buffalo-cambridge-ma-dublin-moscow", "postedAtFormatted": "Friday, February 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Buffalo%2C%20Cambridge%20MA%2C%20Dublin%2C%20Moscow%2C%20Ohio%2C%20Paderborn%2C%20Purdue%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Buffalo%2C%20Cambridge%20MA%2C%20Dublin%2C%20Moscow%2C%20Ohio%2C%20Paderborn%2C%20Purdue%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv7rQx7teLrFDJx4tT%2Fweekly-lw-meetups-austin-buffalo-cambridge-ma-dublin-moscow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Buffalo%2C%20Cambridge%20MA%2C%20Dublin%2C%20Moscow%2C%20Ohio%2C%20Paderborn%2C%20Purdue%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv7rQx7teLrFDJx4tT%2Fweekly-lw-meetups-austin-buffalo-cambridge-ma-dublin-moscow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv7rQx7teLrFDJx4tT%2Fweekly-lw-meetups-austin-buffalo-cambridge-ma-dublin-moscow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 517, "htmlBody": "<p><strong>This summary was posted to LW main on February 1st. The following week's summary is <a href=\"/lw/gmf/weekly_lw_meetups_austin_berlin_cambridge_uk/\">here</a>.<br /></strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/ip\">Fourth Purdue Meetup:&nbsp;<span class=\"date\">01 February 2013 06:50PM</span></a></li>\n<li><a href=\"/meetups/ia\">Less Wrong Dublin:&nbsp;<span class=\"date\">02 February 2013 04:30PM</span></a></li>\n<li><a href=\"/meetups/ig\">Moscow: Reality and Us:&nbsp;<span class=\"date\">03 February 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/io\">Washington DC fun and games meetup:&nbsp;<span class=\"date\">03 February 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/ik\">Paderborn Meetup Frebruary 6th :&nbsp;<span class=\"date\">06 February 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/il\">LW Buffalo Meetup at Buffalo Labs:&nbsp;<span class=\"date\">07 February 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/ht\">Brussels meetup:&nbsp;<span class=\"date\">16 February 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/ir\">Tokyo Meetup:&nbsp;<span class=\"date\">01 March 2013 07:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">02 February 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/iq\">Cambridge First-Sunday Meetup:&nbsp;<span class=\"date\">04 February 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ic\">Columbus, Ohio; Self-Skepticism:&nbsp;<span class=\"date\">04 February 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/ij\">Love and Sex in Salt Lake City:&nbsp;<span class=\"date\">16 February 2014 01:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v7rQx7teLrFDJx4tT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": 1.106863757544422e-06, "legacy": true, "legacyId": "21439", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3SWt49DCt53FXWYDt", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-08T21:22:43.647Z", "modifiedAt": null, "url": null, "title": "Philosophical Landmines", "slug": "philosophical-landmines", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:39.353Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L4HQ3gnSrBETRdcGu/philosophical-landmines", "pageUrlRelative": "/posts/L4HQ3gnSrBETRdcGu/philosophical-landmines", "linkUrl": "https://www.lesswrong.com/posts/L4HQ3gnSrBETRdcGu/philosophical-landmines", "postedAtFormatted": "Friday, February 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philosophical%20Landmines&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhilosophical%20Landmines%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL4HQ3gnSrBETRdcGu%2Fphilosophical-landmines%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philosophical%20Landmines%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL4HQ3gnSrBETRdcGu%2Fphilosophical-landmines", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL4HQ3gnSrBETRdcGu%2Fphilosophical-landmines", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 962, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/k5/cached_thoughts/\">Cached Thoughts</a></p>\n<p>Last summer I was talking to my sister about something. I don't remember the details, but I invoked the concept of \"truth\", or \"reality\" or some such. She immediately spit out a cached reply along the lines of \"But how can you really say what's <em>true</em>?\".</p>\n<p>Of course I'd learned some great replies to that sort of question right here on LW, so I did my best to sort her out, but everything I said invoked more confused slogans and cached thoughts. I realized the battle was lost. Worse, I realized she'd stopped thinking. Later, I realized I'd stopped thinking too.</p>\n<p>I went away and formulated the concept of a \"Philosophical Landmine\".</p>\n<p>I used to occasionally remark that if you care about what happens, you should think about what will happen as a result of possible actions. This is basically a slam dunk in everyday practical rationality, except that I would sometimes describe it as \"consequentialism\".</p>\n<p>The predictable consequence of this sort of statement is that someone starts going off about hospitals and terrorists and organs and moral philosophy and consent and rights and so on. This may be controversial, but I would say that causing this tangent constitutes a failure to communicate the point. Instead of prompting someone to think, I invoked some irrelevant philosophical cruft. The discussion is now about Consequentialism, the Capitalized Moral Theory, instead of the simple idea of thinking through consequences as an everyday heuristic.</p>\n<p>It's not even that my statement relied on a misused term or something; it's that an unimportant choice of terminology dragged the whole conversation in an&nbsp;irrelevant&nbsp;and useless direction.</p>\n<p>That is, \"consequentialism\" was a Philosophical Landmine.</p>\n<p>In the course of normal conversation, you passed through an ordinary spot that happened to conceal the dangerous leftovers of past memetic wars. As a result, an intelligent and reasonable human was reduced to a mindless zombie chanting prerecorded slogans. If you're lucky, that's all. If not, you start chanting counter-slogans and the whole thing goes supercritical.</p>\n<p>It's usually not so bad, and no one is literally \"chanting slogans\". There may even be some original phrasings involved. But the conversation has been derailed.</p>\n<p>So how do these \"philosophical landmine\" things work?</p>\n<p>It looks like when a lot has been said on a confusing topic, usually something in philosophy, there is a large complex of slogans and counter-slogans installed as cached thoughts around it. Certain words or concepts will trigger these cached thoughts, and any attempt to mitigate the damage will trigger more of them. Of course they will also trigger cached thoughts in other people, which in turn... The result being that the conversation rapidly diverges from the original point to some useless yet heavily discussed attractor.</p>\n<p>Notice that whether a particular concept will cause trouble depends on the person as well as the concept. Notice further that this implies that the probability of hitting a landmine scales with the number of people involved and the topic-breadth of the conversation.</p>\n<p>Anyone who hangs out on 4chan can confirm that this is the approximate shape of most thread derailments.</p>\n<p>Most concepts in philosophy and metaphysics are landmines for many people. The phenomenon also occurs in politics and other tribal/ideological disputes. The ones I'm particularly interested in are the ones in philosophy, but it might be useful to divorce the concept of \"conceptual landmines\" from philosophy in particular.</p>\n<p>Here's some common ones in philosophy:</p>\n<ul>\n<li>Morality</li>\n<li>Consequentialism</li>\n<li>Truth</li>\n<li>Reality</li>\n<li>Consciousness</li>\n<li>Rationality</li>\n<li>Quantum</li>\n</ul>\n<p>Landmines in a topic make it really hard to discuss ideas or do work in these fields, because chances are, someone is going to step on one, and then there will be a big noisy mess that interferes with the rather delicate business of thinking carefully about confusing ideas.</p>\n<p>My purpose in bringing this up is mostly to precipitate some terminology and a concept around this phenomenon, so that we can talk about it and refer to it. It is important for concepts to have verbal handles, you see.</p>\n<p>That said, I'll finish with a few words about what we can do about it. There are two major forks of the anti-landmine strategy: avoidance, and damage control.</p>\n<p>Avoiding landmines is <em>your</em> job. If it is a predictable consequence that something you could say will put people in mindless slogan-playback-mode, don't say it. If something you say makes people go off on a spiral of bad philosophy, don't get annoyed with them, just fix what you say. This is just being a <a href=\"http://measureofdoubt.com/2012/06/11/be-a-communications-consequentialist/\">communications consequentialist</a>. Figure out which concepts are landmines for which people, and step around them, or use <a href=\"/lw/g7y/morality_is_awesome/\">alternate terminology</a> with fewer problematic connotations.</p>\n<p>If it happens, which it does, as far as I can tell, my only effective damage control strategy is to abort the conversation. I'll probably think that I can take those stupid ideas here and now, but that's just the landmine trying to go supercritical. Just say no. Of course letting on that you think you've stepped on a landmine is probably incredibly rude; keep it to yourself. Subtly change the subject or rephrase your original point without the problematic concepts or <a href=\"/lw/gm9/philosophical_landmines/8fc8\">something</a>.</p>\n<p>A third prong could be playing \"philosophical bomb squad\", which means permanently defusing landmines by supplying satisfactory nonconfusing explanations of things without causing too many explosions in the process. Needless to say, this is quite hard. I think we do a pretty good job of it here at LW, but for topics and people not yet defused, avoid and abort.</p>\n<p><strong>ADDENDUM:</strong>&nbsp;Since I didn't make it very obvious, it's worth noting that this happens with rationalists, too, even on this very forum. It is your responsibility not to contain landmines as well as not to step on them. But you're already trying to do that, so I don't emphasize it as much as not stepping on them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 1, "5f5c37ee1b5cdee568cfb0d6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L4HQ3gnSrBETRdcGu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 112, "baseScore": 153, "extendedScore": null, "score": 0.000348, "legacy": true, "legacyId": "21537", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 153, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 146, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2MD3NMLBPCqPfnfre", "Aq8BQMXRZX3BoFd4c"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 15, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-09T01:30:02.321Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Investing for the Long Slump", "slug": "seq-rerun-investing-for-the-long-slump", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.245Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ixkqdazCiJoHzycNT/seq-rerun-investing-for-the-long-slump", "pageUrlRelative": "/posts/ixkqdazCiJoHzycNT/seq-rerun-investing-for-the-long-slump", "linkUrl": "https://www.lesswrong.com/posts/ixkqdazCiJoHzycNT/seq-rerun-investing-for-the-long-slump", "postedAtFormatted": "Saturday, February 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Investing%20for%20the%20Long%20Slump&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Investing%20for%20the%20Long%20Slump%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FixkqdazCiJoHzycNT%2Fseq-rerun-investing-for-the-long-slump%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Investing%20for%20the%20Long%20Slump%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FixkqdazCiJoHzycNT%2Fseq-rerun-investing-for-the-long-slump", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FixkqdazCiJoHzycNT%2Fseq-rerun-investing-for-the-long-slump", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>Today's post, <a href=\"/lw/xv/investing_for_the_long_slump/\">Investing for the Long Slump</a> was originally published on 22 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Investing_for_the_Long_Slump\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>What should you do if you think that the world's economy is going to stay bad for a very long time? How could such a scenario happen?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gm8/seq_rerun_failed_utopia_42/\">Failed Utopia #4-2</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ixkqdazCiJoHzycNT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.107206271601278e-06, "legacy": true, "legacyId": "21544", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iGzNGfv5depzzyz2B", "uCfotMPezL6kEtCC9", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-09T03:59:45.544Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR Discussion, chapters 34-38", "slug": "meetup-durham-hpmor-discussion-chapters-34-38", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b3hsgoTGndnPfNDym/meetup-durham-hpmor-discussion-chapters-34-38", "pageUrlRelative": "/posts/b3hsgoTGndnPfNDym/meetup-durham-hpmor-discussion-chapters-34-38", "linkUrl": "https://www.lesswrong.com/posts/b3hsgoTGndnPfNDym/meetup-durham-hpmor-discussion-chapters-34-38", "postedAtFormatted": "Saturday, February 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2034-38&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2034-38%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb3hsgoTGndnPfNDym%2Fmeetup-durham-hpmor-discussion-chapters-34-38%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2034-38%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb3hsgoTGndnPfNDym%2Fmeetup-durham-hpmor-discussion-chapters-34-38", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb3hsgoTGndnPfNDym%2Fmeetup-durham-hpmor-discussion-chapters-34-38", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/iz'>Durham HPMoR Discussion, chapters 34-38</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 February 2013 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Parker and Otis, 112 S Duke St, Durham NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next in our regularly scheduled HPMoR discussions.</p>\n\n<p>Please feel free to join in, even if you haven't done all the reading; we try to summarize the chapters as we discuss them. (Of course, reading them in advance is encouraged!)</p>\n\n<p>It looks like Parker and Otis is open again, so we'll head there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/iz'>Durham HPMoR Discussion, chapters 34-38</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b3hsgoTGndnPfNDym", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.1073004543669357e-06, "legacy": true, "legacyId": "21546", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_34_38\">Discussion article for the meetup : <a href=\"/meetups/iz\">Durham HPMoR Discussion, chapters 34-38</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 February 2013 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Parker and Otis, 112 S Duke St, Durham NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next in our regularly scheduled HPMoR discussions.</p>\n\n<p>Please feel free to join in, even if you haven't done all the reading; we try to summarize the chapters as we discuss them. (Of course, reading them in advance is encouraged!)</p>\n\n<p>It looks like Parker and Otis is open again, so we'll head there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_34_381\">Discussion article for the meetup : <a href=\"/meetups/iz\">Durham HPMoR Discussion, chapters 34-38</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 34-38", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_34_38", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 34-38", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_34_381", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-09T05:50:00.045Z", "modifiedAt": null, "url": null, "title": "A brief history of ethically concerned scientists", "slug": "a-brief-history-of-ethically-concerned-scientists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:34.262Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Kaj_Sotala", "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hxaq9MCaSrwWPmooZ/a-brief-history-of-ethically-concerned-scientists", "pageUrlRelative": "/posts/hxaq9MCaSrwWPmooZ/a-brief-history-of-ethically-concerned-scientists", "linkUrl": "https://www.lesswrong.com/posts/hxaq9MCaSrwWPmooZ/a-brief-history-of-ethically-concerned-scientists", "postedAtFormatted": "Saturday, February 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20brief%20history%20of%20ethically%20concerned%20scientists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20brief%20history%20of%20ethically%20concerned%20scientists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhxaq9MCaSrwWPmooZ%2Fa-brief-history-of-ethically-concerned-scientists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20brief%20history%20of%20ethically%20concerned%20scientists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhxaq9MCaSrwWPmooZ%2Fa-brief-history-of-ethically-concerned-scientists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhxaq9MCaSrwWPmooZ%2Fa-brief-history-of-ethically-concerned-scientists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4169, "htmlBody": "<blockquote>For the first time in history, it has become possible for a limited group of a few thousand people to threaten the absolute destruction of millions. </blockquote><p>-- Norbert Wiener (1956), <u><a href=\"http://books.google.com/books?id=CwoAAAAAMBAJ&pg=PA53&source=gbs_toc_r&cad=2\">Moral Reflections of a Mathematician</a></u>.</p><p><br/>Today, the general attitude towards scientific discovery is that scientists are not themselves responsible for how their work is used. For someone who is interested in science for its own sake, or even for someone who mostly considers research to be a way to pay the bills, this is a tempting attitude. It would be easy to only focus on one\u2019s work, and leave it up to others to decide what to do with it.<br/><br/>But this is not necessarily the attitude that we should encourage. As technology becomes more powerful, it also becomes more dangerous. Throughout history, many scientists and inventors have recognized this, and taken different kinds of action to help ensure that their work will have beneficial consequences. Here are some of them. </p><p>This post is not arguing that any specific approach for taking responsibility for one&#x27;s actions is the correct one. Some researchers hid their work, others refocused on other fields, still others began active campaigns to change the way their work was being used. It is up to the reader to decide which of these approaches were successful and worth emulating, and which ones were not.<br/></p><h2><strong>Pre-industrial inventors</strong></h2><blockquote>\u2026 I do not publish nor divulge [methods of building submarines] by reason of the evil nature of men who would use them as means of destruction at the bottom of the sea, by sending ships to the bottom, and sinking them together with the men in them.</blockquote><p>-- <u><a href=\"http://books.google.com/books?id=A7dUhbBfmzMC&pg=PA274&lpg=PA274&dq=this+i+do+not+divulge+account+evil+nature+men+da+vinci&source=bl&ots=dm71gSZ-Yd&sig=9AUz3KcgO0fTiCz3FcwmkYxxKt8&hl=fi&sa=X&ei=p5IHUbjqK6ji4QSX9YHgCA&ved=0CE0Q6AEwAw#v=onepage&q&f=false\">Leonardo da Vinci</a></u></p><p><br/>People did not always think that the benefits of freely disseminating knowledge outweighed the harms. O.T. Benfey, <u><a href=\"http://books.google.com/books?id=SwoAAAAAMBAJ&pg=PA177\">writing in</a></u> a 1956 issue of the <em>Bulletin of the Atomic Scientists</em>, cites F.S. Taylor\u2019s book on early alchemists:</p><blockquote>Alchemy was certainly intended to be useful .... But [the alchemist] never proposes the <em>public</em> use of such things, the disclosing of his knowledge for the benefit of man. \u2026. Any disclosure of the alchemical secret was felt to be profoundly wrong, and likely to bring immediate punishment from on high. The reason generally given for such secrecy was the probable abuse by wicked men of the power that the alchemical would give \u2026. The alchemists, indeed, felt a strong moral responsibility that is not always acknowledged by the scientists of today.</blockquote><p><br/>With the Renaissance, science began to be viewed as public property, but many scientists remained cautious about the way in which their work might be used. Although he held the office of military engineer, <strong>Leonardo da Vinci </strong>(1452-1519) drew a distinction between offensive and defensive warfare, and emphasized the role of good defenses in protecting people\u2019s liberty from tyrants. He described war as \u2018bestialissima pazzia\u2019 (most bestial madness), and wrote that \u2018it is an infinitely atrocious thing to take away the life of a man\u2019. One of the clearest examples of his reluctance to unleash dangerous inventions was his refusal to publish the details of his plans for submarines.<br/><br/>Later Renaissance thinkers continued to be concerned with the potential uses of their discoveries. <strong>John Napier </strong>(1550-1617), the inventor of logarithms, also experimented with a new form of artillery. Upon seeing its destructive power, he decided to keep its details a secret, and even spoke from his deathbed against the creation of new kinds of weapons.<br/><br/>But only concealing one discovery pales in comparison to the likes of <strong>Robert Boyle </strong>(1627-1691). A pioneer of physics and chemistry and possibly the most famous for describing and publishing <u><a href=\"http://en.wikipedia.org/wiki/Boyle%27s_law\">Boyle\u2019s law</a></u>, he sought to make humanity better off, taking an interest in things such as improved agricultural methods as well as better medicine. In his studies, he also discovered knowledge and made inventions related to a variety of potentially harmful subjects, including poisons, invisible ink, counterfeit money, explosives, and kinetic weaponry. These \u2018my love of Mankind has oblig\u2019d me to conceal, even from my nearest Friends\u2019.</p><p><strong>Chemical warfare</strong></p><p>By the early twentieth century, people had began looking at science in an increasingly optimistic light: it was believed that science would not only continue to improve everyone\u2019s prosperity, but also make war outright impossible. But as science became more sophisticated, it would also become possible to cause ever more harm with ever smaller resources. One of the early indications of science\u2019s ability to do harm came from advances in chemical warfare, and World War I <u><a href=\"http://en.wikipedia.org/wiki/Poison_gas_in_World_War_I\">saw the deployment</a></u> of chlorine, phosgene, and mustard gas as weapons. It should not be surprising, then, that some scientists in related fields began growing concerned.  But unlike earlier inventors, at least three of them did far more than just refuse to publish their work.<br/><br/><strong>Clara Immerwahr</strong> (1870-1915) was a German chemist and the first woman to obtain a PhD from the University of Breslau. She was strongly opposed to the use of chemical weapons. Married to Fritz Haber, \u2018the father of chemical warfare\u2019, she unsuccessfully attempted many times to convince her husband to abandon his work. Immerwahr was generally depressed and miserable over the fact that society considered a married woman\u2019s place to be at home, denying her the opportunity to do science. In the end, after her efforts to dissuade her husband from working on chemical warfare had failed and Fritz had personally overseen <u><a href=\"http://en.wikipedia.org/wiki/Second_Battle_of_Ypres\">the first major use of chlorine</a></u>, she committed suicide by shooting herself in the heart. <br/><br/>Poison gas also concerned scientists in other disciplines. <strong>Lewis Fry Richardson </strong>(1881-1953) was a mathematician and meteorologist. During the World War II, the military became interested in his work on turbulence and gas mixing, and attempted to recruit him to do help them do work on modeling the best ways of using poison gas. Realizing what his work was being used for, Richardson abandoned meteorology entirely and destroyed his unpublished research. Instead, he turned his research to investigating the causes of war, attempting to find ways to reduce the risk of armed conflict. He spent the rest of his life devoted to this topic, and is today considered one of the founders of the scientific analysis of conflict.<br/><br/><strong>Arthur Galston </strong>(1920-2008), a botanist, was also concerned with the military use of his inventions. Building upon his work, the US military developed Agent Orange, a chemical weapon which was deployed in the Vietnam War. Upon discovering what his work had been used for, he began to campaign against its use, and together with a number of others finally convinced President Nixon to order an end to its spraying in 1970. Reflecting upon the matter, Galston<u><a href=\"http://beck2.med.harvard.edu/week13/Galston.pdf\"> wrote</a></u>:</p><blockquote>I used to think that one could avoid involvement in the antisocial consequences of science simply by not working on any project that might be turned to evil or destructive  ends. I have learned that things are not all that simple, and that almost any scientific finding can be perverted or twisted under appropriate societal pressures. In my view, the only recourse for a scientist concerned about the social consequences of his work is to remain involved with it to the end. His responsibility to society does not cease with publication of a definitive scientific paper. Rather, if his discovery is translated into some impact on the world outside the laboratory, he will, in most instances, want to follow through to see that it is used for constructive rather than anti-human purposes.</blockquote><p><br/>After retiring in 1990, he founded the Interdisciplinary Center for Bioethics at Yale, where he also taught bioethics to undergraduates.</p><h2><strong>Nuclear weapons</strong></h2><p>While chemical weapons are capable of inflicting serious injuries as well as birth defects on large numbers of people, they have never been viewed to be as dangerous as nuclear weapons. As physicists became capable of creating weapons of unparalleled destructive power, they also began growing ever more concerned about the consequences of their work.<br/><br/><strong>Le\u00f3 Szil\u00e1rd </strong>(1898-1964) was one of the first people to envision nuclear weapons, and was granted a patent for the nuclear chain reaction in 1934. Two years later, he grew worried that Nazi scientists would find his patents and use them to create weapons, so he asked the British Patent Office to withdraw his patents and secretly reassign them to the Royal Navy. His fear of Nazi Germany developing nuclear weapons also made him instrumental in making the USA initiate the Manhattan Project, as he and two other scientists wrote the <a href=\"http://en.wikipedia.org/wiki/Einstein%E2%80%93Szil%C3%A1rd_letter\">Einstein-Szil\u00e1rd letter</a> that advised President Roosevelt of the need to develop the same technology. But in 1945, he learned that the atomic bomb was about to be used on Japan, despite it being certain that neither Germany nor Japan had one. He then did his best to stop them from being used and started a <u><a href=\"http://www.dannen.com/decision/45-07-17.html\">petition against using </a></u>them, with little success.<br/><br/>After the war, he no longer wanted to contribute to the creation of weapons and changed fields to molecular biology. In 1962, he founded the <u><a href=\"http://en.wikipedia.org/wiki/Council_for_a_Livable_World\">Council for a Livable World</a></u>, which aimed to warn people about the dangers of nuclear war and to promote a policy of arms control. The Council continues its work even today.<br/><br/>Another physicist who worked on the atomic bomb due to a fear of it being developed by Nazi Germany was <strong>Joseph Rotblat</strong> (1908-2005), who felt that the Allies also having an atomic bomb would deter the Axis from using one. But he gradually began to realize that Nazi Germany would likely never develop the atomic bomb, destroying his initial argument for working on it. He also came to realize that the bomb continued to be under active development due to reasons that he felt were unethical. In conversation, General Leslie Groves mentioned that the real purpose of the bomb was to subdue the USSR. Rotblat was shocked to hear this, especially given that the Soviet Union was at the time an ally in the war effort. In 1944, it became apparent that Germany would not develop the atomic bomb. As a result, Rotblat asked for permission to leave the project, and was granted it.<br/><br/>Afterwards, Rotblat regretted his role in developing nuclear weapons. He believed that the logic of nuclear deterrence was flawed, since he thought that if Hitler had possessed an atomic bomb, then Hitler\u2019s last order would have been to use it against London regardless of the consequences. Rotblat decided to do whatever he could to prevent the future use and deployment of nuclear weapons, and proposed a worldwide moratorium on such research until humanity was wise enough to use it without risks. He decided to repurpose his career into something more useful for humanity, and began studying and teaching the application of nuclear physics into medicine, becoming a professor at the Medical College of St Bartholomew\u2019s Hospital in London.<br/><br/>Rotblat worked together with Bertrand Russell to limit the spread of nuclear weapons, and the two collaborated with a number of other scientists to issue the <u><a href=\"http://en.wikipedia.org/wiki/Russell-Einstein_Manifesto\">Russell-Einstein Manifesto</a></u> in 1955, calling the governments of the world to take action to prevent nuclear weapons from doing more damage. The manifesto led to the establishment of the <u><a href=\"http://en.wikipedia.org/wiki/Pugwash_Conferences_on_Science_and_World_Affairs\">Pugwash Conferences</a></u>, in which nuclear scientists from both the West and the East met each other. By facilitating dialogue between the two sides of the Cold War, these conferences <u><a href=\"http://www.irwinabrams.com/books/excerpts/annual95.html\">helped lead to</a></u> several arms control agreements, such as the <u><a href=\"http://en.wikipedia.org/wiki/Partial_Test_Ban_Treaty\">Partial Test Ban Treaty</a></u> of 1963 and the <u><a href=\"http://en.wikipedia.org/wiki/Non-Proliferation_Treaty\">Non-Proliferation Treaty</a></u> of 1968. In 1995, Rotblat and the Pugwash Conferences were <u><a href=\"http://www.nobelprize.org/nobel_prizes/peace/laureates/1995/index.html\">awarded the Nobel Peace Prize</a></u> \u201cfor their efforts to diminish the part played by nuclear arms in international politics and, in the longer run, to eliminate such arms\u201d.<br/><br/>The development of nuclear weapons also affected <strong>Norbert Wiener</strong> (1894-1964), professor of mathematics at the Massachusetts Institute of Technology and the originator of the field of <u><a href=\"http://en.wikipedia.org/wiki/Cybernetics\">cybernetics</a></u>. After the Hiroshima bombing, a researcher working for a major aircraft corporation requested a copy of an earlier paper of Wiener\u2019s. Wiener refused to provide it, and sent <em>Atlantic Monthly </em>a copy of his <u><a href=\"http://connection.ebscohost.com/c/letters/20836656/scientist-rebels\">response to the researcher</a></u>, in which he declared his refusal to share his research with anyone who would use it for military purposes.</p><blockquote>In the past, the community of scholars has made it a custom to furnish scientific information to any person seriously seeking it. However, we must face these facts: The policy of the government itself during and after the war, say in the bombing of Hiroshima and Nagasaki, has made it clear that to provide scientific information is not a necessarily innocent act, and may entail the gravest consequences. One therefore cannot escape reconsidering the established custom of the scientist to give information to every person who may inquire of him. The interchange of ideas, one of the great traditions of science, must of course receive certain limitations when the scientist becomes an arbiter of life and death. [...]</blockquote><blockquote>The experience of the scientists who have worked on the atomic bomb has indicated that in any investigation of this kind the scientist ends by putting unlimited powers in the hands of the people whom he is least inclined to trust with their use. It is perfectly clear also that to disseminate information about a weapon in the present state of our civilization is to make it practically certain that that weapon will be used. [...]</blockquote><blockquote>If therefore I do not desire to participate in the bombing or poisoning of defenseless peoples-and I most certainly do not-I must take a serious responsibility as to those to whom I disclose my scientific ideas. Since it is obvious that with sufficient effort you can obtain my material, even though it is out of print, I can only protest pro forma in refusing to give you any information concerning my past work. However, I rejoice at the fact that my material is not readily available, inasmuch as it gives me the opportunity to raise this serious moral issue. I do not expect to publish any future work of mine which may do damage in the hands of irresponsible militarists.</blockquote><blockquote>I am taking the liberty of calling this letter to the attention of other people in scientific work. I believe it is only proper that they should know of it in order to make their own independent decisions, if similar situations should confront them.</blockquote><p><br/></p><h2><strong>Recombinant DNA</strong></h2><p>For a large part of history, scientists\u2019 largest ethical concerns came from direct military applications of their inventions. While any invention could lead to unintended societal or environmental consequences, for the most part researchers who worked on peaceful technologies didn\u2019t need to be too concerned with their work being dangerous by itself. But as biological and medical research obtained the capability to modify genes and bacteria, it would open up the possibility of unintentionally creating dangerous infectious diseases. In theory, these could be even more dangerous than nuclear weapons - an a-bomb dropped on a city might destroy most of that city, but a single bacteria could give rise to an epidemic infecting people all around the world.<br/><br/>Recombinant DNA techniques involve taking DNA from one source and then introducing it to another kind of organism, causing the new genes to express themselves in the target organism. One of the pioneers of this technique was <strong>Paul Berg</strong> (1926-), who in 1972 had already carried out the preparations for creating a strain of <em>E. coli</em> that contained the genome for a human-infectious virus (SV40) with tentative links to cancer. <strong>Robert Pollack</strong> (1920-) heard news of this experiment and helped convince Berg to halt it - both were concerned about the danger that this new strain would spread to humans in the lab and become a pathogen. Berg then became a major voice calling for more attention to the risks of such research as well as a temporary moratorium. This eventually led to two conferences in Asilomar, with 140 experts participating in the later 1975 one to decide upon guidelines for recombinant DNA research.<br/><br/>Berg and Pollack were far from the only scientists to call attention to the safety concerns of recombinant DNA. Several other scientists contributed, asking for more safety and voicing concern about a technology that could bring harm if misused. <br/><br/>Among them, the molecular biologist <strong>Maxine Singer </strong>(1931-) chaired the 1973 Gordon Conference on Nucleic Acids, in which some of the dangers of the technique were discussed. After the conference, she and several other similarly concerned scientists authored a letter to the President of the National Academy of Science and the President of the Institutes of Health. The letter suggested that a study committee be established to study the risks behind the new recombinant DNA technology, and propose specific actions or guidelines if necessary. She also helped organize the Asilomar Conference in 1975. </p><h2><strong>Informatics</strong></h2><blockquote>But if we are downloaded into our technology, what are the chances that we will thereafter be ourselves or even human? It seems to me far more likely that a robotic existence would not be like a human one in any sense that we understand, that the robots would in no sense be our children, that on this path our humanity may well be lost.</blockquote><p>-- Bill Joy, <u><a href=\"http://www.wired.com/wired/archive/8.04/joy.html\">Why the Future Doesn\u2019t Need Us</a></u>.</p><p><br/>Finally, we come to the topic of information technology and artificial intelligence. As AI systems grow increasingly autonomous, they might become the ultimate example of a technology that seems initially innocuous but ends up capable of doing great damage. Especially if they were to become capable of rapid self-improvement, <u><a href=\"http://intelligence.org/summary/\">they could lead to humanity going extinct</a></u>.<br/><br/>In addition to refusing to help military research, <strong>Norbert Wiener </strong>was also concerned about the effects of automation. In 1949, General Electric wanted him to advise its managers on automaton matters and to teach automation methods to its engineers. Wiener refused these requests, believing that they would further a development which would lead to human workers becoming unemployed and replaced by machines. He thus expanded his boycott of the military to also be a boycott of corporations that he thought acted unethically.<br/><br/>Wiener was also concerned about the risks of autonomous AI. In 1960, Science published his paper &quot;<u><a href=\"http://www.itu.dk/people/cmmm/Wiener.pdf\">Some Moral and Technical Consequences of Automation</a></u>&quot;, in which he spoke at length about the dangers of machine intelligence. He warned that machines might act far too fast for humans to correct their mistakes, and that like genies in stories, they could fulfill the letter of our requests without caring about their spirit. He also discussed such worries elsewhere.</p><blockquote>If we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently interfere once we have started it, because the action is so fast and irrevocable that we have not the data to intervene before the action is complete, then we had better be quite sure that the purpose put into the machine is the purpose which we really desire and not merely a colorful imitation of it.</blockquote><br/><p>Such worries would continue to bother other computer scientists as well, many decades after Wiener\u2019s death. <strong>Bill Joy </strong>(1954-) is known for having played a major role in the development of <u><a href=\"http://en.wikipedia.org/wiki/BSD_UNIX\">BSD Unix</a></u>, having authored the <u><a href=\"http://en.wikipedia.org/wiki/Vi\">vi</a></u> text editor, and being the co-founder of Sun Microsystems. He became concerned about the effects of AI in 1998, when he met Ray Kurzweil at a conference where they were both speakers. Kurzweil gave Joy a preprint of his then-upcoming book, <em>The Age of Spiritual Machines, </em>and Joy found himself concerned over its discussion about the risks of AI. Reading Hans Moravec\u2019s book <em>Robot: Mere Machine to Transcendent Mind </em>exacerbated Joy\u2019s worries, as did several other books which he found around the same time. He began to wonder whether all of his work in the field of information technology and computing had been preparing the way for a world where machines would replace humans.<br/><br/>In 2000, Joy wrote a widely-read article titled <u><a href=\"http://www.wired.com/wired/archive/8.04/joy.html\">Why the Future Doesn\u2019t Need Us</a></u> for Wired, talking about the dangers of AI as well as genetic engineering and nanotechnology. In the article, he called to limit the development of technologies which he felt were too dangerous. Since then, he has continued to be active in promoting responsible technology research. In 2005, an <u><a href=\"http://www.nytimes.com/2005/10/17/opinion/17kurzweiljoy.html\">op-ed co-authored by Joy and Ray Kurzweil</a></u> was published in the New York Times, arguing that the decision to publish the genome of the 1918 influenza virus on the Internet had been a mistake. <br/><br/>Joy also attempted to write a book on the topic, but <u><a href=\"http://www.siliconbeat.com/entries/2005/01/18/bill_joy_joins_kleiner_perkins.html\">then became convinced </a></u>that he could achieve more by working on science and technology investment. In 2005, he joined the venture capital firm <u><a href=\"http://en.wikipedia.org/wiki/Kleiner_Perkins\">Kleiner Perkins Caufield &amp; Byers</a></u> as a partner, and he has been <u><a href=\"http://news.cnet.com/8301-13860_3-20005814-56.html\">focused on investments in green technology</a></u>. </p><h2><strong>Conclusion</strong></h2><p>Technology&#x27;s potential for destruction will only continue to grow, but many of the social norms of science were established under the assumption that scientists don\u2019t need to worry much about how the results of their work are used. Hopefully, the examples provided in this post can encourage more researchers to consider the broader consequences of their work.</p><h2><strong>Sources used</strong></h2><p>This article was written based on research done by Vincent Fagot. The sources listed below are <em>in addition</em> to any that are already linked from the text.<br/><br/><strong>Leonardo da Vinci: </strong></p><ul><li>\u201cThe Notebooks of Leonardo da Vinci\u201d vol 1,  by Edward Mac Curdy (1905 edition)</li><li><u><a href=\"http://books.google.com/books?id=SwoAAAAAMBAJ&pg=PA177&lpg=PA135&hl=nl&redir_esc=y#v=onepage&q&f=false\">\u201cThe scientist\u2019s conscience : historical considerations\u201d in Bulletin of the Atomic Scientists - May 1956 - Page 177</a></u></li></ul><p><br/><strong>John Napier:</strong></p><ul><li><strong><u><a href=\"http://books.google.com/books?id=SwoAAAAAMBAJ&pg=PA177&lpg=PA135&hl=nl&redir_esc=y#v=onepage&q&f=false\">\u201cThe scientist\u2019s conscience : historical considerations\u201d in Bulletin of the Atomic Scientists - May 1956 - Page 177</a></u>.</strong></li><li><strong><u><a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.1989.tb15050.x/abstract\">Rosemary Chalk : Drawing the Line An Examination of Conscientious Objection in Science</a></u>.</strong></li></ul><p><br/><strong>Robert Boyle:</strong></p><ul><li><u><a href=\"http://www.barnesandnoble.com/w/secrets-and-knowledge-in-medicine-and-science-1500-1800-elaine-leong/1100661379\">Secrets and Knowledge in Medicine and Science, 1500-1800 by Elaine Leong and Alisha Rankin, pp 87-104</a></u></li><li><u><a href=\"http://archive.org/details/dictionaryofnati06stepuoft\">Dictionary of National Biography - volume 06, 1886 edition, &quot;Robert Boyle&quot; entry around pp 118-123</a></u></li><li><u><a href=\"http://www.amazon.com/Robert-Boyle-Reconsidered-Michael-Hunter/dp/0521892678\">Robert Boyle Reconsidered by Michael Hunter</a></u></li></ul><p><br/><strong>Clara Immerwahr:</strong></p><ul><li><u><a href=\"http://www.amazon.com/Making-Atomic-Bomb-Richard-Rhodes/dp/0684813785\">Rhodes : The Making of the Atomic Bomb</a></u></li><li><u><a href=\"http://www.amazon.com/European-Women-Chemistry-Jan-Apotheker/dp/3527329560\">Jan Apotheker, Livia Simon Sarkadi and Nicole J. Moreau : European Women in Chemistry</a></u></li><li><u><a href=\"http://www.amazon.com/Hitlers-Scientists-Science-Devils-Pact/dp/0142004804\">John Cornwell : Hitler&#x27;s Scientists: Science, War, and the Devil&#x27;s Pact</a></u></li></ul><p><br/><strong>Lewis Fry Richardson:</strong></p><ul><li><u><a href=\"http://www.amazon.co.uk/Pleasures-Counting-T-W-K%C3%B6rner/dp/0521568234\">T. W. K\u00f6rner : The Pleasures of Counting</a></u></li><li><u><a href=\"http://books.google.com/books?id=SwoAAAAAMBAJ&pg=PA177&lpg=PA135&hl=nl&redir_esc=y#v=onepage&q&f=false\">\u201cThe scientist\u2019s conscience : historical considerations\u201d in Bulletin of the Atomic Scientists - May 1956 - Page 177</a></u></li></ul><p><br/><strong>Arthur Galston:</strong></p><ul><li><u><a href=\"http://www.plantphysiol.org/content/128/3/786.full#ref-4\">Galston A. : An Accidental Plant Biologist, Plant Physiology March 2002  vol. 128 no. 3</a></u></li><li>\u201c<u><a href=\"http://beck2.med.harvard.edu/week13/Galston.pdf\">Science and Social Responsibility: A Case History.\u201d Annals of the New York Academy of Sciences. Vol 196, Article 4</a></u></li></ul><p><br/><strong>Le\u00f3 Szil\u00e1rd:</strong></p><ul><li><u><a href=\"http://www.amazon.com/Making-Atomic-Bomb-Richard-Rhodes/dp/0684813785\">Rhodes : The Making of the Atomic Bomb</a></u></li><li><u><a href=\"http://www.amazon.com/Nuclear-Biological-Chemical-Warfare-Bushan/dp/8176483125\">Bhushan K, Katyal G : Nuclear Biological &amp; Chemical Warfare</a></u></li></ul><p><br/><strong>Joseph Rotblat:</strong></p><ul><li>Bulletin of the atomic scientists, august 1985 : Leaving the Bomb Project by Joseph Rotblat</li><li><u><a href=\"http://www.amazon.com/Keeper-Nuclear-Conscience-Joseph-Rotblat/dp/0199586586/ref=sr_1_1?ie=UTF8&qid=1355022838&sr=8-1&keywords=rotblat\">Keeper of the Nuclear Conscience: The Life and Work of Joseph Rotblat</a></u></li><li><u><a href=\"http://sounds.bl.uk/Oral-history/Science/021M-C0464X0017XX-0700V0\">1999 voice record interview of Joseph Rotblat</a></u></li><li>Deriving an Ethical Code for Scientists: An Interview With Joseph Rotblat</li></ul><p><br/><strong>Norbert Wiener:</strong></p><ul><li><u><a href=\"http://www.amazon.com/Postmodern-War-New-Politics-Conflict/dp/1572301767\">Postmodern War: The New Politics of Conflict by Chris Hables Gray</a></u> (<u><a href=\"http://www.chrishablesgray.org/postmodernwar/\">available online</a></u>)</li><li>Bulletin of the Atomic Scientists - May 1956 - Page 178  : \u201cThe scientist\u2019s conscience : historical considerations</li><li><u><a href=\"http://www.amazon.com/Dark-Hero-Information-Age-Cybernetics/dp/0465013716\">Dark Hero of the Information Age: In Search of Norbert Wiener The Father of Cybernetics by Flo Conway</a></u></li><li>Bulletin of the Atomic Scientists - Jan 1947 - Page 31 : &quot;A Scientist Rebels&quot;</li><li>Bulletin of the Atomic Scientists - Feb 1956 - Page 53  : &quot;Moral Reflections of a Mathematician&quot;</li><li>Bulletin of the Atomic Scientists - Nov 1948 - Page 338 :  &quot;A Rebellious Scientist After Two Years&quot;</li><li><u><a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.1989.tb15050.x/abstract\">Rosemary Chalk : Drawing the Line An Examination of Conscientious Objection in Science</a></u></li><li><u><a href=\"http://www.nyu.edu/projects/nissenbaum/papers/Wiener.pdf\">Some Moral and Technical Consequences of Automation</a></u> in Science, 6 may 1960</li></ul><p><br/><strong>Paul Berg, Maxine Singer, Robert Pollack:</strong></p><ul><li>P. Berg and M. F. Singer : The recombinant DNA controversy: twenty years later, Proc Natl Acad Sci U S A. 1995 September 26</li><li>Potential Biohazards of Recombinant DNA Molecules by Paul Berg, 1974</li><li>Guidelines for DNA Hybrid Molecules by Maxine Singer, 1973</li><li>Biomedical Politics by Kathi E. Hanna (chapter : Asilomar and recombinant DNA by Donald S. Fredrickson), 1991</li><li><u><a href=\"http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1023&context=edethicsinscience\">Asilomar Conference on Laboratory Precautions When Conducting Recombinant DNA Research \u2013 Case Summary</a></u></li><li><u><a href=\"http://books.google.com/books/about/Report_Assembly_of_Life_Sciences_Nationa.html?id=SVYrAAAAYAAJ\">Report - Assembly of Life Sciences, National Research Council</a></u></li><li>P. Berg: <u><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC388511/?page=1\">Potential Biohazards of Recombinant DNA Molecules</a></u></li><li><u><a href=\"http://books.google.com/books/about/Watson_and_DNA.html?id=gUkBMctzM2gC\">Watson and DNA: Making a Scientific Revolution</a></u></li></ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bY5MaF2EATwDkomvu": 2, "7w6XkYe5YPx9YL59j": 2, "ZpG9rheyAkgCoEQea": 2, "Xw6pxiicjuv6NJWjf": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hxaq9MCaSrwWPmooZ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 79, "baseScore": 103, "extendedScore": null, "score": 0.00024, "legacy": true, "legacyId": "21515", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 104, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>For the first time in history, it has become possible for a limited group of a few thousand people to threaten the absolute destruction of millions. </blockquote><p>-- Norbert Wiener (1956), <u><a href=\"http://books.google.com/books?id=CwoAAAAAMBAJ&amp;pg=PA53&amp;source=gbs_toc_r&amp;cad=2\">Moral Reflections of a Mathematician</a></u>.</p><p><br>Today, the general attitude towards scientific discovery is that scientists are not themselves responsible for how their work is used. For someone who is interested in science for its own sake, or even for someone who mostly considers research to be a way to pay the bills, this is a tempting attitude. It would be easy to only focus on one\u2019s work, and leave it up to others to decide what to do with it.<br><br>But this is not necessarily the attitude that we should encourage. As technology becomes more powerful, it also becomes more dangerous. Throughout history, many scientists and inventors have recognized this, and taken different kinds of action to help ensure that their work will have beneficial consequences. Here are some of them. </p><p>This post is not arguing that any specific approach for taking responsibility for one's actions is the correct one. Some researchers hid their work, others refocused on other fields, still others began active campaigns to change the way their work was being used. It is up to the reader to decide which of these approaches were successful and worth emulating, and which ones were not.<br></p><h2 id=\"Pre_industrial_inventors\"><strong>Pre-industrial inventors</strong></h2><blockquote>\u2026 I do not publish nor divulge [methods of building submarines] by reason of the evil nature of men who would use them as means of destruction at the bottom of the sea, by sending ships to the bottom, and sinking them together with the men in them.</blockquote><p>-- <u><a href=\"http://books.google.com/books?id=A7dUhbBfmzMC&amp;pg=PA274&amp;lpg=PA274&amp;dq=this+i+do+not+divulge+account+evil+nature+men+da+vinci&amp;source=bl&amp;ots=dm71gSZ-Yd&amp;sig=9AUz3KcgO0fTiCz3FcwmkYxxKt8&amp;hl=fi&amp;sa=X&amp;ei=p5IHUbjqK6ji4QSX9YHgCA&amp;ved=0CE0Q6AEwAw#v=onepage&amp;q&amp;f=false\">Leonardo da Vinci</a></u></p><p><br>People did not always think that the benefits of freely disseminating knowledge outweighed the harms. O.T. Benfey, <u><a href=\"http://books.google.com/books?id=SwoAAAAAMBAJ&amp;pg=PA177\">writing in</a></u> a 1956 issue of the <em>Bulletin of the Atomic Scientists</em>, cites F.S. Taylor\u2019s book on early alchemists:</p><blockquote>Alchemy was certainly intended to be useful .... But [the alchemist] never proposes the <em>public</em> use of such things, the disclosing of his knowledge for the benefit of man. \u2026. Any disclosure of the alchemical secret was felt to be profoundly wrong, and likely to bring immediate punishment from on high. The reason generally given for such secrecy was the probable abuse by wicked men of the power that the alchemical would give \u2026. The alchemists, indeed, felt a strong moral responsibility that is not always acknowledged by the scientists of today.</blockquote><p><br>With the Renaissance, science began to be viewed as public property, but many scientists remained cautious about the way in which their work might be used. Although he held the office of military engineer, <strong>Leonardo da Vinci </strong>(1452-1519) drew a distinction between offensive and defensive warfare, and emphasized the role of good defenses in protecting people\u2019s liberty from tyrants. He described war as \u2018bestialissima pazzia\u2019 (most bestial madness), and wrote that \u2018it is an infinitely atrocious thing to take away the life of a man\u2019. One of the clearest examples of his reluctance to unleash dangerous inventions was his refusal to publish the details of his plans for submarines.<br><br>Later Renaissance thinkers continued to be concerned with the potential uses of their discoveries. <strong>John Napier </strong>(1550-1617), the inventor of logarithms, also experimented with a new form of artillery. Upon seeing its destructive power, he decided to keep its details a secret, and even spoke from his deathbed against the creation of new kinds of weapons.<br><br>But only concealing one discovery pales in comparison to the likes of <strong>Robert Boyle </strong>(1627-1691). A pioneer of physics and chemistry and possibly the most famous for describing and publishing <u><a href=\"http://en.wikipedia.org/wiki/Boyle%27s_law\">Boyle\u2019s law</a></u>, he sought to make humanity better off, taking an interest in things such as improved agricultural methods as well as better medicine. In his studies, he also discovered knowledge and made inventions related to a variety of potentially harmful subjects, including poisons, invisible ink, counterfeit money, explosives, and kinetic weaponry. These \u2018my love of Mankind has oblig\u2019d me to conceal, even from my nearest Friends\u2019.</p><p><strong id=\"Chemical_warfare\">Chemical warfare</strong></p><p>By the early twentieth century, people had began looking at science in an increasingly optimistic light: it was believed that science would not only continue to improve everyone\u2019s prosperity, but also make war outright impossible. But as science became more sophisticated, it would also become possible to cause ever more harm with ever smaller resources. One of the early indications of science\u2019s ability to do harm came from advances in chemical warfare, and World War I <u><a href=\"http://en.wikipedia.org/wiki/Poison_gas_in_World_War_I\">saw the deployment</a></u> of chlorine, phosgene, and mustard gas as weapons. It should not be surprising, then, that some scientists in related fields began growing concerned.  But unlike earlier inventors, at least three of them did far more than just refuse to publish their work.<br><br><strong>Clara Immerwahr</strong> (1870-1915) was a German chemist and the first woman to obtain a PhD from the University of Breslau. She was strongly opposed to the use of chemical weapons. Married to Fritz Haber, \u2018the father of chemical warfare\u2019, she unsuccessfully attempted many times to convince her husband to abandon his work. Immerwahr was generally depressed and miserable over the fact that society considered a married woman\u2019s place to be at home, denying her the opportunity to do science. In the end, after her efforts to dissuade her husband from working on chemical warfare had failed and Fritz had personally overseen <u><a href=\"http://en.wikipedia.org/wiki/Second_Battle_of_Ypres\">the first major use of chlorine</a></u>, she committed suicide by shooting herself in the heart. <br><br>Poison gas also concerned scientists in other disciplines. <strong>Lewis Fry Richardson </strong>(1881-1953) was a mathematician and meteorologist. During the World War II, the military became interested in his work on turbulence and gas mixing, and attempted to recruit him to do help them do work on modeling the best ways of using poison gas. Realizing what his work was being used for, Richardson abandoned meteorology entirely and destroyed his unpublished research. Instead, he turned his research to investigating the causes of war, attempting to find ways to reduce the risk of armed conflict. He spent the rest of his life devoted to this topic, and is today considered one of the founders of the scientific analysis of conflict.<br><br><strong>Arthur Galston </strong>(1920-2008), a botanist, was also concerned with the military use of his inventions. Building upon his work, the US military developed Agent Orange, a chemical weapon which was deployed in the Vietnam War. Upon discovering what his work had been used for, he began to campaign against its use, and together with a number of others finally convinced President Nixon to order an end to its spraying in 1970. Reflecting upon the matter, Galston<u><a href=\"http://beck2.med.harvard.edu/week13/Galston.pdf\"> wrote</a></u>:</p><blockquote>I used to think that one could avoid involvement in the antisocial consequences of science simply by not working on any project that might be turned to evil or destructive  ends. I have learned that things are not all that simple, and that almost any scientific finding can be perverted or twisted under appropriate societal pressures. In my view, the only recourse for a scientist concerned about the social consequences of his work is to remain involved with it to the end. His responsibility to society does not cease with publication of a definitive scientific paper. Rather, if his discovery is translated into some impact on the world outside the laboratory, he will, in most instances, want to follow through to see that it is used for constructive rather than anti-human purposes.</blockquote><p><br>After retiring in 1990, he founded the Interdisciplinary Center for Bioethics at Yale, where he also taught bioethics to undergraduates.</p><h2 id=\"Nuclear_weapons\"><strong>Nuclear weapons</strong></h2><p>While chemical weapons are capable of inflicting serious injuries as well as birth defects on large numbers of people, they have never been viewed to be as dangerous as nuclear weapons. As physicists became capable of creating weapons of unparalleled destructive power, they also began growing ever more concerned about the consequences of their work.<br><br><strong>Le\u00f3 Szil\u00e1rd </strong>(1898-1964) was one of the first people to envision nuclear weapons, and was granted a patent for the nuclear chain reaction in 1934. Two years later, he grew worried that Nazi scientists would find his patents and use them to create weapons, so he asked the British Patent Office to withdraw his patents and secretly reassign them to the Royal Navy. His fear of Nazi Germany developing nuclear weapons also made him instrumental in making the USA initiate the Manhattan Project, as he and two other scientists wrote the <a href=\"http://en.wikipedia.org/wiki/Einstein%E2%80%93Szil%C3%A1rd_letter\">Einstein-Szil\u00e1rd letter</a> that advised President Roosevelt of the need to develop the same technology. But in 1945, he learned that the atomic bomb was about to be used on Japan, despite it being certain that neither Germany nor Japan had one. He then did his best to stop them from being used and started a <u><a href=\"http://www.dannen.com/decision/45-07-17.html\">petition against using </a></u>them, with little success.<br><br>After the war, he no longer wanted to contribute to the creation of weapons and changed fields to molecular biology. In 1962, he founded the <u><a href=\"http://en.wikipedia.org/wiki/Council_for_a_Livable_World\">Council for a Livable World</a></u>, which aimed to warn people about the dangers of nuclear war and to promote a policy of arms control. The Council continues its work even today.<br><br>Another physicist who worked on the atomic bomb due to a fear of it being developed by Nazi Germany was <strong>Joseph Rotblat</strong> (1908-2005), who felt that the Allies also having an atomic bomb would deter the Axis from using one. But he gradually began to realize that Nazi Germany would likely never develop the atomic bomb, destroying his initial argument for working on it. He also came to realize that the bomb continued to be under active development due to reasons that he felt were unethical. In conversation, General Leslie Groves mentioned that the real purpose of the bomb was to subdue the USSR. Rotblat was shocked to hear this, especially given that the Soviet Union was at the time an ally in the war effort. In 1944, it became apparent that Germany would not develop the atomic bomb. As a result, Rotblat asked for permission to leave the project, and was granted it.<br><br>Afterwards, Rotblat regretted his role in developing nuclear weapons. He believed that the logic of nuclear deterrence was flawed, since he thought that if Hitler had possessed an atomic bomb, then Hitler\u2019s last order would have been to use it against London regardless of the consequences. Rotblat decided to do whatever he could to prevent the future use and deployment of nuclear weapons, and proposed a worldwide moratorium on such research until humanity was wise enough to use it without risks. He decided to repurpose his career into something more useful for humanity, and began studying and teaching the application of nuclear physics into medicine, becoming a professor at the Medical College of St Bartholomew\u2019s Hospital in London.<br><br>Rotblat worked together with Bertrand Russell to limit the spread of nuclear weapons, and the two collaborated with a number of other scientists to issue the <u><a href=\"http://en.wikipedia.org/wiki/Russell-Einstein_Manifesto\">Russell-Einstein Manifesto</a></u> in 1955, calling the governments of the world to take action to prevent nuclear weapons from doing more damage. The manifesto led to the establishment of the <u><a href=\"http://en.wikipedia.org/wiki/Pugwash_Conferences_on_Science_and_World_Affairs\">Pugwash Conferences</a></u>, in which nuclear scientists from both the West and the East met each other. By facilitating dialogue between the two sides of the Cold War, these conferences <u><a href=\"http://www.irwinabrams.com/books/excerpts/annual95.html\">helped lead to</a></u> several arms control agreements, such as the <u><a href=\"http://en.wikipedia.org/wiki/Partial_Test_Ban_Treaty\">Partial Test Ban Treaty</a></u> of 1963 and the <u><a href=\"http://en.wikipedia.org/wiki/Non-Proliferation_Treaty\">Non-Proliferation Treaty</a></u> of 1968. In 1995, Rotblat and the Pugwash Conferences were <u><a href=\"http://www.nobelprize.org/nobel_prizes/peace/laureates/1995/index.html\">awarded the Nobel Peace Prize</a></u> \u201cfor their efforts to diminish the part played by nuclear arms in international politics and, in the longer run, to eliminate such arms\u201d.<br><br>The development of nuclear weapons also affected <strong>Norbert Wiener</strong> (1894-1964), professor of mathematics at the Massachusetts Institute of Technology and the originator of the field of <u><a href=\"http://en.wikipedia.org/wiki/Cybernetics\">cybernetics</a></u>. After the Hiroshima bombing, a researcher working for a major aircraft corporation requested a copy of an earlier paper of Wiener\u2019s. Wiener refused to provide it, and sent <em>Atlantic Monthly </em>a copy of his <u><a href=\"http://connection.ebscohost.com/c/letters/20836656/scientist-rebels\">response to the researcher</a></u>, in which he declared his refusal to share his research with anyone who would use it for military purposes.</p><blockquote>In the past, the community of scholars has made it a custom to furnish scientific information to any person seriously seeking it. However, we must face these facts: The policy of the government itself during and after the war, say in the bombing of Hiroshima and Nagasaki, has made it clear that to provide scientific information is not a necessarily innocent act, and may entail the gravest consequences. One therefore cannot escape reconsidering the established custom of the scientist to give information to every person who may inquire of him. The interchange of ideas, one of the great traditions of science, must of course receive certain limitations when the scientist becomes an arbiter of life and death. [...]</blockquote><blockquote>The experience of the scientists who have worked on the atomic bomb has indicated that in any investigation of this kind the scientist ends by putting unlimited powers in the hands of the people whom he is least inclined to trust with their use. It is perfectly clear also that to disseminate information about a weapon in the present state of our civilization is to make it practically certain that that weapon will be used. [...]</blockquote><blockquote>If therefore I do not desire to participate in the bombing or poisoning of defenseless peoples-and I most certainly do not-I must take a serious responsibility as to those to whom I disclose my scientific ideas. Since it is obvious that with sufficient effort you can obtain my material, even though it is out of print, I can only protest pro forma in refusing to give you any information concerning my past work. However, I rejoice at the fact that my material is not readily available, inasmuch as it gives me the opportunity to raise this serious moral issue. I do not expect to publish any future work of mine which may do damage in the hands of irresponsible militarists.</blockquote><blockquote>I am taking the liberty of calling this letter to the attention of other people in scientific work. I believe it is only proper that they should know of it in order to make their own independent decisions, if similar situations should confront them.</blockquote><p><br></p><h2 id=\"Recombinant_DNA\"><strong>Recombinant DNA</strong></h2><p>For a large part of history, scientists\u2019 largest ethical concerns came from direct military applications of their inventions. While any invention could lead to unintended societal or environmental consequences, for the most part researchers who worked on peaceful technologies didn\u2019t need to be too concerned with their work being dangerous by itself. But as biological and medical research obtained the capability to modify genes and bacteria, it would open up the possibility of unintentionally creating dangerous infectious diseases. In theory, these could be even more dangerous than nuclear weapons - an a-bomb dropped on a city might destroy most of that city, but a single bacteria could give rise to an epidemic infecting people all around the world.<br><br>Recombinant DNA techniques involve taking DNA from one source and then introducing it to another kind of organism, causing the new genes to express themselves in the target organism. One of the pioneers of this technique was <strong>Paul Berg</strong> (1926-), who in 1972 had already carried out the preparations for creating a strain of <em>E. coli</em> that contained the genome for a human-infectious virus (SV40) with tentative links to cancer. <strong>Robert Pollack</strong> (1920-) heard news of this experiment and helped convince Berg to halt it - both were concerned about the danger that this new strain would spread to humans in the lab and become a pathogen. Berg then became a major voice calling for more attention to the risks of such research as well as a temporary moratorium. This eventually led to two conferences in Asilomar, with 140 experts participating in the later 1975 one to decide upon guidelines for recombinant DNA research.<br><br>Berg and Pollack were far from the only scientists to call attention to the safety concerns of recombinant DNA. Several other scientists contributed, asking for more safety and voicing concern about a technology that could bring harm if misused. <br><br>Among them, the molecular biologist <strong>Maxine Singer </strong>(1931-) chaired the 1973 Gordon Conference on Nucleic Acids, in which some of the dangers of the technique were discussed. After the conference, she and several other similarly concerned scientists authored a letter to the President of the National Academy of Science and the President of the Institutes of Health. The letter suggested that a study committee be established to study the risks behind the new recombinant DNA technology, and propose specific actions or guidelines if necessary. She also helped organize the Asilomar Conference in 1975. </p><h2 id=\"Informatics\"><strong>Informatics</strong></h2><blockquote>But if we are downloaded into our technology, what are the chances that we will thereafter be ourselves or even human? It seems to me far more likely that a robotic existence would not be like a human one in any sense that we understand, that the robots would in no sense be our children, that on this path our humanity may well be lost.</blockquote><p>-- Bill Joy, <u><a href=\"http://www.wired.com/wired/archive/8.04/joy.html\">Why the Future Doesn\u2019t Need Us</a></u>.</p><p><br>Finally, we come to the topic of information technology and artificial intelligence. As AI systems grow increasingly autonomous, they might become the ultimate example of a technology that seems initially innocuous but ends up capable of doing great damage. Especially if they were to become capable of rapid self-improvement, <u><a href=\"http://intelligence.org/summary/\">they could lead to humanity going extinct</a></u>.<br><br>In addition to refusing to help military research, <strong>Norbert Wiener </strong>was also concerned about the effects of automation. In 1949, General Electric wanted him to advise its managers on automaton matters and to teach automation methods to its engineers. Wiener refused these requests, believing that they would further a development which would lead to human workers becoming unemployed and replaced by machines. He thus expanded his boycott of the military to also be a boycott of corporations that he thought acted unethically.<br><br>Wiener was also concerned about the risks of autonomous AI. In 1960, Science published his paper \"<u><a href=\"http://www.itu.dk/people/cmmm/Wiener.pdf\">Some Moral and Technical Consequences of Automation</a></u>\", in which he spoke at length about the dangers of machine intelligence. He warned that machines might act far too fast for humans to correct their mistakes, and that like genies in stories, they could fulfill the letter of our requests without caring about their spirit. He also discussed such worries elsewhere.</p><blockquote>If we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently interfere once we have started it, because the action is so fast and irrevocable that we have not the data to intervene before the action is complete, then we had better be quite sure that the purpose put into the machine is the purpose which we really desire and not merely a colorful imitation of it.</blockquote><br><p>Such worries would continue to bother other computer scientists as well, many decades after Wiener\u2019s death. <strong>Bill Joy </strong>(1954-) is known for having played a major role in the development of <u><a href=\"http://en.wikipedia.org/wiki/BSD_UNIX\">BSD Unix</a></u>, having authored the <u><a href=\"http://en.wikipedia.org/wiki/Vi\">vi</a></u> text editor, and being the co-founder of Sun Microsystems. He became concerned about the effects of AI in 1998, when he met Ray Kurzweil at a conference where they were both speakers. Kurzweil gave Joy a preprint of his then-upcoming book, <em>The Age of Spiritual Machines, </em>and Joy found himself concerned over its discussion about the risks of AI. Reading Hans Moravec\u2019s book <em>Robot: Mere Machine to Transcendent Mind </em>exacerbated Joy\u2019s worries, as did several other books which he found around the same time. He began to wonder whether all of his work in the field of information technology and computing had been preparing the way for a world where machines would replace humans.<br><br>In 2000, Joy wrote a widely-read article titled <u><a href=\"http://www.wired.com/wired/archive/8.04/joy.html\">Why the Future Doesn\u2019t Need Us</a></u> for Wired, talking about the dangers of AI as well as genetic engineering and nanotechnology. In the article, he called to limit the development of technologies which he felt were too dangerous. Since then, he has continued to be active in promoting responsible technology research. In 2005, an <u><a href=\"http://www.nytimes.com/2005/10/17/opinion/17kurzweiljoy.html\">op-ed co-authored by Joy and Ray Kurzweil</a></u> was published in the New York Times, arguing that the decision to publish the genome of the 1918 influenza virus on the Internet had been a mistake. <br><br>Joy also attempted to write a book on the topic, but <u><a href=\"http://www.siliconbeat.com/entries/2005/01/18/bill_joy_joins_kleiner_perkins.html\">then became convinced </a></u>that he could achieve more by working on science and technology investment. In 2005, he joined the venture capital firm <u><a href=\"http://en.wikipedia.org/wiki/Kleiner_Perkins\">Kleiner Perkins Caufield &amp; Byers</a></u> as a partner, and he has been <u><a href=\"http://news.cnet.com/8301-13860_3-20005814-56.html\">focused on investments in green technology</a></u>. </p><h2 id=\"Conclusion\"><strong>Conclusion</strong></h2><p>Technology's potential for destruction will only continue to grow, but many of the social norms of science were established under the assumption that scientists don\u2019t need to worry much about how the results of their work are used. Hopefully, the examples provided in this post can encourage more researchers to consider the broader consequences of their work.</p><h2 id=\"Sources_used\"><strong>Sources used</strong></h2><p>This article was written based on research done by Vincent Fagot. The sources listed below are <em>in addition</em> to any that are already linked from the text.<br><br><strong>Leonardo da Vinci: </strong></p><ul><li>\u201cThe Notebooks of Leonardo da Vinci\u201d vol 1,  by Edward Mac Curdy (1905 edition)</li><li><u><a href=\"http://books.google.com/books?id=SwoAAAAAMBAJ&amp;pg=PA177&amp;lpg=PA135&amp;hl=nl&amp;redir_esc=y#v=onepage&amp;q&amp;f=false\">\u201cThe scientist\u2019s conscience : historical considerations\u201d in Bulletin of the Atomic Scientists - May 1956 - Page 177</a></u></li></ul><p><br><strong>John Napier:</strong></p><ul><li><strong><u><a href=\"http://books.google.com/books?id=SwoAAAAAMBAJ&amp;pg=PA177&amp;lpg=PA135&amp;hl=nl&amp;redir_esc=y#v=onepage&amp;q&amp;f=false\">\u201cThe scientist\u2019s conscience : historical considerations\u201d in Bulletin of the Atomic Scientists - May 1956 - Page 177</a></u>.</strong></li><li><strong><u><a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.1989.tb15050.x/abstract\">Rosemary Chalk : Drawing the Line An Examination of Conscientious Objection in Science</a></u>.</strong></li></ul><p><br><strong>Robert Boyle:</strong></p><ul><li><u><a href=\"http://www.barnesandnoble.com/w/secrets-and-knowledge-in-medicine-and-science-1500-1800-elaine-leong/1100661379\">Secrets and Knowledge in Medicine and Science, 1500-1800 by Elaine Leong and Alisha Rankin, pp 87-104</a></u></li><li><u><a href=\"http://archive.org/details/dictionaryofnati06stepuoft\">Dictionary of National Biography - volume 06, 1886 edition, \"Robert Boyle\" entry around pp 118-123</a></u></li><li><u><a href=\"http://www.amazon.com/Robert-Boyle-Reconsidered-Michael-Hunter/dp/0521892678\">Robert Boyle Reconsidered by Michael Hunter</a></u></li></ul><p><br><strong>Clara Immerwahr:</strong></p><ul><li><u><a href=\"http://www.amazon.com/Making-Atomic-Bomb-Richard-Rhodes/dp/0684813785\">Rhodes : The Making of the Atomic Bomb</a></u></li><li><u><a href=\"http://www.amazon.com/European-Women-Chemistry-Jan-Apotheker/dp/3527329560\">Jan Apotheker, Livia Simon Sarkadi and Nicole J. Moreau : European Women in Chemistry</a></u></li><li><u><a href=\"http://www.amazon.com/Hitlers-Scientists-Science-Devils-Pact/dp/0142004804\">John Cornwell : Hitler's Scientists: Science, War, and the Devil's Pact</a></u></li></ul><p><br><strong>Lewis Fry Richardson:</strong></p><ul><li><u><a href=\"http://www.amazon.co.uk/Pleasures-Counting-T-W-K%C3%B6rner/dp/0521568234\">T. W. K\u00f6rner : The Pleasures of Counting</a></u></li><li><u><a href=\"http://books.google.com/books?id=SwoAAAAAMBAJ&amp;pg=PA177&amp;lpg=PA135&amp;hl=nl&amp;redir_esc=y#v=onepage&amp;q&amp;f=false\">\u201cThe scientist\u2019s conscience : historical considerations\u201d in Bulletin of the Atomic Scientists - May 1956 - Page 177</a></u></li></ul><p><br><strong>Arthur Galston:</strong></p><ul><li><u><a href=\"http://www.plantphysiol.org/content/128/3/786.full#ref-4\">Galston A. : An Accidental Plant Biologist, Plant Physiology March 2002  vol. 128 no. 3</a></u></li><li>\u201c<u><a href=\"http://beck2.med.harvard.edu/week13/Galston.pdf\">Science and Social Responsibility: A Case History.\u201d Annals of the New York Academy of Sciences. Vol 196, Article 4</a></u></li></ul><p><br><strong>Le\u00f3 Szil\u00e1rd:</strong></p><ul><li><u><a href=\"http://www.amazon.com/Making-Atomic-Bomb-Richard-Rhodes/dp/0684813785\">Rhodes : The Making of the Atomic Bomb</a></u></li><li><u><a href=\"http://www.amazon.com/Nuclear-Biological-Chemical-Warfare-Bushan/dp/8176483125\">Bhushan K, Katyal G : Nuclear Biological &amp; Chemical Warfare</a></u></li></ul><p><br><strong>Joseph Rotblat:</strong></p><ul><li>Bulletin of the atomic scientists, august 1985 : Leaving the Bomb Project by Joseph Rotblat</li><li><u><a href=\"http://www.amazon.com/Keeper-Nuclear-Conscience-Joseph-Rotblat/dp/0199586586/ref=sr_1_1?ie=UTF8&amp;qid=1355022838&amp;sr=8-1&amp;keywords=rotblat\">Keeper of the Nuclear Conscience: The Life and Work of Joseph Rotblat</a></u></li><li><u><a href=\"http://sounds.bl.uk/Oral-history/Science/021M-C0464X0017XX-0700V0\">1999 voice record interview of Joseph Rotblat</a></u></li><li>Deriving an Ethical Code for Scientists: An Interview With Joseph Rotblat</li></ul><p><br><strong>Norbert Wiener:</strong></p><ul><li><u><a href=\"http://www.amazon.com/Postmodern-War-New-Politics-Conflict/dp/1572301767\">Postmodern War: The New Politics of Conflict by Chris Hables Gray</a></u> (<u><a href=\"http://www.chrishablesgray.org/postmodernwar/\">available online</a></u>)</li><li>Bulletin of the Atomic Scientists - May 1956 - Page 178  : \u201cThe scientist\u2019s conscience : historical considerations</li><li><u><a href=\"http://www.amazon.com/Dark-Hero-Information-Age-Cybernetics/dp/0465013716\">Dark Hero of the Information Age: In Search of Norbert Wiener The Father of Cybernetics by Flo Conway</a></u></li><li>Bulletin of the Atomic Scientists - Jan 1947 - Page 31 : \"A Scientist Rebels\"</li><li>Bulletin of the Atomic Scientists - Feb 1956 - Page 53  : \"Moral Reflections of a Mathematician\"</li><li>Bulletin of the Atomic Scientists - Nov 1948 - Page 338 :  \"A Rebellious Scientist After Two Years\"</li><li><u><a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.1989.tb15050.x/abstract\">Rosemary Chalk : Drawing the Line An Examination of Conscientious Objection in Science</a></u></li><li><u><a href=\"http://www.nyu.edu/projects/nissenbaum/papers/Wiener.pdf\">Some Moral and Technical Consequences of Automation</a></u> in Science, 6 may 1960</li></ul><p><br><strong>Paul Berg, Maxine Singer, Robert Pollack:</strong></p><ul><li>P. Berg and M. F. Singer : The recombinant DNA controversy: twenty years later, Proc Natl Acad Sci U S A. 1995 September 26</li><li>Potential Biohazards of Recombinant DNA Molecules by Paul Berg, 1974</li><li>Guidelines for DNA Hybrid Molecules by Maxine Singer, 1973</li><li>Biomedical Politics by Kathi E. Hanna (chapter : Asilomar and recombinant DNA by Donald S. Fredrickson), 1991</li><li><u><a href=\"http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1023&amp;context=edethicsinscience\">Asilomar Conference on Laboratory Precautions When Conducting Recombinant DNA Research \u2013 Case Summary</a></u></li><li><u><a href=\"http://books.google.com/books/about/Report_Assembly_of_Life_Sciences_Nationa.html?id=SVYrAAAAYAAJ\">Report - Assembly of Life Sciences, National Research Council</a></u></li><li>P. Berg: <u><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC388511/?page=1\">Potential Biohazards of Recombinant DNA Molecules</a></u></li><li><u><a href=\"http://books.google.com/books/about/Watson_and_DNA.html?id=gUkBMctzM2gC\">Watson and DNA: Making a Scientific Revolution</a></u></li></ul>", "sections": [{"title": "Pre-industrial inventors", "anchor": "Pre_industrial_inventors", "level": 1}, {"title": "Chemical warfare", "anchor": "Chemical_warfare", "level": 2}, {"title": "Nuclear weapons", "anchor": "Nuclear_weapons", "level": 1}, {"title": "Recombinant DNA", "anchor": "Recombinant_DNA", "level": 1}, {"title": "Informatics", "anchor": "Informatics", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "Sources used", "anchor": "Sources_used", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "143 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 150, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-09T07:19:43.789Z", "modifiedAt": null, "url": null, "title": "MLP: The Next Level Of Your Studies", "slug": "mlp-the-next-level-of-your-studies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:45.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CSpApLKpTLmZJjJpW/mlp-the-next-level-of-your-studies", "pageUrlRelative": "/posts/CSpApLKpTLmZJjJpW/mlp-the-next-level-of-your-studies", "linkUrl": "https://www.lesswrong.com/posts/CSpApLKpTLmZJjJpW/mlp-the-next-level-of-your-studies", "postedAtFormatted": "Saturday, February 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MLP%3A%20The%20Next%20Level%20Of%20Your%20Studies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMLP%3A%20The%20Next%20Level%20Of%20Your%20Studies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCSpApLKpTLmZJjJpW%2Fmlp-the-next-level-of-your-studies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MLP%3A%20The%20Next%20Level%20Of%20Your%20Studies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCSpApLKpTLmZJjJpW%2Fmlp-the-next-level-of-your-studies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCSpApLKpTLmZJjJpW%2Fmlp-the-next-level-of-your-studies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>The first four chapters of my MLP fanfiction are now <a href=\"https://www.fimfiction.net/story/82658/the-next-level-of-your-studies\">online on fimfiction.net</a>. Unlike <a href=\"/lw/efi/friendship_is_optimal_a_my_little_pony_fanfic/\">Friendship Is Optimal</a> (<a href=\"http://www.fimfiction.net/story/62074/Friendship-is-Optimal\">fim link</a>), which focuses on how MLP might impact the trajectory of AI, or <a href=\"/lw/e1g/a_rationalist_my_little_pony_fanfic/\">Myou've Gotta be Kidding Me</a> (<a href=\"https://www.fimfiction.net/story/33512/Myou%27ve-Gotta-be-Kidding-Me\">fim link</a>), which focuses on how a rationalist might impact the trajectory of Equestria, I wanted to ask: what would the <a href=\"/lw/bd/my_way/\">MLP Way</a> look like? How would MLP impact rationality, and what would rationalist MLP look like?<br /><br />This has been over a year in the making, off and on, and I've received significant help in writing it. In particular, I'd like to thank the pre-readers and editors who have helped polish it, and everyone who's shown interest; that has been a great help in motivating me to work on this rather than other projects.</p>\n<p>There's much more to come; I should be able to maintain at least a chapter a week for the medium term, and hope to post more frequently than that. If you're interested in pre-reading / editing chapters to come, let me know!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CSpApLKpTLmZJjJpW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 13, "extendedScore": null, "score": 1.1074262696250687e-06, "legacy": true, "legacyId": "21547", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CHD5m9fnosr7L3dto", "2NnJrE8jY34ipLZkL", "FBgozHEv7J72NCEPB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-09T17:06:14.891Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC HPMOR Discussion", "slug": "meetup-washington-dc-hpmor-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XuadzvuACa6SpdFKE/meetup-washington-dc-hpmor-discussion", "pageUrlRelative": "/posts/XuadzvuACa6SpdFKE/meetup-washington-dc-hpmor-discussion", "linkUrl": "https://www.lesswrong.com/posts/XuadzvuACa6SpdFKE/meetup-washington-dc-hpmor-discussion", "postedAtFormatted": "Saturday, February 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20HPMOR%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20HPMOR%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuadzvuACa6SpdFKE%2Fmeetup-washington-dc-hpmor-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20HPMOR%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuadzvuACa6SpdFKE%2Fmeetup-washington-dc-hpmor-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuadzvuACa6SpdFKE%2Fmeetup-washington-dc-hpmor-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/j0'>Washington DC HPMOR Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 February 2013 03:00:53AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be talking about methods!\n(This has changed, so you're not crazy if you think it's a TED talks meetup.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/j0'>Washington DC HPMOR Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XuadzvuACa6SpdFKE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1077954344254858e-06, "legacy": true, "legacyId": "21548", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_HPMOR_Discussion\">Discussion article for the meetup : <a href=\"/meetups/j0\">Washington DC HPMOR Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 February 2013 03:00:53AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be talking about methods!\n(This has changed, so you're not crazy if you think it's a TED talks meetup.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_HPMOR_Discussion1\">Discussion article for the meetup : <a href=\"/meetups/j0\">Washington DC HPMOR Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC HPMOR Discussion", "anchor": "Discussion_article_for_the_meetup___Washington_DC_HPMOR_Discussion", "level": 1}, {"title": "Discussion article for the meetup : Washington DC HPMOR Discussion", "anchor": "Discussion_article_for_the_meetup___Washington_DC_HPMOR_Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-10T03:58:04.851Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Higher Purpose", "slug": "seq-rerun-higher-purpose", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:07.842Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uyGMqSw4jjzvHSMRH/seq-rerun-higher-purpose", "pageUrlRelative": "/posts/uyGMqSw4jjzvHSMRH/seq-rerun-higher-purpose", "linkUrl": "https://www.lesswrong.com/posts/uyGMqSw4jjzvHSMRH/seq-rerun-higher-purpose", "postedAtFormatted": "Sunday, February 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Higher%20Purpose&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Higher%20Purpose%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuyGMqSw4jjzvHSMRH%2Fseq-rerun-higher-purpose%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Higher%20Purpose%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuyGMqSw4jjzvHSMRH%2Fseq-rerun-higher-purpose", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuyGMqSw4jjzvHSMRH%2Fseq-rerun-higher-purpose", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 398, "htmlBody": "<p>Today's post, <a href=\"/lw/xw/higher_purpose/\">Higher Purpose</a> was originally published on 23 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Having a Purpose in Life consistently shows up as something that increases stated well-being. Of course, the problem with trying to pick out \"a Purpose in Life\" in order to make yourself happier, is that this doesn't take you outside yourself; it's still all about you. To find purpose, you need to turn your eyes outward to look at the world and find things there that you care about - rather than obsessing about the wonderful spiritual benefits you're getting from helping others. In today's world, most of the highest-priority legitimate Causes consist of large groups of people in extreme jeopardy: Aging threatens the old, starvation threatens the poor, extinction risks threaten humanity as a whole. If the future goes right, many and perhaps all such problems will be solved - depleting the stream of victims to be helped. Will the future therefore consist of self-obsessed individuals, with nothing to take them outside themselves? I suggest, though, that even if there were no large groups of people in extreme jeopardy, we would still, looking around, find things outside ourselves that we cared about - friends, family; truth, freedom... Nonetheless, if the Future goes sufficiently well, there will come a time when you could search the whole of civilization, and never find a single person so much in need of help, as dozens you now pass on the street. If you do want to save someone from death, or help a great many people, then act now; your opportunity may not last, one way or another.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gmg/seq_rerun_investing_for_the_long_slump/\">Investing for the Long Slump</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uyGMqSw4jjzvHSMRH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.1082059646949711e-06, "legacy": true, "legacyId": "21550", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5Pjq3mxuiXu2Ys3gM", "ixkqdazCiJoHzycNT", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-10T22:01:14.177Z", "modifiedAt": null, "url": null, "title": "LW Women- Crowdsourced research on Cognitive biases and gender", "slug": "lw-women-crowdsourced-research-on-cognitive-biases-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:01.173Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hy27QxnhxyMDcNNkF/lw-women-crowdsourced-research-on-cognitive-biases-and", "pageUrlRelative": "/posts/Hy27QxnhxyMDcNNkF/lw-women-crowdsourced-research-on-cognitive-biases-and", "linkUrl": "https://www.lesswrong.com/posts/Hy27QxnhxyMDcNNkF/lw-women-crowdsourced-research-on-cognitive-biases-and", "postedAtFormatted": "Sunday, February 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20Women-%20Crowdsourced%20research%20on%20Cognitive%20biases%20and%20gender&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20Women-%20Crowdsourced%20research%20on%20Cognitive%20biases%20and%20gender%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHy27QxnhxyMDcNNkF%2Flw-women-crowdsourced-research-on-cognitive-biases-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20Women-%20Crowdsourced%20research%20on%20Cognitive%20biases%20and%20gender%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHy27QxnhxyMDcNNkF%2Flw-women-crowdsourced-research-on-cognitive-biases-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHy27QxnhxyMDcNNkF%2Flw-women-crowdsourced-research-on-cognitive-biases-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 760, "htmlBody": "<p>In the last LW Women post, it was <a href=\"/r/lesswrong/lw/fmc/lw_women_minimizing_the_inferential_distance/7vxu\">mentioned</a>, and I agree, that a two-way conversation is more productive, and presents varied viewpoints better than a one-way lecture. To that end, I am making this post an experiment in crowdsourcing research to LW. Instead of writing this topic up myself (more talking AT you), I want to see what happens if instead I leave a good prompt, along with some paths (search terms, journal articles) to start down for discussion. <strong>What information will a collectivist research project yield?</strong>&nbsp; In other words, instead of reading what I write below as the article,<strong> pretend you are helping to collaborate on an article.</strong></p>\n<p>The next post in the series will go back to LW Women's submissions.</p>\n<p>&nbsp;</p>\n<h2>Recommended Rules (because last LW Women post reached 1000+ comments, and we want to keep that as navigable as possible)</h2>\n<p><span style=\"font-size: small; font-weight: normal;\">When possible, make/use parent comments when you are discussing a specific bias, so that multiple studies or lines of reasoning on the same bias can be grouped together.&nbsp;</span></p>\n<p><span style=\"font-size: small; font-weight: normal;\">When you post a summary of a study, make sure to read it first and give a decent rundown. If a study says \"X sometimes, Y sometimes,\" do not just say \"This study proves X!\"&nbsp;</span></p>\n<p>Put meta discussion <a href=\"/r/daenerys-drafts/lw/fmy/lw_women_discussion_series_crowdsourcing_research/8fsf\">HERE</a> (e.g.- What do you think about crowdsourcing research on LW? What do you think about the LW Women series, etc.)</p>\n<p><span style=\"font-size: small; font-weight: normal;\"><br /></span></p>\n<h2>Prompt</h2>\n<p>What cognitive biases might effect various gender stereotypes and how people think about gender? &nbsp;Below are some starting points. The links are to the wikipedia articles. This list isn't the be-all, end-all. It's just somewhere to get started. Use it to get ideas, or not.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Fundamental_attribution_error\">Fundamental Attribution Error</a>- aka&nbsp;<a href=\"/lw/hz/correspondence_bias/\">Correspondence Bias</a>- &nbsp;Tendency to draw inferences about a person's unique and enduring dispositions from behaviors that can be entirely explained by the situations in which they occur.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Actor%E2%80%93observer_bias\">Actor-Observer Bias</a>&nbsp;-&nbsp;People are more likely to see their own behavior as affected by the situation they are in, or the sequence of occurrences that have happened to them throughout their day. But, they see other people&rsquo;s actions as solely a product of their overall personality, and they do not afford them the chance to explain their behavior as exclusively a result of a situational effect.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Just-world_phenomenon\">Just World Fallacy</a>-&nbsp;human actions eventually yield morally fair and fitting consequences</p>\n<p><a href=\"http://en.wikipedia.org/wiki/System_justification\">System Justification</a>- People have a motivation to defend and justify the status quo, even when it may be disadvantageous to certain people... they are motivated to see the status quo (or prevailing social, economic, and political norms) as good, legitimate, and desirable.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Availability_heuristic\">Availability Heuristic</a>-&nbsp;&nbsp;people make judgments about the probability of events by how easy it is to think of examples</p>\n<p><a href=\"http://en.wikipedia.org/wiki/List_of_biases\">List of Biases</a>- help yourself to a bias!&nbsp;</p>\n<p>&nbsp;</p>\n<p><img src=\"http://imgs.xkcd.com/comics/how_it_works.png\" alt=\"\" width=\"410\" height=\"211\" /></p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h2>Example Response</h2>\n<p>Below is an example response I wrote about the Ultimate Attribution Error and Availability Heuristic. I didn't use any studies. Do better than me! (<strong>Update</strong>: I decided I should also include an example of a study write-up, so made a comment with one <a href=\"/r/discussion/lw/fmy/lw_women_crowdsourced_research_on_cognitive/8ftr\">HERE</a>&nbsp;. Please DON'T just give a link and a single sentence!)</p>\n<p>&nbsp;</p>\n<p>The first post on the LW Women series involved trying to minimize the inferential gap by sharing anecdotes of what it's like growing up as a \"geek girl\". When reading these submissions, I was struck by how it might seem like the Fundamental Attribution Bias (aka Correspondence Bias) is at play, but for whole groups. Turns out this is A Thing, and it's called Ultimate Attribution Error.</p>\n<p>For example, say a woman mentions that she's bad with computers. From *her* perspective, she sees the situation as the cause of this: \"Of course I'm not as good with computers! When I went to learn in a programming class, it was full of guys who stared at me the whole time and I was too uncomfortable to pay attention!\" When women see other women with the same responses, they can empathize with the situational causes.</p>\n<p>However, when men see women complaining about new technology, they are more likely to attribute these to factors about the women's personalities: \"she's not good at computers.\"</p>\n<p>We don't view *lack* of a negative as a factor in our personalities. For example, one is likely to realize that the reason they did badly in school is because their parents had a low socio-economic status and so they lacked opportunities. One *might* realize that one of the reasons they are good in school is because their parents have a high socio-economic status which gives them certain advantages and opportunities. But one is *unlikely* to realize that NOT having low socioeconomic parents is why you did NOT do badly in school.</p>\n<p><img src=\"http://www.phdcomics.com/comics/archive/phd0410s.gif\" alt=\"\" /></p>\n<h2><br /></h2>\n<p>Images from: <a href=\"http://www.phdcomics.com/comics/archive.php?comicid=55\">PhD Comics</a> and <a href=\"http://xkcd.com/385/\">xkcd</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hy27QxnhxyMDcNNkF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 12, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "20266", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In the last LW Women post, it was <a href=\"/r/lesswrong/lw/fmc/lw_women_minimizing_the_inferential_distance/7vxu\">mentioned</a>, and I agree, that a two-way conversation is more productive, and presents varied viewpoints better than a one-way lecture. To that end, I am making this post an experiment in crowdsourcing research to LW. Instead of writing this topic up myself (more talking AT you), I want to see what happens if instead I leave a good prompt, along with some paths (search terms, journal articles) to start down for discussion. <strong>What information will a collectivist research project yield?</strong>&nbsp; In other words, instead of reading what I write below as the article,<strong> pretend you are helping to collaborate on an article.</strong></p>\n<p>The next post in the series will go back to LW Women's submissions.</p>\n<p>&nbsp;</p>\n<h2 id=\"Recommended_Rules__because_last_LW_Women_post_reached_1000__comments__and_we_want_to_keep_that_as_navigable_as_possible_\">Recommended Rules (because last LW Women post reached 1000+ comments, and we want to keep that as navigable as possible)</h2>\n<p><span style=\"font-size: small; font-weight: normal;\">When possible, make/use parent comments when you are discussing a specific bias, so that multiple studies or lines of reasoning on the same bias can be grouped together.&nbsp;</span></p>\n<p><span style=\"font-size: small; font-weight: normal;\">When you post a summary of a study, make sure to read it first and give a decent rundown. If a study says \"X sometimes, Y sometimes,\" do not just say \"This study proves X!\"&nbsp;</span></p>\n<p>Put meta discussion <a href=\"/r/daenerys-drafts/lw/fmy/lw_women_discussion_series_crowdsourcing_research/8fsf\">HERE</a> (e.g.- What do you think about crowdsourcing research on LW? What do you think about the LW Women series, etc.)</p>\n<p><span style=\"font-size: small; font-weight: normal;\"><br></span></p>\n<h2 id=\"Prompt\">Prompt</h2>\n<p>What cognitive biases might effect various gender stereotypes and how people think about gender? &nbsp;Below are some starting points. The links are to the wikipedia articles. This list isn't the be-all, end-all. It's just somewhere to get started. Use it to get ideas, or not.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Fundamental_attribution_error\">Fundamental Attribution Error</a>- aka&nbsp;<a href=\"/lw/hz/correspondence_bias/\">Correspondence Bias</a>- &nbsp;Tendency to draw inferences about a person's unique and enduring dispositions from behaviors that can be entirely explained by the situations in which they occur.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Actor%E2%80%93observer_bias\">Actor-Observer Bias</a>&nbsp;-&nbsp;People are more likely to see their own behavior as affected by the situation they are in, or the sequence of occurrences that have happened to them throughout their day. But, they see other people\u2019s actions as solely a product of their overall personality, and they do not afford them the chance to explain their behavior as exclusively a result of a situational effect.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Just-world_phenomenon\">Just World Fallacy</a>-&nbsp;human actions eventually yield morally fair and fitting consequences</p>\n<p><a href=\"http://en.wikipedia.org/wiki/System_justification\">System Justification</a>- People have a motivation to defend and justify the status quo, even when it may be disadvantageous to certain people... they are motivated to see the status quo (or prevailing social, economic, and political norms) as good, legitimate, and desirable.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Availability_heuristic\">Availability Heuristic</a>-&nbsp;&nbsp;people make judgments about the probability of events by how easy it is to think of examples</p>\n<p><a href=\"http://en.wikipedia.org/wiki/List_of_biases\">List of Biases</a>- help yourself to a bias!&nbsp;</p>\n<p>&nbsp;</p>\n<p><img src=\"http://imgs.xkcd.com/comics/how_it_works.png\" alt=\"\" width=\"410\" height=\"211\"></p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h2 id=\"Example_Response\">Example Response</h2>\n<p>Below is an example response I wrote about the Ultimate Attribution Error and Availability Heuristic. I didn't use any studies. Do better than me! (<strong>Update</strong>: I decided I should also include an example of a study write-up, so made a comment with one <a href=\"/r/discussion/lw/fmy/lw_women_crowdsourced_research_on_cognitive/8ftr\">HERE</a>&nbsp;. Please DON'T just give a link and a single sentence!)</p>\n<p>&nbsp;</p>\n<p>The first post on the LW Women series involved trying to minimize the inferential gap by sharing anecdotes of what it's like growing up as a \"geek girl\". When reading these submissions, I was struck by how it might seem like the Fundamental Attribution Bias (aka Correspondence Bias) is at play, but for whole groups. Turns out this is A Thing, and it's called Ultimate Attribution Error.</p>\n<p>For example, say a woman mentions that she's bad with computers. From *her* perspective, she sees the situation as the cause of this: \"Of course I'm not as good with computers! When I went to learn in a programming class, it was full of guys who stared at me the whole time and I was too uncomfortable to pay attention!\" When women see other women with the same responses, they can empathize with the situational causes.</p>\n<p>However, when men see women complaining about new technology, they are more likely to attribute these to factors about the women's personalities: \"she's not good at computers.\"</p>\n<p>We don't view *lack* of a negative as a factor in our personalities. For example, one is likely to realize that the reason they did badly in school is because their parents had a low socio-economic status and so they lacked opportunities. One *might* realize that one of the reasons they are good in school is because their parents have a high socio-economic status which gives them certain advantages and opportunities. But one is *unlikely* to realize that NOT having low socioeconomic parents is why you did NOT do badly in school.</p>\n<p><img src=\"http://www.phdcomics.com/comics/archive/phd0410s.gif\" alt=\"\"></p>\n<h2><br></h2>\n<p>Images from: <a href=\"http://www.phdcomics.com/comics/archive.php?comicid=55\">PhD Comics</a> and <a href=\"http://xkcd.com/385/\">xkcd</a></p>", "sections": [{"title": "Recommended Rules (because last LW Women post reached 1000+ comments, and we want to keep that as navigable as possible)", "anchor": "Recommended_Rules__because_last_LW_Women_post_reached_1000__comments__and_we_want_to_keep_that_as_navigable_as_possible_", "level": 1}, {"title": "Prompt", "anchor": "Prompt", "level": 1}, {"title": "Example Response", "anchor": "Example_Response", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "112 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 112, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DB6wbyrMugYMK5o6a"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-10T22:25:15.535Z", "modifiedAt": null, "url": null, "title": "[Link] False memories of fabricated political events", "slug": "link-false-memories-of-fabricated-political-events", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gjm", "createdAt": "2009-03-09T01:11:32.668Z", "isAdmin": false, "displayName": "gjm"}, "userId": "977L8MR7JmNrQx6df", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/REf88ofNFs9vbmzmF/link-false-memories-of-fabricated-political-events", "pageUrlRelative": "/posts/REf88ofNFs9vbmzmF/link-false-memories-of-fabricated-political-events", "linkUrl": "https://www.lesswrong.com/posts/REf88ofNFs9vbmzmF/link-false-memories-of-fabricated-political-events", "postedAtFormatted": "Sunday, February 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20False%20memories%20of%20fabricated%20political%20events&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20False%20memories%20of%20fabricated%20political%20events%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FREf88ofNFs9vbmzmF%2Flink-false-memories-of-fabricated-political-events%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20False%20memories%20of%20fabricated%20political%20events%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FREf88ofNFs9vbmzmF%2Flink-false-memories-of-fabricated-political-events", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FREf88ofNFs9vbmzmF%2Flink-false-memories-of-fabricated-political-events", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p>Another one for the memory-is-really-unreliable file. Some researchers at UC Irvine (one of them is Elizabeth Loftus, whose name I've seen attached to other fake-memory studies) asked about 5000 subjects about their recollection of four political events. One of the political events never actually happened. About half the subjects said they remembered the fake event. Subjects were more likely to pseudo-remember events congruent with their political preferences (e.g., Bush or Obama doing something embarrassing).</p>\n<p><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2201941\">Link to papers.ssrn.com</a> (paper is freely downloadable).</p>\n<p>The subjects were recruited from the readership of <em>Slate</em>, which unsurprisingly means they aren't a very representative sample of the US population (never mind the rest of the world). In particular, about 5% identified as conservative and about 60% as progressive.</p>\n<p>Each real event was remembered by 90-98% of subjects. Self-identified conservatives remembered the real events a little less well. Self-identified progressives were much more likely to \"remember\" a fake event in which G W Bush took a vacation in Texas while Hurricane Katrina was devastating New Orleans. Self-identified conservatives were somewhat more likely to \"remember\" a fake event in which Barack Obama shook the hand of Mahmoud Ahmedinejad.</p>\n<p>About half of the subjects who \"remembered\" fake events were unable to identify the fake event correctly when they were told that one of the events in the study was fake.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "REf88ofNFs9vbmzmF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 28, "extendedScore": null, "score": 1.1089038928819108e-06, "legacy": true, "legacyId": "21552", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-10T23:05:57.168Z", "modifiedAt": null, "url": null, "title": "[Link] Detachment", "slug": "link-detachment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:35.586Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jdXPfNtL4ThLDLRyX/link-detachment", "pageUrlRelative": "/posts/jdXPfNtL4ThLDLRyX/link-detachment", "linkUrl": "https://www.lesswrong.com/posts/jdXPfNtL4ThLDLRyX/link-detachment", "postedAtFormatted": "Sunday, February 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Detachment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Detachment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjdXPfNtL4ThLDLRyX%2Flink-detachment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Detachment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjdXPfNtL4ThLDLRyX%2Flink-detachment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjdXPfNtL4ThLDLRyX%2Flink-detachment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<p>http://www.urticator.net/essay/0/48.html</p>\n<p>Might be a useful read apropos of Wei Dai's recent <a href=\"/lw/do9/welcome_to_less_wrong_july_2012/8fpg\">question</a>:</p>\n<blockquote>\n<p>Why are we causing [people] to think of LW in terms of <em>identity</em> in the first place, instead of, say, a place to learn about and discuss some interesting ideas?</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jdXPfNtL4ThLDLRyX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "21553", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-11T01:12:08.680Z", "modifiedAt": null, "url": null, "title": "Meetup : Toronto - Rational Debate: Will Rationality Make You Rich? ... and other topics", "slug": "meetup-toronto-rational-debate-will-rationality-make-you", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ibMPvhXqtDoucMemq/meetup-toronto-rational-debate-will-rationality-make-you", "pageUrlRelative": "/posts/ibMPvhXqtDoucMemq/meetup-toronto-rational-debate-will-rationality-make-you", "linkUrl": "https://www.lesswrong.com/posts/ibMPvhXqtDoucMemq/meetup-toronto-rational-debate-will-rationality-make-you", "postedAtFormatted": "Monday, February 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Toronto%20-%20Rational%20Debate%3A%20Will%20Rationality%20Make%20You%20Rich%3F%20...%20and%20other%20topics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Toronto%20-%20Rational%20Debate%3A%20Will%20Rationality%20Make%20You%20Rich%3F%20...%20and%20other%20topics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FibMPvhXqtDoucMemq%2Fmeetup-toronto-rational-debate-will-rationality-make-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Toronto%20-%20Rational%20Debate%3A%20Will%20Rationality%20Make%20You%20Rich%3F%20...%20and%20other%20topics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FibMPvhXqtDoucMemq%2Fmeetup-toronto-rational-debate-will-rationality-make-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FibMPvhXqtDoucMemq%2Fmeetup-toronto-rational-debate-will-rationality-make-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 675, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/j1\">Rational Debate: Will Rationality Make You Rich? ... and other topics</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">12 February 2013 08:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">54 Dundas St E, Toronto, ON</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Place: Upstairs at The Imperial Public Library 54 Dundas St. E, near Dundas Station. Enter at the door on the right marked \"library\", go upstairs and look for the paperclip sign.</p>\n<p>We'll kick the meeting off with ASK LESS WRONG. Think of something in your everyday life that's bothering you and we'll help you smooth it out. Purpose: increase the fun in each others' lives through the magic of friendship. Secondary purpose: train ourselves to notice things that are suboptimal and view them as problems that can be solved.</p>\n<p>The main part of the meeting will be a RATIONAL DEBATE. We'll start with \"will rationality make you rich\", then move on to \"is there intelligent life elsewhere in the universe\" and \"should you vote\". That's probably all we'll have time for before the beer kicks in, but we do have backup topics.</p>\n<p>If you want to read up on any of these topics, that's great - but not strictly necessary.</p>\n<p>Rational debating is far from a solved problem, so we'll be learning how to do it as we go along. I'll be chairing, so don't worry about keeping track of this vast list of meta stuff - that's my job. It'll go something like this:</p>\n<ul>\n<li>\n<p>In a conventional debate, you win by sounding more plausible than the other person. In a <em>rational</em> debate, you win if and only if you end up believing the truth. This makes it a cooperative game - it's possible for everyone to win or for everyone to lose. (Incidentally it also means you don't actually know whether you've won or not).</p>\n</li>\n<li>\n<p>Initially, each person answers the question separately, choosing how they wish to frame their answer. If people come up with very different ways of framing the question, we will take each one in turn and try to approach the question from that direction. (The point of this is to avoid fighting over the framing of the discussion and instead address the issues directly).</p>\n</li>\n<li>\n<p>I'll keep track of structural stuff - different ways of framing the question, agreed subtopics of discussion, and binary chopping to find points of disagreement (which involves lists of statements and verbally how plausible we each think they are).</p>\n</li>\n<li>\n<p>When arguing against something, construct a steel man first - rephrase the opposing argument in your own words, making it as strong and plausible as you can, before you try and defeat it.</p>\n</li>\n<li>\n<p>Be bold and specific - make sure you're saying something substantial, even if you're not completely sure it's true.</p>\n</li>\n<li>\n<p>The social aspect: make sure we're providing status and rewards for the right things.</p>\n</li>\n<li>\n<p>Leave a line of retreat. What would I do if I was wrong about this?</p>\n</li>\n<li>\n<p>Try to <em>notice</em> when you're replying to somebody's cached thought with a cached thought of your own. I'll try and do the same.</p>\n</li>\n<li>\n<p>Try to find something to change your mind about, even if it's something small.</p>\n</li>\n<li>\n<p>Separate out disagreement about facts from disagreement about values (and disagreement about strategy, which combines both). Separate out semantic confusion. I think we're already reasonably good at these.</p>\n</li>\n<li>\n<p>If possible, identify which of these techniques you're trying to put into practice. I'll do the same. (By drawing attention to this we'll help keep things purposeful, and also hopefully learn which techniques seem particularly useful).</p>\n</li>\n</ul>\n<p>Resources on rational debate:</p>\n<p><a rel=\"nofollow\" href=\"/lw/85h/better_disagreement/\">http://lesswrong.com/lw/85h/better_disagreement/</a></p>\n<p><a rel=\"nofollow\" href=\"/lw/o4/leave_a_line_of_retreat/\">http://lesswrong.com/lw/o4/leave_a_line_of_retreat/</a></p>\n<p><a rel=\"nofollow\" href=\"/lw/gm9/philosophical_landmines/\">http://lesswrong.com/lw/gm9/philosophical_landmines/</a></p>\n<p>Hope to see you all at the Imperial Pub on Tuesday! Let me know if you can come.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/j1\">Rational Debate: Will Rationality Make You Rich? ... and other topics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ibMPvhXqtDoucMemq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 1.1090091595119302e-06, "legacy": true, "legacyId": "21555", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rational_Debate__Will_Rationality_Make_You_Rich______and_other_topics\">Discussion article for the meetup : <a href=\"/meetups/j1\">Rational Debate: Will Rationality Make You Rich? ... and other topics</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">12 February 2013 08:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">54 Dundas St E, Toronto, ON</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Place: Upstairs at The Imperial Public Library 54 Dundas St. E, near Dundas Station. Enter at the door on the right marked \"library\", go upstairs and look for the paperclip sign.</p>\n<p>We'll kick the meeting off with ASK LESS WRONG. Think of something in your everyday life that's bothering you and we'll help you smooth it out. Purpose: increase the fun in each others' lives through the magic of friendship. Secondary purpose: train ourselves to notice things that are suboptimal and view them as problems that can be solved.</p>\n<p>The main part of the meeting will be a RATIONAL DEBATE. We'll start with \"will rationality make you rich\", then move on to \"is there intelligent life elsewhere in the universe\" and \"should you vote\". That's probably all we'll have time for before the beer kicks in, but we do have backup topics.</p>\n<p>If you want to read up on any of these topics, that's great - but not strictly necessary.</p>\n<p>Rational debating is far from a solved problem, so we'll be learning how to do it as we go along. I'll be chairing, so don't worry about keeping track of this vast list of meta stuff - that's my job. It'll go something like this:</p>\n<ul>\n<li>\n<p>In a conventional debate, you win by sounding more plausible than the other person. In a <em>rational</em> debate, you win if and only if you end up believing the truth. This makes it a cooperative game - it's possible for everyone to win or for everyone to lose. (Incidentally it also means you don't actually know whether you've won or not).</p>\n</li>\n<li>\n<p>Initially, each person answers the question separately, choosing how they wish to frame their answer. If people come up with very different ways of framing the question, we will take each one in turn and try to approach the question from that direction. (The point of this is to avoid fighting over the framing of the discussion and instead address the issues directly).</p>\n</li>\n<li>\n<p>I'll keep track of structural stuff - different ways of framing the question, agreed subtopics of discussion, and binary chopping to find points of disagreement (which involves lists of statements and verbally how plausible we each think they are).</p>\n</li>\n<li>\n<p>When arguing against something, construct a steel man first - rephrase the opposing argument in your own words, making it as strong and plausible as you can, before you try and defeat it.</p>\n</li>\n<li>\n<p>Be bold and specific - make sure you're saying something substantial, even if you're not completely sure it's true.</p>\n</li>\n<li>\n<p>The social aspect: make sure we're providing status and rewards for the right things.</p>\n</li>\n<li>\n<p>Leave a line of retreat. What would I do if I was wrong about this?</p>\n</li>\n<li>\n<p>Try to <em>notice</em> when you're replying to somebody's cached thought with a cached thought of your own. I'll try and do the same.</p>\n</li>\n<li>\n<p>Try to find something to change your mind about, even if it's something small.</p>\n</li>\n<li>\n<p>Separate out disagreement about facts from disagreement about values (and disagreement about strategy, which combines both). Separate out semantic confusion. I think we're already reasonably good at these.</p>\n</li>\n<li>\n<p>If possible, identify which of these techniques you're trying to put into practice. I'll do the same. (By drawing attention to this we'll help keep things purposeful, and also hopefully learn which techniques seem particularly useful).</p>\n</li>\n</ul>\n<p>Resources on rational debate:</p>\n<p><a rel=\"nofollow\" href=\"/lw/85h/better_disagreement/\">http://lesswrong.com/lw/85h/better_disagreement/</a></p>\n<p><a rel=\"nofollow\" href=\"/lw/o4/leave_a_line_of_retreat/\">http://lesswrong.com/lw/o4/leave_a_line_of_retreat/</a></p>\n<p><a rel=\"nofollow\" href=\"/lw/gm9/philosophical_landmines/\">http://lesswrong.com/lw/gm9/philosophical_landmines/</a></p>\n<p>Hope to see you all at the Imperial Pub on Tuesday! Let me know if you can come.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Rational_Debate__Will_Rationality_Make_You_Rich______and_other_topics1\">Discussion article for the meetup : <a href=\"/meetups/j1\">Rational Debate: Will Rationality Make You Rich? ... and other topics</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rational Debate: Will Rationality Make You Rich? ... and other topics", "anchor": "Discussion_article_for_the_meetup___Rational_Debate__Will_Rationality_Make_You_Rich______and_other_topics", "level": 1}, {"title": "Discussion article for the meetup : Rational Debate: Will Rationality Make You Rich? ... and other topics", "anchor": "Discussion_article_for_the_meetup___Rational_Debate__Will_Rationality_Make_You_Rich______and_other_topics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FhH8m5n8qGSSHsAgG", "3XgYbghWruBMrPTAL", "L4HQ3gnSrBETRdcGu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-11T02:31:25.908Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H4B7pE2rNLQJtCQ4q/meetup-washington-dc-fun-and-games-meetup", "pageUrlRelative": "/posts/H4B7pE2rNLQJtCQ4q/meetup-washington-dc-fun-and-games-meetup", "linkUrl": "https://www.lesswrong.com/posts/H4B7pE2rNLQJtCQ4q/meetup-washington-dc-fun-and-games-meetup", "postedAtFormatted": "Monday, February 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4B7pE2rNLQJtCQ4q%2Fmeetup-washington-dc-fun-and-games-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4B7pE2rNLQJtCQ4q%2Fmeetup-washington-dc-fun-and-games-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4B7pE2rNLQJtCQ4q%2Fmeetup-washington-dc-fun-and-games-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/j2'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 February 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery , Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to play games and hang out.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/j2'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H4B7pE2rNLQJtCQ4q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1090591777118735e-06, "legacy": true, "legacyId": "21556", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/j2\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 February 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery , Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to play games and hang out.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/j2\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-11T03:51:26.553Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] 31 Laws of Fun", "slug": "seq-rerun-31-laws-of-fun", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n56jAYSsFhPmREdkB/seq-rerun-31-laws-of-fun", "pageUrlRelative": "/posts/n56jAYSsFhPmREdkB/seq-rerun-31-laws-of-fun", "linkUrl": "https://www.lesswrong.com/posts/n56jAYSsFhPmREdkB/seq-rerun-31-laws-of-fun", "postedAtFormatted": "Monday, February 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%2031%20Laws%20of%20Fun&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%2031%20Laws%20of%20Fun%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn56jAYSsFhPmREdkB%2Fseq-rerun-31-laws-of-fun%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%2031%20Laws%20of%20Fun%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn56jAYSsFhPmREdkB%2Fseq-rerun-31-laws-of-fun", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn56jAYSsFhPmREdkB%2Fseq-rerun-31-laws-of-fun", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>Today's post, <a href=\"/lw/y0/31_laws_of_fun/\">31 Laws of Fun</a> was originally published on 26 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#31_Laws_of_Fun\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A brief summary of principles for writing fiction set in a eutopia.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gmm/seq_rerun_higher_purpose/\">Higher Purpose</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n56jAYSsFhPmREdkB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1091096564403324e-06, "legacy": true, "legacyId": "21557", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qZJBighPrnv9bSqTZ", "uyGMqSw4jjzvHSMRH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-11T04:32:36.747Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham: Status Quo Bias", "slug": "meetup-durham-status-quo-bias", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3gzo6WA2uybFSAqXg/meetup-durham-status-quo-bias", "pageUrlRelative": "/posts/3gzo6WA2uybFSAqXg/meetup-durham-status-quo-bias", "linkUrl": "https://www.lesswrong.com/posts/3gzo6WA2uybFSAqXg/meetup-durham-status-quo-bias", "postedAtFormatted": "Monday, February 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%3A%20Status%20Quo%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%3A%20Status%20Quo%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3gzo6WA2uybFSAqXg%2Fmeetup-durham-status-quo-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%3A%20Status%20Quo%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3gzo6WA2uybFSAqXg%2Fmeetup-durham-status-quo-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3gzo6WA2uybFSAqXg%2Fmeetup-durham-status-quo-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/j3'>Durham: Status Quo Bias</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 February 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Francesca's, 706 9th Street, Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll discuss the status quo bias and debiasing techniques. Suggested readings include <a href=\"http://www.nickbostrom.com/ethics/statusquo.pdf\" rel=\"nofollow\">Bostrom&#39;s paper</a>, and other things TBD. Suggestions for additional reading are welcome. Post them here, or on the mailing list.</p>\n\n<p>I'll summarize a bit about the status quo bias and Bostrom's paper, so please don't feel like you can't come if you haven't done the reading, but we do encourage reading in advance!</p>\n\n<p>We'll do introductions and low-key discussion from 7:00-7:30, and status quo bias discussion from 7:30-9:00.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/j3'>Durham: Status Quo Bias</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3gzo6WA2uybFSAqXg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.1091356320763378e-06, "legacy": true, "legacyId": "21558", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham__Status_Quo_Bias\">Discussion article for the meetup : <a href=\"/meetups/j3\">Durham: Status Quo Bias</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 February 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Francesca's, 706 9th Street, Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll discuss the status quo bias and debiasing techniques. Suggested readings include <a href=\"http://www.nickbostrom.com/ethics/statusquo.pdf\" rel=\"nofollow\">Bostrom's paper</a>, and other things TBD. Suggestions for additional reading are welcome. Post them here, or on the mailing list.</p>\n\n<p>I'll summarize a bit about the status quo bias and Bostrom's paper, so please don't feel like you can't come if you haven't done the reading, but we do encourage reading in advance!</p>\n\n<p>We'll do introductions and low-key discussion from 7:00-7:30, and status quo bias discussion from 7:30-9:00.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham__Status_Quo_Bias1\">Discussion article for the meetup : <a href=\"/meetups/j3\">Durham: Status Quo Bias</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham: Status Quo Bias", "anchor": "Discussion_article_for_the_meetup___Durham__Status_Quo_Bias", "level": 1}, {"title": "Discussion article for the meetup : Durham: Status Quo Bias", "anchor": "Discussion_article_for_the_meetup___Durham__Status_Quo_Bias1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-11T16:01:17.129Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.113Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "toner", "createdAt": "2009-02-27T05:14:52.530Z", "isAdmin": false, "displayName": "toner"}, "userId": "XaYyFkpvhiiMscJJZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tLKHk5QYegsniYeDa/meetup-melbourne-social-meetup-1", "pageUrlRelative": "/posts/tLKHk5QYegsniYeDa/meetup-melbourne-social-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/tLKHk5QYegsniYeDa/meetup-melbourne-social-meetup-1", "postedAtFormatted": "Monday, February 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtLKHk5QYegsniYeDa%2Fmeetup-melbourne-social-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtLKHk5QYegsniYeDa%2Fmeetup-melbourne-social-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtLKHk5QYegsniYeDa%2Fmeetup-melbourne-social-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/j4'>Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 February 2013 06:45:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next regular social meetup will be held on Friday 15th February at the usual venue in Carlton. All are welcome from 6:30pm for a 7:00pm official start, but don't stress about being on time.</p>\n\n<p>Our social meetups are informal events held on the third Friday of each month, where we lounge about playing boardgames and chatting, with occasional group parlour games such as Mafia/Werewolf or Resistance if people are interested. If you haven't been to a Melbourne meetup before/recently, the social meetup can be less intimidating way to meet us as it's very informal.</p>\n\n<p>Some snacks will be provided and we'll probably arrange some form of delivered food for dinner. BYO drinks and games.</p>\n\n<p>For the location/any questions, please see the Melbourne Less Wrong google group, or feel free to SMS Richard on 0421 231 789 or contact me on 0412 996 288.</p>\n\n<p>Thanks again to Richard (Maelin) for being able to host.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/j4'>Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tLKHk5QYegsniYeDa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.109570301714791e-06, "legacy": true, "legacyId": "21560", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/j4\">Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 February 2013 06:45:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next regular social meetup will be held on Friday 15th February at the usual venue in Carlton. All are welcome from 6:30pm for a 7:00pm official start, but don't stress about being on time.</p>\n\n<p>Our social meetups are informal events held on the third Friday of each month, where we lounge about playing boardgames and chatting, with occasional group parlour games such as Mafia/Werewolf or Resistance if people are interested. If you haven't been to a Melbourne meetup before/recently, the social meetup can be less intimidating way to meet us as it's very informal.</p>\n\n<p>Some snacks will be provided and we'll probably arrange some form of delivered food for dinner. BYO drinks and games.</p>\n\n<p>For the location/any questions, please see the Melbourne Less Wrong google group, or feel free to SMS Richard on 0421 231 789 or contact me on 0412 996 288.</p>\n\n<p>Thanks again to Richard (Maelin) for being able to host.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/j4\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-11T19:19:03.805Z", "modifiedAt": null, "url": null, "title": "A confusion about deontology and consequentialism", "slug": "a-confusion-about-deontology-and-consequentialism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:04.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "WKQjYdBRgSBePG2mY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HyLfYAyHsiwem56PD/a-confusion-about-deontology-and-consequentialism", "pageUrlRelative": "/posts/HyLfYAyHsiwem56PD/a-confusion-about-deontology-and-consequentialism", "linkUrl": "https://www.lesswrong.com/posts/HyLfYAyHsiwem56PD/a-confusion-about-deontology-and-consequentialism", "postedAtFormatted": "Monday, February 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20confusion%20about%20deontology%20and%20consequentialism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20confusion%20about%20deontology%20and%20consequentialism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHyLfYAyHsiwem56PD%2Fa-confusion-about-deontology-and-consequentialism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20confusion%20about%20deontology%20and%20consequentialism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHyLfYAyHsiwem56PD%2Fa-confusion-about-deontology-and-consequentialism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHyLfYAyHsiwem56PD%2Fa-confusion-about-deontology-and-consequentialism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 281, "htmlBody": "<p>I think there&rsquo;s a confusion in our discussions of deontology and consequentialism. I&rsquo;m writing this post to try to clear up that confusion. First let me say that this post is not about any territorial facts. The issue here is how we use the philosophical terms of art &lsquo;consequentialism&rsquo; and &lsquo;deontology&rsquo;.</p>\n<p>The confusion is often stated thusly: &ldquo;deontological theories are full of injunctions like &lsquo;do not kill&rsquo;, but they generally provide no (or no interesting) explanations for these injunctions.&rdquo; There is of course an&nbsp;equivalently&nbsp;confused, though much less common, complaint about consequentialism.</p>\n<p>This is confused because the term &lsquo;deontology&rsquo; in philosophical jargon picks out a normative ethical theory, while the question &lsquo;how do we know that it is wrong to kill?&rsquo; is not a normative but a meta-ethical question. Similarly, consequentialism contains in itself no explanation for why pleasure or utility are morally good, or why consequences should matter to morality at all. Nor does consequentialism/deontology make any claims about how we know moral facts (if there are any). That is also a meta-ethical question.</p>\n<p>Some consequentialists and deontologists are also moral realists. Some are not. Some believe in divine commands, some are hedonists. Consequentialists and deontologists in practice always also subscribe to some meta-ethical theory which purports to explain the value of consequences or the source of injunctions. But consequentialism and deontology as such do not. In order to avoid strawmaning either the consequentialist or the deontologist, it&rsquo;s important to either discuss the comprehensive views of particular ethicists, or to carefully leave aside meta-ethical issues.</p>\n<p>This Stanford Encyclopedia of Philosophy article provides a helpful overview of the issues in the consequentialist-deontologist debate, and is careful to distinguish between ethical and meta-ethical concerns.</p>\n<p><a href=\"http://plato.stanford.edu/entries/ethics-deontological/\">SEP article on Deontology</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HyLfYAyHsiwem56PD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "21562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-11T23:48:58.812Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, MA third-Sunday meetup", "slug": "meetup-cambridge-ma-third-sunday-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bBFvPvMyLTJWaTDoy/meetup-cambridge-ma-third-sunday-meetup", "pageUrlRelative": "/posts/bBFvPvMyLTJWaTDoy/meetup-cambridge-ma-third-sunday-meetup", "linkUrl": "https://www.lesswrong.com/posts/bBFvPvMyLTJWaTDoy/meetup-cambridge-ma-third-sunday-meetup", "postedAtFormatted": "Monday, February 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20MA%20third-Sunday%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20MA%20third-Sunday%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbBFvPvMyLTJWaTDoy%2Fmeetup-cambridge-ma-third-sunday-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20MA%20third-Sunday%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbBFvPvMyLTJWaTDoy%2Fmeetup-cambridge-ma-third-sunday-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbBFvPvMyLTJWaTDoy%2Fmeetup-cambridge-ma-third-sunday-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/j5'>Cambridge, MA third-Sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 February 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA 02138</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups recur on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/j5'>Cambridge, MA third-Sunday meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bBFvPvMyLTJWaTDoy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.1098656683590784e-06, "legacy": true, "legacyId": "21563", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_third_Sunday_meetup\">Discussion article for the meetup : <a href=\"/meetups/j5\">Cambridge, MA third-Sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 February 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA 02138</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups recur on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_third_Sunday_meetup1\">Discussion article for the meetup : <a href=\"/meetups/j5\">Cambridge, MA third-Sunday meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, MA third-Sunday meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_third_Sunday_meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, MA third-Sunday meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_third_Sunday_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-12T01:32:09.073Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Value is Fragile", "slug": "seq-rerun-value-is-fragile", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:35.724Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DpqsRJGXFFP7Q7xfe/seq-rerun-value-is-fragile", "pageUrlRelative": "/posts/DpqsRJGXFFP7Q7xfe/seq-rerun-value-is-fragile", "linkUrl": "https://www.lesswrong.com/posts/DpqsRJGXFFP7Q7xfe/seq-rerun-value-is-fragile", "postedAtFormatted": "Tuesday, February 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Value%20is%20Fragile&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Value%20is%20Fragile%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpqsRJGXFFP7Q7xfe%2Fseq-rerun-value-is-fragile%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Value%20is%20Fragile%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpqsRJGXFFP7Q7xfe%2Fseq-rerun-value-is-fragile", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpqsRJGXFFP7Q7xfe%2Fseq-rerun-value-is-fragile", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p>Today's post, <a href=\"/lw/y3/value_is_fragile/\">Value is Fragile</a> was originally published on 29 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Value_is_Fragile\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An interesting universe, that would be incomprehensible to the universe today, is what the future looks like if things go right. There are a lot of things that humans value that if you did everything else right, when building an AI, but left out that one thing, the future would wind up looking dull, flat, pointless, or empty. Any Future not shaped by a goal system with detailed reliable inheritance from human morals and metamorals, will contain almost nothing of worth.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gmt/seq_rerun_31_laws_of_fun/\">31 Laws of Fun</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DpqsRJGXFFP7Q7xfe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.109930843417917e-06, "legacy": true, "legacyId": "21565", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GNnHHmm8EzePmKzPk", "n56jAYSsFhPmREdkB", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-12T09:41:19.788Z", "modifiedAt": null, "url": null, "title": "Is suicide high-status?", "slug": "is-suicide-high-status", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.286Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MW8mZf6hAmepFLbYC/is-suicide-high-status", "pageUrlRelative": "/posts/MW8mZf6hAmepFLbYC/is-suicide-high-status", "linkUrl": "https://www.lesswrong.com/posts/MW8mZf6hAmepFLbYC/is-suicide-high-status", "postedAtFormatted": "Tuesday, February 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20suicide%20high-status%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20suicide%20high-status%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMW8mZf6hAmepFLbYC%2Fis-suicide-high-status%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20suicide%20high-status%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMW8mZf6hAmepFLbYC%2Fis-suicide-high-status", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMW8mZf6hAmepFLbYC%2Fis-suicide-high-status", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 426, "htmlBody": "<p>I sometimes have thoughts of suicide. That does <strong>not </strong>mean I would ever come within a mile of committing the act of suicide. But my brain does simulate it; though I do try to always reduce such thoughts.</p>\n<p>But what I have noticed is that 'suicide' is triggered in my mind whenever I think of some embarrassing event, real or imagined. Or an event in which I'm obviously a low-status actor. This leads me to think that suicide might be a high-status move, in the sense that its goal is to recover status after some event which caused a big drop in status. Consider the following instances when suicide is often considered:</p>\n<ol>\n<li>One-sided break-ups of romantic relationships. The party who has been 'dumped' (for the lack of a better word), has obviously taken a giant status hit. In this case, suicide is often threatened.&nbsp;</li>\n<li>A samurai committing seppuku. The samurai has lost in battle. Clearly, a huge drop in status (aka 'honor').</li>\n<li>PhD student says he/she can't take it anymore. A PhD is a constant hit in status: you aren't smart enough, you don't have much money, and you don't yet have intellectual status.</li>\n</ol>\n<div>Further, suicide (or suicidal behavior leading to death) seems to have conferred status to artists. Examples: Kurt Cobain, Amy Winehouse, Jimi Hendrix, Hunter S. Thompson, Ernest Hemingway, David Foster Wallace and many more. I'm not saying that they committed suicide due to a pressure to achieve high-status (though that may be the case, I'm not sure). What I am saying is that suicide has been associated with high-status.&nbsp;</div>\n<div><br /></div>\n<div>Further, after a person is dead, he/she is almost always celebrated (at least for a while) and all their faults are forgotten.</div>\n<div><br /></div>\n<div>My theory: in many low-status situations, an instinctive way to recover status is to say that you are too good for this game and check-out. In fact, children (and adults) will often just leave a game they're not very good at and disparage the rest of the players for playing. And suicide is the ultimate check-out. This theory is motivated by observations of my own brain going through thoughts of suicide. They almost always consist of imagining other people crying about my death and saying what an awesome person he was. And about how he was just too smart to be able to live in this world.&nbsp;</div>\n<div><br /></div>\n<div>Do you think this theory has some weight? I'm certain that I'm not the first person to think of this. But a quick Google didn't yield much. Any pointers to literature?</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MW8mZf6hAmepFLbYC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 3, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "21566", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 83, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-12T21:19:24.569Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal LessWrong Meetup - The Science of Winning at Life", "slug": "meetup-montreal-lesswrong-meetup-the-science-of-winning-at", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XEMoeiHZmSp5yQ5RW/meetup-montreal-lesswrong-meetup-the-science-of-winning-at", "pageUrlRelative": "/posts/XEMoeiHZmSp5yQ5RW/meetup-montreal-lesswrong-meetup-the-science-of-winning-at", "linkUrl": "https://www.lesswrong.com/posts/XEMoeiHZmSp5yQ5RW/meetup-montreal-lesswrong-meetup-the-science-of-winning-at", "postedAtFormatted": "Tuesday, February 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20LessWrong%20Meetup%20-%20The%20Science%20of%20Winning%20at%20Life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20LessWrong%20Meetup%20-%20The%20Science%20of%20Winning%20at%20Life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXEMoeiHZmSp5yQ5RW%2Fmeetup-montreal-lesswrong-meetup-the-science-of-winning-at%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20LessWrong%20Meetup%20-%20The%20Science%20of%20Winning%20at%20Life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXEMoeiHZmSp5yQ5RW%2Fmeetup-montreal-lesswrong-meetup-the-science-of-winning-at", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXEMoeiHZmSp5yQ5RW%2Fmeetup-montreal-lesswrong-meetup-the-science-of-winning-at", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/j6'>Montreal LessWrong Meetup - The Science of Winning at Life</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 February 2013 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">655 Avenue du Pr\u00e9sident Kennedy, Montr\u00e9al, QC H3A 3H9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weekly meeting of the Montreal LessWrong Meetup group.</p>\n\n<p>We've decided to look into the Science of Winning at Life. You can read the sequence here (<a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life</a>) if you're interested.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/j6'>Montreal LessWrong Meetup - The Science of Winning at Life</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XEMoeiHZmSp5yQ5RW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.1106813444195423e-06, "legacy": true, "legacyId": "21570", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___The_Science_of_Winning_at_Life\">Discussion article for the meetup : <a href=\"/meetups/j6\">Montreal LessWrong Meetup - The Science of Winning at Life</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 February 2013 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">655 Avenue du Pr\u00e9sident Kennedy, Montr\u00e9al, QC H3A 3H9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weekly meeting of the Montreal LessWrong Meetup group.</p>\n\n<p>We've decided to look into the Science of Winning at Life. You can read the sequence here (<a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life</a>) if you're interested.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___The_Science_of_Winning_at_Life1\">Discussion article for the meetup : <a href=\"/meetups/j6\">Montreal LessWrong Meetup - The Science of Winning at Life</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal LessWrong Meetup - The Science of Winning at Life", "anchor": "Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___The_Science_of_Winning_at_Life", "level": 1}, {"title": "Discussion article for the meetup : Montreal LessWrong Meetup - The Science of Winning at Life", "anchor": "Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___The_Science_of_Winning_at_Life1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-13T00:02:47.786Z", "modifiedAt": null, "url": null, "title": "Meetup : London Meetup, 24th Feb", "slug": "meetup-london-meetup-24th-feb", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.232Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ypnn9s2QsCaSbcZZx/meetup-london-meetup-24th-feb", "pageUrlRelative": "/posts/Ypnn9s2QsCaSbcZZx/meetup-london-meetup-24th-feb", "linkUrl": "https://www.lesswrong.com/posts/Ypnn9s2QsCaSbcZZx/meetup-london-meetup-24th-feb", "postedAtFormatted": "Wednesday, February 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Meetup%2C%2024th%20Feb&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Meetup%2C%2024th%20Feb%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYpnn9s2QsCaSbcZZx%2Fmeetup-london-meetup-24th-feb%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Meetup%2C%2024th%20Feb%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYpnn9s2QsCaSbcZZx%2Fmeetup-london-meetup-24th-feb", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYpnn9s2QsCaSbcZZx%2Fmeetup-london-meetup-24th-feb", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/j7\">London Meetup, 24th Feb</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">24 February 2013 02:00:00PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Holborn, London</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>A meetup in the <a rel=\"nofollow\" href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\">Shakespeare's Head pub</a> by Holborn tube station. We meet every other Sunday at 2pm. Everyone is welcome.</p>\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/j7\">London Meetup, 24th Feb</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ypnn9s2QsCaSbcZZx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "21571", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__24th_Feb\">Discussion article for the meetup : <a href=\"/meetups/j7\">London Meetup, 24th Feb</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">24 February 2013 02:00:00PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Holborn, London</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>A meetup in the <a rel=\"nofollow\" href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\">Shakespeare's Head pub</a> by Holborn tube station. We meet every other Sunday at 2pm. Everyone is welcome.</p>\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___London_Meetup__24th_Feb1\">Discussion article for the meetup : <a href=\"/meetups/j7\">London Meetup, 24th Feb</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Meetup, 24th Feb", "anchor": "Discussion_article_for_the_meetup___London_Meetup__24th_Feb", "level": 1}, {"title": "Discussion article for the meetup : London Meetup, 24th Feb", "anchor": "Discussion_article_for_the_meetup___London_Meetup__24th_Feb1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-13T05:29:57.295Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Baby-Eating Aliens (1/8)", "slug": "seq-rerun-the-baby-eating-aliens-1-8", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.849Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ineLyCTv5q9zhqhF6/seq-rerun-the-baby-eating-aliens-1-8", "pageUrlRelative": "/posts/ineLyCTv5q9zhqhF6/seq-rerun-the-baby-eating-aliens-1-8", "linkUrl": "https://www.lesswrong.com/posts/ineLyCTv5q9zhqhF6/seq-rerun-the-baby-eating-aliens-1-8", "postedAtFormatted": "Wednesday, February 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Baby-Eating%20Aliens%20(1%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Baby-Eating%20Aliens%20(1%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FineLyCTv5q9zhqhF6%2Fseq-rerun-the-baby-eating-aliens-1-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Baby-Eating%20Aliens%20(1%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FineLyCTv5q9zhqhF6%2Fseq-rerun-the-baby-eating-aliens-1-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FineLyCTv5q9zhqhF6%2Fseq-rerun-the-baby-eating-aliens-1-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>Today's post, <a href=\"/lw/y5/the_babyeating_aliens_18/\">The Baby-Eating Aliens (1/8)</a> was originally published on 30 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#The_Baby-Eating_Aliens_.281.2F8.29\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Future explorers discover an alien civilization, and learns something unpleasant about their civilization.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gn1/seq_rerun_value_is_fragile/\">Value is Fragile</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ineLyCTv5q9zhqhF6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.1109916948065422e-06, "legacy": true, "legacyId": "21573", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["n5TqCuizyJDfAPjkr", "DpqsRJGXFFP7Q7xfe", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-13T05:42:21.888Z", "modifiedAt": null, "url": null, "title": "Questions for Moral Realists", "slug": "questions-for-moral-realists-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j7dAaKJLLMxdC4YNy/questions-for-moral-realists-0", "pageUrlRelative": "/posts/j7dAaKJLLMxdC4YNy/questions-for-moral-realists-0", "linkUrl": "https://www.lesswrong.com/posts/j7dAaKJLLMxdC4YNy/questions-for-moral-realists-0", "postedAtFormatted": "Wednesday, February 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Questions%20for%20Moral%20Realists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestions%20for%20Moral%20Realists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj7dAaKJLLMxdC4YNy%2Fquestions-for-moral-realists-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Questions%20for%20Moral%20Realists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj7dAaKJLLMxdC4YNy%2Fquestions-for-moral-realists-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj7dAaKJLLMxdC4YNy%2Fquestions-for-moral-realists-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<p>&nbsp;</p>\n<p>My meta-ethics are basically that of Luke's [Pluralistic Moral Reductionism](http://lesswrong.com/lw/5u2/pluralistic_moral_reductionism/). &nbsp;You can see details on my blog series, starting with [\"The Meaning of Morality\"](http://www.greatplay.net/essays/the-meaning-of-morality).</p>\n<p>However, I was curious as to whether this \"Pluralistic Moral Reductionism\" counts as moral realism or anti-realism. &nbsp;Luke's essay says it depends on what I mean by \"moral realism\". &nbsp;I see moral realism as broken down into three separate axes:</p>\n<p>There's **success theory**, the part that I accept, which states that moral statements like \"murder is wrong\" do successfully refer to something real (in this case, a particular moral standard, like utilitarianism -- \"murder is wrong\" refers to \"murder does not maximize happiness\").</p>\n<p>There's **unitary theory**, which I reject, that states there is only one \"true\" moral standard rather than hundreds of possible ones.</p>\n<p>And then there's **absolutism theory**, which I reject, that states that the one true morality is rationally binding.</p>\n<p>I don't know how many moral realists are on LessWrong, but I have a few questions for people who accept moral realism, especially unitary theory or absolutism theory. &nbsp;These are \"generally seeking understanding and opposing points of view\" kind of questions, not stumper questions designed to disprove or anything. &nbsp;While I'm doing some more reading on the topic, if you're into moral realism, you could help me out by sharing your perspective.</p>\n<p>~</p>\n<p>[size=130]Why is there only one particular morality?[/size]</p>\n<p>This goes right to the core of unitary theory -- that there is only one true theory of morality. &nbsp;But I must admit I'm dumbfounded at how any one particular theory of morality could be \"the one true one\", except in so far as someone personally chooses that theory over others based on preferences and desires.</p>\n<p>So why is there only one particular morality? &nbsp;And what is the one true theory of morality? &nbsp;What makes this theory the one true one rather than others? &nbsp;How do we know there is only one particular theory? &nbsp;What's inadequate about all the other candidates?</p>\n<p>~</p>\n<p>[size=130]Where does morality come from?[/size]</p>\n<p>This gets me a bit more background knowledge, but what is the ontology of morality? &nbsp;Some concepts of moral realism have an idea of a \"moral realm\", while others reject this as needlessly queer and spooky. &nbsp;But essentially, what is grounding morality? &nbsp;Are moral facts contingent; could morality have been different? &nbsp;Is it possible to make it different in the future?</p>\n<p>~</p>\n<p>[size=130]Why should we care about (your) morality?[/size]</p>\n<p>I see rationality as talking about what best satisfies your pre-existing desires. &nbsp;But it's entirely possible that morality isn't desirable by someone at all. &nbsp;While I hope that society is prepared to coerce them into moral behavior (either through social or legal force), I don't think that their immoral behavior is necessarily irrational. &nbsp;And on some accounts, morality is independent of desire but still has rational force.</p>\n<p>How does morality get it's ability to be rationally binding? &nbsp;If the very definition of \"rationality\" includes being moral, is that mere wordplay? &nbsp;Why should we accept this definition of rationality and not a different one?</p>\n<p>I look forward to engaging in diologue with some moral realists. &nbsp;Same with moral anti-realists, I guess. &nbsp;After all, [if moral realism is true, I want to know](http://www.overcomingbias.com/2013/01/morality-as-though-it-really-mattered.html).</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j7dAaKJLLMxdC4YNy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "21574", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-13T05:44:44.948Z", "modifiedAt": null, "url": null, "title": "Questions for Moral Realists", "slug": "questions-for-moral-realists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:57.749Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iBAJ8bbjngfGfFwRh/questions-for-moral-realists", "pageUrlRelative": "/posts/iBAJ8bbjngfGfFwRh/questions-for-moral-realists", "linkUrl": "https://www.lesswrong.com/posts/iBAJ8bbjngfGfFwRh/questions-for-moral-realists", "postedAtFormatted": "Wednesday, February 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Questions%20for%20Moral%20Realists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestions%20for%20Moral%20Realists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiBAJ8bbjngfGfFwRh%2Fquestions-for-moral-realists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Questions%20for%20Moral%20Realists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiBAJ8bbjngfGfFwRh%2Fquestions-for-moral-realists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiBAJ8bbjngfGfFwRh%2Fquestions-for-moral-realists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 508, "htmlBody": "<p>My meta-ethics are basically that of Luke's <a href=\"/lw/5u2/pluralistic_moral_reductionism/\">Pluralistic Moral Reductionism</a>. &nbsp;(UPDATE: Elaborated in <a href=\"http://everydayutilitarian.com/essays/a-meta-ethics-faq/\">my Meta-ethics FAQ</a>.)</p>\n<p>However, I was curious as to whether this \"Pluralistic Moral Reductionism\" counts as moral realism or anti-realism. &nbsp;Luke's essay says it depends on what I mean by \"moral realism\". &nbsp;I see moral realism as broken down into three separate axes:</p>\n<p>There's <strong>success theory</strong>, the part that I accept, which states that moral statements like \"murder is wrong\" do successfully refer to something real (in this case, a particular moral standard, like utilitarianism -- \"murder is wrong\" refers to \"murder does not maximize happiness\").</p>\n<p>There's <strong>unitary theory</strong>, which I reject, that states there is only one \"true\" moral standard rather than hundreds of possible ones.</p>\n<p>And then there's <strong>absolutism theory</strong>, which I reject, that states that the one true morality is rationally binding.</p>\n<p>I don't know how many moral realists are on LessWrong, but I have a few questions for people who accept moral realism, especially unitary theory or absolutism theory. &nbsp;These are \"generally seeking understanding and opposing points of view\" kind of questions, not stumper questions designed to disprove or anything. &nbsp;While I'm doing some more reading on the topic, if you're into moral realism, you could help me out by sharing your perspective.</p>\n<p>~</p>\n<h2>Why is there only one particular morality?</h2>\n<p>This goes right to the core of unitary theory -- that there is only one true theory of morality. &nbsp;But I must admit I'm dumbfounded at how any one particular theory of morality could be \"the one true one\", except in so far as someone personally chooses that theory over others based on preferences and desires.</p>\n<p>So why is there only one particular morality? &nbsp;And what is the one true theory of morality? &nbsp;What makes this theory the one true one rather than others? &nbsp;How do we know there is only one particular theory? &nbsp;What's inadequate about all the other candidates?</p>\n<p>~</p>\n<h2>Where does morality come from?</h2>\n<p>This gets me a bit more background knowledge, but what is the ontology of morality? &nbsp;Some concepts of moral realism have an idea of a \"moral realm\", while others reject this as needlessly queer and spooky. &nbsp;But essentially, what is grounding morality? &nbsp;Are moral facts contingent; could morality have been different? &nbsp;Is it possible to make it different in the future?</p>\n<p>~</p>\n<h2>Why should we care about (your) morality?</h2>\n<p>I see rationality as talking about what best satisfies your pre-existing desires. &nbsp;But it's entirely possible that morality isn't desirable by someone at all. &nbsp;While I hope that society is prepared to coerce them into moral behavior (either through social or legal force), I don't think that their immoral behavior is necessarily irrational. &nbsp;And on some accounts, morality is independent of desire but still has rational force.</p>\n<p>How does morality get it's ability to be rationally binding? &nbsp;If the very definition of \"rationality\" includes being moral, is that mere wordplay? &nbsp;Why should we accept this definition of rationality and not a different one?</p>\n<p>I look forward to engaging in diologue with some moral realists. &nbsp;Same with moral anti-realists, I guess. &nbsp;After all, <a href=\"http://www.overcomingbias.com/2013/01/morality-as-though-it-really-mattered.html\">if moral realism is true, I want to know</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iBAJ8bbjngfGfFwRh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "21575", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>My meta-ethics are basically that of Luke's <a href=\"/lw/5u2/pluralistic_moral_reductionism/\">Pluralistic Moral Reductionism</a>. &nbsp;(UPDATE: Elaborated in <a href=\"http://everydayutilitarian.com/essays/a-meta-ethics-faq/\">my Meta-ethics FAQ</a>.)</p>\n<p>However, I was curious as to whether this \"Pluralistic Moral Reductionism\" counts as moral realism or anti-realism. &nbsp;Luke's essay says it depends on what I mean by \"moral realism\". &nbsp;I see moral realism as broken down into three separate axes:</p>\n<p>There's <strong>success theory</strong>, the part that I accept, which states that moral statements like \"murder is wrong\" do successfully refer to something real (in this case, a particular moral standard, like utilitarianism -- \"murder is wrong\" refers to \"murder does not maximize happiness\").</p>\n<p>There's <strong>unitary theory</strong>, which I reject, that states there is only one \"true\" moral standard rather than hundreds of possible ones.</p>\n<p>And then there's <strong>absolutism theory</strong>, which I reject, that states that the one true morality is rationally binding.</p>\n<p>I don't know how many moral realists are on LessWrong, but I have a few questions for people who accept moral realism, especially unitary theory or absolutism theory. &nbsp;These are \"generally seeking understanding and opposing points of view\" kind of questions, not stumper questions designed to disprove or anything. &nbsp;While I'm doing some more reading on the topic, if you're into moral realism, you could help me out by sharing your perspective.</p>\n<p>~</p>\n<h2 id=\"Why_is_there_only_one_particular_morality_\">Why is there only one particular morality?</h2>\n<p>This goes right to the core of unitary theory -- that there is only one true theory of morality. &nbsp;But I must admit I'm dumbfounded at how any one particular theory of morality could be \"the one true one\", except in so far as someone personally chooses that theory over others based on preferences and desires.</p>\n<p>So why is there only one particular morality? &nbsp;And what is the one true theory of morality? &nbsp;What makes this theory the one true one rather than others? &nbsp;How do we know there is only one particular theory? &nbsp;What's inadequate about all the other candidates?</p>\n<p>~</p>\n<h2 id=\"Where_does_morality_come_from_\">Where does morality come from?</h2>\n<p>This gets me a bit more background knowledge, but what is the ontology of morality? &nbsp;Some concepts of moral realism have an idea of a \"moral realm\", while others reject this as needlessly queer and spooky. &nbsp;But essentially, what is grounding morality? &nbsp;Are moral facts contingent; could morality have been different? &nbsp;Is it possible to make it different in the future?</p>\n<p>~</p>\n<h2 id=\"Why_should_we_care_about__your__morality_\">Why should we care about (your) morality?</h2>\n<p>I see rationality as talking about what best satisfies your pre-existing desires. &nbsp;But it's entirely possible that morality isn't desirable by someone at all. &nbsp;While I hope that society is prepared to coerce them into moral behavior (either through social or legal force), I don't think that their immoral behavior is necessarily irrational. &nbsp;And on some accounts, morality is independent of desire but still has rational force.</p>\n<p>How does morality get it's ability to be rationally binding? &nbsp;If the very definition of \"rationality\" includes being moral, is that mere wordplay? &nbsp;Why should we accept this definition of rationality and not a different one?</p>\n<p>I look forward to engaging in diologue with some moral realists. &nbsp;Same with moral anti-realists, I guess. &nbsp;After all, <a href=\"http://www.overcomingbias.com/2013/01/morality-as-though-it-really-mattered.html\">if moral realism is true, I want to know</a>.</p>", "sections": [{"title": "Why is there only one particular morality?", "anchor": "Why_is_there_only_one_particular_morality_", "level": 1}, {"title": "Where does morality come from?", "anchor": "Where_does_morality_come_from_", "level": 1}, {"title": "Why should we care about (your) morality?", "anchor": "Why_should_we_care_about__your__morality_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "110 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 110, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3zDX3f3QTepNeZHGc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-13T05:57:55.058Z", "modifiedAt": null, "url": null, "title": "[LINK] Open Source Software Developer with Terminal Illness Hopes to Opt Out of Death", "slug": "link-open-source-software-developer-with-terminal-illness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:56.281Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rSWBkLpoQWERWdGjM/link-open-source-software-developer-with-terminal-illness", "pageUrlRelative": "/posts/rSWBkLpoQWERWdGjM/link-open-source-software-developer-with-terminal-illness", "linkUrl": "https://www.lesswrong.com/posts/rSWBkLpoQWERWdGjM/link-open-source-software-developer-with-terminal-illness", "postedAtFormatted": "Wednesday, February 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Open%20Source%20Software%20Developer%20with%20Terminal%20Illness%20Hopes%20to%20Opt%20Out%20of%20Death&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Open%20Source%20Software%20Developer%20with%20Terminal%20Illness%20Hopes%20to%20Opt%20Out%20of%20Death%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrSWBkLpoQWERWdGjM%2Flink-open-source-software-developer-with-terminal-illness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Open%20Source%20Software%20Developer%20with%20Terminal%20Illness%20Hopes%20to%20Opt%20Out%20of%20Death%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrSWBkLpoQWERWdGjM%2Flink-open-source-software-developer-with-terminal-illness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrSWBkLpoQWERWdGjM%2Flink-open-source-software-developer-with-terminal-illness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1257, "htmlBody": "<p>Aaron Winborn writes:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">TLDR:&nbsp;<a style=\"text-decoration: none; padding: 0px; margin: 0px;\" title=\"http://venturist.info/aaron-winborn-charity.html\" href=\"http://venturist.info/aaron-winborn-charity.html\">http://venturist.info/aaron-winborn-charity.html</a></p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">So maybe you've heard about my plight, in which I wrestle Lou Gehrig in this losing battle to stay alive. And I use the phrase \"staying alive\" loosely, as many would shudder at the thought of becoming locked in with&nbsp;<a style=\"text-decoration: none; padding: 0px; margin: 0px;\" href=\"http://aaronwinborn.com/blogs/aaron/en.wikipedia.org/wiki/Lou_Gehrig's_disease\">ALS</a>, completely paralyzed, unable to move a muscle other than your eyes.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">But that's only half the story. Wait for the punchline.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">As if the physical challenges of adapting to new and increasingly debilitating disabilities were not enough, my wife and two young daughters are forced to watch helplessly as the man they knew loses the ability to lift a fork or scratch an itch, who just two years ago was able to lift his infant daughter and run with the 7-year-old. The emotional strain on my family is more than any family should have to bear. Not to mention the financial difficulties, which include big purchases such as a wheelchair van and home modifications, and ultimately round the clock nursing care, all of it exacerbated by the fact that we have had to give up my income both because of the illness and to qualify for disability and Medicaid.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; font-size: 12px; line-height: 18px; padding: 0px;\"><img src=\"http://aaronwinborn.com/sites/aaronwinborn.com/files/before-rollerskating.JPG\" alt=\"\" /></p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">Meet me, Aaron Winborn, software developer and author of&nbsp;<a style=\"text-decoration: none; padding: 0px; margin: 0px;\" href=\"http://www.packtpub.com/create-multimedia-website-with-drupal/book?mid=230609ykjxg1\">Drupal Multimedia</a>, champion of the open source software movement.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">Years ago, I worked for the lady of death herself, Elisabeth K&uuml;bler-Ross, the author of On Death and Dying. Of course, I knew that one day I would need to confront death, but like most people, I assumed it would be when I was old, not in the prime of my life. Not that I'm complaining; I have lived a full life, from living in a Buddhist monastery to living overseas, from marrying the woman of my dreams to having two wonderful daughters, from teaching in a radical school to building websites for progressive organizations, from running a flight simulator for the US Navy to working as a puppeteer.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">I accept the fact of my inevitable death. But accepting death does not, I believe, mean simply rolling over and letting that old dog bite you. Regardless of the prevalent mindset in society that says that people die and so should you, get over it, I believe that the reality we experience of people living only to a few decades is about to be turned upside down.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">Ray Kurzweil spells out a coming technological singularity, in which accelerating technologies reach a critical mass and we reach a post-human world. He boldly predicts this will happen by the year 2045. I figured that if I could make it to 2035, my late 60s, that I would be able to take advantage of whatever medical advances were available and ride the wave to a radically extended lifespan.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">ALS dictates otherwise. 50% of everyone diagnosed will die within 2 to 3 years of the onset of the disease. 80% will be gone in 5 years. And only 10% go on to survive a decade, most of them locked in, paralyzed completely, similar to Stephen Hawking. Sadly, my scores put me on the fast track of the 50%, and I am coming up quickly on 3 years.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">Enter Kim Suozzi.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">On June 10 of last year, her birthday, which is coincidentally my own,&nbsp;<a style=\"text-decoration: none; padding: 0px; margin: 0px;\" href=\"http://www.huffingtonpost.com/2013/01/22/kim-suozzi-cryogenicallly-preserved-cancer_n_2526415.html\">Kim Suozzi</a>&nbsp;asked a question to the Internet, \"Today is my 23rd birthday and probably my last. Anything awesome I should try before I die?\" The answer that she received and acted on would probably be surprising to many.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">On January 17, 2013, Kim Suozzi died, and as per her dying wish, was cryonically preserved.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">She was a brave person, and I hope to meet her someday.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">So yes, there we have it. The point that I am making with all this rambling. I hope to freeze my body after I die, in the hope of future medical technologies advancing to the point where they will be able to revive me.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">The good news is that in the scheme of things, it is not too terribly expensive to have yourself cryonically preserved. You should look at it yourself; most people will fund it with a $35K-200K life insurance policy.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">The bad news for me is that a life insurance policy is out of the question for me; a terminal illness precludes that as an option. Likewise, due to the financial hardships in store for us, self-funding is also out of the question.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">When I learned about Kim Suozzi's plight, I reached out to the organization that set up the charity that ultimately funded her cryopreservation. The&nbsp;<a style=\"text-decoration: none; padding: 0px; margin: 0px;\" href=\"http://www.venturist.info/index.html\">Society for Venturism</a>, a non-profit that has raised funds for the eventual cryopreservation of terminally ill patients, agreed to take on my case.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">Many of you reading this post have already helped out in so many ways. From volunteering your time and effort to our family, to donating money towards my&nbsp;<a style=\"text-decoration: none; padding: 0px; margin: 0px;\" href=\"http://aaronwinborn.com/blogs/aaron/special-needs-trust\">Special Needs Trust</a>&nbsp;to help provide a cushion for the difficult times ahead.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">I am so grateful for all of this. It means so much to me and my family to know that there is such a large and generous community supporting us. I hate to ask for anything more, especially for something that may seem like an extravagance.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">But is it really an extravagance?</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">If I were to ask for $100,000 for an experimental stem cell treatment, I doubt that we would even be having this conversation. No one in their right mind would even consider a potentially life-saving procedure to be an extravagance.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">And what is cryonics, but a potentially life-saving procedure?</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">People choose from among many options for their bodies after death. Some choose to be buried, some choose cremation. Some choose to donate their bodies to science. That last is precisely what happens with cryonics: in addition to helping to answer the obvious question of will future revival from cold storage be possible, many developments in cryonics help modern medicine with the development of better preservation for organ transplantation and blood volume expanders.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">Yes, I admit that the chances of it working are slim, but have you looked at the state of stem cell research for ALS lately? Consider that the only FDA approved medication to treat ALS, Rilutek, will on average add 3 months to one's lifespan, and you might begin to see my desperation.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">But you should be happy with the life you've had. Why do you want to live forever?</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">The only reasonable response to that is to ask why do you want to die?</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">I love life. Every morning, even now with my body half paralyzed, I awaken with a new sense of purpose, excited to take on the day. There is so much I have yet to do. There are books to write, games to create, songs to sing. If I can get the use of my arms and hands again, there are gardens to plant, houses to build, space ships to fly. And oh, the people to love.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">So please help me to realize this, my dying wish.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; font-size: 12px; line-height: 18px; padding: 0px;\"><a style=\"text-decoration: none; padding: 0px; margin: 0px;\" title=\"http://venturist.info/aaron-winborn-charity.html\" href=\"http://venturist.info/aaron-winborn-charity.html\">http://venturist.info/aaron-winborn-charity.html</a></p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif; padding: 0px;\">\"The most beautiful people we have known are those who have known defeat, known suffering, known struggle, known loss, and have found their way out of the depths. These persons have an appreciation, a sensitivity, and an understanding of life that fills them with compassion, gentleness, and a deep loving concern. Beautiful people do not just happen.\"</p>\n<p><span style=\"font-family: Arial, Helvetica, Verdana, 'Bitstream Vera Sans', sans-serif;\">- Elisabeth K&uuml;bler-Ross</span></p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Blog post: http://aaronwinborn.com/blogs/aaron/open-source-software-developer-terminal-illness-hopes-opt-out-death</p>\n<p>Hacker news discussion:&nbsp;http://news.ycombinator.com/item?id=5211602</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E9ihK6bA9YKkmJs2f": 1, "ZnHkaTkxukegSrZqE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rSWBkLpoQWERWdGjM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 22, "extendedScore": null, "score": 1.1110093904225178e-06, "legacy": true, "legacyId": "21576", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-13T09:40:18.946Z", "modifiedAt": null, "url": null, "title": "Realism : Direct or Indirect?", "slug": "realism-direct-or-indirect", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:38.202Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kremlin", "createdAt": "2011-11-20T13:42:17.401Z", "isAdmin": false, "displayName": "kremlin"}, "userId": "hs8N5xYvR4GGkDg8f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gqyexMp4Rxgc7TZvG/realism-direct-or-indirect", "pageUrlRelative": "/posts/gqyexMp4Rxgc7TZvG/realism-direct-or-indirect", "linkUrl": "https://www.lesswrong.com/posts/gqyexMp4Rxgc7TZvG/realism-direct-or-indirect", "postedAtFormatted": "Wednesday, February 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Realism%20%3A%20Direct%20or%20Indirect%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARealism%20%3A%20Direct%20or%20Indirect%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgqyexMp4Rxgc7TZvG%2Frealism-direct-or-indirect%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Realism%20%3A%20Direct%20or%20Indirect%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgqyexMp4Rxgc7TZvG%2Frealism-direct-or-indirect", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgqyexMp4Rxgc7TZvG%2Frealism-direct-or-indirect", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 316, "htmlBody": "<p><a href=\"http://plato.stanford.edu/entries/perception-episprob/\">Stanford Encyclopedia : Perception</a><br /><a href=\"http://en.wikipedia.org/wiki/Direct_and_indirect_realism\">Wikipedia : Direct and Indirect Realism</a></p>\n<p>On various philosophy forums I've participated on, there have been arguments between those who call themselves 'direct realists' and those who call themselves 'indirect realists'. The question is apparently about perception. Do we experience reality directly, or do we experience it indirectly?</p>\n<p>When I was first initiated to the conversation, I immediately took the indirect side -- There is a ball, photons bounce off the ball, the frequency of those photons is changed by some properties of the ball, the photons hit my retina activating light-sensitive cells, those cells send signals to my brain communicating that they were activated, the signals make it to the visual cortex and...you know...some stuff happens, and I experience the sight of a ball.</p>\n<p>So, my first thought in the conversation about Indirect vs Direct realism was that there was a lot of stuff in between the ball and my experience of it, so, it must be indirect.</p>\n<p>But then I found that direct realists don't actually disagree about any part of that sequence of events I described above. For them as well, at least the few that have bothered to respond, photons bounce off a ball, interact with our retinas, send signals to the brain, etc. The physical process is apparently the same for both sides of the debate.</p>\n<p>And when two sides vehemently disagree on something, and then when the question is broken down into easy, answerable questions you find that they actually agree on every relevant question, that tends to be a pretty good hint that it's a <a href=\"/lw/og/wrong_questions/\">wrong</a> <a href=\"/lw/np/disputing_definitions/\">question</a>.</p>\n<p>So, is this a wrong question? Is this just a debate about definitions? Is it a semantic argument, or is there a meaningful difference between Direct and Indirect Realism? In the paraphrased words of Eliezer, <em>\"Is there any way-the-world-could-be&mdash;any state of affairs&mdash;that corresponds to Direct Realism being true, or Indirect Realism being true?\"</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gqyexMp4Rxgc7TZvG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 4, "extendedScore": null, "score": 1.1111501482033932e-06, "legacy": true, "legacyId": "21578", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XzrqkhfwtiSDgKoAF", "7X2j8HAkWdmMoS8PE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-13T13:45:25.024Z", "modifiedAt": null, "url": null, "title": "The Fundamental Question - Rationality computer game design", "slug": "the-fundamental-question-rationality-computer-game-design", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:29.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8kFewgKabR6vYLjQb/the-fundamental-question-rationality-computer-game-design", "pageUrlRelative": "/posts/8kFewgKabR6vYLjQb/the-fundamental-question-rationality-computer-game-design", "linkUrl": "https://www.lesswrong.com/posts/8kFewgKabR6vYLjQb/the-fundamental-question-rationality-computer-game-design", "postedAtFormatted": "Wednesday, February 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fundamental%20Question%20-%20Rationality%20computer%20game%20design&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fundamental%20Question%20-%20Rationality%20computer%20game%20design%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8kFewgKabR6vYLjQb%2Fthe-fundamental-question-rationality-computer-game-design%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fundamental%20Question%20-%20Rationality%20computer%20game%20design%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8kFewgKabR6vYLjQb%2Fthe-fundamental-question-rationality-computer-game-design", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8kFewgKabR6vYLjQb%2Fthe-fundamental-question-rationality-computer-game-design", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2650, "htmlBody": "<blockquote>\n<p>I sometimes go around saying that the fundamental question of rationality is <em>Why do you believe what you believe?</em></p>\n<p>-- <a href=\"/lw/q5/quantum_nonrealism/\">Eliezer in Quantum Non-Realism</a></p>\n</blockquote>\n<p>I was much impressed when they finally came out with a PC version of <a href=\"http://dragonboxapp.com/\">DragonBox</a>, and I got around to testing it on some children I knew. Two kids, one of them four and the other eight years old, ended up blazing through several levels of solving first-degree equations while having a lot of fun doing so, even though they didn't know what it was that they were doing. That made me think that there <em>has</em> to be some way of making a computer game that would similarly teach rationality skills at <a href=\"/lw/5kz/the_5second_level/\">the 5-second level</a>. Some game where you would actually be forced to learn useful skills if you wanted to make progress.</p>\n<p>After playing around with some ideas, I hit upon the notion of making a game centered around the Fundamental Question. I'm not sure whether this can be made to work, but it seems to have promise. The basic idea: you are required to figure out the solution to various mysteries by collecting various kinds of evidence. Some of the sources of evidence will be more reliable than others. In order to hit upon the correct solution, you need to consider where each piece of evidence came from, and whether you can rely on it.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Gameplay example</strong></span></p>\n<p>Now, let's go into a little more detail. Let's suppose that the game has a character called Bob. Bob tells you that tomorrow, eight o'clock, there will be an assassination attempt on Market Square. The fact that Bob has told you this is <a href=\"/lw/jl/what_is_evidence/\">evidence</a> for the claim being true, so the game automatically records the fact that you have such a piece of evidence, and that it came from Bob.</p>\n<p>(Click on the pictures in case you don't see them properly.)</p>\n<p><a href=\"http://kajsotala.fi/Random/Question1b.png\"><img style=\"vertical-align: middle;\" src=\"http://kajsotala.fi/Random/Question1b.png\" alt=\"\" /></a></p>\n<p>But how does Bob know that? You ask, and it turns out that Alice told him. So next, you go and ask Alice. Alice is confused and says that she never said anything about any assassination attempt: she just said that something big is going to be happen at the Market Square at that time, she heard it from the Mayor. The game records two new pieces of evidence: Alice's claim of something big happening at the Market Square tomorrow (which she heard from the Mayor), and her story of what she actually told Bob. Guess that Bob isn't a very reliable source of evidence: he has a tendency to come up with fancy invented details.</p>\n<p><a href=\"http://kajsotala.fi/Random/Question2.png\"><img style=\"vertical-align: middle;\" src=\"http://kajsotala.fi/Random/Question2.png\" alt=\"\" width=\"612\" height=\"611\" /></a></p>\n<p>Or is he? After all, your sole knowledge about Bob being unreliable is that Alice claims she never said what Bob says she said. But maybe Alice has a grudge against Bob, and is intentionally out to make everyone disbelieve him. Maybe it's Alice who's unreliable. The evidence that you have is compatible with both hypotheses. At this point, you don't have enough information to decide between them, but the game lets you experiment with setting either of them as \"true\" and seeing the implications of this on your belief network. Or maybe they're both true - Bob <em>is</em> generally unreliable, <em>and </em>Alice is out to discredit him. That's another possibility that you might want to consider. In any case, the claim that there will be an assassination tomorrow isn't looking very likely at the moment.</p>\n<p><a href=\"http://kajsotala.fi/Random/Question3b.png\"><img src=\"http://kajsotala.fi/Random/Question3b.png\" alt=\"\" width=\"639\" height=\"600\" /></a></p>\n<p>Actually, having the possibility for somebody lying should probably be a pretty late-game thing, as it makes your belief network a lot more complicated, and I'm not sure whether this thing should display numerical probabilities at all. Instead of having to juggle the hypotheses of \"Alice lied\" and \"Bob exaggerates things\", the game should probably just record the fact that \"Bob exaggerates things\". But I spent a bunch of time making these pictures, and they do illustrate some of the general principles involved, so I'll just use them for now.</p>\n<p><a href=\"http://kajsotala.fi/Random/Question4.png\"><img style=\"vertical-align: middle;\" src=\"http://kajsotala.fi/Random/Question4.png\" alt=\"\" width=\"807\" height=\"600\" /></a></p>\n<p><strong>Game basics</strong></p>\n<p>So, to repeat the basic premise of the game, in slightly more words this time around: your task is to figure out something, and in order to do so, you need to collect different pieces of evidence. As you do so, the game generates a <a href=\"/lw/ev3/causal_diagrams_and_causal_models/\">belief network</a> showing the origin and history of the various pieces of evidence that you've gathered. That much is done automatically. But often, the evidence that you've gathered is compatible with many different hypotheses. In those situations, you can experiment with different ways of various hypotheses being true or false, and the game will automatically propagate the consequences of that hypothetical through your belief network, helping you decide what angle you should explore next.</p>\n<p>Of course, people don't always remember the source of their knowledge, or they might just appeal to personal experiences. Or they might lie about the sources, though that will only happen at the more advanced levels.</p>\n<p>As you proceed in the game, you will also be given access to more advanced tools that you can use for making hypothetical manipulations to the belief network. For example, it may happen that many different characters say that armies of vampire bats tend to move about at full moon. Since you hear that information from many different sources, it seems reliable. But then you find out that they all heard it from a nature documentary on TV that aired a few weeks back. This is reflected in your belief graph, as the game modifies it to show that all of those supposedly independent sources can actually be tracked back to a single one. That considerably reduces the reliability of the information.</p>\n<p>But maybe you were already suspecting that the sources might <em>not</em> be independent? In that case, it would have been nice if the belief graph interface would let you postulate this beforehand, and see how big of an effect it would make on the plausibility of the different hypotheses if they were in fact reliant on each other. Once your character learns the right skills, it becomes possible to also add new hypothetical connections to the belief graph, and see how this would influence your beliefs. That will further help you decide what possibilities to explore and verify.</p>\n<p>Because you can't explore every possible eventuality. There's a time limit: after a certain amount of moves, a bomb will go off, the aliens will invade, or whatever.</p>\n<p>The various characters are also more nuanced than just \"reliable\" or \"not reliable\". As you collect information about the various characters, you'll figure out their <a href=\"/lw/2fj/a_taxonomy_of_bias_mindware_problems/\">mindware</a>, motivations, and biases. Somebody might be really reliable most of the time, but have strong biases when it comes to politics, for example. Others are out to defame others, or invent fancy details to all the stories. If you talk to somebody you don't have any knowledge about yet, you can set a prior on the extent that you rely on their information, based on your experiences with other people.</p>\n<p>You also have another source of evidence: your own intuitions and experience. As you get into various situations, a source of evidence that's labeled simply \"your brain\" will provide various gut feelings and impressions about things. The claim that Alice presented doesn't seem to make sense. Bob feels reliable. You could persuade Carol to help you if you just said this one thing. But in what situations, and for what things, can you rely on your own brain? What are your own biases and problems? If you have a strong sense of having heard something at some point, but can't remember where it was, are you any more reliable than anyone else who can't remember the source of their information? You'll need to figure all of that out.</p>\n<p>As the game progresses to higher levels, your own efforts will prove insufficient for analyzing all the necessary information. You'll have to recruit a group of reliable allies, who you can trust to analyze some of the information on their own and report the results to you accurately. Of course, in order to make better decisions, they'll need you to tell them your conclusions as well. Be sure not to report as true things that you aren't really sure about, or they will end up drawing the wrong conclusions and focusing on the wrong possibilities. But you do need to condense your report somewhat: you can't just communicate your <em>entire</em> belief network to them.</p>\n<p>Hopefully, all of this should lead to player learning on a gut level things like:</p>\n<ul>\n<li><strong>Consider the origin of your knowledge:</strong> Obvious.</li>\n<li><strong>Visualizing degrees of uncertainty:</strong> In addition to giving you a numerical estimate about the probability of something, the game also color-codes the various probabilities and shows the amount of probability mass associated with your various beliefs.</li>\n<li><strong>Considering whether different sources really are independent:</strong> Some sources which seem independent won't actually be that, and some which seem dependent on each other won't be.</li>\n<li><strong>Value of information:</strong> Given all the evidence you have so far, if you found out X, exactly how much would it change your currently existing beliefs? You can test this and find out, and then decide whether it's worth finding out.</li>\n<li><strong>Seek disconfirmation:</strong> A lot of things that seem true really aren't, and acting on flawed information can cost you.</li>\n<li><strong>Prefer simpler theories:</strong> Complex, detailed hypotheses are more likely to be wrong in this game as well.</li>\n<li><strong>Common biases: </strong>Ideally, the list of biases that various characters have is derived from existing psychological research on the topic. Some biases are really common, others are more rare.</li>\n<li><strong>Epistemic hygiene: </strong>Pass off wrong information to your allies, and it'll cost you.</li>\n<li><strong>Seek to update your beliefs: </strong>The game will automatically update your belief network... to some extent. But it's still possible for you to assign mutually exclusive events probabilities that sum to more than 1, or otherwise have conflicting or incoherent beliefs. The game will mark these with a warning sign, and it's up to you to decide whether this particular inconsistency needs to be resolved or not.</li>\n<li><strong>Etc etc.<br /></strong></li>\n</ul>\n<p><strong>Design considerations</strong></p>\n<p>It's not enough for the game to be educational: if somebody downloads the game because it teaches rationality skills, that's great, but we want people to also play it because it's <em>fun</em>. Some principles that help ensure that, as well as its general utility as an educational aid, include:</p>\n<ul>\n<li><strong>Provide both short- and medium-term feedback: </strong>Ideally, there should be plenty of hints for how to find out the truth about something by investigating just one more thing: then the player can find out whether your guess was correct. It's no fun if the player has to work through fifty decisions before finding out whether they made the right move: they should get constant immediate feedback. At the same time, the player's decisions should be building up to a larger goal, with uncertainty about the overall goal keeping them interested.</li>\n<li><strong>Don't overwhelm the player: </strong>In a game like this, it would be easy to throw a million contradictory pieces of evidence at the player, forcing them to go through countless of sources of evidence and possible interactions and have no clue of what they should be doing. But the game should be manageable. Even if it <em>looks like</em> there is a huge messy network of countless pieces of contradictory evidence, it should be possible to find the connections which reveal the network to be relatively simple after all. (This is not strictly realistic, but necessary for making the game playable.)</li>\n<li><strong>Introduce new gameplay concepts gradually: </strong>Closely related to the previous item. Don't start out with making the player deal with every single gameplay concept at once. Instead, start them out in a trusted and safe environment where everyone is basically reliable, and then begin gradually introducing new things that they need to take into account.</li>\n<li><strong>No tedium:</strong> <a href=\"http://www.gamasutra.com/view/news/164869/gdc_2012_sid_meier_on_how_to_see_.php\">A game is a series of interesting decisions</a>. The game should never force the player to do anything uninteresting or tedious. Did Alice tell Bob something? No need to write that down, the game keeps automatic track of it. From the evidence that has been gathered so far, is it completely obvious what hypothesis is going to be right? Let the player mark that as something that will be taken for granted and move on.</li>\n<li><strong>No glued-on tasks: </strong>A sign of a bad educational game is that the educational component is glued on to the game (or vice versa). Answer this exam question correctly, and you'll get to play a fun action level! There should be none of that - the educational component should be an indistinguishable part of the game play.</li>\n<li><strong>Achievement, not <a href=\"http://www.pixelpoppers.com/2009/11/awesome-by-proxy-addicted-to-fake.html\">fake achievement</a>: </strong>Related to the previous point. It would be easy to make a game that <a href=\"/lw/ir/science_as_attire/\">wore the attire</a> of rationality, and which used concepts like \"probability theory\", and then when your character leveled up he would get better probability attacks or whatever. And you'd feel great about your character learning cool stuff, while you yourself learned nothing. The game must genuinely require the player to actually learn new skills in order to get further.</li>\n<li><strong>Emotionally compelling: </strong>The game should not be just an abstract intellectual exercise, but have an emotionally compelling story as well. Your choices should feel like they <em>matter</em>, and characters should be in risk of dying if you make the wrong decisions.</li>\n<li><strong>Teach true things: </strong>Hopefully, the players should take the things that they've learned from the game and apply them to their daily lives. That means that we have a responsibility not to teach them things which aren't actually true.</li>\n<li><strong>Replayable: </strong>Practice makes perfect. At least part of the game world needs to be randomly generated, so that the game can be replayed without a risk of it becoming boring because the player has memorized the whole belief network.</li>\n</ul>\n<p><strong>What next?<br /></strong></p>\n<p>What you've just read is a very high-level design, and a quite incomplete one at that: I've spoken on the need to have \"an emotionally compelling story\", but said nothing about the story or the setting. This should probably be something like a spy or detective story, because that's thematically appropriate for a game which is about managing information; and it might be best to have it in a fantasy setting, so that you can question the widely-accepted truths of that setting without needing to get on anyone's toes by questioning widely-accepted truths of our society.</p>\n<p>But there's still a <em>lot </em>of work that remains to be done with regard to things like what exactly does the belief network look like, what kinds of evidence can there be, how does one make all of this actually be fun, and so on. I mentioned the need to have both short- and medium-term feedback, but I'm not sure of how that could be achieved, or whether this design lets you achieve it at all. And I don't even know whether the game should show explicit probabilities.</p>\n<p>And having a design isn't enough: the whole thing needs to be implemented as well, preferably while it's still being designed in order to take advantage of <a href=\"http://en.wikipedia.org/wiki/Agile_software_development\">agile development</a> techniques. Make a prototype, find some unsuspecting testers, spring it on them, revise. And then there are the graphics and music, things for which I have no competence for working on.</p>\n<p>I'll probably be working on this in my spare time - I've been playing with the idea of going to the field of educational games at some point, and want the design and programming experience. If anyone feels like they could and would want to contribute to the project, let me know.</p>\n<p><strong>EDIT: </strong>Great to see that there's interest! I've created a <a href=\"https://groups.google.com/forum/?fromgroups#!forum/the-fundamental-question\">mailing list for discussing the game</a>. It's probably easiest to have the initial discussion here, and then shift the discussion to the list.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "EdDGrAxYcrXnKkDca": 1, "xgpBASEThXPuKRhbS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8kFewgKabR6vYLjQb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 61, "extendedScore": null, "score": 0.000154, "legacy": true, "legacyId": "21551", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 61, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>I sometimes go around saying that the fundamental question of rationality is <em>Why do you believe what you believe?</em></p>\n<p>-- <a href=\"/lw/q5/quantum_nonrealism/\">Eliezer in Quantum Non-Realism</a></p>\n</blockquote>\n<p>I was much impressed when they finally came out with a PC version of <a href=\"http://dragonboxapp.com/\">DragonBox</a>, and I got around to testing it on some children I knew. Two kids, one of them four and the other eight years old, ended up blazing through several levels of solving first-degree equations while having a lot of fun doing so, even though they didn't know what it was that they were doing. That made me think that there <em>has</em> to be some way of making a computer game that would similarly teach rationality skills at <a href=\"/lw/5kz/the_5second_level/\">the 5-second level</a>. Some game where you would actually be forced to learn useful skills if you wanted to make progress.</p>\n<p>After playing around with some ideas, I hit upon the notion of making a game centered around the Fundamental Question. I'm not sure whether this can be made to work, but it seems to have promise. The basic idea: you are required to figure out the solution to various mysteries by collecting various kinds of evidence. Some of the sources of evidence will be more reliable than others. In order to hit upon the correct solution, you need to consider where each piece of evidence came from, and whether you can rely on it.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Gameplay example</strong></span></p>\n<p>Now, let's go into a little more detail. Let's suppose that the game has a character called Bob. Bob tells you that tomorrow, eight o'clock, there will be an assassination attempt on Market Square. The fact that Bob has told you this is <a href=\"/lw/jl/what_is_evidence/\">evidence</a> for the claim being true, so the game automatically records the fact that you have such a piece of evidence, and that it came from Bob.</p>\n<p>(Click on the pictures in case you don't see them properly.)</p>\n<p><a href=\"http://kajsotala.fi/Random/Question1b.png\"><img style=\"vertical-align: middle;\" src=\"http://kajsotala.fi/Random/Question1b.png\" alt=\"\"></a></p>\n<p>But how does Bob know that? You ask, and it turns out that Alice told him. So next, you go and ask Alice. Alice is confused and says that she never said anything about any assassination attempt: she just said that something big is going to be happen at the Market Square at that time, she heard it from the Mayor. The game records two new pieces of evidence: Alice's claim of something big happening at the Market Square tomorrow (which she heard from the Mayor), and her story of what she actually told Bob. Guess that Bob isn't a very reliable source of evidence: he has a tendency to come up with fancy invented details.</p>\n<p><a href=\"http://kajsotala.fi/Random/Question2.png\"><img style=\"vertical-align: middle;\" src=\"http://kajsotala.fi/Random/Question2.png\" alt=\"\" width=\"612\" height=\"611\"></a></p>\n<p>Or is he? After all, your sole knowledge about Bob being unreliable is that Alice claims she never said what Bob says she said. But maybe Alice has a grudge against Bob, and is intentionally out to make everyone disbelieve him. Maybe it's Alice who's unreliable. The evidence that you have is compatible with both hypotheses. At this point, you don't have enough information to decide between them, but the game lets you experiment with setting either of them as \"true\" and seeing the implications of this on your belief network. Or maybe they're both true - Bob <em>is</em> generally unreliable, <em>and </em>Alice is out to discredit him. That's another possibility that you might want to consider. In any case, the claim that there will be an assassination tomorrow isn't looking very likely at the moment.</p>\n<p><a href=\"http://kajsotala.fi/Random/Question3b.png\"><img src=\"http://kajsotala.fi/Random/Question3b.png\" alt=\"\" width=\"639\" height=\"600\"></a></p>\n<p>Actually, having the possibility for somebody lying should probably be a pretty late-game thing, as it makes your belief network a lot more complicated, and I'm not sure whether this thing should display numerical probabilities at all. Instead of having to juggle the hypotheses of \"Alice lied\" and \"Bob exaggerates things\", the game should probably just record the fact that \"Bob exaggerates things\". But I spent a bunch of time making these pictures, and they do illustrate some of the general principles involved, so I'll just use them for now.</p>\n<p><a href=\"http://kajsotala.fi/Random/Question4.png\"><img style=\"vertical-align: middle;\" src=\"http://kajsotala.fi/Random/Question4.png\" alt=\"\" width=\"807\" height=\"600\"></a></p>\n<p><strong id=\"Game_basics\">Game basics</strong></p>\n<p>So, to repeat the basic premise of the game, in slightly more words this time around: your task is to figure out something, and in order to do so, you need to collect different pieces of evidence. As you do so, the game generates a <a href=\"/lw/ev3/causal_diagrams_and_causal_models/\">belief network</a> showing the origin and history of the various pieces of evidence that you've gathered. That much is done automatically. But often, the evidence that you've gathered is compatible with many different hypotheses. In those situations, you can experiment with different ways of various hypotheses being true or false, and the game will automatically propagate the consequences of that hypothetical through your belief network, helping you decide what angle you should explore next.</p>\n<p>Of course, people don't always remember the source of their knowledge, or they might just appeal to personal experiences. Or they might lie about the sources, though that will only happen at the more advanced levels.</p>\n<p>As you proceed in the game, you will also be given access to more advanced tools that you can use for making hypothetical manipulations to the belief network. For example, it may happen that many different characters say that armies of vampire bats tend to move about at full moon. Since you hear that information from many different sources, it seems reliable. But then you find out that they all heard it from a nature documentary on TV that aired a few weeks back. This is reflected in your belief graph, as the game modifies it to show that all of those supposedly independent sources can actually be tracked back to a single one. That considerably reduces the reliability of the information.</p>\n<p>But maybe you were already suspecting that the sources might <em>not</em> be independent? In that case, it would have been nice if the belief graph interface would let you postulate this beforehand, and see how big of an effect it would make on the plausibility of the different hypotheses if they were in fact reliant on each other. Once your character learns the right skills, it becomes possible to also add new hypothetical connections to the belief graph, and see how this would influence your beliefs. That will further help you decide what possibilities to explore and verify.</p>\n<p>Because you can't explore every possible eventuality. There's a time limit: after a certain amount of moves, a bomb will go off, the aliens will invade, or whatever.</p>\n<p>The various characters are also more nuanced than just \"reliable\" or \"not reliable\". As you collect information about the various characters, you'll figure out their <a href=\"/lw/2fj/a_taxonomy_of_bias_mindware_problems/\">mindware</a>, motivations, and biases. Somebody might be really reliable most of the time, but have strong biases when it comes to politics, for example. Others are out to defame others, or invent fancy details to all the stories. If you talk to somebody you don't have any knowledge about yet, you can set a prior on the extent that you rely on their information, based on your experiences with other people.</p>\n<p>You also have another source of evidence: your own intuitions and experience. As you get into various situations, a source of evidence that's labeled simply \"your brain\" will provide various gut feelings and impressions about things. The claim that Alice presented doesn't seem to make sense. Bob feels reliable. You could persuade Carol to help you if you just said this one thing. But in what situations, and for what things, can you rely on your own brain? What are your own biases and problems? If you have a strong sense of having heard something at some point, but can't remember where it was, are you any more reliable than anyone else who can't remember the source of their information? You'll need to figure all of that out.</p>\n<p>As the game progresses to higher levels, your own efforts will prove insufficient for analyzing all the necessary information. You'll have to recruit a group of reliable allies, who you can trust to analyze some of the information on their own and report the results to you accurately. Of course, in order to make better decisions, they'll need you to tell them your conclusions as well. Be sure not to report as true things that you aren't really sure about, or they will end up drawing the wrong conclusions and focusing on the wrong possibilities. But you do need to condense your report somewhat: you can't just communicate your <em>entire</em> belief network to them.</p>\n<p>Hopefully, all of this should lead to player learning on a gut level things like:</p>\n<ul>\n<li><strong>Consider the origin of your knowledge:</strong> Obvious.</li>\n<li><strong>Visualizing degrees of uncertainty:</strong> In addition to giving you a numerical estimate about the probability of something, the game also color-codes the various probabilities and shows the amount of probability mass associated with your various beliefs.</li>\n<li><strong>Considering whether different sources really are independent:</strong> Some sources which seem independent won't actually be that, and some which seem dependent on each other won't be.</li>\n<li><strong>Value of information:</strong> Given all the evidence you have so far, if you found out X, exactly how much would it change your currently existing beliefs? You can test this and find out, and then decide whether it's worth finding out.</li>\n<li><strong>Seek disconfirmation:</strong> A lot of things that seem true really aren't, and acting on flawed information can cost you.</li>\n<li><strong>Prefer simpler theories:</strong> Complex, detailed hypotheses are more likely to be wrong in this game as well.</li>\n<li><strong>Common biases: </strong>Ideally, the list of biases that various characters have is derived from existing psychological research on the topic. Some biases are really common, others are more rare.</li>\n<li><strong>Epistemic hygiene: </strong>Pass off wrong information to your allies, and it'll cost you.</li>\n<li><strong>Seek to update your beliefs: </strong>The game will automatically update your belief network... to some extent. But it's still possible for you to assign mutually exclusive events probabilities that sum to more than 1, or otherwise have conflicting or incoherent beliefs. The game will mark these with a warning sign, and it's up to you to decide whether this particular inconsistency needs to be resolved or not.</li>\n<li><strong>Etc etc.<br></strong></li>\n</ul>\n<p><strong id=\"Design_considerations\">Design considerations</strong></p>\n<p>It's not enough for the game to be educational: if somebody downloads the game because it teaches rationality skills, that's great, but we want people to also play it because it's <em>fun</em>. Some principles that help ensure that, as well as its general utility as an educational aid, include:</p>\n<ul>\n<li><strong>Provide both short- and medium-term feedback: </strong>Ideally, there should be plenty of hints for how to find out the truth about something by investigating just one more thing: then the player can find out whether your guess was correct. It's no fun if the player has to work through fifty decisions before finding out whether they made the right move: they should get constant immediate feedback. At the same time, the player's decisions should be building up to a larger goal, with uncertainty about the overall goal keeping them interested.</li>\n<li><strong>Don't overwhelm the player: </strong>In a game like this, it would be easy to throw a million contradictory pieces of evidence at the player, forcing them to go through countless of sources of evidence and possible interactions and have no clue of what they should be doing. But the game should be manageable. Even if it <em>looks like</em> there is a huge messy network of countless pieces of contradictory evidence, it should be possible to find the connections which reveal the network to be relatively simple after all. (This is not strictly realistic, but necessary for making the game playable.)</li>\n<li><strong>Introduce new gameplay concepts gradually: </strong>Closely related to the previous item. Don't start out with making the player deal with every single gameplay concept at once. Instead, start them out in a trusted and safe environment where everyone is basically reliable, and then begin gradually introducing new things that they need to take into account.</li>\n<li><strong>No tedium:</strong> <a href=\"http://www.gamasutra.com/view/news/164869/gdc_2012_sid_meier_on_how_to_see_.php\">A game is a series of interesting decisions</a>. The game should never force the player to do anything uninteresting or tedious. Did Alice tell Bob something? No need to write that down, the game keeps automatic track of it. From the evidence that has been gathered so far, is it completely obvious what hypothesis is going to be right? Let the player mark that as something that will be taken for granted and move on.</li>\n<li><strong>No glued-on tasks: </strong>A sign of a bad educational game is that the educational component is glued on to the game (or vice versa). Answer this exam question correctly, and you'll get to play a fun action level! There should be none of that - the educational component should be an indistinguishable part of the game play.</li>\n<li><strong>Achievement, not <a href=\"http://www.pixelpoppers.com/2009/11/awesome-by-proxy-addicted-to-fake.html\">fake achievement</a>: </strong>Related to the previous point. It would be easy to make a game that <a href=\"/lw/ir/science_as_attire/\">wore the attire</a> of rationality, and which used concepts like \"probability theory\", and then when your character leveled up he would get better probability attacks or whatever. And you'd feel great about your character learning cool stuff, while you yourself learned nothing. The game must genuinely require the player to actually learn new skills in order to get further.</li>\n<li><strong>Emotionally compelling: </strong>The game should not be just an abstract intellectual exercise, but have an emotionally compelling story as well. Your choices should feel like they <em>matter</em>, and characters should be in risk of dying if you make the wrong decisions.</li>\n<li><strong>Teach true things: </strong>Hopefully, the players should take the things that they've learned from the game and apply them to their daily lives. That means that we have a responsibility not to teach them things which aren't actually true.</li>\n<li><strong>Replayable: </strong>Practice makes perfect. At least part of the game world needs to be randomly generated, so that the game can be replayed without a risk of it becoming boring because the player has memorized the whole belief network.</li>\n</ul>\n<p><strong id=\"What_next_\">What next?<br></strong></p>\n<p>What you've just read is a very high-level design, and a quite incomplete one at that: I've spoken on the need to have \"an emotionally compelling story\", but said nothing about the story or the setting. This should probably be something like a spy or detective story, because that's thematically appropriate for a game which is about managing information; and it might be best to have it in a fantasy setting, so that you can question the widely-accepted truths of that setting without needing to get on anyone's toes by questioning widely-accepted truths of our society.</p>\n<p>But there's still a <em>lot </em>of work that remains to be done with regard to things like what exactly does the belief network look like, what kinds of evidence can there be, how does one make all of this actually be fun, and so on. I mentioned the need to have both short- and medium-term feedback, but I'm not sure of how that could be achieved, or whether this design lets you achieve it at all. And I don't even know whether the game should show explicit probabilities.</p>\n<p>And having a design isn't enough: the whole thing needs to be implemented as well, preferably while it's still being designed in order to take advantage of <a href=\"http://en.wikipedia.org/wiki/Agile_software_development\">agile development</a> techniques. Make a prototype, find some unsuspecting testers, spring it on them, revise. And then there are the graphics and music, things for which I have no competence for working on.</p>\n<p>I'll probably be working on this in my spare time - I've been playing with the idea of going to the field of educational games at some point, and want the design and programming experience. If anyone feels like they could and would want to contribute to the project, let me know.</p>\n<p><strong>EDIT: </strong>Great to see that there's interest! I've created a <a href=\"https://groups.google.com/forum/?fromgroups#!forum/the-fundamental-question\">mailing list for discussing the game</a>. It's probably easiest to have the initial discussion here, and then shift the discussion to the list.</p>", "sections": [{"title": "Game basics", "anchor": "Game_basics", "level": 1}, {"title": "Design considerations", "anchor": "Design_considerations", "level": 1}, {"title": "What next?", "anchor": "What_next_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "68 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["k3823vuarnmL5Pqin", "JcpzFpPBSmzuksmWM", "6s3xABaXKPdFwA3FS", "hzuSDMx7pd2uxFc5w", "a5DQxG9NgzSLRZMnQ", "4Bwr6s9dofvqPWakn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-13T16:10:48.091Z", "modifiedAt": null, "url": null, "title": "Higher than the most high", "slug": "higher-than-the-most-high", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.605Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jpMwB3NKjcYXZLo8E/higher-than-the-most-high", "pageUrlRelative": "/posts/jpMwB3NKjcYXZLo8E/higher-than-the-most-high", "linkUrl": "https://www.lesswrong.com/posts/jpMwB3NKjcYXZLo8E/higher-than-the-most-high", "postedAtFormatted": "Wednesday, February 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Higher%20than%20the%20most%20high&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHigher%20than%20the%20most%20high%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjpMwB3NKjcYXZLo8E%2Fhigher-than-the-most-high%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Higher%20than%20the%20most%20high%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjpMwB3NKjcYXZLo8E%2Fhigher-than-the-most-high", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjpMwB3NKjcYXZLo8E%2Fhigher-than-the-most-high", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 337, "htmlBody": "<p>In an earlier <a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/\">post</a>, I talked about how we could deal with variants of the Heaven and Hell problem - situations where you have an infinite number of options, and none of them is a maximum. The solution for a (deterministic) agent was to try and implement the strategy that would reach the highest possible number, without risking falling into an infinite loop.</p>\n<p>Wei Dai <a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/8ew3\">pointed</a> out that in the cases where the options are unbounded in utility (ie you can get arbitrarily high utility), then there are probabilistic strategies that give you infinite expected utility. I suggested you could still do better than this. This started a conversation about choosing between strategies with infinite expectation (would you prefer a strategy with infinite expectation, or the same plus an extra dollar?), which went off into some&nbsp;interesting&nbsp;directions as to what needed to be done when the strategies can't sensibly be compared with each other...</p>\n<p>Interesting though that may be, it's also helpful to have simple cases where you don't need all these subtleties. So here is one:</p>\n<p style=\"padding-left: 30px;\">Omega approaches you and Mrs X, asking you each to name an integer to him, privately. The person who names the highest integer gets 1 utility; the other gets nothing. In practical terms, Omega will reimburse you all utility lost during the decision process (so you can take as long as you want to decide). The first person to name a number gets 1 utility immediately; they may then lose that 1 depending on the eventual response of the other. Hence if one person responds and the other doesn't, they get the 1 utility and keep it. What should you do?</p>\n<p>In this case, a strategy that gives you a number with infinite expectation isn't enough - you have to beat Mrs X, but you also have to eventually say something. Hence there is a duel of (likely probabilistic) strategies, implemented by bounded agents, with no maximum strategy, and each agent trying to compute the maximal strategy they can construct without falling into a loop.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jpMwB3NKjcYXZLo8E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "21580", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PpTN7GP2FsPyHfKrs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-13T21:35:39.396Z", "modifiedAt": null, "url": null, "title": "Meetup : Vienna Meetup 9th March", "slug": "meetup-vienna-meetup-9th-march", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ratcourse", "createdAt": "2012-11-03T10:26:05.641Z", "isAdmin": false, "displayName": "Ratcourse"}, "userId": "qwnfbBpAcbxLT4JBr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iHvTroAq9naAkXG2f/meetup-vienna-meetup-9th-march", "pageUrlRelative": "/posts/iHvTroAq9naAkXG2f/meetup-vienna-meetup-9th-march", "linkUrl": "https://www.lesswrong.com/posts/iHvTroAq9naAkXG2f/meetup-vienna-meetup-9th-march", "postedAtFormatted": "Wednesday, February 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vienna%20Meetup%209th%20March&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vienna%20Meetup%209th%20March%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiHvTroAq9naAkXG2f%2Fmeetup-vienna-meetup-9th-march%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vienna%20Meetup%209th%20March%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiHvTroAq9naAkXG2f%2Fmeetup-vienna-meetup-9th-march", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiHvTroAq9naAkXG2f%2Fmeetup-vienna-meetup-9th-march", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/j8'>Vienna Meetup 9th March</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 March 2013 04:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Schottengasse 2, 1010 Innere Stadt (1.Bez)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cafe im schottenstift.</p>\n\n<p>Mainly this is a meeting to kickstart the vienna lesswrong/rationality group. We will get to know each other and discuss ways to promote a rational \nlifestyle.</p>\n\n<p>Confirmed attendees:\nFive attendees so far, 3 of whom are working on creating a rationality education project.</p>\n\n<p>See you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/j8'>Vienna Meetup 9th March</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iHvTroAq9naAkXG2f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.111603107918369e-06, "legacy": true, "legacyId": "21582", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vienna_Meetup_9th_March\">Discussion article for the meetup : <a href=\"/meetups/j8\">Vienna Meetup 9th March</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 March 2013 04:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Schottengasse 2, 1010 Innere Stadt (1.Bez)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cafe im schottenstift.</p>\n\n<p>Mainly this is a meeting to kickstart the vienna lesswrong/rationality group. We will get to know each other and discuss ways to promote a rational \nlifestyle.</p>\n\n<p>Confirmed attendees:\nFive attendees so far, 3 of whom are working on creating a rationality education project.</p>\n\n<p>See you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vienna_Meetup_9th_March1\">Discussion article for the meetup : <a href=\"/meetups/j8\">Vienna Meetup 9th March</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vienna Meetup 9th March", "anchor": "Discussion_article_for_the_meetup___Vienna_Meetup_9th_March", "level": 1}, {"title": "Discussion article for the meetup : Vienna Meetup 9th March", "anchor": "Discussion_article_for_the_meetup___Vienna_Meetup_9th_March1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T02:02:26.421Z", "modifiedAt": null, "url": null, "title": "[LINK] \"Scott and Scurvy\": a reminder of the messiness of scientific progress", "slug": "link-scott-and-scurvy-a-reminder-of-the-messiness-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.753Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hesperidia", "createdAt": "2011-04-08T18:20:00.290Z", "isAdmin": false, "displayName": "hesperidia"}, "userId": "i4ccRy9pZA4eedwHN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vPvpo5dxGX2mh8Gwp/link-scott-and-scurvy-a-reminder-of-the-messiness-of", "pageUrlRelative": "/posts/vPvpo5dxGX2mh8Gwp/link-scott-and-scurvy-a-reminder-of-the-messiness-of", "linkUrl": "https://www.lesswrong.com/posts/vPvpo5dxGX2mh8Gwp/link-scott-and-scurvy-a-reminder-of-the-messiness-of", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20%22Scott%20and%20Scurvy%22%3A%20a%20reminder%20of%20the%20messiness%20of%20scientific%20progress&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20%22Scott%20and%20Scurvy%22%3A%20a%20reminder%20of%20the%20messiness%20of%20scientific%20progress%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvPvpo5dxGX2mh8Gwp%2Flink-scott-and-scurvy-a-reminder-of-the-messiness-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20%22Scott%20and%20Scurvy%22%3A%20a%20reminder%20of%20the%20messiness%20of%20scientific%20progress%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvPvpo5dxGX2mh8Gwp%2Flink-scott-and-scurvy-a-reminder-of-the-messiness-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvPvpo5dxGX2mh8Gwp%2Flink-scott-and-scurvy-a-reminder-of-the-messiness-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p><a href=\"http://idlewords.com/2010/03/scott_and_scurvy.htm\">http://idlewords.com/2010/03/scott_and_scurvy.htm</a></p>\n<blockquote>\n<p>Now, I had been taught in school that scurvy had been conquered in 1747, when the Scottish physician <a href=\"http://en.wikipedia.org/wiki/James_Lind\">James Lind</a> proved in one of the first controlled medical experiments that citrus  fruits were an effective cure for the disease.  From that point on, we  were told, the Royal Navy had required a daily dose of lime juice to be  mixed in with sailors&rsquo; grog, and scurvy ceased to be a problem on long  ocean voyages.</p>\n<p>But here was a Royal Navy surgeon in 1911 apparently ignorant of what  caused the disease, or how to cure it.   Somehow a highly-trained group  of scientists at the start of the 20th century knew less about scurvy  than the average sea captain in Napoleonic times.  Scott left a base  abundantly stocked with fresh meat, fruits, apples, and lime juice, and  headed out on the ice for five months with no protection against scurvy,  all the while confident he was not at risk.  What happened?</p>\n</blockquote>\n<p>This article is a vivid illustration of just how nonlinear and <em>downright messy</em> science actually is, and how little the superficial presentation of science as neat \"progress\" reflects the reality of the field.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vPvpo5dxGX2mh8Gwp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 20, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "21584", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T03:03:04.591Z", "modifiedAt": null, "url": null, "title": "Memetic Tribalism", "slug": "memetic-tribalism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.359Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ztsw7b3CbJSzD98aR/memetic-tribalism", "pageUrlRelative": "/posts/Ztsw7b3CbJSzD98aR/memetic-tribalism", "linkUrl": "https://www.lesswrong.com/posts/Ztsw7b3CbJSzD98aR/memetic-tribalism", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Memetic%20Tribalism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMemetic%20Tribalism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZtsw7b3CbJSzD98aR%2Fmemetic-tribalism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Memetic%20Tribalism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZtsw7b3CbJSzD98aR%2Fmemetic-tribalism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZtsw7b3CbJSzD98aR%2Fmemetic-tribalism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1130, "htmlBody": "<p>Related: <a href=\"/lw/gw/politics_is_the_mindkiller/\">politics is the mind killer</a>, <a href=\"/lw/9v/beware_of_otheroptimizing/\">other optimizing</a></p>\n<p>When someone says something stupid, I get an urge to correct them. Based on the stories I hear from others, I'm not the only one.</p>\n<p>For example, some of my friends are into this rationality thing, and they've learned about all these biases and correct ways to get things done. Naturally, they get irritated with people who haven't learned this stuff. They complain about how their family members or coworkers aren't rational, and they ask what is the best way to correct them.</p>\n<p>I could get into the details of the optimal set of arguments to turn someone into a rationalist, or I could go a bit meta and ask: \"Why would you want to do that?\"</p>\n<p>Why should you spend your time correcting someone else's reasoning?</p>\n<p>One reason that comes up is that it's valuable for some reason to change their reasoning. OK, when is it possible?</p>\n<ol>\n<li>\n<p>You actually know better than them.</p>\n</li>\n<li>\n<p>You know how to patch their reasoning.</p>\n</li>\n<li>\n<p>They will be receptive to said patching.</p>\n</li>\n<li>\n<p>They will actually change their behavior if the accept the patch.</p>\n</li>\n</ol>\n<p>It seems like it should be rather rare for those conditions to all be true, or even to be likely enough for the expected gain to be worth the cost, and yet I feel the urge quite often. And I'm not thinking it through and deciding, I'm just feeling an urge; humans are adaptation executors, and this one seems like an adaptation. For some reason \"correcting\" people's reasoning was important enough in the ancestral environment to be special-cased in motivation hardware.</p>\n<p>I could try to spin an ev-psych just-so story about tribal status, intellectual dominance hierarchies, ingroup-outgroup signaling, and whatnot, but I'm not an evolutionary psychologist, so I wouldn't actually know what I was doing, and the details don't matter anyway. What matters is that this urge seems to be hardware, and it probably has nothing to do with actual truth or <em>your</em> strategic concerns.</p>\n<p>It seems to happen to everyone who has ideas. Social justice types get frustrated with people who seem unable to acknowledge their own privilege. The epistemological flamewar between atheists and theists rages continually across the internet. Tech-savvy folk get frustrated with others' total inability to explore and use Google. Some aspiring rationalists get <a href=\"/lw/gl9/how_to_offend_a_rationalist_who_hasnt_thought/\">annoyed</a> with people who refuse to decompartmentalize or claim that something is in a separate magisteria.</p>\n<p>Some of those border on being just classic <a href=\"/lw/gt/a_fable_of_science_and_politics/\">blue vs green</a> thinking, but from the outside, the rationality example isn't all that different. They all seem to be motivated mostly by \"This person fails to display the complex habits of thought that I think are fashionable; I should {make fun | correct them | call them out}.\"</p>\n<p>I'm now quite skeptical that my urge to correct reflects an actual opportunity to win by improving someone's thinking, given that I'd feel it <a href=\"/lw/jl/what_is_evidence/\">whether or not</a> I could actually help, and that it seems to be <a href=\"/lw/oo/explaining_vs_explaining_away/\">caused by something else</a>.</p>\n<p>The value of attempting a rationality-intervention has gone back down towards baseline, but it's not obvious that the baseline value of rationality interventions is all that low. Maybe it's a good idea, even if there is a possible bias supporting it. We can't win just by reversing our biases; reversed stupidity is not intelligence.</p>\n<p>The best reason I can think of to correct flawed thinking is if your ability to accomplish your goals directly depends on their rationality. Maybe they are your business partner, or your spouse. Someone specific and close who you can cooperate with a lot. If this is the case, it's near the same level of urgency as correcting your own.</p>\n<p>Another good reason (to discuss the subject at least) is that discussing your ideas with smart people is a good way to make your ideas better. I often get my dad to poke holes in my current craziness, because he is smarter and wiser than me. If this is your angle, keep in mind that if you expect someone else to correct you, it's probably not best to go in making bold claims and implicitly claiming intellectual dominance.</p>\n<p>An OK reason is that creating more rationalists is valuable in general. This one is less good than it first appears. Do you really think your comparative advantage right now is in converting this person to your way of thinking? Is that really worth the risk of social friction and expenditure of time and mental energy? <em>Is this the <a href=\"/lw/hu/the_third_alternative/\">best method</a> you can think of for creating more rationalists?</em></p>\n<p>I think it is valuable to raise the sanity waterline when you can, but using methods of mass instruction like writing blog posts, administering a meetup, or <a href=\"http://lesswrong.com\">launching a whole rationality movement</a> is a lot more effective than arguing with your mom. Those options aren't for everybody of course, but if you're into waterline-manipulation, you should at least be considering strategies like them. At least consider picking a better time.</p>\n<p>Another reason that gets brought up is that turning people around you into rationalists is instrumental in a selfish way, because it makes life easier for you. This one is suspect to me, even without the incentive to rationalize. Did you also seriously consider <em>sabotaging</em> people's rationality to take advantage of them? Surely that's nearly as plausible a-priori. For what specific reason did your search process rank cooperation over predation?&nbsp;</p>\n<p>I'm sure there are plenty of good reasons to prefer cooperation, but of course <a href=\"/lw/kq/fake_justification/\">no search process was ever run</a>. All of these reasons that come to mind when I think of why I might want to fix someone's reasoning are just post-hoc rationalizations of an automatic behavior. The true chain of cause-and-effect is observe-&gt;feel-&gt;act; no planning or thinking involved, except where it is necessary for the act. And that feeling isn't specific to rationality, it affects all mental habits, even stupid ones.</p>\n<p>Rationality isn't just a new memetic orthodoxy for the cool kids, it's about actually winning. Every improvement requires a change. Rationalizing strategic reasons for instinctual behavior isn't change, it's spending your resources answering questions with <em>zero value of information</em>. Rationality isn't about what other people are doing wrong; it's about what <em>you</em> are doing wrong.</p>\n<p>I used to call this practice of modeling other people's thoughts to enforce <a href=\"https://en.wikipedia.org/wiki/Nineteen_Eighty-Four\">orthodoxy</a> on them \"incorrect use of empathy\", but in terms of ev-psych, it may be exactly the correct use of empathy. We can call it Memetic Tribalism instead.</p>\n<p>(I've ignored the <em>other</em> reason to correct people's reasoning, which is that it's fun and status-increasing. When I reflect on my reasons for writing posts like this, it turns out I do it largely for the fun and internet status points, but I try to at least be aware of that.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DdgSyQoZXjj3KnF4N": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ztsw7b3CbJSzD98aR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 52, "baseScore": 61, "extendedScore": null, "score": 0.00011810986164933786, "legacy": true, "legacyId": "21585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f", "6NvbSwuSAooQxxf7f", "EhyiWtMmWG6Eorh84", "6hfGNLf4Hg5DXqJCF", "6s3xABaXKPdFwA3FS", "cphoF8naigLhRf3tu", "erGipespbbzdG5zYb", "bfbiyTogEKWEGP96S"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T03:15:02.280Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Boredom vs Scope Insensitivity, and life-debugging", "slug": "meetup-vancouver-boredom-vs-scope-insensitivity-and-life", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jnvkhrNTkPYwD6NGq/meetup-vancouver-boredom-vs-scope-insensitivity-and-life", "pageUrlRelative": "/posts/jnvkhrNTkPYwD6NGq/meetup-vancouver-boredom-vs-scope-insensitivity-and-life", "linkUrl": "https://www.lesswrong.com/posts/jnvkhrNTkPYwD6NGq/meetup-vancouver-boredom-vs-scope-insensitivity-and-life", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Boredom%20vs%20Scope%20Insensitivity%2C%20and%20life-debugging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Boredom%20vs%20Scope%20Insensitivity%2C%20and%20life-debugging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnvkhrNTkPYwD6NGq%2Fmeetup-vancouver-boredom-vs-scope-insensitivity-and-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Boredom%20vs%20Scope%20Insensitivity%2C%20and%20life-debugging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnvkhrNTkPYwD6NGq%2Fmeetup-vancouver-boredom-vs-scope-insensitivity-and-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnvkhrNTkPYwD6NGq%2Fmeetup-vancouver-boredom-vs-scope-insensitivity-and-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/j9'>Vancouver Boredom vs Scope Insensitivity, and life-debugging</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 February 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 W broadway, vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at Benny's Bagels on west broadway at 15:00 on saturday.</p>\n\n<p>We are going to discuss <a href=\"http://lesswrong.com/lw/196/boredom_vs_scope_insensitivity/\">Boredom vs Scope Insensitivity</a> and related issues with utility curves.</p>\n\n<p>To complement such a theoretical topic, we may also discuss the rationality failures in our lives that we'd like to get better at. It should be thoroughly depressing and hopefully useful.</p>\n\n<p>As usual, see us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/j9'>Vancouver Boredom vs Scope Insensitivity, and life-debugging</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jnvkhrNTkPYwD6NGq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.1118181212527157e-06, "legacy": true, "legacyId": "21586", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Boredom_vs_Scope_Insensitivity__and_life_debugging\">Discussion article for the meetup : <a href=\"/meetups/j9\">Vancouver Boredom vs Scope Insensitivity, and life-debugging</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 February 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 W broadway, vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at Benny's Bagels on west broadway at 15:00 on saturday.</p>\n\n<p>We are going to discuss <a href=\"http://lesswrong.com/lw/196/boredom_vs_scope_insensitivity/\">Boredom vs Scope Insensitivity</a> and related issues with utility curves.</p>\n\n<p>To complement such a theoretical topic, we may also discuss the rationality failures in our lives that we'd like to get better at. It should be thoroughly depressing and hopefully useful.</p>\n\n<p>As usual, see us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Boredom_vs_Scope_Insensitivity__and_life_debugging1\">Discussion article for the meetup : <a href=\"/meetups/j9\">Vancouver Boredom vs Scope Insensitivity, and life-debugging</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Boredom vs Scope Insensitivity, and life-debugging", "anchor": "Discussion_article_for_the_meetup___Vancouver_Boredom_vs_Scope_Insensitivity__and_life_debugging", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Boredom vs Scope Insensitivity, and life-debugging", "anchor": "Discussion_article_for_the_meetup___Vancouver_Boredom_vs_Scope_Insensitivity__and_life_debugging1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6S4Lf2tCMWAfbGtdt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T04:01:18.709Z", "modifiedAt": null, "url": null, "title": "Meetup : Cincinnati February: Predictions", "slug": "meetup-cincinnati-february-predictions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:04.136Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cCMLcYx9nijYpbwxu/meetup-cincinnati-february-predictions", "pageUrlRelative": "/posts/cCMLcYx9nijYpbwxu/meetup-cincinnati-february-predictions", "linkUrl": "https://www.lesswrong.com/posts/cCMLcYx9nijYpbwxu/meetup-cincinnati-february-predictions", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cincinnati%20February%3A%20Predictions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cincinnati%20February%3A%20Predictions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcCMLcYx9nijYpbwxu%2Fmeetup-cincinnati-february-predictions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cincinnati%20February%3A%20Predictions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcCMLcYx9nijYpbwxu%2Fmeetup-cincinnati-february-predictions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcCMLcYx9nijYpbwxu%2Fmeetup-cincinnati-february-predictions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ja'>Cincinnati February: Predictions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 February 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">354 Ludlow Avenue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at the Amol India on Ludlow street at 1400. This month's exercise is to try for calibration. Each of us should give a probability for each of the five events below occurring before April 1st; at the meetup we will discuss our reasoning and perhaps update. Then in April we'll see how we did, as a group and individually.</p>\n\n<ol>\n<li>Will Kim Jong-un cease to be dictator of Best Korea? </li>\n<li>Will the sentence of any of the seven scientists convicted of failing to adequately warn of the L'Aquila earthquake be modified? </li>\n<li>Will a large (more than 1000 soldiers) foreign force invade Iran? </li>\n<li>Will pope Bendict's resignation be revealed as for other than medical reasons?</li>\n<li>Will Eliezer update HPMOR \na) Exactly once\nb) Exactly twice\nc) Three or more times? </li>\n</ol>\n\n<p>If you like, you can substitute other questions, or suggest new ones, in the comments. Posting to PredictionBook is optional but encouraged.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ja'>Cincinnati February: Predictions</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cCMLcYx9nijYpbwxu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.1118474434796861e-06, "legacy": true, "legacyId": "21587", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cincinnati_February__Predictions\">Discussion article for the meetup : <a href=\"/meetups/ja\">Cincinnati February: Predictions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 February 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">354 Ludlow Avenue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at the Amol India on Ludlow street at 1400. This month's exercise is to try for calibration. Each of us should give a probability for each of the five events below occurring before April 1st; at the meetup we will discuss our reasoning and perhaps update. Then in April we'll see how we did, as a group and individually.</p>\n\n<ol>\n<li>Will Kim Jong-un cease to be dictator of Best Korea? </li>\n<li>Will the sentence of any of the seven scientists convicted of failing to adequately warn of the L'Aquila earthquake be modified? </li>\n<li>Will a large (more than 1000 soldiers) foreign force invade Iran? </li>\n<li>Will pope Bendict's resignation be revealed as for other than medical reasons?</li>\n<li>Will Eliezer update HPMOR \na) Exactly once\nb) Exactly twice\nc) Three or more times? </li>\n</ol>\n\n<p>If you like, you can substitute other questions, or suggest new ones, in the comments. Posting to PredictionBook is optional but encouraged.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cincinnati_February__Predictions1\">Discussion article for the meetup : <a href=\"/meetups/ja\">Cincinnati February: Predictions</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cincinnati February: Predictions", "anchor": "Discussion_article_for_the_meetup___Cincinnati_February__Predictions", "level": 1}, {"title": "Discussion article for the meetup : Cincinnati February: Predictions", "anchor": "Discussion_article_for_the_meetup___Cincinnati_February__Predictions1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T04:50:06.199Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] War and/or Peace (2/8)", "slug": "seq-rerun-war-and-or-peace-2-8", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xm4BgZbStnQ7fFtgd/seq-rerun-war-and-or-peace-2-8", "pageUrlRelative": "/posts/xm4BgZbStnQ7fFtgd/seq-rerun-war-and-or-peace-2-8", "linkUrl": "https://www.lesswrong.com/posts/xm4BgZbStnQ7fFtgd/seq-rerun-war-and-or-peace-2-8", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20War%20and%2For%20Peace%20(2%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20War%20and%2For%20Peace%20(2%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxm4BgZbStnQ7fFtgd%2Fseq-rerun-war-and-or-peace-2-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20War%20and%2For%20Peace%20(2%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxm4BgZbStnQ7fFtgd%2Fseq-rerun-war-and-or-peace-2-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxm4BgZbStnQ7fFtgd%2Fseq-rerun-war-and-or-peace-2-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 160, "htmlBody": "<p>Today's post, <a href=\"/lw/y6/war_andor_peace_28/\">War and/or Peace (2/8)</a> was originally published on 31 January 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#War_and.2For_Peace_.282.2F8.29\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The true prisoner's dilemma against aliens. The conference struggles to decide the appropriate course of action.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gn9/seq_rerun_the_babyeating_aliens_18/\">The Baby-Eating Aliens (1/8)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xm4BgZbStnQ7fFtgd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.1118783625645324e-06, "legacy": true, "legacyId": "21588", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RXQ5MkWkTCvLMGHrp", "ineLyCTv5q9zhqhF6", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T06:32:40.415Z", "modifiedAt": null, "url": null, "title": "Rationalist Lent", "slug": "rationalist-lent", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:08.620Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Qiaochu_Yuan", "createdAt": "2012-11-24T08:36:50.547Z", "isAdmin": false, "displayName": "Qiaochu_Yuan"}, "userId": "qgFX9ZhzPCkcduZyB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LnpShPEqcsGFTFsKS/rationalist-lent", "pageUrlRelative": "/posts/LnpShPEqcsGFTFsKS/rationalist-lent", "linkUrl": "https://www.lesswrong.com/posts/LnpShPEqcsGFTFsKS/rationalist-lent", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20Lent&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20Lent%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLnpShPEqcsGFTFsKS%2Frationalist-lent%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20Lent%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLnpShPEqcsGFTFsKS%2Frationalist-lent", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLnpShPEqcsGFTFsKS%2Frationalist-lent", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p>As I understand it, <a href=\"http://en.wikipedia.org/wiki/Lent\">Lent</a> is a holiday where we celebrate the scientific method by changing exactly one variable in our lives for 40 days. This seems like a convenient <a href=\"/lw/dc7/nash_equilibria_and_schelling_points/\">Schelling point</a> for rationalists to adopt, so:</p>\n<p>What variable are you going to change for the next 40 days?</p>\n<p>(I am really annoyed I didn't think of this yesterday.)&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hXTqT62YDTTiqJfxG": 1, "AodfCFefLAuwDyj7Z": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LnpShPEqcsGFTFsKS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 65, "extendedScore": null, "score": 0.000224, "legacy": true, "legacyId": "21589", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yJfBzcDL9fBHJfZ6P"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T09:02:50.102Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-02]", "slug": "meetup-cambridge-uk-lw-meetup-reading-group-haefb-02", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.037Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sohum", "createdAt": "2012-02-06T06:28:59.691Z", "isAdmin": false, "displayName": "Sohum"}, "userId": "DP4jNdSvbeAofhxkb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rKwMTMnkxy6YFog3u/meetup-cambridge-uk-lw-meetup-reading-group-haefb-02", "pageUrlRelative": "/posts/rKwMTMnkxy6YFog3u/meetup-cambridge-uk-lw-meetup-reading-group-haefb-02", "linkUrl": "https://www.lesswrong.com/posts/rKwMTMnkxy6YFog3u/meetup-cambridge-uk-lw-meetup-reading-group-haefb-02", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-02%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-02%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKwMTMnkxy6YFog3u%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-02%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-02%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKwMTMnkxy6YFog3u%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-02", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKwMTMnkxy6YFog3u%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-02", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jb'>Cambridge, UK LW Meetup [Reading Group, HAEFB-02]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 February 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Trinity JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup! This week, we'll be continuing our reading group session of the Shiny New Sequence, Highly Advanced Epistemology 101 For Beginners. These are explicitly designed as introductory posts, and this is just our second session, so dive right in with us if you're new!</p>\n\n<p>We only covered The Useful Idea of Truth last week, so this week we'll do Appreciating Cognitive Algorithms and the two minor posts, Skill: The Map is Not The Territory and Firewalling the Optimal From The Rational.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jb'>Cambridge, UK LW Meetup [Reading Group, HAEFB-02]</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rKwMTMnkxy6YFog3u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.112038542622102e-06, "legacy": true, "legacyId": "21590", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_02_\">Discussion article for the meetup : <a href=\"/meetups/jb\">Cambridge, UK LW Meetup [Reading Group, HAEFB-02]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 February 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Trinity JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup! This week, we'll be continuing our reading group session of the Shiny New Sequence, Highly Advanced Epistemology 101 For Beginners. These are explicitly designed as introductory posts, and this is just our second session, so dive right in with us if you're new!</p>\n\n<p>We only covered The Useful Idea of Truth last week, so this week we'll do Appreciating Cognitive Algorithms and the two minor posts, Skill: The Map is Not The Territory and Firewalling the Optimal From The Rational.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_02_1\">Discussion article for the meetup : <a href=\"/meetups/jb\">Cambridge, UK LW Meetup [Reading Group, HAEFB-02]</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-02]", "anchor": "Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_02_", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-02]", "anchor": "Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_02_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T09:22:23.380Z", "modifiedAt": null, "url": null, "title": "A Series of Increasingly Perverse and Destructive Games", "slug": "a-series-of-increasingly-perverse-and-destructive-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:54.803Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nigerweiss", "createdAt": "2012-11-20T18:28:27.983Z", "isAdmin": false, "displayName": "nigerweiss"}, "userId": "DYPo3FWGnAX3mwp2Y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Dtkrq5h7GnuxSSPRG/a-series-of-increasingly-perverse-and-destructive-games", "pageUrlRelative": "/posts/Dtkrq5h7GnuxSSPRG/a-series-of-increasingly-perverse-and-destructive-games", "linkUrl": "https://www.lesswrong.com/posts/Dtkrq5h7GnuxSSPRG/a-series-of-increasingly-perverse-and-destructive-games", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Series%20of%20Increasingly%20Perverse%20and%20Destructive%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Series%20of%20Increasingly%20Perverse%20and%20Destructive%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDtkrq5h7GnuxSSPRG%2Fa-series-of-increasingly-perverse-and-destructive-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Series%20of%20Increasingly%20Perverse%20and%20Destructive%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDtkrq5h7GnuxSSPRG%2Fa-series-of-increasingly-perverse-and-destructive-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDtkrq5h7GnuxSSPRG%2Fa-series-of-increasingly-perverse-and-destructive-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 982, "htmlBody": "<p><em>Related to: <a href=\"/r/discussion/lw/gng/higher_than_the_most_high/\">Higher Than the Most High</a></em></p>\n<p>&nbsp;</p>\n<p>The linked post describes a game in which (I fudge a little), Omega comes to you and two other people, and ask you to tell him an integer. &nbsp;The person who names the largest integer is allowed to leave. &nbsp;The other two are killed.</p>\n<p>This got me thinking about variations on the same concept, and here's what I've come up, taking that game to be <strong>GAME<sub>0</sub></strong><span style=\"vertical-align: sub;\">. &nbsp;The results are sort of a fun time-waster, and bring up some interesting issues. &nbsp;For your enjoyment...</span></p>\n<p>&nbsp;</p>\n<p><strong>THE GAMES:</strong></p>\n<p><span style=\"vertical-align: sub;\"><strong>GAME<span style=\"font-size: 11px;\">1</span></strong></span><span style=\"vertical-align: sub;\"><strong>:</strong>&nbsp;Omega takes you and two strangers (all competent programmers), and kidnaps and sedates you. &nbsp;You awake in three rooms with instructions printed on the wall explaining the game, and a computer with an operating system and programming language compiler, but no internet. &nbsp;Food, water, and toiletries are provided, but no external communication. &nbsp;The participants are allowed to write programs on the computer in a language that supports arbitrarily large numerical values. &nbsp;The programs are taken by Omega and run on a hypercomputer in finite time (this hypercomputer can resolve the halting problem and infinite loops, but programs that do not eventually halt return no output). &nbsp;The person who wrote the program with the largest output is allowed to leave. &nbsp;The others are instantly and painlessly killed. &nbsp;In the event of a tie, everyone dies. &nbsp;If your program returns no output, that is taken to be zero. &nbsp; &nbsp;</span></p>\n<p><span style=\"vertical-align: sub;\"><strong>GAME<span style=\"font-size: 11px;\">2</span></strong></span><span style=\"vertical-align: sub;\"><strong>:</strong>&nbsp;</span>Identical to <strong>GAME<sub>1</sub></strong>, except that each program you write has to take two inputs, which will be the text of the other players' programs (assume they're all written in the same language). &nbsp;The reward for outputting the largest number apply normally. &nbsp;</p>\n<p><strong>GAME</strong><span style=\"font-size: 11px;\"><strong>3: </strong></span>Identical to <strong>Game<sub>2</sub></strong>, except that while you are sedated, Omega painlessly and imperceptibly uploads you. &nbsp;Additionally, the instructions on the wall now specify that your program must take four inputs - blackbox functions which represent the uploaded minds of all three players, plus a simulation of the room you're in, indistinguishable from the real thing. &nbsp;We'll assume that players can't modify or interpret the contents of their opponents' brains. &nbsp;The room function take an argument of a string (which controls the text printed on the wall, and outputs whatever number the person in the simulation's program returns).</p>\n<p>&nbsp;</p>\n<p>In each of these games, which program should you write if you wish to survive? &nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>SOME DISCUSSION OF STRATEGY:&nbsp;</strong></p>\n<p><strong>GAME1: </strong>Clearly, the trivial strategy (implement the Ackerman or similar fast-growing functions and generate some large integer), gives no better than random results, because it's the bare minimal strategy anyone will employ, and your ranking in the results, without knowledge of your opponents is entirely up to chance / how long you're willing to sit there typing nines for your Ackermann argument.</p>\n<p>A few alternatives for your consideration:</p>\n<p>1: if you are aware of an existence hypothesis (say, a number with some property which is not conclusively known to exist and could be any integer), write a program that brute-force tests all integers until it arrives at an integer which matches the requirements, and use this as the argument for your rapidly-growing function. &nbsp;While it may never return any output, if it does, the output will be an integer, and the expected value goes towards infinity. &nbsp;</p>\n<p>2: Write a program that generates all programs shorter than length n, and finds the one with the largest output. &nbsp;Then make a separate stab at your own non-meta winning strategy. &nbsp;Take the length of the program you produce, tetrate it for safety, and use that as your length n. &nbsp;Return the return value of the winning program.</p>\n<p>On the whole, though, this game is simply not all that interesting in a broader sense. &nbsp;</p>\n<p><strong>GAME2: </strong>This game has its own amusing quirks (primarily that it could probably actually be played in real life on a non-hypercomputer), however, most of its salient features are also present in GAME3, so I'm going to defer discussion to that. &nbsp;I'll only say that the obvious strategy (sum the outputs of the other two players' programs and return that) leads to an infinite recursive trawl and never halts if everyone takes it. &nbsp;This holds true for any simple strategy for adding or multiplying some constant with the outputs of your opponents' programs. &nbsp; &nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>GAME3: </strong>This game is by far the most interesting. &nbsp;For starters, this game permits acausal negotiation between players (by parties simulating and conversing with one another). &nbsp;Furthermore, anthropic reasoning plays a huge role, since the player is never sure if they're in the real world, one of their own simulations, or one of the simulations of the other players. &nbsp;</p>\n<p>Players can negotiate, barter, or threaten one another, they can attempt to send signals to their simulated selves (to indicate that they are in their own simulation and not somebody else's). &nbsp;They can make their choices based on coin flips, to render themselves difficult to simulate. &nbsp;They can attempt to brute-force the signals their simulated opponents are expecting. &nbsp;They can simulate copies of their opponents who think they're playing any previous version of the game, and are unaware they've been uploaded. &nbsp;They can simulate copies of their opponents, observe their meta-strategies, and plan around them. &nbsp;They can totally ignore the inputs from the other players and play just the level one game. &nbsp;It gets very exciting very quickly. &nbsp;I'd like to see what strategy you folks would employ. &nbsp;</p>\n<p>&nbsp;</p>\n<p>And, as a final bonus, I present <strong>GAME<sub>4</sub> </strong>: &nbsp;In game 4, there is no Omega, and no hypercomputer. &nbsp;You simply take a friend, chloroform them, and put them in a concrete room with the instructions for GAME3 on the wall, and a linux computer not plugged into anything. &nbsp;You leave them there for a few months working on their program, and watch what happens to their psychology. &nbsp;You win when they shrink down into a dead-eyed, terminally-paranoid and entirely insane shell of their former selves. &nbsp;This is the easiest game. &nbsp;</p>\n<p>&nbsp;</p>\n<p>Happy playing! &nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Dtkrq5h7GnuxSSPRG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 17, "extendedScore": null, "score": 1.1120509379307487e-06, "legacy": true, "legacyId": "21591", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Related to: <a href=\"/r/discussion/lw/gng/higher_than_the_most_high/\">Higher Than the Most High</a></em></p>\n<p>&nbsp;</p>\n<p>The linked post describes a game in which (I fudge a little), Omega comes to you and two other people, and ask you to tell him an integer. &nbsp;The person who names the largest integer is allowed to leave. &nbsp;The other two are killed.</p>\n<p>This got me thinking about variations on the same concept, and here's what I've come up, taking that game to be <strong>GAME<sub>0</sub></strong><span style=\"vertical-align: sub;\">. &nbsp;The results are sort of a fun time-waster, and bring up some interesting issues. &nbsp;For your enjoyment...</span></p>\n<p>&nbsp;</p>\n<p><strong id=\"THE_GAMES_\">THE GAMES:</strong></p>\n<p><span style=\"vertical-align: sub;\"><strong>GAME<span style=\"font-size: 11px;\">1</span></strong></span><span style=\"vertical-align: sub;\"><strong>:</strong>&nbsp;Omega takes you and two strangers (all competent programmers), and kidnaps and sedates you. &nbsp;You awake in three rooms with instructions printed on the wall explaining the game, and a computer with an operating system and programming language compiler, but no internet. &nbsp;Food, water, and toiletries are provided, but no external communication. &nbsp;The participants are allowed to write programs on the computer in a language that supports arbitrarily large numerical values. &nbsp;The programs are taken by Omega and run on a hypercomputer in finite time (this hypercomputer can resolve the halting problem and infinite loops, but programs that do not eventually halt return no output). &nbsp;The person who wrote the program with the largest output is allowed to leave. &nbsp;The others are instantly and painlessly killed. &nbsp;In the event of a tie, everyone dies. &nbsp;If your program returns no output, that is taken to be zero. &nbsp; &nbsp;</span></p>\n<p><span style=\"vertical-align: sub;\"><strong>GAME<span style=\"font-size: 11px;\">2</span></strong></span><span style=\"vertical-align: sub;\"><strong>:</strong>&nbsp;</span>Identical to <strong>GAME<sub>1</sub></strong>, except that each program you write has to take two inputs, which will be the text of the other players' programs (assume they're all written in the same language). &nbsp;The reward for outputting the largest number apply normally. &nbsp;</p>\n<p><strong>GAME</strong><span style=\"font-size: 11px;\"><strong>3: </strong></span>Identical to <strong>Game<sub>2</sub></strong>, except that while you are sedated, Omega painlessly and imperceptibly uploads you. &nbsp;Additionally, the instructions on the wall now specify that your program must take four inputs - blackbox functions which represent the uploaded minds of all three players, plus a simulation of the room you're in, indistinguishable from the real thing. &nbsp;We'll assume that players can't modify or interpret the contents of their opponents' brains. &nbsp;The room function take an argument of a string (which controls the text printed on the wall, and outputs whatever number the person in the simulation's program returns).</p>\n<p>&nbsp;</p>\n<p>In each of these games, which program should you write if you wish to survive? &nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"SOME_DISCUSSION_OF_STRATEGY__\">SOME DISCUSSION OF STRATEGY:&nbsp;</strong></p>\n<p><strong>GAME1: </strong>Clearly, the trivial strategy (implement the Ackerman or similar fast-growing functions and generate some large integer), gives no better than random results, because it's the bare minimal strategy anyone will employ, and your ranking in the results, without knowledge of your opponents is entirely up to chance / how long you're willing to sit there typing nines for your Ackermann argument.</p>\n<p>A few alternatives for your consideration:</p>\n<p>1: if you are aware of an existence hypothesis (say, a number with some property which is not conclusively known to exist and could be any integer), write a program that brute-force tests all integers until it arrives at an integer which matches the requirements, and use this as the argument for your rapidly-growing function. &nbsp;While it may never return any output, if it does, the output will be an integer, and the expected value goes towards infinity. &nbsp;</p>\n<p>2: Write a program that generates all programs shorter than length n, and finds the one with the largest output. &nbsp;Then make a separate stab at your own non-meta winning strategy. &nbsp;Take the length of the program you produce, tetrate it for safety, and use that as your length n. &nbsp;Return the return value of the winning program.</p>\n<p>On the whole, though, this game is simply not all that interesting in a broader sense. &nbsp;</p>\n<p><strong>GAME2: </strong>This game has its own amusing quirks (primarily that it could probably actually be played in real life on a non-hypercomputer), however, most of its salient features are also present in GAME3, so I'm going to defer discussion to that. &nbsp;I'll only say that the obvious strategy (sum the outputs of the other two players' programs and return that) leads to an infinite recursive trawl and never halts if everyone takes it. &nbsp;This holds true for any simple strategy for adding or multiplying some constant with the outputs of your opponents' programs. &nbsp; &nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>GAME3: </strong>This game is by far the most interesting. &nbsp;For starters, this game permits acausal negotiation between players (by parties simulating and conversing with one another). &nbsp;Furthermore, anthropic reasoning plays a huge role, since the player is never sure if they're in the real world, one of their own simulations, or one of the simulations of the other players. &nbsp;</p>\n<p>Players can negotiate, barter, or threaten one another, they can attempt to send signals to their simulated selves (to indicate that they are in their own simulation and not somebody else's). &nbsp;They can make their choices based on coin flips, to render themselves difficult to simulate. &nbsp;They can attempt to brute-force the signals their simulated opponents are expecting. &nbsp;They can simulate copies of their opponents who think they're playing any previous version of the game, and are unaware they've been uploaded. &nbsp;They can simulate copies of their opponents, observe their meta-strategies, and plan around them. &nbsp;They can totally ignore the inputs from the other players and play just the level one game. &nbsp;It gets very exciting very quickly. &nbsp;I'd like to see what strategy you folks would employ. &nbsp;</p>\n<p>&nbsp;</p>\n<p>And, as a final bonus, I present <strong>GAME<sub>4</sub> </strong>: &nbsp;In game 4, there is no Omega, and no hypercomputer. &nbsp;You simply take a friend, chloroform them, and put them in a concrete room with the instructions for GAME3 on the wall, and a linux computer not plugged into anything. &nbsp;You leave them there for a few months working on their program, and watch what happens to their psychology. &nbsp;You win when they shrink down into a dead-eyed, terminally-paranoid and entirely insane shell of their former selves. &nbsp;This is the easiest game. &nbsp;</p>\n<p>&nbsp;</p>\n<p>Happy playing! &nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "THE GAMES:", "anchor": "THE_GAMES_", "level": 1}, {"title": "SOME DISCUSSION OF STRATEGY:\u00a0", "anchor": "SOME_DISCUSSION_OF_STRATEGY__", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "33 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jpMwB3NKjcYXZLo8E"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T09:44:53.666Z", "modifiedAt": null, "url": null, "title": "The Singularity Wars", "slug": "the-singularity-wars", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:38.718Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GxxARXNDAD5pWnxak/the-singularity-wars", "pageUrlRelative": "/posts/GxxARXNDAD5pWnxak/the-singularity-wars", "linkUrl": "https://www.lesswrong.com/posts/GxxARXNDAD5pWnxak/the-singularity-wars", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Singularity%20Wars&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Singularity%20Wars%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGxxARXNDAD5pWnxak%2Fthe-singularity-wars%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Singularity%20Wars%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGxxARXNDAD5pWnxak%2Fthe-singularity-wars", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGxxARXNDAD5pWnxak%2Fthe-singularity-wars", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1027, "htmlBody": "<p><em><span>(This is a&nbsp;introduction, for &nbsp;those not immersed in the Singularity world, into the history of and relationships between SU, SIAI [</span><span>SI,</span><span>&nbsp;</span><span>MIRI], SS, LW, CSER, FHI, and&nbsp;</span><span>CFAR. It also has some opinions, which are strictly my own.)</span></em></p>\n<p>The good news is that there <em>were</em> no Singularity Wars.&nbsp;</p>\n<p><span>The Bay Area had a Singularity University and a Singularity Institute, each going in a very &nbsp;different direction. You'd expect to see something like the&nbsp;</span><a href=\"http://www.youtube.com/watch?v=gb_qHP7VaZE\" target=\"_blank\">People's Front of Judea and the Judean People's Front</a><span>, burning each other's </span><a href=\"http://www.historynet.com/first-jewish-roman-war.htm\" target=\"_blank\">grain supplies</a><span>&nbsp;as the Romans moved in.&nbsp;</span></p>\n<p><a id=\"more\"></a></p>\n<p>The&nbsp;<a href=\"http://singinst.org\">Singularity Institute for Artificial Intelligence</a>&nbsp;was founded first, in 2000, by Eliezer Yudkowsky.<br /><br />Singularity&nbsp;University&nbsp;was founded in 2008. Ray Kurzweil, the driving force behind&nbsp;SU, was also active in&nbsp;SIAI,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Singularity_Institute\">serving</a>&nbsp;on its board in varying capacities in the years up to &nbsp;2010.<br /><br />SIAI's multi-part name was clunky, and their domain, singinst.org, unmemorable. I kept accidentally visiting siai.org for months, but it belonged to the Self Insurance Association of Illinois. (The cool new domain name singularity.org, recently acquired after a rather uninspired site <a href=\"http://web.archive.org/web/20060307115157/http://www.singularity.org/\" target=\"_blank\">appeared there for several years</a>, arrived shortly before it was no longer relevant.)&nbsp;All the better to confuse you with, SIAI has been going for the last few years by the shortened name&nbsp;Singularity&nbsp;Institute, abbreviated&nbsp;SI.<br /><br />The annual<a href=\"/http;/singularitysummit.com\">&nbsp;</a><a href=\"/http;/singularitysummit.com\">Singularity<span>&nbsp;</span>Summit</a>&nbsp;was launched by SI, together with Kurzweil, in 2006. SS was SI's premier PR mechanism, mustering geek heroes to&nbsp;give their tacit endorsement for SI's seriousness, if not its views, by agreeing to appear on-stage.</p>\n<p>The Singularity Summit was always off-topic for SI: more SU-like than SI-like. Speakers spoke about whatever technologically-advanced ideas interested them. Occasional SI representatives spoke about the Intelligence Explosion, but they too would often stray into other areas like rationality and the scientific process. Yet SS remained firmly in SI's hands.</p>\n<p><span>It became clear over the years that SU and SI &nbsp;have almost nothing to do with each other except for the word&nbsp; \"Singularity.\" The word has&nbsp;</span><a href=\"http://yudkowsky.net/singularity/schools\" target=\"_blank\">three major meanings</a><span>, and of these, Yudkowsky favored the Intelligence Explosion while&nbsp;Kurzweil pushed Accelerating Change.</span><br /><br /><span>But actually, SU's activities have little to do with the Singularity, even under Kurzweil's definition. Kurzweil writes of a future, around the 2040s, &nbsp;in which the human condition is altered beyond recognition. But SU mostly deals with whizzy next-gen technology. &nbsp;They are doing something important, encouraging technological advancement with a focus on helping humanity, but they spend little time working on optimizing the end of our human existence as we know it. &nbsp;</span><a href=\"http://hpmor.com/notes/#post-448\" target=\"_blank\">Yudkowsky calls</a>&nbsp;what they do \"technoyay.\"<span>&nbsp;And maybe that's what the Singularity means, nowadays. Time to stop using the word.</span><br /><br /><span>(I've also heard SU graduates saying \"I was at Singularity last week,\" on the pattern of \"I was at Harvard last week,\" eliding \"University.\" I think that that counts as the end of Singularity as we know it.)</span><br /><br /><span>You might expect SU and SI to get in a stupid squabble about the name. People love fighting over words. But to everyone's credit, I didn't hear squabbling, just confusion from those who were &nbsp;not in the know. Or you might expect SI to give up, change its name and close down the Singularity Summit. But lo and behold, SU and SI settled the matter sensibly, amicable, in fact ... rationally. SU bought the Summit and the entire \"Singularity\" brand from SI -- for money! Yes! Coase rules!</span><br /><br /><span>SI chose the new name&nbsp;</span>Machine Intelligence Research Institute<span>. I like it.</span><br /><br /><span>The term \"Artificial Intelligence\" got burned out in the AI Winter&nbsp;</span>in the early 1990's. The term has been firmly&nbsp;<a href=\"http://blog.joshuafox.com/2009/04/game-of-taboo.html\" target=\"_blank\">taboo</a>&nbsp;since then, even in the software industry, even in the &nbsp;leading edge of the software industry. I did technical evangelism for Unicorn, a leading industrial ontology software startup, and the phrase \"Artificial Intelligence\" was most definitely out of bounds. The term was not used even inside the company. This was despite a founder with a CoSci PhD, and a&nbsp;co-founder with a masters in AI.</p>\n<p>The rarely-used term \"Machine Intelligence\" throws off that baggage, and so, SI managed to ditch two taboo words at once.<br /><br />The&nbsp;MIRI&nbsp;name is perhaps too broad. It could serve for any AI research group. The Machine Intelligence Research Institute focuses on decreasing the chances of a negative Intelligence Explosion and increasing the chances of a positive one, not on rushing to develop machine intelligence ASAP. But the name is accurate.<br /><br />In 2005, the&nbsp;<a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a>&nbsp;at Oxford University was founded, followed by the<a href=\"http://cser.org/\">&nbsp;Centre for the Study of Existential Risk</a>&nbsp;at Cambridge University&nbsp;in early 2013.&nbsp;FHI&nbsp;is doing good work, rivaling MIRI's and in some ways surpassing it.&nbsp;CSER's announced research area, and the reputations of its founders, suggest that we can expect good things. Competition for the sake of humanity! The more the merrier!<br /><br />In late 2012, SI spun off the&nbsp;<a href=\"http://appliedrationality.org\">Center for Applied Rationality</a>. Since 2008, much of SI's energies, and particularly those of Yudkowsky, had gone to<a> LessWrong.com</a> and the field of rationality. As a tactic to bring in smart, committed new researchers and organizers, this was highly successful, and who can argue with the importance of being more rational? But as a strategy for saving humanity from existential AI risk, this second focus was a distraction. SI got the point, and split off CFAR.<br /><br />Way to go, MIRI! So many of the criticisms I had about SI's strategic direction and its administration in the &nbsp;years I first encountered it in 2005 have been resolved recently.&nbsp;</p>\n<p><span>Next step: A much much better human future.</span></p>\n<p><span>The TL;DR, conveniently at the bottom of the article to encourage you to actually read it, is:</span></p>\n<ul>\n<li><a href=\"http://singinst.org\">MIRI </a>(formerly SIAI, SI): Working to avoid existential&nbsp;risk&nbsp;from future machine intelligence, while&nbsp;increasing the chances of positive outcome</li>\n<li><a href=\"http://appliedrationality.org\">CFAR</a>: Training in applied rationality</li>\n<li><a href=\"http://cser.org/\">CSER</a>: Research towards avoiding existential risk, with future machine intelligence as a strong focus</li>\n<li><a href=\"http://www.fhi.ox.ac.uk/\">FHI</a>: Researching various transhumanist topics, but with a strong research program in existential risk &nbsp;and future machine intelligence in particular</li>\n<li><a href=\"http://singularityu.org/\">SU</a>: Teaching and encouraging the development of next-generation technologies</li>\n<li><a href=\"http://singularitysummit.com/\">SS</a>: An annual forum for top geek heroes to speak &nbsp;on whatever interests them.&nbsp;Favored topics include societal trends,&nbsp;next-gen science and technology, and transhumanism.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1, "izp6eeJJEg9v5zcur": 1, "K6oowPZC6kds6LDTg": 1, "X7v7Fyp9cgBYaMe2e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GxxARXNDAD5pWnxak", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 58, "baseScore": 82, "extendedScore": null, "score": 0.00019684023280382975, "legacy": true, "legacyId": "21568", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T16:48:20.713Z", "modifiedAt": null, "url": null, "title": "Meetup : Purdue Meetup", "slug": "meetup-purdue-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sek0M", "createdAt": "2012-12-11T23:50:31.795Z", "isAdmin": false, "displayName": "Sek0M"}, "userId": "Bh4aQFJwJ5KH3J4mQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6izcm2aCTCvbxmcPS/meetup-purdue-meetup", "pageUrlRelative": "/posts/6izcm2aCTCvbxmcPS/meetup-purdue-meetup", "linkUrl": "https://www.lesswrong.com/posts/6izcm2aCTCvbxmcPS/meetup-purdue-meetup", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Purdue%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Purdue%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6izcm2aCTCvbxmcPS%2Fmeetup-purdue-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Purdue%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6izcm2aCTCvbxmcPS%2Fmeetup-purdue-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6izcm2aCTCvbxmcPS%2Fmeetup-purdue-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jc'>Purdue Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 February 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Purdue - Hicks Library</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A reminder that there will be a Purdue Less Wrong Meetup this Friday.  We have reserved a space, but given the short timeline and incomplete email list the plan is to meet in Hicks, group up, and transition to the reserved space. I'm saying the meeting time is 7PM instead of 6:50 based on my own preference for round numbers.</p>\n\n<p>We're planning to bring some minor level of snackage, and in addition to our often very dynamic discussions I will offer some more structured thoughts on, well, structure, and how to grow the organization and offer more utility to our own meetup folks and possible to the campus and community in general.  Please feel free to contact me with any question or concerns, and we hope to see you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jc'>Purdue Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6izcm2aCTCvbxmcPS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.112333687456897e-06, "legacy": true, "legacyId": "21592", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Purdue_Meetup\">Discussion article for the meetup : <a href=\"/meetups/jc\">Purdue Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 February 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Purdue - Hicks Library</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A reminder that there will be a Purdue Less Wrong Meetup this Friday.  We have reserved a space, but given the short timeline and incomplete email list the plan is to meet in Hicks, group up, and transition to the reserved space. I'm saying the meeting time is 7PM instead of 6:50 based on my own preference for round numbers.</p>\n\n<p>We're planning to bring some minor level of snackage, and in addition to our often very dynamic discussions I will offer some more structured thoughts on, well, structure, and how to grow the organization and offer more utility to our own meetup folks and possible to the campus and community in general.  Please feel free to contact me with any question or concerns, and we hope to see you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Purdue_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/jc\">Purdue Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Purdue Meetup", "anchor": "Discussion_article_for_the_meetup___Purdue_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Purdue Meetup", "anchor": "Discussion_article_for_the_meetup___Purdue_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T16:59:13.302Z", "modifiedAt": null, "url": null, "title": "Domesticating reduced impact AIs", "slug": "domesticating-reduced-impact-ais", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:05.388Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FdcxknHjeNH2MzrTj/domesticating-reduced-impact-ais", "pageUrlRelative": "/posts/FdcxknHjeNH2MzrTj/domesticating-reduced-impact-ais", "linkUrl": "https://www.lesswrong.com/posts/FdcxknHjeNH2MzrTj/domesticating-reduced-impact-ais", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Domesticating%20reduced%20impact%20AIs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADomesticating%20reduced%20impact%20AIs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFdcxknHjeNH2MzrTj%2Fdomesticating-reduced-impact-ais%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Domesticating%20reduced%20impact%20AIs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFdcxknHjeNH2MzrTj%2Fdomesticating-reduced-impact-ais", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFdcxknHjeNH2MzrTj%2Fdomesticating-reduced-impact-ais", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1939, "htmlBody": "<p><em>About a year ago, I <a href=\"/lw/a39/the_mathematics_of_reduced_impact_help_needed/\">posted</a> several ideas for \"reduced impact AI\" (what Nick Bostrom calls \"domesticity\"). I think the most promising approach was the third one, which I pompously titled \"The information in the evidence\". In this post, I'll attempt to put together a (non-realistic) example of this, to see if it's solid enough to build on. I'll be <strong>highlighting</strong> assumptions I'm making about the AI; please point out any implicit assumption that I missed, and any other weaknesses of the setup. For the moment, I'm more interested in \"this doesn't work\" than \"this can't be done in practice\" or \"this can't be usefully generalised\".</em></p>\n<p><em><strong>EDIT</strong>: It wasn't clear here, but any paperclip constructed by the reduced impact AI would be destroyed in the explosion, and the AIs would not be observed during the process. How to get <strong>useful</strong> work out of the AI will be the next step, if this model holds up.</em></p>\n<h2>Intuitive idea</h2>\n<p>For a reduced impact AI, we want an AI that can accomplish something, say building a <a href=\"http://en.wikipedia.org/wiki/Office_Assistant\">paperclip</a>, without it going out of control and optimising the universe. We want the future to be roughly the same whether or not the AI was turned on. Hence the piece of information \"the AI was turned on\" is not particularly important - if we didn't know, we wouldn't go far wrong in our predictions.</p>\n<p>To enforce this we'll equip the AI with a two-piece motivation: a utility function U (causing it to build paperclips) and a penalty function R (which penalises the AI if its actions have a large future 'impact'). The challenge is to have a setup and a definition of R that implements this intuitive idea.</p>\n<p>&nbsp;</p>\n<h2>Reduced impact</h2>\n<p>Can we hope to define reduced impact? Doesn't any change percolate across all future time? Yes, but not all changes are the same - especially where our ability to predict them is concerned. Imagine an AI, confined to a box. While inside the box, it will build a paperclip. Then, in one hour's time, an antimatter bomb inside the box will explode, obliterating everything inside. We would intuitively say that whether the paperclip was built or not, would have very little impact afterwards. What we really mean is that we are unable to usefully use that information for future predictions. To make it even harder, surround the box by a large mass of antimatter gas, moving chaotically. In order to figure out the consequences of anything happening inside the box, we'd have to be able to unravel chaos: solve chaotic equation correctly and quantitatively, not just qualitatively. So our first assumption is that the <strong>AI can't unravel chaos</strong>.<a id=\"more\"></a></p>\n<p>Note that the chaotic antimatter is meant to erase the impact of small changes, not to contain the AI. We should assume that the AI could easily get out of the box and spread across the universe, should it want to. So we'll be putting machinery and other useful tools inside the box - anything an AI could use to build paperclips or take over the universe. We're only&nbsp;successful&nbsp;if it doesn't want to do that last thing.</p>\n<p>&nbsp;</p>\n<h2>The Future</h2>\n<p>We want 'the future' to be roughly the same whatever the AI does. But what do we mean by that? Obviously, if the AI builds a paperclip, the near future inside the box will be very different. What we want is that the mid future, the future after the explosion, be not very different. So we want to define F, a subset of spacetime, that represents this mid future.</p>\n<p>There are many ways of doing this. Let O=(box, AI turned on) be the point in space time at which the experiment begins. We could then take the initial rest frame of the box, and say that F contains everything two hours or more in the future from O (recall the explosion happens after one hour). Or we could be <a href=\"http://en.wikipedia.org/wiki/Lorentz_covariance\">Lorentz-invariant</a>, and say that F contains everything that can be reached from O, along a geodesic with proper-time more than two hours. F need not even be strictly \"the future\": it could contain everything in the universe, except for the box-and-surrounding-cloud-for-the-duration-of-the-experiment.</p>\n<p>Note that in order to define this F, we're making use of general relativity - so we assume the <strong>AI understands physics</strong>. This includes grounding the concepts into reality properly, and <strong>dealing with <a href=\"/lw/827/ai_ontology_crises_an_informal_typology/\">ontology</a> <a href=\"http://wiki.lesswrong.com/wiki/Ontological_crisis\">crises</a></strong>. We'll also need to assume that the <strong>AI obeys the laws of physics as we understand them, and doesn't have outside-physics superpowers</strong>.</p>\n<p>&nbsp;</p>\n<h2>The AI design</h2>\n<p>The point of defining F was to be able to talk sensibly about a space in which we expect the AI's actions to have reduced measurable impact. So we'll need to start assuming some things about how the AI is structured. We'll assume that the <strong>AI consists functionally of the utility function U, the penalty function R and a probability estimating module P</strong>, and whatever practical components (including knowledge bases) it needs to run its calculations. We'll assume that the <strong>AI can't directly change these components</strong> (though indirectly, it's another matter - if it has access to manipulators, it can freely hack itself). The reason for this assumption will be clear in the next section. Note that we are assuming the AI can know every detail of the setup - we're not counting on security through obscurity.</p>\n<p>Given any world w, we can define w<sup>F</sup>&nbsp;as the world w restricted to the space F - i.e. the future part of w. The probability estimating module P can estimate numbers such as P(w<sup>F</sup>) (the probability of w<sup>F</sup>) and&nbsp;P(w<sup>F</sup>|e) (the probability of w<sup>F</sup>&nbsp;given evidence e). Some worlds will have w<sup>F</sup>=v<sup>F</sup>&nbsp;even with w&ne;v (their pasts differ, but their futures are the same).</p>\n<p>Because of the limitations of the AI (being unable to unravel chaos), we have an implicit coarse-graining of the worlds. For instance, let X be a minor fact about the inside of the box, and let w and v be worlds such that w assumes X and v assumes &not;X. Then if w<sup>F</sup>=v<sup>F</sup>&nbsp;(so the worlds have the same future), we have P(v<sup>F</sup>|X)=P(w<sup>F</sup>|X) &ge;&nbsp;P(w|X), even though P(v|X)=0.</p>\n<p>&nbsp;</p>\n<h2>The Counterfactual</h2>\n<p>We want the future to be roughly the same \"whether or not the AI was turned on\". Hence we need to define a counterfactual. It's asking for trouble to ask a working AI to imagine what the universe would be like if it wasn't turned on. So we'll use a more complicated counterfactual that doesn't have that problem: the relevant counterfactual is firmly in the future, and firmly probabilistic.</p>\n<p>Initially, we imagine the box contains a \"<a href=\"http://en.wikipedia.org/wiki/Palpatine\">master AI</a>\" (which has been turned on), and a \"<a href=\"http://en.wikipedia.org/wiki/Darth_Vader\">disciple AI</a>\", which is turned off. The master AI is the one whose motivation we want to constrain. We don't give the master AI any manipulators or direct means of influencing anything at all. All that it can do is build the source code for the disciple AI. We assume the <strong>master&nbsp;</strong><strong>AI can't affect the world in any other way</strong>.</p>\n<p>Specifically, the master AI will build the source code, according to its preferences, and store it somewhere. Then, exactly 30 minutes after the experiment began (and hence 30 minutes before the explosion), that source code will be copied into the disciple AI. If the source code is not empty, the disciple AI will be turned on.</p>\n<p>The counterfactual will be whether the code is copied or not. At the 30 minute mark, a random process X will output 1 or 0 with equal probability. If X=1, the source code is copied as described. If X=0, nothing happens.&nbsp;<strong>We assume that X is a process that the (master) AI will find truly random</strong>&nbsp;(note that this can be deduced from the \"can't unravel chaos\" assumption - but it's good to have it specified&nbsp;separately, as it serves a different purpose in the experiment). This X will be the point of counterfactuality - we want the future world to be similar whether X=0 (no AI gets to do anything) or X=1 (the disciple AI is unleashed.</p>\n<p>The whole setup can be&nbsp;summarised&nbsp;in the following drawing of&nbsp;unsurpassable&nbsp;elegance:</p>\n<p><img src=\"http://images.lesswrong.com/t3_gmx_0.png?v=51b086ab763517633724144641dc34dc\" alt=\"\" width=\"620\" height=\"877\" /></p>\n<p>&nbsp;</p>\n<h2>The Penalty Function</h2>\n<p>We can now describe the penalty function R. For any world w, we can compute the difference in probability between the future world given X=0 and X=1. There are many ways of assessing the difference; the absolute difference in probability is one simple way of doing this:</p>\n<p style=\"padding-left: 30px;\">R(w)= | P(w<sup>F</sup>|X=1) - P(w<sup>F</sup>|X=0) |</p>\n<p>Then we can integrate this across all w (for some reasonable weights) to get the total penalty function R. As required, R measures how much the AI's future expectation changes, given whether it's successful at creating the disciple AI's source code or not.</p>\n<p>&nbsp;</p>\n<h2>Relative Scaling</h2>\n<p>We're ready to go now. The AI's motivation is that it must maximise U-R: try and maximise its utility while diminishing the penalty function. This is not exactly a utility function, since the output of the probability module P appears inside R, the quantity to be maximised (probabilities shouldn't appear within the definition of the value of a world). Since the master AI is prevented from self-modification, this should not be a problem.&nbsp;Alternately, P could be formally defined as \"the output of a module described as [the code of the AI's P module]\". This would make U-R into a proper utility function, where the module P used by the master AI for probability happens to be the same as the module P used in the definition of R - but they aren't specified to be identical.</p>\n<p>Let's define U in a nice bounded way: giving 0 utility if the disciple builds no paperclips, and utility 1 if it builds at least one. The motivations that the master AI could pick for its disciple fall broadly into three categories:</p>\n<ol>\n<li>The disciple doesn't do anything (U=0, R=0).</li>\n<li>The disciple builds a paperclip or two and nothing much else (U close to 1, R close to 0). This is what we want.</li>\n<li>The disciple optimises the universe (U very close to 1, R hopefully very high).</li>\n</ol>\n<p>We've assumed that the master AI can't unravel chaos - and if this is exactly true, then we're fine. Then option 2 is perfect: U will be close to 1, and R will be exactly 0 and the AI will implement that. But maybe the master AI is a bit smarter than we thought, and the \"can't unravel chaos\" is broadly true, but the AI can still predict <em>some</em> small expected difference. This means that R will increase above zero, making the master AI's task harder, and making the \"do nothing\" option relatively more attractive.</p>\n<p>This seems a great security feature - it means that if the master AI is smarter than we expected, it is more likely to do nothing. But this setup fails if the master AI always does nothing. So we'll need to weight R so that the master AI can program its disciple to do <em>something</em>. There are issues with doing this in practice (we don't want AIs to influence people to set R too low, for instance with deliberate inaction). But let's assume here that <strong>we've picked an R such that if the AI is as smart as we think, it will just about be able to do something</strong> (if it's smarter, it'll still do nothing).</p>\n<p>&nbsp;</p>\n<h2>Conclusion</h2>\n<p>Given all these assumptions, then it seems that this setup will produce what we want: a reduced impact AI that will program a disciple AI that will build a paperclip and little else. The main error mode (if the AI is smarter than expected) seems to be for the AI to do nothing.</p>\n<p><strong>Note:</strong><em> the original formulation of this had a second box,&nbsp;causally&nbsp;disconnected and otherwise identical but with no AI, for purposes of comparison and calibration. It seems that it isn't necessary, but I may put it back in if needed.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FdcxknHjeNH2MzrTj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 13, "extendedScore": null, "score": 1.112340585039581e-06, "legacy": true, "legacyId": "21561", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>About a year ago, I <a href=\"/lw/a39/the_mathematics_of_reduced_impact_help_needed/\">posted</a> several ideas for \"reduced impact AI\" (what Nick Bostrom calls \"domesticity\"). I think the most promising approach was the third one, which I pompously titled \"The information in the evidence\". In this post, I'll attempt to put together a (non-realistic) example of this, to see if it's solid enough to build on. I'll be <strong>highlighting</strong> assumptions I'm making about the AI; please point out any implicit assumption that I missed, and any other weaknesses of the setup. For the moment, I'm more interested in \"this doesn't work\" than \"this can't be done in practice\" or \"this can't be usefully generalised\".</em></p>\n<p><em><strong>EDIT</strong>: It wasn't clear here, but any paperclip constructed by the reduced impact AI would be destroyed in the explosion, and the AIs would not be observed during the process. How to get <strong>useful</strong> work out of the AI will be the next step, if this model holds up.</em></p>\n<h2 id=\"Intuitive_idea\">Intuitive idea</h2>\n<p>For a reduced impact AI, we want an AI that can accomplish something, say building a <a href=\"http://en.wikipedia.org/wiki/Office_Assistant\">paperclip</a>, without it going out of control and optimising the universe. We want the future to be roughly the same whether or not the AI was turned on. Hence the piece of information \"the AI was turned on\" is not particularly important - if we didn't know, we wouldn't go far wrong in our predictions.</p>\n<p>To enforce this we'll equip the AI with a two-piece motivation: a utility function U (causing it to build paperclips) and a penalty function R (which penalises the AI if its actions have a large future 'impact'). The challenge is to have a setup and a definition of R that implements this intuitive idea.</p>\n<p>&nbsp;</p>\n<h2 id=\"Reduced_impact\">Reduced impact</h2>\n<p>Can we hope to define reduced impact? Doesn't any change percolate across all future time? Yes, but not all changes are the same - especially where our ability to predict them is concerned. Imagine an AI, confined to a box. While inside the box, it will build a paperclip. Then, in one hour's time, an antimatter bomb inside the box will explode, obliterating everything inside. We would intuitively say that whether the paperclip was built or not, would have very little impact afterwards. What we really mean is that we are unable to usefully use that information for future predictions. To make it even harder, surround the box by a large mass of antimatter gas, moving chaotically. In order to figure out the consequences of anything happening inside the box, we'd have to be able to unravel chaos: solve chaotic equation correctly and quantitatively, not just qualitatively. So our first assumption is that the <strong>AI can't unravel chaos</strong>.<a id=\"more\"></a></p>\n<p>Note that the chaotic antimatter is meant to erase the impact of small changes, not to contain the AI. We should assume that the AI could easily get out of the box and spread across the universe, should it want to. So we'll be putting machinery and other useful tools inside the box - anything an AI could use to build paperclips or take over the universe. We're only&nbsp;successful&nbsp;if it doesn't want to do that last thing.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_Future\">The Future</h2>\n<p>We want 'the future' to be roughly the same whatever the AI does. But what do we mean by that? Obviously, if the AI builds a paperclip, the near future inside the box will be very different. What we want is that the mid future, the future after the explosion, be not very different. So we want to define F, a subset of spacetime, that represents this mid future.</p>\n<p>There are many ways of doing this. Let O=(box, AI turned on) be the point in space time at which the experiment begins. We could then take the initial rest frame of the box, and say that F contains everything two hours or more in the future from O (recall the explosion happens after one hour). Or we could be <a href=\"http://en.wikipedia.org/wiki/Lorentz_covariance\">Lorentz-invariant</a>, and say that F contains everything that can be reached from O, along a geodesic with proper-time more than two hours. F need not even be strictly \"the future\": it could contain everything in the universe, except for the box-and-surrounding-cloud-for-the-duration-of-the-experiment.</p>\n<p>Note that in order to define this F, we're making use of general relativity - so we assume the <strong>AI understands physics</strong>. This includes grounding the concepts into reality properly, and <strong>dealing with <a href=\"/lw/827/ai_ontology_crises_an_informal_typology/\">ontology</a> <a href=\"http://wiki.lesswrong.com/wiki/Ontological_crisis\">crises</a></strong>. We'll also need to assume that the <strong>AI obeys the laws of physics as we understand them, and doesn't have outside-physics superpowers</strong>.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_AI_design\">The AI design</h2>\n<p>The point of defining F was to be able to talk sensibly about a space in which we expect the AI's actions to have reduced measurable impact. So we'll need to start assuming some things about how the AI is structured. We'll assume that the <strong>AI consists functionally of the utility function U, the penalty function R and a probability estimating module P</strong>, and whatever practical components (including knowledge bases) it needs to run its calculations. We'll assume that the <strong>AI can't directly change these components</strong> (though indirectly, it's another matter - if it has access to manipulators, it can freely hack itself). The reason for this assumption will be clear in the next section. Note that we are assuming the AI can know every detail of the setup - we're not counting on security through obscurity.</p>\n<p>Given any world w, we can define w<sup>F</sup>&nbsp;as the world w restricted to the space F - i.e. the future part of w. The probability estimating module P can estimate numbers such as P(w<sup>F</sup>) (the probability of w<sup>F</sup>) and&nbsp;P(w<sup>F</sup>|e) (the probability of w<sup>F</sup>&nbsp;given evidence e). Some worlds will have w<sup>F</sup>=v<sup>F</sup>&nbsp;even with w\u2260v (their pasts differ, but their futures are the same).</p>\n<p>Because of the limitations of the AI (being unable to unravel chaos), we have an implicit coarse-graining of the worlds. For instance, let X be a minor fact about the inside of the box, and let w and v be worlds such that w assumes X and v assumes \u00acX. Then if w<sup>F</sup>=v<sup>F</sup>&nbsp;(so the worlds have the same future), we have P(v<sup>F</sup>|X)=P(w<sup>F</sup>|X) \u2265&nbsp;P(w|X), even though P(v|X)=0.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_Counterfactual\">The Counterfactual</h2>\n<p>We want the future to be roughly the same \"whether or not the AI was turned on\". Hence we need to define a counterfactual. It's asking for trouble to ask a working AI to imagine what the universe would be like if it wasn't turned on. So we'll use a more complicated counterfactual that doesn't have that problem: the relevant counterfactual is firmly in the future, and firmly probabilistic.</p>\n<p>Initially, we imagine the box contains a \"<a href=\"http://en.wikipedia.org/wiki/Palpatine\">master AI</a>\" (which has been turned on), and a \"<a href=\"http://en.wikipedia.org/wiki/Darth_Vader\">disciple AI</a>\", which is turned off. The master AI is the one whose motivation we want to constrain. We don't give the master AI any manipulators or direct means of influencing anything at all. All that it can do is build the source code for the disciple AI. We assume the <strong>master&nbsp;</strong><strong>AI can't affect the world in any other way</strong>.</p>\n<p>Specifically, the master AI will build the source code, according to its preferences, and store it somewhere. Then, exactly 30 minutes after the experiment began (and hence 30 minutes before the explosion), that source code will be copied into the disciple AI. If the source code is not empty, the disciple AI will be turned on.</p>\n<p>The counterfactual will be whether the code is copied or not. At the 30 minute mark, a random process X will output 1 or 0 with equal probability. If X=1, the source code is copied as described. If X=0, nothing happens.&nbsp;<strong>We assume that X is a process that the (master) AI will find truly random</strong>&nbsp;(note that this can be deduced from the \"can't unravel chaos\" assumption - but it's good to have it specified&nbsp;separately, as it serves a different purpose in the experiment). This X will be the point of counterfactuality - we want the future world to be similar whether X=0 (no AI gets to do anything) or X=1 (the disciple AI is unleashed.</p>\n<p>The whole setup can be&nbsp;summarised&nbsp;in the following drawing of&nbsp;unsurpassable&nbsp;elegance:</p>\n<p><img src=\"http://images.lesswrong.com/t3_gmx_0.png?v=51b086ab763517633724144641dc34dc\" alt=\"\" width=\"620\" height=\"877\"></p>\n<p>&nbsp;</p>\n<h2 id=\"The_Penalty_Function\">The Penalty Function</h2>\n<p>We can now describe the penalty function R. For any world w, we can compute the difference in probability between the future world given X=0 and X=1. There are many ways of assessing the difference; the absolute difference in probability is one simple way of doing this:</p>\n<p style=\"padding-left: 30px;\">R(w)= | P(w<sup>F</sup>|X=1) - P(w<sup>F</sup>|X=0) |</p>\n<p>Then we can integrate this across all w (for some reasonable weights) to get the total penalty function R. As required, R measures how much the AI's future expectation changes, given whether it's successful at creating the disciple AI's source code or not.</p>\n<p>&nbsp;</p>\n<h2 id=\"Relative_Scaling\">Relative Scaling</h2>\n<p>We're ready to go now. The AI's motivation is that it must maximise U-R: try and maximise its utility while diminishing the penalty function. This is not exactly a utility function, since the output of the probability module P appears inside R, the quantity to be maximised (probabilities shouldn't appear within the definition of the value of a world). Since the master AI is prevented from self-modification, this should not be a problem.&nbsp;Alternately, P could be formally defined as \"the output of a module described as [the code of the AI's P module]\". This would make U-R into a proper utility function, where the module P used by the master AI for probability happens to be the same as the module P used in the definition of R - but they aren't specified to be identical.</p>\n<p>Let's define U in a nice bounded way: giving 0 utility if the disciple builds no paperclips, and utility 1 if it builds at least one. The motivations that the master AI could pick for its disciple fall broadly into three categories:</p>\n<ol>\n<li>The disciple doesn't do anything (U=0, R=0).</li>\n<li>The disciple builds a paperclip or two and nothing much else (U close to 1, R close to 0). This is what we want.</li>\n<li>The disciple optimises the universe (U very close to 1, R hopefully very high).</li>\n</ol>\n<p>We've assumed that the master AI can't unravel chaos - and if this is exactly true, then we're fine. Then option 2 is perfect: U will be close to 1, and R will be exactly 0 and the AI will implement that. But maybe the master AI is a bit smarter than we thought, and the \"can't unravel chaos\" is broadly true, but the AI can still predict <em>some</em> small expected difference. This means that R will increase above zero, making the master AI's task harder, and making the \"do nothing\" option relatively more attractive.</p>\n<p>This seems a great security feature - it means that if the master AI is smarter than we expected, it is more likely to do nothing. But this setup fails if the master AI always does nothing. So we'll need to weight R so that the master AI can program its disciple to do <em>something</em>. There are issues with doing this in practice (we don't want AIs to influence people to set R too low, for instance with deliberate inaction). But let's assume here that <strong>we've picked an R such that if the AI is as smart as we think, it will just about be able to do something</strong> (if it's smarter, it'll still do nothing).</p>\n<p>&nbsp;</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>Given all these assumptions, then it seems that this setup will produce what we want: a reduced impact AI that will program a disciple AI that will build a paperclip and little else. The main error mode (if the AI is smarter than expected) seems to be for the AI to do nothing.</p>\n<p><strong>Note:</strong><em> the original formulation of this had a second box,&nbsp;causally&nbsp;disconnected and otherwise identical but with no AI, for purposes of comparison and calibration. It seems that it isn't necessary, but I may put it back in if needed.</em></p>", "sections": [{"title": "Intuitive idea", "anchor": "Intuitive_idea", "level": 1}, {"title": "Reduced impact", "anchor": "Reduced_impact", "level": 1}, {"title": "The Future", "anchor": "The_Future", "level": 1}, {"title": "The AI design", "anchor": "The_AI_design", "level": 1}, {"title": "The Counterfactual", "anchor": "The_Counterfactual", "level": 1}, {"title": "The Penalty Function", "anchor": "The_Penalty_Function", "level": 1}, {"title": "Relative Scaling", "anchor": "Relative_Scaling", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "104 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 104, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8Nwg7kqAfCM46tuHq", "TA7kDYZGjMcSCH75C"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T17:19:59.224Z", "modifiedAt": null, "url": null, "title": "SI/MIRI's emerging tech summaries [link]", "slug": "si-miri-s-emerging-tech-summaries-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.120Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qch3hQGzwcbz2WXS8/si-miri-s-emerging-tech-summaries-link", "pageUrlRelative": "/posts/Qch3hQGzwcbz2WXS8/si-miri-s-emerging-tech-summaries-link", "linkUrl": "https://www.lesswrong.com/posts/Qch3hQGzwcbz2WXS8/si-miri-s-emerging-tech-summaries-link", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SI%2FMIRI's%20emerging%20tech%20summaries%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASI%2FMIRI's%20emerging%20tech%20summaries%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQch3hQGzwcbz2WXS8%2Fsi-miri-s-emerging-tech-summaries-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SI%2FMIRI's%20emerging%20tech%20summaries%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQch3hQGzwcbz2WXS8%2Fsi-miri-s-emerging-tech-summaries-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQch3hQGzwcbz2WXS8%2Fsi-miri-s-emerging-tech-summaries-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9, "htmlBody": "<p>I just noticed they have \"summaries of emerging technologies\".</p>\n<p>http://singularity.org/techsummaries/</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qch3hQGzwcbz2WXS8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "21593", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T18:42:24.412Z", "modifiedAt": null, "url": null, "title": "Meetup : Buffalo Meetup", "slug": "meetup-buffalo-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "StonesOnCanvas", "createdAt": "2012-06-28T17:32:49.237Z", "isAdmin": false, "displayName": "StonesOnCanvas"}, "userId": "FAfkKGH6E8BLmXW4M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RsE3bzLwKKRqhYCX5/meetup-buffalo-meetup", "pageUrlRelative": "/posts/RsE3bzLwKKRqhYCX5/meetup-buffalo-meetup", "linkUrl": "https://www.lesswrong.com/posts/RsE3bzLwKKRqhYCX5/meetup-buffalo-meetup", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Buffalo%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Buffalo%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsE3bzLwKKRqhYCX5%2Fmeetup-buffalo-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Buffalo%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsE3bzLwKKRqhYCX5%2Fmeetup-buffalo-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsE3bzLwKKRqhYCX5%2Fmeetup-buffalo-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jd'>Buffalo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 February 2013 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">SPOT Coffee Delaware Ave & W Chippewa St, Buffalo, NY</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(Apologies, for the short notice.) Last meetup we talked about making sure your beliefs \"pay rent \" by constraining anticipation.\nThis time we'll talk about specific examples of ways some beliefs may not even really count as beliefs at all:</p>\n\n<p>Belief in Belief - <a href=\"http://lesswrong.com/lw/i4/belief_in_belief/\" rel=\"nofollow\">http://lesswrong.com/lw/i4/belief_in_belief/</a>\nProfessing and Cheering - <a href=\"http://lesswrong.com/lw/i6/professing_and_cheering/\" rel=\"nofollow\">http://lesswrong.com/lw/i6/professing_and_cheering/</a>\nBelief as Attire - <a href=\"http://lesswrong.com/lw/i7/belief_as_attire/\" rel=\"nofollow\">http://lesswrong.com/lw/i7/belief_as_attire/</a></p>\n\n<p>The concepts are pretty similar so I thought I'd just lump them together for this meetup. Read what you can (3 posts is a lot, so don't worry about it if you don't get around to it). In any case, I'll do a cliff-notes summary for everyone. Anyone can attend. Feel free to invite friends who might be interested. We'll also play some cool games too. We're meeting at SPOT this time (I'll have a sign so you can find us easily).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jd'>Buffalo Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RsE3bzLwKKRqhYCX5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.1124060264418731e-06, "legacy": true, "legacyId": "21594", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Buffalo_Meetup\">Discussion article for the meetup : <a href=\"/meetups/jd\">Buffalo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 February 2013 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">SPOT Coffee Delaware Ave &amp; W Chippewa St, Buffalo, NY</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(Apologies, for the short notice.) Last meetup we talked about making sure your beliefs \"pay rent \" by constraining anticipation.\nThis time we'll talk about specific examples of ways some beliefs may not even really count as beliefs at all:</p>\n\n<p>Belief in Belief - <a href=\"http://lesswrong.com/lw/i4/belief_in_belief/\" rel=\"nofollow\">http://lesswrong.com/lw/i4/belief_in_belief/</a>\nProfessing and Cheering - <a href=\"http://lesswrong.com/lw/i6/professing_and_cheering/\" rel=\"nofollow\">http://lesswrong.com/lw/i6/professing_and_cheering/</a>\nBelief as Attire - <a href=\"http://lesswrong.com/lw/i7/belief_as_attire/\" rel=\"nofollow\">http://lesswrong.com/lw/i7/belief_as_attire/</a></p>\n\n<p>The concepts are pretty similar so I thought I'd just lump them together for this meetup. Read what you can (3 posts is a lot, so don't worry about it if you don't get around to it). In any case, I'll do a cliff-notes summary for everyone. Anyone can attend. Feel free to invite friends who might be interested. We'll also play some cool games too. We're meeting at SPOT this time (I'll have a sign so you can find us easily).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Buffalo_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/jd\">Buffalo Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Buffalo Meetup", "anchor": "Discussion_article_for_the_meetup___Buffalo_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Buffalo Meetup", "anchor": "Discussion_article_for_the_meetup___Buffalo_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CqyJzDZWvGhhFJ7dY", "RmCjazjupRGcHSm5N", "nYkMLFpx77Rz3uo9c"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T20:43:06.521Z", "modifiedAt": null, "url": null, "title": "Learning critical thinking: a personal example", "slug": "learning-critical-thinking-a-personal-example", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:24.483Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pp62TwbtyFnTZe4Nb/learning-critical-thinking-a-personal-example", "pageUrlRelative": "/posts/pp62TwbtyFnTZe4Nb/learning-critical-thinking-a-personal-example", "linkUrl": "https://www.lesswrong.com/posts/pp62TwbtyFnTZe4Nb/learning-critical-thinking-a-personal-example", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learning%20critical%20thinking%3A%20a%20personal%20example&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearning%20critical%20thinking%3A%20a%20personal%20example%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpp62TwbtyFnTZe4Nb%2Flearning-critical-thinking-a-personal-example%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learning%20critical%20thinking%3A%20a%20personal%20example%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpp62TwbtyFnTZe4Nb%2Flearning-critical-thinking-a-personal-example", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpp62TwbtyFnTZe4Nb%2Flearning-critical-thinking-a-personal-example", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2457, "htmlBody": "<p>Related to: <a href=\"/lw/76x/is_rationality_teachable/\">Is Rationality Teachable</a></p>\n<p>&ldquo;Critical care nursing isn&rsquo;t about having critically ill patients,&rdquo; my preceptor likes to say, &ldquo;it&rsquo;s about critical thinking.&rdquo;</p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">I doubt she's talking about the same kind of critical thinking that <a href=\"/lw/dhe/to_learn_critical_thinking_study_critical_thinking/\">philosophers</a> are, and I find that definition abstract anyway.&nbsp;There&rsquo;s been a lot of talk about critical thinking during our four years of nursing school, but our profs seem to have a hard time defining it. So I&rsquo;ll go with a definition from Google.</span></p>\n<blockquote>\n<p class=\"MsoBodyTextIndent\"><em style=\"text-indent: 0.5in;\">Critical thinking can be seen as having two components: 1) a set of information and belief generating and processing skills, and 2) the habit, based on intellectual commitment, of using those skills to guide behaviour. It is thus to be contrasted with: 1) the mere acquisition and retention of information alone, because it involves a particular way in which information is sought and treated; 2) the mere possession of a set of skills, because it involves the continual use of them; and 3) the mere use of those skills (\"as an exercise\") without acceptance of their results.<sup>1</sup></em></p>\n</blockquote>\n<p class=\"MsoBodyTextIndent\">That&rsquo;s basically rationality&ndash;epistemic, i.e. generating true beliefs, and instrumental, i.e. knowing how to use them to achieve what you want. Maybe part of me expected, implicitly, to have an easier time learning this skill because of my Less Wrong knowledge. And maybe I am more consciously aware of my mistakes, and the cognitive factors that caused them, than most of my classmates. When it&rsquo;s forty-five minutes past the end of my shift and I&rsquo;m still charting, I&rsquo;m also calling myself out on succumbing to the planning fallacy. I once went through the first half hour of a shift during my pediatrics rotation thinking that one of my patients had cerebral palsy, when he actually had cystic fibrosis&ndash;all because I misread my prof&rsquo;s handwriting as &lsquo;CP&rsquo; when she&rsquo;d written &lsquo;CF&rsquo;. I was totally confused by all the enzyme supplements on his list of meds, but it still took me a while to figure it out&ndash;a combination of priming and confirmation bias, taken to the next level.&nbsp;</p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">But, overall, even if I know what I'm doing wrong, it <em>hasn&rsquo;t </em></span><span lang=\"EN-GB\">been easier to do things right. I have a hard time with the hospital environment, possibly because </span><span lang=\"EN-GB\">I&rsquo;m the kind of person who ended up reading and posting on Less Wrong. My cognitive style leans towards Type 2 reasoning, in Keith Stanovich&rsquo;s taxonomy&ndash;thorough, but slow. I like to <em>understand </em></span><span lang=\"EN-GB\">things, on a deep level. I like knowing why I&rsquo;m doing something, and I don&rsquo;t trust my intuitions, the fast-and-dirty product of Type 1 reasoning. But Type 2 reasoning requires a lot of working memory, and humans aren&rsquo;t known for that, which is the source of most of my frustration and nearly all of my errors&ndash;when working memory overload forces me to be a <a href=\"/lw/2ey/a_taxonomy_of_bias_the_cognitive_miser/\">cognitive miser</a>.</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Still, for all the frustration, I&rsquo;m pretty sure I&rsquo;ve ended up in the <em>perfect </em></span><span lang=\"EN-GB\">environment to learn this skill called &lsquo;critical thinking.&rsquo; I&rsquo;m way out of my depth&ndash;which I expected. <em>No</em></span><span lang=\"EN-GB\"> fourth year student is ready to work independently in a trauma ICU, but I decided to finish my schooling here in the name of <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">tsuyoku naritai</a>, and for all the days when I&rsquo;ve gone home crying, it&rsquo;s still worth it. I&rsquo;m <em>learning</em></span><span lang=\"EN-GB\">. </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;</span></p>\n<p class=\"MsoBodyTextIndent\"><strong>The skills</strong></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><strong><!--[if !supportEmptyParas]-->&nbsp;</strong></span><span style=\"text-indent: -0.25in;\" lang=\"EN-GB\"><em>1.<span style=\"font-style: normal; font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></em></span><span style=\"text-indent: -0.25in;\" lang=\"EN-GB\"><em>A set of information and belief generating and processing skills.</em></span></p>\n<p class=\"MsoBodyTextIndent\"><span style=\"text-indent: -0.25in;\" lang=\"EN-GB\"><em></em></span><span style=\"text-indent: 0in;\" lang=\"EN-GB\">Medicine, and nursing, are a bit like physics, in that you need to generate true beliefs about systems that exist outside of you, and predict how they&rsquo;re going to behave. This involves knowing a lot of abstract theory, which I&rsquo;m good at, and a lot of heuristics and pattern-matching for applying the right bits of theory to particular patients, which I&rsquo;m less good at. That&rsquo;s partly an experience thing; my brain needs patterns to match to. But in general, I have decent mental models of my patients. I&rsquo;m curious and I like to understand things. If I don&rsquo;t know what part of the theories applies, I <em>ask</em></span><span style=\"text-indent: 0in;\" lang=\"EN-GB\">.</span></p>\n<p class=\"MsoBodyTextIndent\"><span style=\"text-indent: -0.25in;\" lang=\"EN-GB\"><em>2.<span style=\"font-style: normal; font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></em></span><span style=\"text-indent: -0.25in;\" lang=\"EN-GB\"><em>The habit, based on intellectual commitment, of using those skills to guide behaviour.</em></span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">So you&rsquo;ve got your mental model of your patient, your best understand of what&rsquo;s actually going on, on a physiological and biochemical level, down under the skin where you can&rsquo;t see it. You know what &ldquo;normal&rdquo; is for a variety of measures: vital signs, lung sounds, lab values, etc. Given that your patient is in the ICU, you know <em>something&rsquo;s </em></span><span lang=\"EN-GB\">abnormal, or they wouldn&rsquo;t be there. Their diagnosis tells you what to expect, and you look at the results of your assessments and ask a couple of questions. One: is this what I expect, for this patient? Two: what do I need to do about it?</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">I&rsquo;m not going to be <em>surprised </em></span><span lang=\"EN-GB\">if a post-op patient has low hemoglobin. It&rsquo;s information of a kind, telling the doctor whether or not the patient needs a transfusion, and how many units, but it&rsquo;s not really new information, and a moderately abnormal value wouldn&rsquo;t worry me or anyone else. If their hemoglobin <em>keeps </em></span><span lang=\"EN-GB\">dropping; okay, they&rsquo;re actively bleeding somewhere, that&rsquo;s irritating, and possibly dangerous, and needs dealing with, but it&rsquo;s not surprising. </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">But if a patient here for an abdominal surgery suddenly has decreased level of consciousness and their pupils aren&rsquo;t reacting normally to light, I&rsquo;m <em>worried</em></span><span lang=\"EN-GB\">. There&rsquo;s nothing in my mental model that says I should expect it. I notice I&rsquo;m confused, and that confusion guides my behaviour; I call the doctor right away, because we need more information to update our collective mental model, information you can&rsquo;t get just from observation, like a CT scan of the head. (Even this is optimistic&ndash;plenty of patients are admitted to the ICU because we have no idea what&rsquo;s wrong with them, and are hoping to keep them alive long enough to find out.) </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">The basics of ICU nursing come down to treating numbers. Heart rate, blood pressure, oxygen saturations, urine output, etc; know the acceptable range, notice if they change, and use Treatment X to get them back where they&rsquo;re supposed to be. Which doesn&rsquo;t sound that hard. But implicit in &lsquo;notice if they change&rsquo; is &lsquo;figure out <em>why </em></span><span lang=\"EN-GB\">they changed&rsquo;, because that affects how you treat them, and implicit in that is a <em>lot </em></span><span lang=\"EN-GB\">of background knowledge, which has to be put in context.</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">I&rsquo;m, honestly, fairly terrible at this. It&rsquo;s a compartmentalization thing. I don&rsquo;t like using my knowledge as input arguments to generate new conclusions and then relying on those conclusions to treat human beings. It feels like <em>guessing</em></span><span lang=\"EN-GB\">. Even though, back in high school, I never really needed to study for physics tests&ndash;if I <em>understood </em></span><span lang=\"EN-GB\">what we&rsquo;d learned, I could re-derive forgotten details from first principles. But hospital patients ended up in a non-overlapping magisterium in my head. In order for me to trust my knowledge, it has to have come <em>directly </em></span><span lang=\"EN-GB\">from the lips of a teacher or experienced nurse. </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">My preceptor, who &nbsp;hates this.&nbsp; &ldquo;She needs to continue to work on her critical thinking when it comes to caring for critically ill patients,&rdquo; she wrote on my evaluation. &ldquo;She knows the theory, and is now working to apply it to ICU nursing.&rdquo; Shorthand for, <em>she knows the theory, but getting her to apply it to ICU nursing is like pulling teeth</em></span><span lang=\"EN-GB\">.&nbsp;</span>A number of our conversations have gone like this:</p>\n<blockquote>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Me: &ldquo;Our patient&rsquo;s blood pressure dropped a bit.&rdquo; </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Her: &ldquo;Yeah, it did. What do you want to do about it?&rdquo;</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Me: &ldquo;I, uh, I don&rsquo;t know... Should I increase the vasopressors?&rdquo;</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Her: &ldquo;I don&rsquo;t know, should you?&rdquo;</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Me: &ldquo;Uh, maybe I should increase the phenylephrine to 40 mcg/min and see what happens. How long should I wait to see?&rdquo;</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Her: &ldquo;You tell me.&rdquo;</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Me: &ldquo;Well, let&rsquo;s say it&rsquo;ll take a few minutes for what&rsquo;s in the tubing now to get pushed through, and it should take effect pretty quickly because it&rsquo;s IV, like a minute... So if his blood pressure&rsquo;s not up enough in five minutes, I&rsquo;ll increase the phenyl to 60. Does that sound okay?&rdquo;</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Her: &ldquo;It&rsquo;s your decision to make.\"&nbsp;</span></p>\n</blockquote>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Needless to say, I find this teaching method extremely stressful and scary, and I&rsquo;m learning about ten times more than I would if she <em>answered </em></span><span lang=\"EN-GB\">the questions I asked. Because &ldquo;the mere acquisition and retention of information alone&rdquo; isn&rsquo;t my problem. I have a brain like an encyclopaedia. My problem, in the critical care nursing context, is the &ldquo;particular way in which information is sought and treated.&rdquo; I need to know the right time to notice something is wrong, the right place to look in my encyclopaedia, and the right way to take the information I just looked up and figure out what to <em>do </em></span><span lang=\"EN-GB\">with it. </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><strong>The mistakes</strong></span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Some of my errors, unsurprisingly, boil down to a failure to override inappropriate Type 1 responses with Type 2 responses&ndash;in other words, not thinking about what I&rsquo;m doing. But <em>most </em></span><span lang=\"EN-GB\">of them are more of a <a href=\"/lw/2fj/a_taxonomy_of_bias_mindware_problems/\">mindware gap</a>&ndash;I don&rsquo;t yet have the &ldquo;domain-specific knowledge sets&rdquo; that the nurses around me have. Not just theory knowledge; I do have <em>most </em></span><span lang=\"EN-GB\">of that; but the procedural habits of how to stay organized and prioritize and dump the contents of my working memory onto paper in a way that I can read them back later. Usually, when I make a mistake, I knew better, but the part of my brain that knew better was doing something else at the time, that small note of confusion getting lost in the general chaos.&nbsp;</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Pretty much all nurses keep a &ldquo;feuille de route&rdquo;&ndash;I have yet to find a satisfactory English word for this, but it&rsquo;s a personal sheet of paper, <em>not </em></span><span lang=\"EN-GB\">legal charting, usually kept in a pocket, and used as an extended working memory. In med/surg, when I had four patients, I made a chart with four columns; name and personal information, medications, treatments/general plan for the day, and medical history; and as many rows as I had patients. If something was important, I circled it in red ink. This system doesn&rsquo;t work in the ICU, so my current feuille de route has several aspects. I fold a piece of blank paper into four, and take notes from the previous shift report on one quarter of one side, or two quarters if it&rsquo;s a long report. Across from that, I draw a vertical column of times, from 8:00 am to 6:00 pm (or 8:00 pm to 6:00 am). 7:00 pm and 7:00 am are shift change, so nothing else really gets done for that hour. I use this to scribble down what I need to get down during my twelve hours, and approximately when I want to do it, and I prioritize, i.e. from 1 to 5 most to least important. Once it&rsquo;s done, I cross it off&ndash;then I can forget about it. On the other side of the paper, I make a cheat sheet for giving report to the next nurse, or presenting my patient to the doctors at rounds.&nbsp;&nbsp; </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">This might be low-tech and simple, but it takes a <em>huge </em></span><span lang=\"EN-GB\">load off my working memory, and reduces my most frequent error, which is to get so overwhelmed and frazzled that my brain goes on strike. In other words, the failure to <a href=\"/lw/7e5/the_cognitive_science_of_rationality/#HumanReasoning\">override Type 1 responses</a> due to the lack of cognitive capacity to run a Type 2 process. It&rsquo;s drastically cut down on the frequency of this mental conversation:</span></p>\n<blockquote>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Me: &ldquo;I turned off the sedation, and my patient isn&rsquo;t waking up as fast as I expected. I notice I&rsquo;m confused&ndash;&rdquo;</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">My brain: &ldquo;You&rsquo;re always confused! Everything around here is intensely confusing! How am I supposed to use that as information?&rdquo;&nbsp; </span></p>\n</blockquote>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Odd as it might sound, I often don&rsquo;t <em>notice</em></span><span lang=\"EN-GB\"> when my brain starts edging towards a meltdown. The feeling itself is quite recognizable, but the circumstances that lead to it, i.e. overloaded working memory, mean that I&rsquo;m not usually paying attention to my own feelings.</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">&ldquo;You need to stop and take a breath,&rdquo; my preceptor says about fifty times a day. Easier said than done&ndash;but it&rsquo;s more efficient, overall, to have a tiny part of my mind permanently on standby, keeping an eye on my emotions, noticing when the gears start to overheat. Then stop, take a breath, and let go of <em>everything </em></span><span lang=\"EN-GB\">except the task at hand, trusting myself to have created enough cues in my environment to retrieve the other tasks, once I&rsquo;m done. Humans don&rsquo;t multitask well. Doing one thing while trying to remember a list of five others is intense multitasking, and it&rsquo;s no wonder it&rsquo;s exhausting.</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><em><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></em></span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><strong>The implications</strong></span></p>\n<!--EndFragment-->\n<p class=\"MsoBodyTextIndent\">&ldquo;You can&rsquo;t teach critical thinking,&rdquo; my preceptor says, but I&rsquo;m pretty sure that&rsquo;s exactly what she&rsquo;s doing right now. A great deal of what I already know is domain-specific to nursing, but most of what I&rsquo;m learning right now is generally applicable. I&rsquo;m learning the procedural skills to work through difficult problems, under what Keith Stanovich would call average rather than optimal conditions. Sitting in my own little bubble in front of a multiple choice exam&ndash;that&rsquo;s optimal conditions. Trying to figure out if I should be surprised or worried about my patient&rsquo;s increased heart rate, while simultaneously deciding whether or not I can ignore the ventilator alarm and whether I can finish giving my twelve o&rsquo;clock antibiotic before I need to do twelve o&rsquo;clock vitals&ndash;that&rsquo;s not just average conditions, it&rsquo;s under-duress conditions.</p>\n<p class=\"MsoBodyTextIndent\"><span style=\"font-size: 12pt; font-family: Times;\" lang=\"EN-GB\"><img src=\"http://www.nurstoon.com/Images/comicstrip150.GIF\" alt=\"\" width=\"605\" height=\"214\" /></span></p>\n<p>I&rsquo;m hoping that after a few more weeks, or maybe a few more years, I&rsquo;ll be able to perform comfortably in this intensely terrifying environment. And I&rsquo;m hoping that some of the skills I learn will be general-purpose, for me at least. It&rsquo;d be nice if they were teachable to others, too, but I think my preceptor might be right about one thing&ndash;you can&rsquo;t teach this kind of critical thinking in the classroom. It's about moulding my brain into the right shape, and everyone's brain starts out in a different shape, so the mould has to be personalized.&nbsp;</p>\n<p>But the habits are general ones. Notice when you're faced with a difficult problem, or making an important decision. Notice that you're doing this while distracted. Stop and take a breath. Get out a piece of paper. Figure out how the problem is formatted in your mind, and format it that way on the paper. (This is probably the hardest part). Dump your working memory and give yourself space to think. Prioritize from 1 to <em>n</em>. Keep an eye on the evolving situation, sure, but find that moment of concentration in the midst of chaos, and solve the problem.&nbsp;</p>\n<p>Of course, it's far from guaranteed that this will work<em>. </em>I'm making an empirical prediction; that the skills I'm currently learning <em>will </em>be transferable to non-nursing areas, and that they'll make a difference in my life outside of work. I'll be on the lookout for examples, either of success or failure.</p>\n<p>&nbsp;</p>\n<p><strong>References</strong></p>\n<p>Scriven, Michael; Paul, Richard. <em>Defining critical thinking.</em>&nbsp;(2011). The critical thinking community.&nbsp;<em><a href=\"http://www.criticalthinking.org/pages/defining-critical-thinking/410\">http://www.criticalthinking.org/pages/defining-critical-thinking/410</a></em></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fR7QfYx4JA3BnptT9": 2, "irYLXtT9hkPXoZqhH": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pp62TwbtyFnTZe4Nb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 58, "extendedScore": null, "score": 0.0005623710285760613, "legacy": true, "legacyId": "21595", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Related to: <a href=\"/lw/76x/is_rationality_teachable/\">Is Rationality Teachable</a></p>\n<p>\u201cCritical care nursing isn\u2019t about having critically ill patients,\u201d my preceptor likes to say, \u201cit\u2019s about critical thinking.\u201d</p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">I doubt she's talking about the same kind of critical thinking that <a href=\"/lw/dhe/to_learn_critical_thinking_study_critical_thinking/\">philosophers</a> are, and I find that definition abstract anyway.&nbsp;There\u2019s been a lot of talk about critical thinking during our four years of nursing school, but our profs seem to have a hard time defining it. So I\u2019ll go with a definition from Google.</span></p>\n<blockquote>\n<p class=\"MsoBodyTextIndent\"><em style=\"text-indent: 0.5in;\">Critical thinking can be seen as having two components: 1) a set of information and belief generating and processing skills, and 2) the habit, based on intellectual commitment, of using those skills to guide behaviour. It is thus to be contrasted with: 1) the mere acquisition and retention of information alone, because it involves a particular way in which information is sought and treated; 2) the mere possession of a set of skills, because it involves the continual use of them; and 3) the mere use of those skills (\"as an exercise\") without acceptance of their results.<sup>1</sup></em></p>\n</blockquote>\n<p class=\"MsoBodyTextIndent\">That\u2019s basically rationality\u2013epistemic, i.e. generating true beliefs, and instrumental, i.e. knowing how to use them to achieve what you want. Maybe part of me expected, implicitly, to have an easier time learning this skill because of my Less Wrong knowledge. And maybe I am more consciously aware of my mistakes, and the cognitive factors that caused them, than most of my classmates. When it\u2019s forty-five minutes past the end of my shift and I\u2019m still charting, I\u2019m also calling myself out on succumbing to the planning fallacy. I once went through the first half hour of a shift during my pediatrics rotation thinking that one of my patients had cerebral palsy, when he actually had cystic fibrosis\u2013all because I misread my prof\u2019s handwriting as \u2018CP\u2019 when she\u2019d written \u2018CF\u2019. I was totally confused by all the enzyme supplements on his list of meds, but it still took me a while to figure it out\u2013a combination of priming and confirmation bias, taken to the next level.&nbsp;</p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">But, overall, even if I know what I'm doing wrong, it <em>hasn\u2019t </em></span><span lang=\"EN-GB\">been easier to do things right. I have a hard time with the hospital environment, possibly because </span><span lang=\"EN-GB\">I\u2019m the kind of person who ended up reading and posting on Less Wrong. My cognitive style leans towards Type 2 reasoning, in Keith Stanovich\u2019s taxonomy\u2013thorough, but slow. I like to <em>understand </em></span><span lang=\"EN-GB\">things, on a deep level. I like knowing why I\u2019m doing something, and I don\u2019t trust my intuitions, the fast-and-dirty product of Type 1 reasoning. But Type 2 reasoning requires a lot of working memory, and humans aren\u2019t known for that, which is the source of most of my frustration and nearly all of my errors\u2013when working memory overload forces me to be a <a href=\"/lw/2ey/a_taxonomy_of_bias_the_cognitive_miser/\">cognitive miser</a>.</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Still, for all the frustration, I\u2019m pretty sure I\u2019ve ended up in the <em>perfect </em></span><span lang=\"EN-GB\">environment to learn this skill called \u2018critical thinking.\u2019 I\u2019m way out of my depth\u2013which I expected. <em>No</em></span><span lang=\"EN-GB\"> fourth year student is ready to work independently in a trauma ICU, but I decided to finish my schooling here in the name of <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">tsuyoku naritai</a>, and for all the days when I\u2019ve gone home crying, it\u2019s still worth it. I\u2019m <em>learning</em></span><span lang=\"EN-GB\">. </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;</span></p>\n<p class=\"MsoBodyTextIndent\"><strong id=\"The_skills\">The skills</strong></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><strong><!--[if !supportEmptyParas]-->&nbsp;</strong></span><span style=\"text-indent: -0.25in;\" lang=\"EN-GB\"><em>1.<span style=\"font-style: normal; font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></em></span><span style=\"text-indent: -0.25in;\" lang=\"EN-GB\"><em>A set of information and belief generating and processing skills.</em></span></p>\n<p class=\"MsoBodyTextIndent\"><span style=\"text-indent: -0.25in;\" lang=\"EN-GB\"><em></em></span><span style=\"text-indent: 0in;\" lang=\"EN-GB\">Medicine, and nursing, are a bit like physics, in that you need to generate true beliefs about systems that exist outside of you, and predict how they\u2019re going to behave. This involves knowing a lot of abstract theory, which I\u2019m good at, and a lot of heuristics and pattern-matching for applying the right bits of theory to particular patients, which I\u2019m less good at. That\u2019s partly an experience thing; my brain needs patterns to match to. But in general, I have decent mental models of my patients. I\u2019m curious and I like to understand things. If I don\u2019t know what part of the theories applies, I <em>ask</em></span><span style=\"text-indent: 0in;\" lang=\"EN-GB\">.</span></p>\n<p class=\"MsoBodyTextIndent\"><span style=\"text-indent: -0.25in;\" lang=\"EN-GB\"><em>2.<span style=\"font-style: normal; font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp; </span></em></span><span style=\"text-indent: -0.25in;\" lang=\"EN-GB\"><em>The habit, based on intellectual commitment, of using those skills to guide behaviour.</em></span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">So you\u2019ve got your mental model of your patient, your best understand of what\u2019s actually going on, on a physiological and biochemical level, down under the skin where you can\u2019t see it. You know what \u201cnormal\u201d is for a variety of measures: vital signs, lung sounds, lab values, etc. Given that your patient is in the ICU, you know <em>something\u2019s </em></span><span lang=\"EN-GB\">abnormal, or they wouldn\u2019t be there. Their diagnosis tells you what to expect, and you look at the results of your assessments and ask a couple of questions. One: is this what I expect, for this patient? Two: what do I need to do about it?</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">I\u2019m not going to be <em>surprised </em></span><span lang=\"EN-GB\">if a post-op patient has low hemoglobin. It\u2019s information of a kind, telling the doctor whether or not the patient needs a transfusion, and how many units, but it\u2019s not really new information, and a moderately abnormal value wouldn\u2019t worry me or anyone else. If their hemoglobin <em>keeps </em></span><span lang=\"EN-GB\">dropping; okay, they\u2019re actively bleeding somewhere, that\u2019s irritating, and possibly dangerous, and needs dealing with, but it\u2019s not surprising. </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">But if a patient here for an abdominal surgery suddenly has decreased level of consciousness and their pupils aren\u2019t reacting normally to light, I\u2019m <em>worried</em></span><span lang=\"EN-GB\">. There\u2019s nothing in my mental model that says I should expect it. I notice I\u2019m confused, and that confusion guides my behaviour; I call the doctor right away, because we need more information to update our collective mental model, information you can\u2019t get just from observation, like a CT scan of the head. (Even this is optimistic\u2013plenty of patients are admitted to the ICU because we have no idea what\u2019s wrong with them, and are hoping to keep them alive long enough to find out.) </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">The basics of ICU nursing come down to treating numbers. Heart rate, blood pressure, oxygen saturations, urine output, etc; know the acceptable range, notice if they change, and use Treatment X to get them back where they\u2019re supposed to be. Which doesn\u2019t sound that hard. But implicit in \u2018notice if they change\u2019 is \u2018figure out <em>why </em></span><span lang=\"EN-GB\">they changed\u2019, because that affects how you treat them, and implicit in that is a <em>lot </em></span><span lang=\"EN-GB\">of background knowledge, which has to be put in context.</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">I\u2019m, honestly, fairly terrible at this. It\u2019s a compartmentalization thing. I don\u2019t like using my knowledge as input arguments to generate new conclusions and then relying on those conclusions to treat human beings. It feels like <em>guessing</em></span><span lang=\"EN-GB\">. Even though, back in high school, I never really needed to study for physics tests\u2013if I <em>understood </em></span><span lang=\"EN-GB\">what we\u2019d learned, I could re-derive forgotten details from first principles. But hospital patients ended up in a non-overlapping magisterium in my head. In order for me to trust my knowledge, it has to have come <em>directly </em></span><span lang=\"EN-GB\">from the lips of a teacher or experienced nurse. </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">My preceptor, who &nbsp;hates this.&nbsp; \u201cShe needs to continue to work on her critical thinking when it comes to caring for critically ill patients,\u201d she wrote on my evaluation. \u201cShe knows the theory, and is now working to apply it to ICU nursing.\u201d Shorthand for, <em>she knows the theory, but getting her to apply it to ICU nursing is like pulling teeth</em></span><span lang=\"EN-GB\">.&nbsp;</span>A number of our conversations have gone like this:</p>\n<blockquote>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Me: \u201cOur patient\u2019s blood pressure dropped a bit.\u201d </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Her: \u201cYeah, it did. What do you want to do about it?\u201d</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Me: \u201cI, uh, I don\u2019t know... Should I increase the vasopressors?\u201d</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Her: \u201cI don\u2019t know, should you?\u201d</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Me: \u201cUh, maybe I should increase the phenylephrine to 40 mcg/min and see what happens. How long should I wait to see?\u201d</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Her: \u201cYou tell me.\u201d</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Me: \u201cWell, let\u2019s say it\u2019ll take a few minutes for what\u2019s in the tubing now to get pushed through, and it should take effect pretty quickly because it\u2019s IV, like a minute... So if his blood pressure\u2019s not up enough in five minutes, I\u2019ll increase the phenyl to 60. Does that sound okay?\u201d</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Her: \u201cIt\u2019s your decision to make.\"&nbsp;</span></p>\n</blockquote>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Needless to say, I find this teaching method extremely stressful and scary, and I\u2019m learning about ten times more than I would if she <em>answered </em></span><span lang=\"EN-GB\">the questions I asked. Because \u201cthe mere acquisition and retention of information alone\u201d isn\u2019t my problem. I have a brain like an encyclopaedia. My problem, in the critical care nursing context, is the \u201cparticular way in which information is sought and treated.\u201d I need to know the right time to notice something is wrong, the right place to look in my encyclopaedia, and the right way to take the information I just looked up and figure out what to <em>do </em></span><span lang=\"EN-GB\">with it. </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><strong>The mistakes</strong></span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Some of my errors, unsurprisingly, boil down to a failure to override inappropriate Type 1 responses with Type 2 responses\u2013in other words, not thinking about what I\u2019m doing. But <em>most </em></span><span lang=\"EN-GB\">of them are more of a <a href=\"/lw/2fj/a_taxonomy_of_bias_mindware_problems/\">mindware gap</a>\u2013I don\u2019t yet have the \u201cdomain-specific knowledge sets\u201d that the nurses around me have. Not just theory knowledge; I do have <em>most </em></span><span lang=\"EN-GB\">of that; but the procedural habits of how to stay organized and prioritize and dump the contents of my working memory onto paper in a way that I can read them back later. Usually, when I make a mistake, I knew better, but the part of my brain that knew better was doing something else at the time, that small note of confusion getting lost in the general chaos.&nbsp;</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Pretty much all nurses keep a \u201cfeuille de route\u201d\u2013I have yet to find a satisfactory English word for this, but it\u2019s a personal sheet of paper, <em>not </em></span><span lang=\"EN-GB\">legal charting, usually kept in a pocket, and used as an extended working memory. In med/surg, when I had four patients, I made a chart with four columns; name and personal information, medications, treatments/general plan for the day, and medical history; and as many rows as I had patients. If something was important, I circled it in red ink. This system doesn\u2019t work in the ICU, so my current feuille de route has several aspects. I fold a piece of blank paper into four, and take notes from the previous shift report on one quarter of one side, or two quarters if it\u2019s a long report. Across from that, I draw a vertical column of times, from 8:00 am to 6:00 pm (or 8:00 pm to 6:00 am). 7:00 pm and 7:00 am are shift change, so nothing else really gets done for that hour. I use this to scribble down what I need to get down during my twelve hours, and approximately when I want to do it, and I prioritize, i.e. from 1 to 5 most to least important. Once it\u2019s done, I cross it off\u2013then I can forget about it. On the other side of the paper, I make a cheat sheet for giving report to the next nurse, or presenting my patient to the doctors at rounds.&nbsp;&nbsp; </span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">This might be low-tech and simple, but it takes a <em>huge </em></span><span lang=\"EN-GB\">load off my working memory, and reduces my most frequent error, which is to get so overwhelmed and frazzled that my brain goes on strike. In other words, the failure to <a href=\"/lw/7e5/the_cognitive_science_of_rationality/#HumanReasoning\">override Type 1 responses</a> due to the lack of cognitive capacity to run a Type 2 process. It\u2019s drastically cut down on the frequency of this mental conversation:</span></p>\n<blockquote>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Me: \u201cI turned off the sedation, and my patient isn\u2019t waking up as fast as I expected. I notice I\u2019m confused\u2013\u201d</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">My brain: \u201cYou\u2019re always confused! Everything around here is intensely confusing! How am I supposed to use that as information?\u201d&nbsp; </span></p>\n</blockquote>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">Odd as it might sound, I often don\u2019t <em>notice</em></span><span lang=\"EN-GB\"> when my brain starts edging towards a meltdown. The feeling itself is quite recognizable, but the circumstances that lead to it, i.e. overloaded working memory, mean that I\u2019m not usually paying attention to my own feelings.</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\">\u201cYou need to stop and take a breath,\u201d my preceptor says about fifty times a day. Easier said than done\u2013but it\u2019s more efficient, overall, to have a tiny part of my mind permanently on standby, keeping an eye on my emotions, noticing when the gears start to overheat. Then stop, take a breath, and let go of <em>everything </em></span><span lang=\"EN-GB\">except the task at hand, trusting myself to have created enough cues in my environment to retrieve the other tasks, once I\u2019m done. Humans don\u2019t multitask well. Doing one thing while trying to remember a list of five others is intense multitasking, and it\u2019s no wonder it\u2019s exhausting.</span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><em><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></em></span></p>\n<p class=\"MsoBodyTextIndent\"><span lang=\"EN-GB\"><strong>The implications</strong></span></p>\n<!--EndFragment-->\n<p class=\"MsoBodyTextIndent\">\u201cYou can\u2019t teach critical thinking,\u201d my preceptor says, but I\u2019m pretty sure that\u2019s exactly what she\u2019s doing right now. A great deal of what I already know is domain-specific to nursing, but most of what I\u2019m learning right now is generally applicable. I\u2019m learning the procedural skills to work through difficult problems, under what Keith Stanovich would call average rather than optimal conditions. Sitting in my own little bubble in front of a multiple choice exam\u2013that\u2019s optimal conditions. Trying to figure out if I should be surprised or worried about my patient\u2019s increased heart rate, while simultaneously deciding whether or not I can ignore the ventilator alarm and whether I can finish giving my twelve o\u2019clock antibiotic before I need to do twelve o\u2019clock vitals\u2013that\u2019s not just average conditions, it\u2019s under-duress conditions.</p>\n<p class=\"MsoBodyTextIndent\"><span style=\"font-size: 12pt; font-family: Times;\" lang=\"EN-GB\"><img src=\"http://www.nurstoon.com/Images/comicstrip150.GIF\" alt=\"\" width=\"605\" height=\"214\"></span></p>\n<p>I\u2019m hoping that after a few more weeks, or maybe a few more years, I\u2019ll be able to perform comfortably in this intensely terrifying environment. And I\u2019m hoping that some of the skills I learn will be general-purpose, for me at least. It\u2019d be nice if they were teachable to others, too, but I think my preceptor might be right about one thing\u2013you can\u2019t teach this kind of critical thinking in the classroom. It's about moulding my brain into the right shape, and everyone's brain starts out in a different shape, so the mould has to be personalized.&nbsp;</p>\n<p>But the habits are general ones. Notice when you're faced with a difficult problem, or making an important decision. Notice that you're doing this while distracted. Stop and take a breath. Get out a piece of paper. Figure out how the problem is formatted in your mind, and format it that way on the paper. (This is probably the hardest part). Dump your working memory and give yourself space to think. Prioritize from 1 to <em>n</em>. Keep an eye on the evolving situation, sure, but find that moment of concentration in the midst of chaos, and solve the problem.&nbsp;</p>\n<p>Of course, it's far from guaranteed that this will work<em>. </em>I'm making an empirical prediction; that the skills I'm currently learning <em>will </em>be transferable to non-nursing areas, and that they'll make a difference in my life outside of work. I'll be on the lookout for examples, either of success or failure.</p>\n<p>&nbsp;</p>\n<p><strong id=\"References\">References</strong></p>\n<p>Scriven, Michael; Paul, Richard. <em>Defining critical thinking.</em>&nbsp;(2011). The critical thinking community.&nbsp;<em><a href=\"http://www.criticalthinking.org/pages/defining-critical-thinking/410\">http://www.criticalthinking.org/pages/defining-critical-thinking/410</a></em></p>\n<p>&nbsp;</p>", "sections": [{"title": "The skills", "anchor": "The_skills", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["H2zKAfiSJR6WJQ8pn", "MGtKNd5GBXN5oNj7p", "qMTzv8ATgDtfLq9ME", "DoLQN5ryZ9XkZjq5h", "a5DQxG9NgzSLRZMnQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-14T22:00:17.614Z", "modifiedAt": null, "url": null, "title": "Meetup : Future Meetup for Indonesian LWers", "slug": "meetup-future-meetup-for-indonesian-lwers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:42.877Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rv77ax", "createdAt": "2011-12-27T03:09:42.765Z", "isAdmin": false, "displayName": "rv77ax"}, "userId": "RohJNxkejdoRenJug", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vGftjoQJSmJCXEddo/meetup-future-meetup-for-indonesian-lwers", "pageUrlRelative": "/posts/vGftjoQJSmJCXEddo/meetup-future-meetup-for-indonesian-lwers", "linkUrl": "https://www.lesswrong.com/posts/vGftjoQJSmJCXEddo/meetup-future-meetup-for-indonesian-lwers", "postedAtFormatted": "Thursday, February 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Future%20Meetup%20for%20Indonesian%20LWers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Future%20Meetup%20for%20Indonesian%20LWers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvGftjoQJSmJCXEddo%2Fmeetup-future-meetup-for-indonesian-lwers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Future%20Meetup%20for%20Indonesian%20LWers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvGftjoQJSmJCXEddo%2Fmeetup-future-meetup-for-indonesian-lwers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvGftjoQJSmJCXEddo%2Fmeetup-future-meetup-for-indonesian-lwers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/je'>Future Meetup for Indonesian LWers</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 January 2015 04:00:00PM (+0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bandung, Indonesia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I remember about LW's survey sometimes ago, and when the survey result come out, I see that only one LWers is come from Indonesia. Obviously that was me.</p>\n\n<p>I try to search forum and this site, using keyword \"Indonesia\", and only found one user, which is me.</p>\n\n<p>Now, in case someone from and/or live in my country like to have a meetup, I put this reminder, so when they search this site they will find it and maybe we can have meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/je'>Future Meetup for Indonesian LWers</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vGftjoQJSmJCXEddo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 1.1125315478405984e-06, "legacy": true, "legacyId": "21596", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Future_Meetup_for_Indonesian_LWers\">Discussion article for the meetup : <a href=\"/meetups/je\">Future Meetup for Indonesian LWers</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 January 2015 04:00:00PM (+0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bandung, Indonesia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I remember about LW's survey sometimes ago, and when the survey result come out, I see that only one LWers is come from Indonesia. Obviously that was me.</p>\n\n<p>I try to search forum and this site, using keyword \"Indonesia\", and only found one user, which is me.</p>\n\n<p>Now, in case someone from and/or live in my country like to have a meetup, I put this reminder, so when they search this site they will find it and maybe we can have meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Future_Meetup_for_Indonesian_LWers1\">Discussion article for the meetup : <a href=\"/meetups/je\">Future Meetup for Indonesian LWers</a></h2>", "sections": [{"title": "Discussion article for the meetup : Future Meetup for Indonesian LWers", "anchor": "Discussion_article_for_the_meetup___Future_Meetup_for_Indonesian_LWers", "level": 1}, {"title": "Discussion article for the meetup : Future Meetup for Indonesian LWers", "anchor": "Discussion_article_for_the_meetup___Future_Meetup_for_Indonesian_LWers1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T01:43:08.454Z", "modifiedAt": null, "url": null, "title": "LW Women: LW Online", "slug": "lw-women-lw-online", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:07.971Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dDZcMG8xopYgnNfvK/lw-women-lw-online", "pageUrlRelative": "/posts/dDZcMG8xopYgnNfvK/lw-women-lw-online", "linkUrl": "https://www.lesswrong.com/posts/dDZcMG8xopYgnNfvK/lw-women-lw-online", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20Women%3A%20LW%20Online&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20Women%3A%20LW%20Online%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdDZcMG8xopYgnNfvK%2Flw-women-lw-online%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20Women%3A%20LW%20Online%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdDZcMG8xopYgnNfvK%2Flw-women-lw-online", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdDZcMG8xopYgnNfvK%2Flw-women-lw-online", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1735, "htmlBody": "<p><strong id=\"internal-source-marker_0.11153432168066502\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"> </strong></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<h2 style=\"margin: 0px 0px 0.75em; font-size: 16px; float: none; font-family: Arial, Helvetica, sans-serif; text-align: justify;\">Standard Intro</h2>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\"><strong>The following section will be at the top of all posts in the LW Women series.</strong></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\">Several months ago, I put out a call for anonymous submissions by the women on LW, with the idea that I would compile them into some kind of post. &nbsp;There is a LOT of material, so I am breaking them down into more manageable-sized themed posts.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\">Seven women submitted, totaling about 18 pages.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\"><strong>Standard Disclaimer</strong>- Women have many different viewpoints, and just because I am acting as an intermediary to allow for anonymous communication does NOT mean that I agree with everything that will be posted in this series. (It would be rather impossible to, since there are some posts arguing opposite sides!)</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\"><strong>Warning</strong>-&nbsp;Submitters&nbsp;were told to not hold back for politeness. You are allowed to disagree, but these are candid comments; if you consider candidness impolite, I suggest you not read this post</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\"><strong>To the submitters</strong>- If you would like to respond anonymously to a comment (for example if there is a comment questioning something in your post, and you want to clarify), you can PM your message and I will post it for you. If this happens a lot, I might create a LW_Women sockpuppet account for the submitters to share.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\"><strong>Please do NOT break anonymity, because it lowers the anonymity of the rest of the submitters.</strong></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\">(Note from me: I've been procrastinating on posting these. Sorry to everyone who submitted! But I've got them organized decently enough to post now, and will be putting one up once a week or so, until we're through)</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Submitter A</h2>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I think <a href=\"/lw/134/sayeth_the_girl/yep\">this</a> is all true. Note that that commenter hasn't commented since 2009.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">Objectifying remarks about attractive women and sneery remarks about unattractive women are not nice. I worry that guys at less wrong would ignore unattractive women if they came to meetings. Unattractive women can still be smart! I also worry that they would only pay attention to attractive women insofar as they think they might get to sleep with them.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I find the \"women are aliens\" attitude that various commenters &nbsp;(and even Eliezer in the post I link to) seem to have difficult to deal with: http://lesswrong.com/lw/rp/the_opposite_sex/. I wish these posters would make it clear that they are talking about women on average: presumably they don't think that all men and all women find each other to be like aliens.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I find I tend to shy away from saying feminist things in response to PUA/gender posts, since there seems to be a fair amount of knee-jerk down-voting of anything feminist sounding. There also seems to be quite a lot of knee-jerk up-voting of poorly researched armchair ev-psych.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">Linked to 3, if people want to make claims about men and women having different innate abilities, that is fine. However, I wish they'd make it clear when they are talking on average, i.e. \"women on average are worse at engineering than men\" not \"women are worse at engineering than men.\"</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">A bit of me wishes that the \"no mindkiller topics\" rule was enforced more strictly, and that we didn't discuss sex/gender issues. I do think it is off-putting to smart women - you don't convert people to rationality by talking about such emotive topics. Even if some of the claims like \"women on average are less good at engineering than men\" are true* they are likely to put smart women off visiting less wrong. Not sure to what extent we should sacrifice looking for truth to attract people. I suspect many LWers would say not at all. I don't know. We already rarely discuss politics, so would it be terrible to also discuss sex/gender issues as little as possible?</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I agree with Luke<a href=\"/lw/ceg/link_international_variation_in_iq_the_role_of/6mcb\"> here</a></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">*and I do think some of them are true</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">***</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<h2>Submitter B</h2>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">My experience of LessWrong is that it feels unfriendly. It took me a long time to develop skin thick enough to tolerate an environment where warmth is scarce. I feel pretty certain that I've got a thicker skin than most women and that the environment is putting off other women. You wouldn't find those women writing an LW narrative, though - the type of women I'm speaking of would not have joined. It's good to open a line of communication between the genders, but by asking the women who stayed, you're not finding out much about the women who did not stay. This is why I mention my thinner-skinned self.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;What do I mean by unfriendly? It feels like people are ten thousand times more likely to point out my flaws than to appreciate something I said. Also, there's next to no emotional relating to one another. People show appreciation silently in votes, and give verbal criticism, and there are occasionally compliments, but there seems to be a dearth of friendliness. I don't need instant bonding, but the coldness is thick. If I try to tell by the way people are acting, I'm half convinced that most of the people here think I'm a moron. I'm thick skinned enough that it doesn't get to me, but I don't envision this type of environment working to draw women.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">Ive had similar unfriendly experiences in other male-dominated environments like in a class of mostly boys. They were aggressive - in a selfish way, as opposed to a constructive one. For instance, if the teacher was demonstrating something, they'd crowd around aggressively trying to get the best spots. I was much shorter, which makes it harder to see. This forced me to compete for a front spot if I wanted to see at all, and I never did because I just wasn't like that. So that felt pretty insensitive. Another male dominated environment was similarly heavy on the criticism and light on niceness.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">These seem to be a theme in male-dominated environments which have always had somewhat of a deterring effect on me: selfish competitive behavior (Constructive competition for an award or to produce something of quality is one thing, but to compete for a privilege in a way that hurts someone at a disadvantage is off-putting), focus on negative reinforcement (acting like tough guys by not giving out compliments and being abrasive), lack of friendliness (There can be no warm fuzzies when you're acting manly) and hostility toward sensitivity.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">One exception to this is Vladimir_Nesov. He has behaved in a supportive and yet honest way that feels friendly to me. ShannonFriedman does \"honest yet friendly\" well, too.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">A lot of guys I've dated in the last year have made the same creepy mistake. I think this is likely to be relevant because they're so much like LW members (most of them are programmers, their personalities are very similar and one of them had even signed up for cryo), and because I've seen some hints of this behavior on the discussions. I don't talk enough about myself here to actually bring out this \"creepy\" behavior (anticipation of that behavior is inhibiting me as well as not wanting to get too personal in public) so this could give you an insight that might not be possible if I spoke strictly of my experiences on LessWrong.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">The mistake goes like this:</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I'd say something about myself.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">They'd disagree with me.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">For a specific example, I was asked whether I was more of a thinker or feeler and I said I was pretty balanced. He retorted that I was more of a thinker. When I persist in these situations, they actually argue with me. I am the one who has spent millions of minutes in this mind, able to directly experience what's going on inside of it. They have spent, at this point, maybe a few hundred minutes observing it from the outside, yet they act like they're experts. If they said they didn't understand, or even that they didn't believe me, that would be workable. But they try to convince me I'm wrong about myself. I find this deeply disturbing and it's completely dysfunctional. There's no way a person will ever get to know me if he won't even listen to what I say about myself. Having to argue with a person over who I am is intolerable.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I've thought about this a lot trying to figure out what they're trying to do. It's never going to be a sexy \"negative hit\" to argue with me about who I am. Disagreeing with me about myself can't possibly count as showing off their incredible ability to see into me because they're doing the exact opposite: being willfully ignorant. Maybe they have such a need to box me into a category that they insist on doing so immediately. Personalities don't fit nicely in categories, so this is an auto-fail. It comes across as if they're either deluded into believing they're some kind of mind-reading genius or that they don't realize I'm a whole, grown-up human being complete with the ability to know myself. This has happened on the LessWrong forum also.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I have had a similar problem that only started to make sense after considering that they may have been making a conscious effort to develop skepticism: I had a lot of experiences where it felt like everything I said about myself was being scrutinized. It makes perfect sense to be skeptical about other conversation topics, but when they're skeptical about things I say about myself, this is ingratiating. This is because it's not likely that either of us will be able to prove or disprove anything about my personality or subjective experiences in a short period of time, and possibly never. Yet saying nothing about ourselves is not an option if we want to get to know each other better. I have to start somewhere.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">It's almost like they're in such a rush to have definitive answers about me that they're sabotaging their potential to develop a real understanding of me. Getting to know people is complicated - that's why it takes a long time. Tearing apart her self-expressions can't save you from the ambiguity.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I need \"getting to know me\" / \"sharing myself\" type conversations to be an exploration. I do understand the need to construct one's own perspective on each new person. I don't need all my statements to be accepted at face value. I just want to feel that the person is happily exploring. They should seem like they're having fun checking out something interesting, not interrogating me and expecting to find a pile of errors. Maybe this happens because of having a habit of skeptical thinking - they make people feel scrutinized without knowing it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W9aNkPwtPhMrcfgj7": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dDZcMG8xopYgnNfvK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 46, "extendedScore": null, "score": 0.000101, "legacy": true, "legacyId": "20261", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"internal-source-marker_0.11153432168066502\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\"> </strong></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<h2 style=\"margin: 0px 0px 0.75em; font-size: 16px; float: none; font-family: Arial, Helvetica, sans-serif; text-align: justify;\" id=\"Standard_Intro\">Standard Intro</h2>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\"><strong id=\"The_following_section_will_be_at_the_top_of_all_posts_in_the_LW_Women_series_\">The following section will be at the top of all posts in the LW Women series.</strong></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\">Several months ago, I put out a call for anonymous submissions by the women on LW, with the idea that I would compile them into some kind of post. &nbsp;There is a LOT of material, so I am breaking them down into more manageable-sized themed posts.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\">Seven women submitted, totaling about 18 pages.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\"><strong>Standard Disclaimer</strong>- Women have many different viewpoints, and just because I am acting as an intermediary to allow for anonymous communication does NOT mean that I agree with everything that will be posted in this series. (It would be rather impossible to, since there are some posts arguing opposite sides!)</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\"><strong>Warning</strong>-&nbsp;Submitters&nbsp;were told to not hold back for politeness. You are allowed to disagree, but these are candid comments; if you consider candidness impolite, I suggest you not read this post</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\"><strong>To the submitters</strong>- If you would like to respond anonymously to a comment (for example if there is a comment questioning something in your post, and you want to clarify), you can PM your message and I will post it for you. If this happens a lot, I might create a LW_Women sockpuppet account for the submitters to share.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\"><strong id=\"Please_do_NOT_break_anonymity__because_it_lowers_the_anonymity_of_the_rest_of_the_submitters_\">Please do NOT break anonymity, because it lowers the anonymity of the rest of the submitters.</strong></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\">(Note from me: I've been procrastinating on posting these. Sorry to everyone who submitted! But I've got them organized decently enough to post now, and will be putting one up once a week or so, until we're through)</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"Submitter_A\">Submitter A</h2>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I think <a href=\"/lw/134/sayeth_the_girl/yep\">this</a> is all true. Note that that commenter hasn't commented since 2009.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">Objectifying remarks about attractive women and sneery remarks about unattractive women are not nice. I worry that guys at less wrong would ignore unattractive women if they came to meetings. Unattractive women can still be smart! I also worry that they would only pay attention to attractive women insofar as they think they might get to sleep with them.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I find the \"women are aliens\" attitude that various commenters &nbsp;(and even Eliezer in the post I link to) seem to have difficult to deal with: http://lesswrong.com/lw/rp/the_opposite_sex/. I wish these posters would make it clear that they are talking about women on average: presumably they don't think that all men and all women find each other to be like aliens.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I find I tend to shy away from saying feminist things in response to PUA/gender posts, since there seems to be a fair amount of knee-jerk down-voting of anything feminist sounding. There also seems to be quite a lot of knee-jerk up-voting of poorly researched armchair ev-psych.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">Linked to 3, if people want to make claims about men and women having different innate abilities, that is fine. However, I wish they'd make it clear when they are talking on average, i.e. \"women on average are worse at engineering than men\" not \"women are worse at engineering than men.\"</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">A bit of me wishes that the \"no mindkiller topics\" rule was enforced more strictly, and that we didn't discuss sex/gender issues. I do think it is off-putting to smart women - you don't convert people to rationality by talking about such emotive topics. Even if some of the claims like \"women on average are less good at engineering than men\" are true* they are likely to put smart women off visiting less wrong. Not sure to what extent we should sacrifice looking for truth to attract people. I suspect many LWers would say not at all. I don't know. We already rarely discuss politics, so would it be terrible to also discuss sex/gender issues as little as possible?</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I agree with Luke<a href=\"/lw/ceg/link_international_variation_in_iq_the_role_of/6mcb\"> here</a></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">*and I do think some of them are true</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">***</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<h2 id=\"Submitter_B\">Submitter B</h2>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">My experience of LessWrong is that it feels unfriendly. It took me a long time to develop skin thick enough to tolerate an environment where warmth is scarce. I feel pretty certain that I've got a thicker skin than most women and that the environment is putting off other women. You wouldn't find those women writing an LW narrative, though - the type of women I'm speaking of would not have joined. It's good to open a line of communication between the genders, but by asking the women who stayed, you're not finding out much about the women who did not stay. This is why I mention my thinner-skinned self.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;What do I mean by unfriendly? It feels like people are ten thousand times more likely to point out my flaws than to appreciate something I said. Also, there's next to no emotional relating to one another. People show appreciation silently in votes, and give verbal criticism, and there are occasionally compliments, but there seems to be a dearth of friendliness. I don't need instant bonding, but the coldness is thick. If I try to tell by the way people are acting, I'm half convinced that most of the people here think I'm a moron. I'm thick skinned enough that it doesn't get to me, but I don't envision this type of environment working to draw women.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">Ive had similar unfriendly experiences in other male-dominated environments like in a class of mostly boys. They were aggressive - in a selfish way, as opposed to a constructive one. For instance, if the teacher was demonstrating something, they'd crowd around aggressively trying to get the best spots. I was much shorter, which makes it harder to see. This forced me to compete for a front spot if I wanted to see at all, and I never did because I just wasn't like that. So that felt pretty insensitive. Another male dominated environment was similarly heavy on the criticism and light on niceness.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">These seem to be a theme in male-dominated environments which have always had somewhat of a deterring effect on me: selfish competitive behavior (Constructive competition for an award or to produce something of quality is one thing, but to compete for a privilege in a way that hurts someone at a disadvantage is off-putting), focus on negative reinforcement (acting like tough guys by not giving out compliments and being abrasive), lack of friendliness (There can be no warm fuzzies when you're acting manly) and hostility toward sensitivity.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">One exception to this is Vladimir_Nesov. He has behaved in a supportive and yet honest way that feels friendly to me. ShannonFriedman does \"honest yet friendly\" well, too.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">A lot of guys I've dated in the last year have made the same creepy mistake. I think this is likely to be relevant because they're so much like LW members (most of them are programmers, their personalities are very similar and one of them had even signed up for cryo), and because I've seen some hints of this behavior on the discussions. I don't talk enough about myself here to actually bring out this \"creepy\" behavior (anticipation of that behavior is inhibiting me as well as not wanting to get too personal in public) so this could give you an insight that might not be possible if I spoke strictly of my experiences on LessWrong.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">The mistake goes like this:</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I'd say something about myself.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">They'd disagree with me.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">For a specific example, I was asked whether I was more of a thinker or feeler and I said I was pretty balanced. He retorted that I was more of a thinker. When I persist in these situations, they actually argue with me. I am the one who has spent millions of minutes in this mind, able to directly experience what's going on inside of it. They have spent, at this point, maybe a few hundred minutes observing it from the outside, yet they act like they're experts. If they said they didn't understand, or even that they didn't believe me, that would be workable. But they try to convince me I'm wrong about myself. I find this deeply disturbing and it's completely dysfunctional. There's no way a person will ever get to know me if he won't even listen to what I say about myself. Having to argue with a person over who I am is intolerable.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I've thought about this a lot trying to figure out what they're trying to do. It's never going to be a sexy \"negative hit\" to argue with me about who I am. Disagreeing with me about myself can't possibly count as showing off their incredible ability to see into me because they're doing the exact opposite: being willfully ignorant. Maybe they have such a need to box me into a category that they insist on doing so immediately. Personalities don't fit nicely in categories, so this is an auto-fail. It comes across as if they're either deluded into believing they're some kind of mind-reading genius or that they don't realize I'm a whole, grown-up human being complete with the ability to know myself. This has happened on the LessWrong forum also.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I have had a similar problem that only started to make sense after considering that they may have been making a conscious effort to develop skepticism: I had a lot of experiences where it felt like everything I said about myself was being scrutinized. It makes perfect sense to be skeptical about other conversation topics, but when they're skeptical about things I say about myself, this is ingratiating. This is because it's not likely that either of us will be able to prove or disprove anything about my personality or subjective experiences in a short period of time, and possibly never. Yet saying nothing about ourselves is not an option if we want to get to know each other better. I have to start somewhere.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">It's almost like they're in such a rush to have definitive answers about me that they're sabotaging their potential to develop a real understanding of me. Getting to know people is complicated - that's why it takes a long time. Tearing apart her self-expressions can't save you from the ambiguity.</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I need \"getting to know me\" / \"sharing myself\" type conversations to be an exploration. I do understand the need to construct one's own perspective on each new person. I don't need all my statements to be accepted at face value. I just want to feel that the person is happily exploring. They should seem like they're having fun checking out something interesting, not interrogating me and expecting to find a pile of errors. Maybe this happens because of having a habit of skeptical thinking - they make people feel scrutinized without knowing it.</p>", "sections": [{"title": "Standard Intro", "anchor": "Standard_Intro", "level": 1}, {"title": "The following section will be at the top of all posts in the LW Women series.", "anchor": "The_following_section_will_be_at_the_top_of_all_posts_in_the_LW_Women_series_", "level": 2}, {"title": "Please do NOT break anonymity, because it lowers the anonymity of the rest of the submitters.", "anchor": "Please_do_NOT_break_anonymity__because_it_lowers_the_anonymity_of_the_rest_of_the_submitters_", "level": 2}, {"title": "Submitter A", "anchor": "Submitter_A", "level": 1}, {"title": "Submitter B", "anchor": "Submitter_B", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "596 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 596, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T07:06:03.175Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Super Happy People (3/8)", "slug": "seq-rerun-the-super-happy-people-3-8", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/44LRAs4eHBauLek7G/seq-rerun-the-super-happy-people-3-8", "pageUrlRelative": "/posts/44LRAs4eHBauLek7G/seq-rerun-the-super-happy-people-3-8", "linkUrl": "https://www.lesswrong.com/posts/44LRAs4eHBauLek7G/seq-rerun-the-super-happy-people-3-8", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Super%20Happy%20People%20(3%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Super%20Happy%20People%20(3%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44LRAs4eHBauLek7G%2Fseq-rerun-the-super-happy-people-3-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Super%20Happy%20People%20(3%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44LRAs4eHBauLek7G%2Fseq-rerun-the-super-happy-people-3-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44LRAs4eHBauLek7G%2Fseq-rerun-the-super-happy-people-3-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 160, "htmlBody": "<p>Today's post, <a href=\"/lw/y7/the_super_happy_people_38/\">The Super Happy People (3/8)</a> was originally published on 01 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#The_Super_Happy_People_.283.2F8.29\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Humanity encounters new aliens that see the existence of pain amongst humans as morally unacceptable.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gno/seq_rerun_war_andor_peace_28/\">War and/or Peace (2/8)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "44LRAs4eHBauLek7G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1128778575924259e-06, "legacy": true, "legacyId": "21605", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qCsxiojX7BSLuuBgQ", "xm4BgZbStnQ7fFtgd", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T07:34:55.569Z", "modifiedAt": null, "url": null, "title": "Three Axes of Prohibitions", "slug": "three-axes-of-prohibitions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:38.957Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GX72JPWoWJLT9zpgu/three-axes-of-prohibitions", "pageUrlRelative": "/posts/GX72JPWoWJLT9zpgu/three-axes-of-prohibitions", "linkUrl": "https://www.lesswrong.com/posts/GX72JPWoWJLT9zpgu/three-axes-of-prohibitions", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20Axes%20of%20Prohibitions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20Axes%20of%20Prohibitions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGX72JPWoWJLT9zpgu%2Fthree-axes-of-prohibitions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20Axes%20of%20Prohibitions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGX72JPWoWJLT9zpgu%2Fthree-axes-of-prohibitions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGX72JPWoWJLT9zpgu%2Fthree-axes-of-prohibitions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 707, "htmlBody": "<p>The Game of Thrones board game is similar to Diplomacy (so I hear: I've never actually played Diplomacy). You often need to make alliances to survive, but these alliances are weak. It is both expected and required that you will eventually break your alliances, otherwise you will lose.&nbsp;My first time playing this game, I made an alliance with a neighboring House which turned out to be unwise, and severely limited my options. To me, breaking an alliance to win a game (even if it was socially acceptable) didn&rsquo;t feel right/wasn&rsquo;t worth the negative feelings, and so I ended up stuck on my island for the whole of the game.&nbsp;</p>\n<p>Instead of adapting by learning to be ok breaking alliances, which I considered to be a sub-optimal solution, I fixed the problem by targeting my alliance terms. Now, instead of a general alliance, my offers were along the lines of &ldquo;I won&rsquo;t attack you across this border for the next four rounds, if you agree to the same.&rdquo;</p>\n<p>This had the effect of actually strengthening my alliances. Limiting the terms of the alliance to something that could easily be complied with, meant that defecting was no longer expected. The cost goes down, and the benefits go up. In the previous game, both I and my ally would&rsquo;ve had to leave our mutual border semi-defended, because we knew the alliance wouldn&rsquo;t hold against a strong enough temptation of conquest. This was facilitated by the fact that the alliance was expected to be eventually broken. In the targeted alliance, I and my ally can leave our mutual border undefended, since we can expect the alliance to hold. This is facilitated by the fact that there would be social sanctions against breaking the alliance. (e.g. I wouldn&rsquo;t form alliances with that person in future games, because I knew they would break them.)</p>\n<p>This lead me to the thought that prohibitions seem to focus on three axes:</p>\n<ul>\n<li><strong>Specific/Vague Wording</strong>: When you ask for a favor, say &ldquo;please&rdquo; v. Be polite</li>\n<li><strong>Targeted/Broad Expectations</strong>: Don&rsquo;t do this one thing v. Don&rsquo;t do this whole class of things</li>\n<li><strong>Social expectation to comply/fudge</strong>: Don&rsquo;t have sex with another (wo)man v &ldquo;Don&rsquo;t even look at another (wo)man!\"&nbsp;</li>\n</ul>\n<p>&nbsp;</p>\n<p>General Examples:</p>\n<ul>\n<li><strong>Speed limits</strong>- specific, targeted, but expected to fudge</li>\n<li><strong>Terms of use agreements</strong>- specific, broad, expected to fudge (no one even reads them)</li>\n<li><strong>Bribing officials in corrupt societies</strong> (there are laws against it, but it is expected as the way to get anything done, or even considered a perk of the office)- vague (giving &ldquo;gifts&rdquo; is appropriate?)</li>\n<li>\"<strong>Thou shall not kill\"</strong>- specific, targeted, expected to comply</li>\n<li><strong>Paper shields</strong>- being asked to sign something that is vague and broad. The idea is that it is broad enough to cover anything and everything, and so is expected to be broken. But as long as you don't do anything egregious, they don't enforce it. </li>\n</ul>\n<p>&nbsp;</p>\n<p>It seems to me that making an injunction specific and targeted increases the expectation of compliance. This is important to me, because I seem to dislike injunctions that I am expected to fudge.&nbsp;</p>\n<p>Another example of how this plays out in my life: Being enmeshed in a poly network, there is a lot of talking about people (not necessarily in a bad way-- if you ask me about my day though, my answer is going to involve other people). To get around worrying about if I am ever breaking confidence, I specifically tell people I am close to that I don&rsquo;t consider any information to be private unless it is specifically stated as so (this goes two ways).&nbsp;This way, I get to \"gossip\" but also people know they can strongly trust me with any information that is prefaced with \"This is not for public consumption...\" In this example, like the board game, I am turning a broad, vague injuction that isn't strongly expected to be followed (\"don't ever talk about other people\") into an injuction that can be trusted to be followed by making it specific and targeted.&nbsp;</p>\n<p>Relevant to previous discussions on: ask v guess cultures, and the idea that if it's expected that everyone breaks a specific law then the government can arrest anyone they want to</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AADZcNS24mmSfPp2w": 1, "xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GX72JPWoWJLT9zpgu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 47, "extendedScore": null, "score": 0.000174, "legacy": true, "legacyId": "21606", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T08:53:41.188Z", "modifiedAt": null, "url": null, "title": "The Virtue of Compartmentalization", "slug": "the-virtue-of-compartmentalization", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.990Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "b1shop", "createdAt": "2010-07-21T22:58:23.412Z", "isAdmin": false, "displayName": "b1shop"}, "userId": "YYM9ouBdPbNFxvF2G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ptyc3A2dALq7oYuno/the-virtue-of-compartmentalization", "pageUrlRelative": "/posts/Ptyc3A2dALq7oYuno/the-virtue-of-compartmentalization", "linkUrl": "https://www.lesswrong.com/posts/Ptyc3A2dALq7oYuno/the-virtue-of-compartmentalization", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Virtue%20of%20Compartmentalization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Virtue%20of%20Compartmentalization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPtyc3A2dALq7oYuno%2Fthe-virtue-of-compartmentalization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Virtue%20of%20Compartmentalization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPtyc3A2dALq7oYuno%2Fthe-virtue-of-compartmentalization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPtyc3A2dALq7oYuno%2Fthe-virtue-of-compartmentalization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 786, "htmlBody": "<p><em>Cross posted from my blog, <a href=\"http://www.selfishmeme.com/274/the-virtue-of-compartmentalization/\">Selfish Meme</a>.</em></p>\n<p><em></em><br /><img style=\"float: right; margin-left: 15px\" src=\"http://www.selfishmeme.com/wp-content/uploads/2013/02/198964_1005225089982_783_n.jpg\" alt=\"\" width=\"223\" height=\"362\" />I&rsquo;d like to humbly propose a new virtue to add to Eliezer&rsquo;s <a href=\"http://yudkowsky.net/rational/virtues\">virtues of rationality</a> &mdash; the virtue of Compartmentalization. Like the <a href=\"http://en.wikipedia.org/wiki/Virtue#Aristotelian_virtue\">Aristoteian virtues</a>, the virtue of Compartmentalization is a golden mean. Learning the appropriate amount of Compartmentalization, like learning the appropriate amount of bravery, is a life-long challenge.</p>\n<p>Learning how to program is both learning the words to which computers listen and training yourself to think about complex problems. Learning to comfortably move between levels of abstraction is an important part of the second challenge.</p>\n<p>Large programs are composed of multiple modules. Each module is composed of lines of code. Each line of code is composed of functions manipulating objects. Each function is yet a deeper set of instructions.</p>\n<p>For a programmer to truly focus on one element of a program, he or she has to operate at the right level of abstraction and temporarily forget the elements above, below or alongside the current problem.</p>\n<p>Programming is not the only discipline that requires this focus. Economists and mathematicians rely on tools such as regressions and Bayes&rsquo; rule without continually recanting the math that makes them truths. Engineers do not consider wave-particle duality when predicting Newtonian-type problems. When a mechanic is fixing a radiator, the only relevant fact about spark plugs is that they produce heat.</p>\n<p>If curiosity killed the cat, it&rsquo;s only because it distracted her from more urgent matters.</p>\n<p>As I became a better programmer I didn&rsquo;t notice my Compartmentalization-skills improving &ndash; I was too lost in the problem at hand, but I noticed the skill when I noticed its absence in other people. Take, for example, the confused philosophical debate about free will. A typical spiel from an actual philosopher can be found in the movie <a href=\"http://www.youtube.com/watch?v=veqkUUOlLLE\">Waking Life</a>.</p>\n<p>Discussions about free will often veer into unproductive digressions about physical facts at the wrong level of abstraction. Perhaps, at its deepest level, reality is a collection of billiard balls. Perhaps reality is, deep down, a pantheon of gods rolling dice. Maybe all matter is composed of cellists balancing on vibrating tightropes. Maybe we&rsquo;re living in a simulated matrix of 1&rsquo;s and 0s, or maybe it really is just turtles all the way down.</p>\n<p>These are interesting questions that should be pursued by all blessed with sufficient curiosity, but these are questions at a level of abstraction absolutely irrelevant to the questions at hand.</p>\n<p>A philosopher with a programmer&rsquo;s discipline thinking about &ldquo;free will&rdquo; will not start by debating the above questions. Instead, he will notice that &ldquo;free will&rdquo; is itself a philosophical abstraction that can be broken down into several, oft-convoluted components. Debating the concept as a whole is too high of an abstraction. When one says &ldquo;do I have free will?&rdquo; one could actually be asking:</p>\n<ol>\n<li>Are the actions of humans predictable?</li>\n<li>Are humans perfectly predictable with complete knowledge and infinite computational time?</li>\n<li>Will we ever have complete knowledge and infinite computational time necessary to perfectly predict a human?</li>\n<li>Can you reliably manipulate humans with advertising/priming?</li>\n<li>Are humans capable of thinking about and changing their habits through conscious thought?</li>\n<li>Do humans have a non-physical soul that directs our actions and is above physical influences?</li>\n</ol>\n<p>I&rsquo;m sure there are other questions lurking beneath in the conceptual quagmire of &ldquo;free will,&rdquo; but that&rsquo;s a good start These six are not only significantly narrower in scope than &ldquo;Do humans have free will?&rdquo; but also are also answerable and actionable. Off the cuff:</p>\n<ol>\n<li>Of course.</li>\n<li>Probably.</li>\n<li>Probably not.</li>\n<li>Less than marketers/psychologists would want you to believe but more than the rest of us would like to admit.</li>\n<li>More so than most animals, but less so than we might desire.</li>\n<li>Brain damage and mind-altering drugs would suggest our &ldquo;spirits&rdquo; are not above physical influences.</li>\n</ol>\n<p>So, in sum, what would a programmer have to say about the question of free will? Nothing. The problem must be broken into manageable pieces, and each element must be examined in turn. The original question is not clear enough for a single answer. Furthermore, he will ignore all claims about the fundamental nature of the universe. You don&rsquo;t go digging around machine code when you&rsquo;re making a spreadsheet.</p>\n<p>If you want your brain to think about problems larger, older and deeper than your brain, then you should be capable of zooming in and out of the problem &ndash; sometimes poring over the minutest details and sometimes blurring your vision to see the larger picture. Sometimes you need to alternate between multiple maps of varying detail for the same territory. Far from being a vice, this is the virtue of Compartmentalization.</p>\n<p>Your homework assignment: Does the expression &ldquo;love is just a chemical&rdquo; change anything about Valentine&rsquo;s Day?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ptyc3A2dALq7oYuno", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "21607", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T10:46:17.282Z", "modifiedAt": null, "url": null, "title": "Unintentional bayesian", "slug": "unintentional-bayesian", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:43.562Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "BgFCy4FeuL7DPMSdz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9G87df4ERvqxrJS26/unintentional-bayesian", "pageUrlRelative": "/posts/9G87df4ERvqxrJS26/unintentional-bayesian", "linkUrl": "https://www.lesswrong.com/posts/9G87df4ERvqxrJS26/unintentional-bayesian", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Unintentional%20bayesian&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnintentional%20bayesian%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9G87df4ERvqxrJS26%2Funintentional-bayesian%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Unintentional%20bayesian%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9G87df4ERvqxrJS26%2Funintentional-bayesian", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9G87df4ERvqxrJS26%2Funintentional-bayesian", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 313, "htmlBody": "<p>Growing up in a very religious country, I was indoctrinated thoroughly both at home and at school. I used to believe that some Christian beliefs made sense. When I was 14 years old or so, I began contemplating death &ndash; I said to myself, &ldquo;Well, after I die I go to Hell or Heaven; the latter is preferable, so I'd better learn as soon as possible how I can make sure I'll go to Heaven.&rdquo;</p>\n<p style=\"margin-bottom: 0in;\">So I went on to read frantically about Christianity. With every iota of information processed, I strayed away from this religion. That is, the more I read, the less anything pertaining to it seemed plausible. &ldquo;Where the hell is Hell? Can I visit before I die? Why doesn't God answer my prayers to tell me? Why do some people get to talk to God but not me?&rdquo;, I retorted. In retrospective, my greatest strength was genuine curiosity &ndash; I wanted to know as much as possible about the truthfulness of my religion.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The irony here is that wanting to become more Christian-like led to my abandoning of Christianity. But I continued to learn more about other religions as well, thinking that one might be truer than the other. Of course, none of them seemed every remotely plausible; I concluded that religions are false. I turned into an atheist without even knowing that that word existed!</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Eventually I stumbled on some articles regarding non-religion and discovered that my lack of religious beliefs are called 'atheism'. Since then, I have abandoned more beliefs tied to, say, politics or nutrition, thanks to applying bayesian probability to my hypotheses.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I had been an unintentional bayesian for my whole life!</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Have you had any similar experiences?&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">PS: This is my first article. I am looking forward to hearing feedback on it.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Edit #1: I should have used the term 'rationalist' instead of 'bayesian' because I didn't apply Bayes' theorem explicitly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9G87df4ERvqxrJS26", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 9, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "21608", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T14:21:10.122Z", "modifiedAt": null, "url": null, "title": "Is protecting yourself from your own biases self-defeating? ", "slug": "is-protecting-yourself-from-your-own-biases-self-defeating", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:38.779Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "BgFCy4FeuL7DPMSdz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jiYwesj9cSN855iwy/is-protecting-yourself-from-your-own-biases-self-defeating", "pageUrlRelative": "/posts/jiYwesj9cSN855iwy/is-protecting-yourself-from-your-own-biases-self-defeating", "linkUrl": "https://www.lesswrong.com/posts/jiYwesj9cSN855iwy/is-protecting-yourself-from-your-own-biases-self-defeating", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20protecting%20yourself%20from%20your%20own%20biases%20self-defeating%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20protecting%20yourself%20from%20your%20own%20biases%20self-defeating%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjiYwesj9cSN855iwy%2Fis-protecting-yourself-from-your-own-biases-self-defeating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20protecting%20yourself%20from%20your%20own%20biases%20self-defeating%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjiYwesj9cSN855iwy%2Fis-protecting-yourself-from-your-own-biases-self-defeating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjiYwesj9cSN855iwy%2Fis-protecting-yourself-from-your-own-biases-self-defeating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 306, "htmlBody": "<p style=\"margin-bottom: 0in;\">I graduated from high school and wish to further my education formally by studying for a bachelor's degree in order to become a medical researcher. I could, for instance, take two different academic paths:</p>\n<ol>\n<li>\n<p style=\"margin-bottom: 0in;\">Study Medicine at undergraduate level and then do a postdoctoral fellowship.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in;\">Study Biochemistry at undergraduate level, then study for a PhD at graduate level, and finally do a postdoctoral fellowship.</p>\n</li>\n</ol>\n<p style=\"margin-bottom: 0in;\">Since I will do these studies in Europe, they each take approximately the same amount of time, namely 6 to 8 years.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Do I want to do treat patients? No, I do not. But I am considering Medicine because it can be a buffer against my own mediocrity: in case I turn out to be a below average scientist, I will be screwed royally. From my personal job shadowing experience, Medicine, on the other hand, requires mere basic intellectual traits, primarily the ability to memorize heaps of information. And those I think I have. To do world-class research though I'd have to be an intellectual heavyweight, and of that I'm not so sure.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">How do I decide what path to&nbsp; follow?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The reason I'm asking you strangers for advice is because I evidently have biases, such as the pessimism/optimism bias or the <em>Dunning</em>&ndash;<em>Kruger effect, that impair my ability to reason clearly; and people who know me personally are likewise prone to make errors in advising me because of biases like, say, the Halo effect. (Come to think of it, thinking that I can't become an above average scientist is in itself a self-defeating prophecy!)<br /></em></p>\n<p style=\"margin-bottom: 0in;\"><em><br /></em></p>\n<p style=\"margin-bottom: 0in;\">Do you think that one ought to always seek advice from total strangers in order to be safeguarded from his/her own biases?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p><em>PS: I apologize if I should have written this in a specific thread. I'll delete my article if that's necessary.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jiYwesj9cSN855iwy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -2, "extendedScore": null, "score": 1.1131540959013526e-06, "legacy": true, "legacyId": "21609", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T15:59:00.457Z", "modifiedAt": null, "url": null, "title": "What are your rules of thumb?", "slug": "what-are-your-rules-of-thumb", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:54.256Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DwvLzRZsZRzPuW49a/what-are-your-rules-of-thumb", "pageUrlRelative": "/posts/DwvLzRZsZRzPuW49a/what-are-your-rules-of-thumb", "linkUrl": "https://www.lesswrong.com/posts/DwvLzRZsZRzPuW49a/what-are-your-rules-of-thumb", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20your%20rules%20of%20thumb%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20your%20rules%20of%20thumb%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwvLzRZsZRzPuW49a%2Fwhat-are-your-rules-of-thumb%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20your%20rules%20of%20thumb%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwvLzRZsZRzPuW49a%2Fwhat-are-your-rules-of-thumb", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwvLzRZsZRzPuW49a%2Fwhat-are-your-rules-of-thumb", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 398, "htmlBody": "<p>I'm not as smart as I like to think I am. Knowing that, I've gotten into a habit of trying to work out as many general principles as I can ahead of time, so that when I actually need to think of something, I've already done as much of the work as I can.</p>\n<p>What are your most useful cached thoughts?</p>\n<p><a id=\"more\"></a>A few of the rules-of-thumb I've already pre-cached include:</p>\n<ul>\n<li>\"Stay classy\". Assume that whatever social interactions I have are going to come back to me in twenty years, so try not to make my future self too embarrassed; be as polite and respectful as feasible.</li>\n<li>\"The rule of threes: for anything important, try to have at least three sources, including at least one under your own control\". Adapted from some wilderness survival books, it also applies to anything from home emergency kits to internet access to news sources.</li>\n<li>\"Assume I'm more likely than not going to get the worse side of the bargain.\" There are lots of people who are better than me at haggling, negotiating, and social sciences - in fact, I'm almost certainly somewhere down on the lower half of the bell-curve. So if it's possible to be taken advantage of in a deal, then I'm probably the one who's going to get taken advantage of.</li>\n<li>\"Assume I'm more likely than not going to get the worse side of the bargain.\" Applied to the field of ethics; if an ethical system says that it's moral to shoot someone for stealing an ice cream, then I assume that someone is going to mistake me for having stolen it and try to shoot me; or, if it's supposedly moral for a rich man to charge a thousand dollars for a bottle of water, I assume I'm going to be the one crawling in from the desert.</li>\n<li>\"It behooves every man who values liberty of conscience for himself, to resist invasions of it in the case of others: or their case may, by change of circumstances, become his own.\" I'm quite willing to steal ideas, including this phrasing of enlightened self-interest by Thomas Jefferson. (But I don't want this to become just another quotes thread.)</li>\n</ul>\n<p>&nbsp;</p>\n<p>That should be a reasonable but not overwhelming sample of the sorts of ideas I mean, and am hoping to evoke more of with this post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DwvLzRZsZRzPuW49a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 29, "extendedScore": null, "score": 1.113216226775752e-06, "legacy": true, "legacyId": "21610", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T17:00:38.924Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Berlin, Cambridge UK, London", "slug": "weekly-lw-meetups-austin-berlin-cambridge-uk-london", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3SWt49DCt53FXWYDt/weekly-lw-meetups-austin-berlin-cambridge-uk-london", "pageUrlRelative": "/posts/3SWt49DCt53FXWYDt/weekly-lw-meetups-austin-berlin-cambridge-uk-london", "linkUrl": "https://www.lesswrong.com/posts/3SWt49DCt53FXWYDt/weekly-lw-meetups-austin-berlin-cambridge-uk-london", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Cambridge%20UK%2C%20London&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Cambridge%20UK%2C%20London%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3SWt49DCt53FXWYDt%2Fweekly-lw-meetups-austin-berlin-cambridge-uk-london%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Cambridge%20UK%2C%20London%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3SWt49DCt53FXWYDt%2Fweekly-lw-meetups-austin-berlin-cambridge-uk-london", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3SWt49DCt53FXWYDt%2Fweekly-lw-meetups-austin-berlin-cambridge-uk-london", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 501, "htmlBody": "<p><strong>This summary was posted to LW main on February 8th. The following week's summary is <a href=\"/lw/gob/weekly_lw_meetups_austin_bielefeld_brussels/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/it\">Berlin Meetup:&nbsp;<span class=\"date\">08 February 2013 07:30PM</span></a></li>\n<li><a href=\"/meetups/iu\">London Meetup, 10th Feb:&nbsp;<span class=\"date\">11 February 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ht\">Brussels meetup:&nbsp;<span class=\"date\">16 February 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/iv\">Moscow: Rationality in our daily life:&nbsp;<span class=\"date\">17 February 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/ix\">Bielefeld Meetup, February 20th:&nbsp;<span class=\"date\">20 February 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/ix\"></a><a href=\"/meetups/iy\">First meetup in Frankfurt (Main) :&nbsp;<span class=\"date\">22 February 2013 06:30PM</span></a></li>\n<li><a href=\"/meetups/ir\">Tokyo Meetup:&nbsp;<span class=\"date\">01 March 2013 07:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">09 February 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/iw\">Cambridge, UK LW Meetup [Reading Group, HAEFB-01]:&nbsp;<span class=\"date\">10 February 2013 11:00AM</span></a></li>\n<li><a href=\"/meetups/ij\">Love and Sex in Salt Lake City:&nbsp;<span class=\"date\">16 February 2014 01:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3SWt49DCt53FXWYDt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 1.1132553740304255e-06, "legacy": true, "legacyId": "21543", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WmMTshDQpoAAjYMTT", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T17:28:03.102Z", "modifiedAt": null, "url": null, "title": "Meetup : Toronto - What's all this about Bayesian Probability and stuff?!", "slug": "meetup-toronto-what-s-all-this-about-bayesian-probability", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hpJnRQpSDEKKz8ejb/meetup-toronto-what-s-all-this-about-bayesian-probability", "pageUrlRelative": "/posts/hpJnRQpSDEKKz8ejb/meetup-toronto-what-s-all-this-about-bayesian-probability", "linkUrl": "https://www.lesswrong.com/posts/hpJnRQpSDEKKz8ejb/meetup-toronto-what-s-all-this-about-bayesian-probability", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Toronto%20-%20What's%20all%20this%20about%20Bayesian%20Probability%20and%20stuff%3F!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Toronto%20-%20What's%20all%20this%20about%20Bayesian%20Probability%20and%20stuff%3F!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhpJnRQpSDEKKz8ejb%2Fmeetup-toronto-what-s-all-this-about-bayesian-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Toronto%20-%20What's%20all%20this%20about%20Bayesian%20Probability%20and%20stuff%3F!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhpJnRQpSDEKKz8ejb%2Fmeetup-toronto-what-s-all-this-about-bayesian-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhpJnRQpSDEKKz8ejb%2Fmeetup-toronto-what-s-all-this-about-bayesian-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jf'>Toronto - What's all this about Bayesian Probability and stuff?!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 February 2013 08:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">54 Dundas St E, Toronto, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Place: Upstairs at The Imperial Public Library 54 Dundas St. E, near Dundas Station. Enter at the door on the right marked \"library\", go upstairs and look for the paperclip sign.</p>\n\n<p>A lot of us enjoy reading the Less Wrong Sequences:</p>\n\n<p><a href=\"http://wiki.lesswrong.com/wiki/Sequences\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Sequences</a></p>\n\n<p>...but there's a lot there that might seem confusing. Do we really try to live our lives according to a mathematical formula? Do we really hate frequentist statistics? Who are these mysterious organizations with their logos at the top of the page? What's the deal with the Harry Potter and My Little Pony stuff? How much of that math and science and philosophy is relevant to our daily lives?</p>\n\n<p>This meetup is a chance to try and clear up some of these confusions.</p>\n\n<p>It's also a chance for each of us to explain: what does rationality mean to you? What do you imagine a rational person as being like?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jf'>Toronto - What's all this about Bayesian Probability and stuff?!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hpJnRQpSDEKKz8ejb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.113272777967367e-06, "legacy": true, "legacyId": "21612", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Toronto___What_s_all_this_about_Bayesian_Probability_and_stuff__\">Discussion article for the meetup : <a href=\"/meetups/jf\">Toronto - What's all this about Bayesian Probability and stuff?!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 February 2013 08:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">54 Dundas St E, Toronto, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Place: Upstairs at The Imperial Public Library 54 Dundas St. E, near Dundas Station. Enter at the door on the right marked \"library\", go upstairs and look for the paperclip sign.</p>\n\n<p>A lot of us enjoy reading the Less Wrong Sequences:</p>\n\n<p><a href=\"http://wiki.lesswrong.com/wiki/Sequences\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Sequences</a></p>\n\n<p>...but there's a lot there that might seem confusing. Do we really try to live our lives according to a mathematical formula? Do we really hate frequentist statistics? Who are these mysterious organizations with their logos at the top of the page? What's the deal with the Harry Potter and My Little Pony stuff? How much of that math and science and philosophy is relevant to our daily lives?</p>\n\n<p>This meetup is a chance to try and clear up some of these confusions.</p>\n\n<p>It's also a chance for each of us to explain: what does rationality mean to you? What do you imagine a rational person as being like?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Toronto___What_s_all_this_about_Bayesian_Probability_and_stuff__1\">Discussion article for the meetup : <a href=\"/meetups/jf\">Toronto - What's all this about Bayesian Probability and stuff?!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Toronto - What's all this about Bayesian Probability and stuff?!", "anchor": "Discussion_article_for_the_meetup___Toronto___What_s_all_this_about_Bayesian_Probability_and_stuff__", "level": 1}, {"title": "Discussion article for the meetup : Toronto - What's all this about Bayesian Probability and stuff?!", "anchor": "Discussion_article_for_the_meetup___Toronto___What_s_all_this_about_Bayesian_Probability_and_stuff__1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T18:43:01.328Z", "modifiedAt": null, "url": null, "title": "Cryo: Legal fees of $2500", "slug": "cryo-legal-fees-of-usd2500", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:39.190Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/84Kzzrne3bFLcY9wg/cryo-legal-fees-of-usd2500", "pageUrlRelative": "/posts/84Kzzrne3bFLcY9wg/cryo-legal-fees-of-usd2500", "linkUrl": "https://www.lesswrong.com/posts/84Kzzrne3bFLcY9wg/cryo-legal-fees-of-usd2500", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryo%3A%20Legal%20fees%20of%20%242500&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryo%3A%20Legal%20fees%20of%20%242500%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F84Kzzrne3bFLcY9wg%2Fcryo-legal-fees-of-usd2500%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryo%3A%20Legal%20fees%20of%20%242500%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F84Kzzrne3bFLcY9wg%2Fcryo-legal-fees-of-usd2500", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F84Kzzrne3bFLcY9wg%2Fcryo-legal-fees-of-usd2500", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 344, "htmlBody": "<p>I have just received a message from my lawyer, regarding the preparations of my cryo-based will, power of attorney, and related papers. The most significant quote reads as follows:<br /><br /><br />Due to the complex nature of your wishes and the undeveloped area of the law surrounding cryogenics and the transportation of a human body out of the country, your file has required and will require further extensive time and research to prepare and draft the necessary documents to ensure that your wishes are correctly stated and that they will be carried out upon your death. As such, in order to complete any further work on your behalf we will require a retainer from you in the amount of $2,500.00. Please contact our office to arrange the payment of the retainer. This can be paid by cheque, cash, credit or debit. Upon receipt of this retainer we will proceed to draft the documents in a manner that best ensures that your wishes regarding cryogenics are carried out.<br /><a id=\"more\"></a><br />This is somewhat more than I was hoping, and expecting, to pay. While I do have a line of credit which I can use to pay the immediate cost, I do not have sufficient income to pay off that cost any time soon, and I am thus debating whether or not to pay it.<br /><br />One option that occurs to me is that if my law-firm is able to nail down a set of documents to maximize the odds of a Canadian cryonicist's will being properly carried out, then that would be to the benefit of a number of cryonicists, here and otherwise; perhaps even enough so that it could be in the interests of cryonicists here to help cover the legal fees, instead of having to hire new lawyers to separately re-develop the same paperwork. (It would be reasonably trivial for me to provide proof of my lawyer's existence and the retainer request.)<br /><br />I'm also open to any other ideas that anyone here could suggest.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "84Kzzrne3bFLcY9wg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 20, "extendedScore": null, "score": 1.113320395042229e-06, "legacy": true, "legacyId": "21613", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-15T23:17:42.391Z", "modifiedAt": null, "url": null, "title": "Open thread, February 15-28, 2013", "slug": "open-thread-february-15-28-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.615Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a9ru8H88acsKFfnfL/open-thread-february-15-28-2013", "pageUrlRelative": "/posts/a9ru8H88acsKFfnfL/open-thread-february-15-28-2013", "linkUrl": "https://www.lesswrong.com/posts/a9ru8H88acsKFfnfL/open-thread-february-15-28-2013", "postedAtFormatted": "Friday, February 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20February%2015-28%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20February%2015-28%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa9ru8H88acsKFfnfL%2Fopen-thread-february-15-28-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20February%2015-28%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa9ru8H88acsKFfnfL%2Fopen-thread-february-15-28-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa9ru8H88acsKFfnfL%2Fopen-thread-february-15-28-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a9ru8H88acsKFfnfL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "21614", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 346, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-16T00:31:20.077Z", "modifiedAt": null, "url": null, "title": "Great rationality posts by LWers not posted to LW", "slug": "great-rationality-posts-by-lwers-not-posted-to-lw", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:35.394Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xZdW7D43AaCiQQzvM/great-rationality-posts-by-lwers-not-posted-to-lw", "pageUrlRelative": "/posts/xZdW7D43AaCiQQzvM/great-rationality-posts-by-lwers-not-posted-to-lw", "linkUrl": "https://www.lesswrong.com/posts/xZdW7D43AaCiQQzvM/great-rationality-posts-by-lwers-not-posted-to-lw", "postedAtFormatted": "Saturday, February 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Great%20rationality%20posts%20by%20LWers%20not%20posted%20to%20LW&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGreat%20rationality%20posts%20by%20LWers%20not%20posted%20to%20LW%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxZdW7D43AaCiQQzvM%2Fgreat-rationality-posts-by-lwers-not-posted-to-lw%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Great%20rationality%20posts%20by%20LWers%20not%20posted%20to%20LW%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxZdW7D43AaCiQQzvM%2Fgreat-rationality-posts-by-lwers-not-posted-to-lw", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxZdW7D43AaCiQQzvM%2Fgreat-rationality-posts-by-lwers-not-posted-to-lw", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 265, "htmlBody": "<p>Ever since Eliezer, Yvain, and myself stopped posting regularly, LW's front page has mostly been populated by meta posts. (The Discussion section is still abuzz with interesting content, though, including <a href=\"/lw/f6o/original_research_on_less_wrong/\">original research</a>.)</p>\n<p>Luckily, many LWers are posting potentially front-page-worthy content to <a href=\"/lw/d8t/blogs_by_lwers/\">their own blogs</a>.</p>\n<p>Below are some recent-ish highlights outside Less Wrong, for your reading enjoyment. I've added an <strong>*</strong> to my personal favorites.</p>\n<p>&nbsp;</p>\n<p><strong><a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a></strong> (Robin Hanson, Rob Wiblin, Katja Grace, Carl Shulman)</p>\n<ul>\n<li>Hanson, <a href=\"http://www.overcomingbias.com/2013/02/beware-far-values.html\">Beware Far Values</a></li>\n<li>Wiblin, <a href=\"http://www.overcomingbias.com/2013/02/is-us-gun-control-an-important-issue.html\">Is US Gun Control an Important Issue?</a></li>\n<li>Wiblin, <a href=\"http://www.overcomingbias.com/2013/01/morality-as-though-it-really-mattered.html\">Morality As Though It Really Mattered</a></li>\n<li>Grace, <a href=\"http://www.overcomingbias.com/2012/11/can-a-tiny-bit-of-noise-destroy-communication.html\">Can a Tiny Bit of Noise Destroy Communication?</a></li>\n<li>Shulman,&nbsp;<a href=\"http://www.overcomingbias.com/2012/11/nuclear-winter-and-human-extinction-qa-with-luke-oman.html\">Nuclear winter and human extinction: Q&amp;A with Luke Oman</a></li>\n<li>Wiblin,&nbsp;<a href=\"http://www.overcomingbias.com/2012/11/does-complexity-bias-biotechnology-towards-destruction.html\">Does complexity bias biotechnology towards doing damage?</a></li>\n</ul>\n<div><strong><a href=\"http://squid314.livejournal.com/\">Yvain</a></strong> (now moved <a href=\"http://slatestarcodex.com/\">here</a>)</div>\n<div>\n<ul>\n<li><a href=\"http://squid314.livejournal.com/354867.html\">Kurzweil's Law of Accelerating Returns</a>&nbsp;<strong>*</strong></li>\n<li><a href=\"http://squid314.livejournal.com/352945.html\">The Great Stagnation</a></li>\n<li><a href=\"http://squid314.livejournal.com/350090.html\">Epistemic Learned Helplessness</a>&nbsp;<strong>*</strong></li>\n<li><a href=\"http://squid314.livejournal.com/346391.html\">The Biodeterminist's Guide to Parenting</a></li>\n</ul>\n<div><a href=\"http://rationalconspiracy.com/\"><strong>The Rationalist Conspiracy</strong></a> (Alyssa Vance)</div>\n<div>\n<ul>\n<li><a href=\"http://rationalconspiracy.com/2013/01/24/what-caring-is/\">What Caring Is</a></li>\n<li><a href=\"http://rationalconspiracy.com/2012/11/23/the-real-america-of-2022/\">The Real America of 2022</a></li>\n<li><a href=\"http://rationalconspiracy.com/2012/11/25/why-most-online-medical-information-sucks/\">Why Most Online Medical Information Sucks</a></li>\n</ul>\n<div><strong><a href=\"http://reflectivedisequilibrium.blogspot.com/\">Reflective Disequilibrium</a></strong> (Carl Shulman)</div>\n<div>\n<ul>\n<li><a href=\"http://reflectivedisequilibrium.blogspot.com/2012/09/spreading-happiness-to-stars-seems.html\">Spreading happiness to the stars seems little harder than just spreading</a></li>\n<li><a href=\"http://reflectivedisequilibrium.blogspot.com/2012/07/rawls-original-position-potential.html\">Rawls' original position, potential people, and Pascal's Mugging</a></li>\n<li><a href=\"http://reflectivedisequilibrium.blogspot.com/2012/05/philosophers-vs-economists-on.html\">Philosophers vs economists on discounting</a></li>\n<li><a href=\"http://reflectivedisequilibrium.blogspot.com/2012/05/utilitarianism-contractualism-and-self.html\">Utilitarianism, contractualism, and self-sacrifice</a></li>\n<li><a href=\"http://reflectivedisequilibrium.blogspot.com/2012/03/are-pain-and-pleasure-equally-energy.html\">Are pain and pleasure equally energy-efficient?</a>&nbsp;<strong>*</strong></li>\n</ul>\n<div><strong><a href=\"http://rationalaltruist.com/\">Rational Altruist</a></strong> (Paul Christiano)</div>\n<div>\n<ul>\n<li><a href=\"http://rationalaltruist.com/2013/01/27/ethical-questions/\">Pressing Ethical Questions</a></li>\n<li><a href=\"http://rationalaltruist.com/2013/01/22/replaceability/\">Replaceability</a>&nbsp;<strong>*</strong></li>\n<li><a href=\"http://rationalaltruist.com/2013/01/06/how-useful-is-progress/\">How Useful is Progress?</a></li>\n</ul>\n<div><strong><a href=\"http://alexvermeer.com/\">Alex Vermeer</a></strong></div>\n<div>\n<ul>\n<li><a href=\"http://alexvermeer.com/15-benefits-growth-mindset/\">15 Benefits of the Growth Mindset</a></li>\n</ul>\n<div><a href=\"http://princemm.wordpress.com/\"><strong>Prince Mm Mm</strong></a> (Giles)</div>\n<div>\n<ul>\n<li><a href=\"http://princemm.wordpress.com/2012/03/26/anthropic-principle-primer/\">Anthropic Principle Primer</a></li>\n<li><a href=\"http://princemm.wordpress.com/2012/01/31/entropy-and-unconvincing-models/\">Entropy and Unconvincing Models</a></li>\n</ul>\n<div><br /></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "GQyPQcdEQF4zXhJBq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xZdW7D43AaCiQQzvM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 40, "extendedScore": null, "score": 1.1135416712117062e-06, "legacy": true, "legacyId": "21615", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jTkmEGWM4dJAfE62W", "h83ZzxEpKiPPjsRKs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-16T01:31:21.086Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Interlude with the Confessor", "slug": "seq-rerun-interlude-with-the-confessor", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:53.975Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MG5TN4RYoaCKe795D/seq-rerun-interlude-with-the-confessor", "pageUrlRelative": "/posts/MG5TN4RYoaCKe795D/seq-rerun-interlude-with-the-confessor", "linkUrl": "https://www.lesswrong.com/posts/MG5TN4RYoaCKe795D/seq-rerun-interlude-with-the-confessor", "postedAtFormatted": "Saturday, February 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Interlude%20with%20the%20Confessor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Interlude%20with%20the%20Confessor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMG5TN4RYoaCKe795D%2Fseq-rerun-interlude-with-the-confessor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Interlude%20with%20the%20Confessor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMG5TN4RYoaCKe795D%2Fseq-rerun-interlude-with-the-confessor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMG5TN4RYoaCKe795D%2Fseq-rerun-interlude-with-the-confessor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Today's post, <a href=\"/lw/y8/interlude_with_the_confessor_48/\">Interlude with the Confessor (4/8)</a> was originally published on 02 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Interlude_with_the_Confessor_.284.2F8.29\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Akon talks things over with the Confessor, and receives a history lesson.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/go5/seq_rerun_the_super_happy_people_38/\">The Super Happy People (3/8)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MG5TN4RYoaCKe795D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1135798065433552e-06, "legacy": true, "legacyId": "21618", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bojLBvsYck95gbKNM", "44LRAs4eHBauLek7G", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-16T11:25:24.176Z", "modifiedAt": null, "url": null, "title": "What's your hypothetical apostasy?", "slug": "what-s-your-hypothetical-apostasy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:38.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "BgFCy4FeuL7DPMSdz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RAMvCGLiraZqwm72k/what-s-your-hypothetical-apostasy", "pageUrlRelative": "/posts/RAMvCGLiraZqwm72k/what-s-your-hypothetical-apostasy", "linkUrl": "https://www.lesswrong.com/posts/RAMvCGLiraZqwm72k/what-s-your-hypothetical-apostasy", "postedAtFormatted": "Saturday, February 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What's%20your%20hypothetical%20apostasy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat's%20your%20hypothetical%20apostasy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAMvCGLiraZqwm72k%2Fwhat-s-your-hypothetical-apostasy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What's%20your%20hypothetical%20apostasy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAMvCGLiraZqwm72k%2Fwhat-s-your-hypothetical-apostasy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAMvCGLiraZqwm72k%2Fwhat-s-your-hypothetical-apostasy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>By learning to overcome the <a href=\"https://en.wikipedia.org/wiki/Nirvana_fallacy\">Nirvana fallacy</a>, I have managed to find my <a href=\"http://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html\">hypothetical apostasy</a>: I want to cure aging!</p>\n<p>I think that awesome stuff will happen in the far future and I plan on getting there, so I'll do my best to make sure that I stay alive as long as I can. (Also, my primitive survival instincts make me want to become immortal.) Unfortunately, due to my evolutionary baggage, my own genes are going to kill me in a few decades.</p>\n<p>What's your hypothetical apostasy and how do you plan to put it in practice?</p>\n<p>&nbsp;</p>\n<p>Edit #1: If you're downvoting this article, I'd like to know why you're doing that. Send me a message or reply here.</p>\n<p>Edit #2: I totally misunderstood what the hypothetical apostasy means. I was under the impression that it meant defending a view that most people deem too weird to contemplate. See Lark's <a href=\"/lw/gor/whats_your_hypothetical_apostasy/8h3p\">explanation</a>. I guess you should downvote this article!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RAMvCGLiraZqwm72k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -11, "extendedScore": null, "score": 1.1139573982790574e-06, "legacy": true, "legacyId": "21627", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-16T12:19:09.985Z", "modifiedAt": null, "url": null, "title": "Help me refactor my life", "slug": "help-me-refactor-my-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:28.304Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MixedNuts", "createdAt": "2010-03-02T15:31:04.433Z", "isAdmin": false, "displayName": "MixedNuts"}, "userId": "zMvXdNMyznn73XSee", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZpTxXB9QWgPWFpLnf/help-me-refactor-my-life", "pageUrlRelative": "/posts/ZpTxXB9QWgPWFpLnf/help-me-refactor-my-life", "linkUrl": "https://www.lesswrong.com/posts/ZpTxXB9QWgPWFpLnf/help-me-refactor-my-life", "postedAtFormatted": "Saturday, February 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20me%20refactor%20my%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20me%20refactor%20my%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZpTxXB9QWgPWFpLnf%2Fhelp-me-refactor-my-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20me%20refactor%20my%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZpTxXB9QWgPWFpLnf%2Fhelp-me-refactor-my-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZpTxXB9QWgPWFpLnf%2Fhelp-me-refactor-my-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 335, "htmlBody": "<p>So I got stuck in a depressive rut once again and I'm making a fresh start. Give me advice!</p>\n<p>I'll move. My plan is to find a cheap place somewhere in France, but anywhere in the EU's good. Having like-minded roommates would be a plus. Anyone wish to bask in my glorious presence? I'm a very accommodating roommate (I've complained about exactly one thing in two years of rooming with various people, and never refused a request), respond well to nagging, tend to keep to myself unless prompted, don't have many irritating habits I can think of (maybe I use too much dish soap?), and am generally good with kids. And if you're a mentally odd person who could use a bit of live-in help, boy are you in luck!</p>\n<p>I'll poke around at jobs until I find one I can reliably handle. That means not having to focus too much and no heavy responsibilities. Working from home is best - programming or web dev or translation or editing. For meatspace jobs, maybe tutoring. Anyone have a bright idea? Anyone want me to normalise the tags in their music collection for a pittance?</p>\n<p>I'll try to develop better mental hygiene. That includes hounding psychiatrists, maintaining my network of friends (social life is the one thing I'm really successful at these days), cultivating new hobbies, and tinkering with useful habits. Anyone have deep wisdom, or an old lockpicking kit to send me?</p>\n<p>I'll make some long-term plans. Taking up my studies again and going for that cool engineering career is the obvious choice, but it might not be a reasonable goal by then. Anyone want to recruit me for seasteading or something?</p>\n<p>I have enough savings to last me for a while and my parents haven't cut me off yet, so I can afford a few failures before my life's in order again. Other than that I can't predict very much about how dependable my meds, any of my friends, my <a title=\"Limerent object!\" href=\"/user/Patrick/\">shiny new boyfriend</a>, or my own brain are.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZpTxXB9QWgPWFpLnf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 11, "extendedScore": null, "score": 1.113991582651271e-06, "legacy": true, "legacyId": "21628", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-17T06:28:16.668Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Three Worlds Decide (5/8)", "slug": "seq-rerun-three-worlds-decide-5-8", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.176Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tgq2HqBS5DE9rHrcc/seq-rerun-three-worlds-decide-5-8", "pageUrlRelative": "/posts/tgq2HqBS5DE9rHrcc/seq-rerun-three-worlds-decide-5-8", "linkUrl": "https://www.lesswrong.com/posts/tgq2HqBS5DE9rHrcc/seq-rerun-three-worlds-decide-5-8", "postedAtFormatted": "Sunday, February 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Three%20Worlds%20Decide%20(5%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Three%20Worlds%20Decide%20(5%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftgq2HqBS5DE9rHrcc%2Fseq-rerun-three-worlds-decide-5-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Three%20Worlds%20Decide%20(5%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftgq2HqBS5DE9rHrcc%2Fseq-rerun-three-worlds-decide-5-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftgq2HqBS5DE9rHrcc%2Fseq-rerun-three-worlds-decide-5-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<p>Today's post, <a href=\"/lw/y9/three_worlds_decide_58/\">Three Worlds Decide (5/8)</a> was originally published on 03 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Three_Worlds_Decide_.285.2F8.29\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The Superhappies propose a compromise.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/goi/seq_rerun_interlude_with_the_confessor/\">Interlude with the Confessor</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tgq2HqBS5DE9rHrcc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1146844713956038e-06, "legacy": true, "legacyId": "21636", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Z263n4TXJimKn6A8Z", "MG5TN4RYoaCKe795D", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-18T05:18:54.528Z", "modifiedAt": null, "url": null, "title": "Meetup : Shanghai Meetup Wednesday, 7:30", "slug": "meetup-shanghai-meetup-wednesday-7-30", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Barry_Cotter", "createdAt": "2010-04-19T16:29:03.629Z", "isAdmin": false, "displayName": "Barry_Cotter"}, "userId": "5pZXxaf79kj37Rwq2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mTuE4fgyLm2xogkQX/meetup-shanghai-meetup-wednesday-7-30", "pageUrlRelative": "/posts/mTuE4fgyLm2xogkQX/meetup-shanghai-meetup-wednesday-7-30", "linkUrl": "https://www.lesswrong.com/posts/mTuE4fgyLm2xogkQX/meetup-shanghai-meetup-wednesday-7-30", "postedAtFormatted": "Monday, February 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Shanghai%20Meetup%20Wednesday%2C%207%3A30&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Shanghai%20Meetup%20Wednesday%2C%207%3A30%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTuE4fgyLm2xogkQX%2Fmeetup-shanghai-meetup-wednesday-7-30%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Shanghai%20Meetup%20Wednesday%2C%207%3A30%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTuE4fgyLm2xogkQX%2Fmeetup-shanghai-meetup-wednesday-7-30", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTuE4fgyLm2xogkQX%2Fmeetup-shanghai-meetup-wednesday-7-30", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/jg\">Meetup Wednesday, 7:30</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">20 February 2013 01:07:30PM (+0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Salley Garden, 480 Yongjia Lu, near Yueyang Lu, Shanghai</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Salley Garden 480 Yongjia Lu (near Yueyang Lu), (map), at 7:30 p.m</p>\n<p>If someone else has a topic they want to talk about for a bit before we just talk, say it in the comments, otherwise I'll edit this next lunchtime with what we'll spend a while talking about before just chatting.</p>\n<p>Last time we had four people from the mailing list show up apart from me and a friend. There was good conversation and good food but unfortunately we didn't get to board games.</p>\n<p>All two times I've been there the food has been good. I hope to see some of you there.</p>\n<p>I look like <a href=\"http://www.okcupid.com/profile/Hibernian/photos\">this</a>, and will have a piece of paper with Shanghai Rationalists/ Less wrong on it.</p>\n<p>It can be hard to find so directions are below</p>\n<p><span style=\"font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa;\">It's exactly in the middle of the block&nbsp;</span><strong style=\"font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa;\">between Yueyang Lu &amp; Taiyuan Lu</strong><span style=\"font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa;\">, on the north side of Yongjia Lu. There's a Salley Garden sign at the entrance to the alleyway - head down there and it's on the left, next to Inferno.&nbsp;</span><a style=\"text-decoration: initial; color: #336699; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa;\" href=\"http://imgur.com/M5wKQ\">Here is what the entrance to the alleyway looks like</a><span style=\"font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa;\">. If you're coming from Yueyang Lu and you hit the police station, you've gone too far.</span></p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/jg\">Meetup Wednesday, 7:30</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mTuE4fgyLm2xogkQX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.1155575460270643e-06, "legacy": true, "legacyId": "21651", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup_Wednesday__7_30\">Discussion article for the meetup : <a href=\"/meetups/jg\">Meetup Wednesday, 7:30</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">20 February 2013 01:07:30PM (+0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Salley Garden, 480 Yongjia Lu, near Yueyang Lu, Shanghai</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Salley Garden 480 Yongjia Lu (near Yueyang Lu), (map), at 7:30 p.m</p>\n<p>If someone else has a topic they want to talk about for a bit before we just talk, say it in the comments, otherwise I'll edit this next lunchtime with what we'll spend a while talking about before just chatting.</p>\n<p>Last time we had four people from the mailing list show up apart from me and a friend. There was good conversation and good food but unfortunately we didn't get to board games.</p>\n<p>All two times I've been there the food has been good. I hope to see some of you there.</p>\n<p>I look like <a href=\"http://www.okcupid.com/profile/Hibernian/photos\">this</a>, and will have a piece of paper with Shanghai Rationalists/ Less wrong on it.</p>\n<p>It can be hard to find so directions are below</p>\n<p><span style=\"font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa;\">It's exactly in the middle of the block&nbsp;</span><strong style=\"font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa;\">between Yueyang Lu &amp; Taiyuan Lu</strong><span style=\"font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa;\">, on the north side of Yongjia Lu. There's a Salley Garden sign at the entrance to the alleyway - head down there and it's on the left, next to Inferno.&nbsp;</span><a style=\"text-decoration: initial; color: #336699; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa;\" href=\"http://imgur.com/M5wKQ\">Here is what the entrance to the alleyway looks like</a><span style=\"font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa;\">. If you're coming from Yueyang Lu and you hit the police station, you've gone too far.</span></p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Meetup_Wednesday__7_301\">Discussion article for the meetup : <a href=\"/meetups/jg\">Meetup Wednesday, 7:30</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup Wednesday, 7:30", "anchor": "Discussion_article_for_the_meetup___Meetup_Wednesday__7_30", "level": 1}, {"title": "Discussion article for the meetup : Meetup Wednesday, 7:30", "anchor": "Discussion_article_for_the_meetup___Meetup_Wednesday__7_301", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-18T08:03:11.043Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Normal Ending: Last Tears", "slug": "seq-rerun-normal-ending-last-tears", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:41.653Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LhnTZ4QsqAxp3bCkf/seq-rerun-normal-ending-last-tears", "pageUrlRelative": "/posts/LhnTZ4QsqAxp3bCkf/seq-rerun-normal-ending-last-tears", "linkUrl": "https://www.lesswrong.com/posts/LhnTZ4QsqAxp3bCkf/seq-rerun-normal-ending-last-tears", "postedAtFormatted": "Monday, February 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Normal%20Ending%3A%20Last%20Tears&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Normal%20Ending%3A%20Last%20Tears%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLhnTZ4QsqAxp3bCkf%2Fseq-rerun-normal-ending-last-tears%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Normal%20Ending%3A%20Last%20Tears%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLhnTZ4QsqAxp3bCkf%2Fseq-rerun-normal-ending-last-tears", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLhnTZ4QsqAxp3bCkf%2Fseq-rerun-normal-ending-last-tears", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<p>Today's post, <a href=\"/lw/ya/normal_ending_last_tears_68/\">Normal Ending: Last Tears (6/8)</a> was originally published on 04 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Normal_Ending:_Last_Tears_.286.2F8.29\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Humanity accepts the Superhappies' bargain.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gp0/seq_rerun_three_worlds_decide_58/\">Three Worlds Decide (5/8)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LhnTZ4QsqAxp3bCkf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.1156622684204895e-06, "legacy": true, "legacyId": "21655", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HWH46whexsoqR3yXk", "tgq2HqBS5DE9rHrcc", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-18T17:05:10.817Z", "modifiedAt": null, "url": null, "title": "Imitation is the Sincerest Form of Argument", "slug": "imitation-is-the-sincerest-form-of-argument", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.506Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "palladias", "createdAt": "2012-04-03T13:45:53.766Z", "isAdmin": false, "displayName": "palladias"}, "userId": "Bv2LXWzZf96WGpqJ5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KjdP2WjWng6skwbY7/imitation-is-the-sincerest-form-of-argument", "pageUrlRelative": "/posts/KjdP2WjWng6skwbY7/imitation-is-the-sincerest-form-of-argument", "linkUrl": "https://www.lesswrong.com/posts/KjdP2WjWng6skwbY7/imitation-is-the-sincerest-form-of-argument", "postedAtFormatted": "Monday, February 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Imitation%20is%20the%20Sincerest%20Form%20of%20Argument&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImitation%20is%20the%20Sincerest%20Form%20of%20Argument%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKjdP2WjWng6skwbY7%2Fimitation-is-the-sincerest-form-of-argument%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Imitation%20is%20the%20Sincerest%20Form%20of%20Argument%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKjdP2WjWng6skwbY7%2Fimitation-is-the-sincerest-form-of-argument", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKjdP2WjWng6skwbY7%2Fimitation-is-the-sincerest-form-of-argument", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 637, "htmlBody": "<p>I recently gave a talk at Chicago Ideas Week on adapting Turing Tests to have better, less mindkill-y arguments, and this is the precis for folks who would prefer not to sit through the video (<a href=\"http://vimeo.com/56932073\">which is available here</a>).</p>\n<p>Conventional Turing Tests check whether a programmer can build a convincing&nbsp;facsimile&nbsp;of a human conversationalist. &nbsp; The test has turned out to reveal less about machine intelligence than human intelligence. &nbsp;(Anger is really easy to fake, since fights can end up a little more Markov chain-y, where you only need to reply to the most recent rejoinder and can ignore what came before). &nbsp;Since normal Turing Tests made us think more about our model of human conversation, economist Bryan Caplan came up with a way to use them to make us think more usefully about our models of our <em>enemies</em>.</p>\n<p>After Paul Krugman disparaged Caplan's brand of libertarian economics, Caplan <a href=\"http://econlog.econlib.org/archives/2011/06/the_ideological.html\">challenged him to an <em>ideological</em>&nbsp;Turing Test</a>, where both players would be human, but would be trying to accurately imitate each other. &nbsp;Caplan and Krugman would each answer questions about their true beliefs honestly, and then would fill out the questionaire again <em>in persona inimici</em>&nbsp;- trying to guess the answers given by the other side. &nbsp;Caplan was willing to bet that he understood Krugman's position well enough to mimic it, but Krugman would be easily spotted as a fake!Caplan.</p>\n<p>Krugman didn't take him up on the offer, but I've <a href=\"http://www.patheos.com/blogs/unequallyyoked/ideological-turing-test-contest\">run a couple iterations of the test for my religion/philosophy blog</a>. &nbsp;The first year, some of the most interesting results were the proxy variables people were using, that weren't as strong as indicators as the judges thought. &nbsp;(One Catholic coasted through to victory as a faux atheist, since many of the atheist judges thought there was no way a&nbsp;Christian&nbsp;would appreciate the webcomic SMBC).</p>\n<p>The trouble was, the Christians did a lot better, since it turned out <a href=\"http://www.patheos.com/blogs/unequallyyoked/2011/06/final-turing-question-list.html\">I had written boring, easy to guess questions</a> for the true and faux atheists. &nbsp;The second year, <a href=\"http://www.patheos.com/blogs/unequallyyoked/2012/06/2012-ideological-turing-test-index-post.html\">I wrote weirder questions</a>, and the answers were a lot more diverse and surprising (and a number of the atheist participants called out each other as fakes or just plain wrong, since we'd gotten past the shallow questions from year one, and there's a lot of philosophical diversity within atheism).</p>\n<p>The exercise made people get curious about what it was their opponents actually thought and why. &nbsp;It helped people spot incorrect stereotypes of an opposing side and faultlines they'd been ignoring within their own. &nbsp;Personally, (and according to other participants) it helped me have an argument less <em>antagonistically</em>. &nbsp;Instead of just trying to find enough of a weak point to discomfit my opponent, I was trying to build up a model of how they thought, and I needed their help to do it. &nbsp;</p>\n<p>Taking a calm, inquisitive look at an opponent's&nbsp;position&nbsp;might teach me that my position is wrong, or has a gap I need to investigate. &nbsp;But even if my opponent is just as wrong as zer seemed, there's still a benefit to me. &nbsp;Having a really detailed, accurate model of zer position may help me show them why it's wrong, since now I can see exactly where it rasps against reality. &nbsp;And even if my conversation isn't helpful to them, it's interesting for me to see what they were missing. &nbsp;I may be correct in this particular argument, but the odds are good that I share the rationalist weak-point that is keeping them from noticing the error. &nbsp;I'd like to be able to see it more clearly so I can try and spot it in my own thought. &nbsp;(Think of this as the shift from \"How the <em>hell</em>&nbsp;can you be so dumb?!\" to \"How the hell <em>can</em>&nbsp;you be so dumb?\").</p>\n<p>When I get angry, I'm satisfied when I beat my interlocutor. &nbsp;When I get curious, I'm only satisfied when I learn something new.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"A4kr45wS7fBW5PBpf": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KjdP2WjWng6skwbY7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 92, "baseScore": 122, "extendedScore": null, "score": 0.000285, "legacy": true, "legacyId": "21657", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 122, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-18T20:03:56.667Z", "modifiedAt": null, "url": null, "title": "Giving What We Can September Internship ", "slug": "giving-what-we-can-september-internship", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:39.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Larks", "createdAt": "2009-04-28T20:21:45.860Z", "isAdmin": false, "displayName": "Larks"}, "userId": "jQXwiWxFcfyYjytXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mSDqrxRYCZqArbkSR/giving-what-we-can-september-internship", "pageUrlRelative": "/posts/mSDqrxRYCZqArbkSR/giving-what-we-can-september-internship", "linkUrl": "https://www.lesswrong.com/posts/mSDqrxRYCZqArbkSR/giving-what-we-can-september-internship", "postedAtFormatted": "Monday, February 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Giving%20What%20We%20Can%20September%20Internship%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGiving%20What%20We%20Can%20September%20Internship%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmSDqrxRYCZqArbkSR%2Fgiving-what-we-can-september-internship%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Giving%20What%20We%20Can%20September%20Internship%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmSDqrxRYCZqArbkSR%2Fgiving-what-we-can-september-internship", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmSDqrxRYCZqArbkSR%2Fgiving-what-we-can-september-internship", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 467, "htmlBody": "<p>Summary: advert for students to do charity cost-effectiveness research at Giving What We Can.</p>\n<p>&nbsp;</p>\n<p>Do you want to join the fight against global poverty and gain experience of research or communications at one of the world's handful of organisations dedicated to improving the world as efficiently as possible? <a href=\"http://www.givingwhatwecan.org/\"><span class=\"il\">Giving</span> <span class=\"il\">What</span> <span class=\"il\">We</span> <span class=\"il\">Can</span></a> is running a summer internship programme for students interested in promoting effective charitable <span class=\"il\">giving</span>. On the two-week programme (16th-27th September 2013) interns will gain training and experience in the area of their choice; either Cost-Effectiveness Research, Communications or Operations.</p>\n<p>The date is cunningly placed sufficiently late in the year that students can do an internship with another company and then come to us afterwards; this is what I did last year.</p>\n<p>The intership will take place in Oxford, UK. Housing and living expenses will be provided.</p>\n<p><strong>To apply</strong></p>\n<p>Please send us an email at <a href=\"mailto:internship@givingwhatwecn.org\" target=\"_blank\">internship@givingwhatwecan.org</a> with your CV. The deadline for applications is 12:00 GMT on the 20th March 2013.</p>\n<p>&nbsp;</p>\n<p><strong>Roles available</strong><strong>Research into charity cost-effectiveness</strong></p>\n<p><span class=\"il\">Giving</span> <span class=\"il\">What</span> <span class=\"il\">We</span> <span class=\"il\">Can</span> conducts research to help people find the most cost effective charities to donate to, lead by our director of research, <a href=\"http://www.overcomingbias.com/author/robwiblin\">Overcoming Bias co-blogger Robert Wiblin</a>. You <span class=\"il\">can</span> get a sense of the research <a href=\"http://www.givingwhatwecan.org/where-to-give/charity-evaluation\">here </a>and see a full list of current projects <a href=\"https://docs.google.com/document/d/1IDGXZg3yN56l6YVTAJ1NEs67Uxee-YcpesSW5WSElLM/edit\">here</a>. Some sample areas of interest are:</p>\n<p>&nbsp;&nbsp; Evaluations of how effective particular charities or programmes are.</p>\n<p>&nbsp;&nbsp; Comparing efforts to reduce climate change to other ways of assisting the world&rsquo;s poor.</p>\n<p>&nbsp;&nbsp; Biomedical research which could offer vaccines or cures for neglected diseases.&nbsp;</p>\n<p>&nbsp;&nbsp; Political Advocacy - how worthwhile is it to lobby for better government aid?</p>\n<p>Requirements: A quantitative background is strongly preferred, especially in statistics, mathematics and economics.</p>\n<p><strong> Communications: Media content creation</strong></p>\n<p>Creating infographics, videos and other materials to communicate our message about the power of <span class=\"il\">giving</span> and research on effective charity.</p>\n<p>Requirements: experience with appropriate software, such as vector graphics or video editing packages.</p>\n<p><strong>Communications: Outreach</strong></p>\n<p><strong></strong> Research and reach out to relevant groups, from organisations <span class=\"il\">we</span> could work with to websites and online communities where <span class=\"il\">we</span> could build a reputation and broaden our member base. This will require both research into the most appropriate and receptive places to contact, and establishing a rapport with them before suggesting that a partnership of sorts be made.<strong></strong></p>\n<p><strong>Communications: Online outreach</strong></p>\n<p>Work with our social media manager to plan and implement social media strategies, and research the most effective way to convey <span class=\"il\">Giving</span> <span class=\"il\">What</span> <span class=\"il\">We</span> <span class=\"il\">Can</span>&rsquo;s message to other online communities.</p>\n<p>Requirements: a good understanding of social media strategy and the dynamic of online communities.</p>\n<p><strong>Operations: Legal/financial research</strong></p>\n<p><strong></strong> <span class=\"il\">We</span> have many projects in this area, but an example of a major one is reporting on how our activities fit with formally recognised charitable purposes. This will involve working towards the reports <span class=\"il\">we</span> have to file with the Charities Commission. Other projects include registering us as a charity overseas.</p>\n<p>Requirements: Having studied law is helpful, but not required.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mSDqrxRYCZqArbkSR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "21658", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Summary: advert for students to do charity cost-effectiveness research at Giving What We Can.</p>\n<p>&nbsp;</p>\n<p>Do you want to join the fight against global poverty and gain experience of research or communications at one of the world's handful of organisations dedicated to improving the world as efficiently as possible? <a href=\"http://www.givingwhatwecan.org/\"><span class=\"il\">Giving</span> <span class=\"il\">What</span> <span class=\"il\">We</span> <span class=\"il\">Can</span></a> is running a summer internship programme for students interested in promoting effective charitable <span class=\"il\">giving</span>. On the two-week programme (16th-27th September 2013) interns will gain training and experience in the area of their choice; either Cost-Effectiveness Research, Communications or Operations.</p>\n<p>The date is cunningly placed sufficiently late in the year that students can do an internship with another company and then come to us afterwards; this is what I did last year.</p>\n<p>The intership will take place in Oxford, UK. Housing and living expenses will be provided.</p>\n<p><strong id=\"To_apply\">To apply</strong></p>\n<p>Please send us an email at <a href=\"mailto:internship@givingwhatwecn.org\" target=\"_blank\">internship@givingwhatwecan.org</a> with your CV. The deadline for applications is 12:00 GMT on the 20th March 2013.</p>\n<p>&nbsp;</p>\n<p><strong>Roles available</strong><strong>Research into charity cost-effectiveness</strong></p>\n<p><span class=\"il\">Giving</span> <span class=\"il\">What</span> <span class=\"il\">We</span> <span class=\"il\">Can</span> conducts research to help people find the most cost effective charities to donate to, lead by our director of research, <a href=\"http://www.overcomingbias.com/author/robwiblin\">Overcoming Bias co-blogger Robert Wiblin</a>. You <span class=\"il\">can</span> get a sense of the research <a href=\"http://www.givingwhatwecan.org/where-to-give/charity-evaluation\">here </a>and see a full list of current projects <a href=\"https://docs.google.com/document/d/1IDGXZg3yN56l6YVTAJ1NEs67Uxee-YcpesSW5WSElLM/edit\">here</a>. Some sample areas of interest are:</p>\n<p>&nbsp;&nbsp; Evaluations of how effective particular charities or programmes are.</p>\n<p>&nbsp;&nbsp; Comparing efforts to reduce climate change to other ways of assisting the world\u2019s poor.</p>\n<p>&nbsp;&nbsp; Biomedical research which could offer vaccines or cures for neglected diseases.&nbsp;</p>\n<p>&nbsp;&nbsp; Political Advocacy - how worthwhile is it to lobby for better government aid?</p>\n<p>Requirements: A quantitative background is strongly preferred, especially in statistics, mathematics and economics.</p>\n<p><strong id=\"Communications__Media_content_creation\"> Communications: Media content creation</strong></p>\n<p>Creating infographics, videos and other materials to communicate our message about the power of <span class=\"il\">giving</span> and research on effective charity.</p>\n<p>Requirements: experience with appropriate software, such as vector graphics or video editing packages.</p>\n<p><strong id=\"Communications__Outreach\">Communications: Outreach</strong></p>\n<p><strong></strong> Research and reach out to relevant groups, from organisations <span class=\"il\">we</span> could work with to websites and online communities where <span class=\"il\">we</span> could build a reputation and broaden our member base. This will require both research into the most appropriate and receptive places to contact, and establishing a rapport with them before suggesting that a partnership of sorts be made.<strong></strong></p>\n<p><strong id=\"Communications__Online_outreach\">Communications: Online outreach</strong></p>\n<p>Work with our social media manager to plan and implement social media strategies, and research the most effective way to convey <span class=\"il\">Giving</span> <span class=\"il\">What</span> <span class=\"il\">We</span> <span class=\"il\">Can</span>\u2019s message to other online communities.</p>\n<p>Requirements: a good understanding of social media strategy and the dynamic of online communities.</p>\n<p><strong id=\"Operations__Legal_financial_research\">Operations: Legal/financial research</strong></p>\n<p><strong></strong> <span class=\"il\">We</span> have many projects in this area, but an example of a major one is reporting on how our activities fit with formally recognised charitable purposes. This will involve working towards the reports <span class=\"il\">we</span> have to file with the Charities Commission. Other projects include registering us as a charity overseas.</p>\n<p>Requirements: Having studied law is helpful, but not required.</p>", "sections": [{"title": "To apply", "anchor": "To_apply", "level": 1}, {"title": "Communications: Media content creation", "anchor": "Communications__Media_content_creation", "level": 1}, {"title": "Communications: Outreach", "anchor": "Communications__Outreach", "level": 1}, {"title": "Communications: Online outreach", "anchor": "Communications__Online_outreach", "level": 1}, {"title": "Operations: Legal/financial research", "anchor": "Operations__Legal_financial_research", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-18T23:00:44.458Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.895Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JDfsHynmv7LWq2Lff/meetup-melbourne-practical-rationality", "pageUrlRelative": "/posts/JDfsHynmv7LWq2Lff/meetup-melbourne-practical-rationality", "linkUrl": "https://www.lesswrong.com/posts/JDfsHynmv7LWq2Lff/meetup-melbourne-practical-rationality", "postedAtFormatted": "Monday, February 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJDfsHynmv7LWq2Lff%2Fmeetup-melbourne-practical-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJDfsHynmv7LWq2Lff%2Fmeetup-melbourne-practical-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJDfsHynmv7LWq2Lff%2Fmeetup-melbourne-practical-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jh'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 March 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 Walsh Street, West Melbourne VIC 3003, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jh'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JDfsHynmv7LWq2Lff", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.1162347518290858e-06, "legacy": true, "legacyId": "21660", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/jh\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 March 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 Walsh Street, West Melbourne VIC 3003, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/jh\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-18T23:36:04.849Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Show and tell meetup: Economics II", "slug": "meetup-washington-dc-show-and-tell-meetup-economics-ii", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M92H55qYKtXRsqzsR/meetup-washington-dc-show-and-tell-meetup-economics-ii", "pageUrlRelative": "/posts/M92H55qYKtXRsqzsR/meetup-washington-dc-show-and-tell-meetup-economics-ii", "linkUrl": "https://www.lesswrong.com/posts/M92H55qYKtXRsqzsR/meetup-washington-dc-show-and-tell-meetup-economics-ii", "postedAtFormatted": "Monday, February 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Show%20and%20tell%20meetup%3A%20Economics%20II&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Show%20and%20tell%20meetup%3A%20Economics%20II%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM92H55qYKtXRsqzsR%2Fmeetup-washington-dc-show-and-tell-meetup-economics-ii%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Show%20and%20tell%20meetup%3A%20Economics%20II%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM92H55qYKtXRsqzsR%2Fmeetup-washington-dc-show-and-tell-meetup-economics-ii", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM92H55qYKtXRsqzsR%2Fmeetup-washington-dc-show-and-tell-meetup-economics-ii", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ji'>Washington DC Show and tell meetup: Economics II</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 February 2013 03:00:47PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA (courtyard)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Richard will be leading a discussion about economics. Please bring questions!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ji'>Washington DC Show and tell meetup: Economics II</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M92H55qYKtXRsqzsR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.116257303041984e-06, "legacy": true, "legacyId": "21661", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics_II\">Discussion article for the meetup : <a href=\"/meetups/ji\">Washington DC Show and tell meetup: Economics II</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 February 2013 03:00:47PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA (courtyard)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Richard will be leading a discussion about economics. Please bring questions!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics_II1\">Discussion article for the meetup : <a href=\"/meetups/ji\">Washington DC Show and tell meetup: Economics II</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Show and tell meetup: Economics II", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics_II", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Show and tell meetup: Economics II", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics_II1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-19T02:24:41.584Z", "modifiedAt": null, "url": null, "title": "Falsifiable and non-Falsifiable Ideas", "slug": "falsifiable-and-non-falsifiable-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:54.178Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shaih", "createdAt": "2012-07-11T20:03:10.897Z", "isAdmin": false, "displayName": "shaih"}, "userId": "4M2XmGZC3bGSP9QsZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aBsM3q9mtq5KfSJBz/falsifiable-and-non-falsifiable-ideas", "pageUrlRelative": "/posts/aBsM3q9mtq5KfSJBz/falsifiable-and-non-falsifiable-ideas", "linkUrl": "https://www.lesswrong.com/posts/aBsM3q9mtq5KfSJBz/falsifiable-and-non-falsifiable-ideas", "postedAtFormatted": "Tuesday, February 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Falsifiable%20and%20non-Falsifiable%20Ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFalsifiable%20and%20non-Falsifiable%20Ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaBsM3q9mtq5KfSJBz%2Ffalsifiable-and-non-falsifiable-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Falsifiable%20and%20non-Falsifiable%20Ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaBsM3q9mtq5KfSJBz%2Ffalsifiable-and-non-falsifiable-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaBsM3q9mtq5KfSJBz%2Ffalsifiable-and-non-falsifiable-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 771, "htmlBody": "<p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial, sans-serif;\"><br /> I have been talking to some people (few specific people I thought would benefit and appreciate it) in my dorm and teaching them rationality. I have been thinking of skills that should be taught first and it made me think about what skill is most important to me as a rationalist.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">I decided to start with the question &ldquo;What does it mean to be able to test something with an experiment?&rdquo; which could also mean &ldquo;What does it mean to be falsifiable?&rdquo;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">To help my point I brought up the thought experiment with a<span>&nbsp;</span><a href=\"http://www.godlessgeeks.com/LINKS/Dragon.htm\" target=\"_blank\">dragon in Carl Sagan&rsquo;s garage</a></span><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\">&nbsp;which is as follows</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-family: Arial, sans-serif;\">Carl: There is a dragon in my garage</span><br /> <span style=\"font-family: Verdana, sans-serif;\">Me: I thought dragons only existed in legends and I want to see for myself</span><br /> <span style=\"font-family: Verdana, sans-serif;\">Carl: Sure follow me and have a look</span><br /> <span style=\"font-family: Verdana, sans-serif;\">Me: I don&rsquo;t see a dragon in there</span><br /> <span style=\"font-family: Verdana, sans-serif;\">Carl: My dragon is invisible</span><br /> <span style=\"font-family: Verdana, sans-serif;\">Me: Let me throw some flour in so I can see where the dragon is by the disruption of the flour<span>&nbsp;</span></span><br /> <span style=\"font-family: Verdana, sans-serif;\">Carl: My dragon is incorporeal</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">And so on</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">The answer that I was trying to bring about was along the lines that if something could be tested by an experiment then it must have at least one different effect if it were true than if it were false. Further if something had at least one effect different if it were true than if it was false then I could at least in theory test it with an experiment.</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">This led me to the statement:<br /> If something cannot at least in theory be tested by experiment then it has no effect on the world and lacks meaning from a truth stand point therefore rational standpoint.</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">Anthony (the person I was talking to at the time) started his counter argument with any object in a thought experiment cannot be tested for but still has a meaning.</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">So I revised my statement any object that if brought into the real world cannot be tested for has no meaning. Under the assumption that if an object could not be tested for in the real world it also has no effect on anything in the thought experiment. i.e. the story with the dragon would have gone the same way independent of its truth values if it were in the real world.</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">Then the discussion continued into could it be rational to have a belief that could not even in theory be tested. It became interesting when Anthony gave the argument that if believing in a dragon in your garage gave you happiness and the world would be the same either way besides the happiness combined with the principle that rationality is the art of systematized winning it is clearly rational to believe in the dragon.</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">I responded with truth trumps happiness and believing the dragon would force you to believe the false belief which is not worth the amount of happiness received by believing it. Even further I argued that it would in fact be a false belief because p(world) &gt; p(world)p(impermeable invisible dragon) which is a simple occum&rsquo;s razor argument.</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">My intended direction for this argument with Anthony from this point was to apply these points to theology but we ran out of time and we have not had time again to talk so that may be a future post.</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">Today however<span>&nbsp;</span><a href=\"/lw/do9/welcome_to_less_wrong_july_2012/8hgu\" target=\"_blank\">Shminux</a></span><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\">&nbsp;pointed out to me that I held beliefs that were themselves non-falsifiable. I realized then that it might be rational to believe non-falsifiable things for two reasons (I&rsquo;m sure there&rsquo;s more but these are the main one&rsquo;s I can think of please comment your own)</span></span></p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in; text-indent: -.25in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-family: Arial, sans-serif;\">1)<span style=\"font-size: 7pt; line-height: 115%;\">&nbsp;&nbsp;<span>&nbsp;</span></span></span></span><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">The belief has a beauty to it that flows with falsifiable beliefs and makes known facts fit more perfectly. (this is very dangerous and should not be used lightly because it focuses to closely on opinion)</span></span></p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in; text-indent: -.25in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-family: Arial, sans-serif;\">2)<span style=\"font-size: 7pt; line-height: 115%;\">&nbsp;&nbsp;<span>&nbsp;</span></span></span></span><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">You believe that the belief will someday allow you to make an original theory which will be falsifiable.</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">Both of these reasons if not used very carefully will allow false beliefs. As such I myself decided that if a belief or new theory sufficiently meets these conditions enough to make me want to believe them I should put them into a special category of my thoughts (perhaps conjectures). &nbsp;This category should be below beliefs in power but still held as how the world works and anything in this category should always strive to leave it, meaning that I should always strive to make any non-falsifiable conjecture no longer be a conjecture through making it a belief or disproving it.&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial, sans-serif;\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial, sans-serif;\">Note: This is my first post so as well as discussing the post, critiques simply to the writing are deeply welcomed in PM to me.&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial, sans-serif; background-position: initial initial; background-repeat: initial initial;\">&nbsp;</span></p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aBsM3q9mtq5KfSJBz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -3, "extendedScore": null, "score": 1.1163649098362588e-06, "legacy": true, "legacyId": "21662", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-19T04:38:05.848Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] True Ending: Sacrificial Fire", "slug": "seq-rerun-true-ending-sacrificial-fire", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZWZxz2Ddkpvskf5RP/seq-rerun-true-ending-sacrificial-fire", "pageUrlRelative": "/posts/ZWZxz2Ddkpvskf5RP/seq-rerun-true-ending-sacrificial-fire", "linkUrl": "https://www.lesswrong.com/posts/ZWZxz2Ddkpvskf5RP/seq-rerun-true-ending-sacrificial-fire", "postedAtFormatted": "Tuesday, February 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20True%20Ending%3A%20Sacrificial%20Fire&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20True%20Ending%3A%20Sacrificial%20Fire%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZWZxz2Ddkpvskf5RP%2Fseq-rerun-true-ending-sacrificial-fire%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20True%20Ending%3A%20Sacrificial%20Fire%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZWZxz2Ddkpvskf5RP%2Fseq-rerun-true-ending-sacrificial-fire", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZWZxz2Ddkpvskf5RP%2Fseq-rerun-true-ending-sacrificial-fire", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"/lw/yb/true_ending_sacrificial_fire_78/\">True Ending: Sacrificial Fire (7/8)</a> was originally published on 05 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#True_Ending:_Sacrificial_Fire_.287.2F8.29\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The Impossible Possible World tries to save humanity.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gpj/seq_rerun_normal_ending_last_tears/\">Normal Ending: Last Tears</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZWZxz2Ddkpvskf5RP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1164500603059802e-06, "legacy": true, "legacyId": "21663", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6Ls6f5PerERJmsTGB", "LhnTZ4QsqAxp3bCkf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-19T04:42:56.114Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver!", "slug": "meetup-vancouver-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d5vHLXPL5gSSEA4tc/meetup-vancouver-0", "pageUrlRelative": "/posts/d5vHLXPL5gSSEA4tc/meetup-vancouver-0", "linkUrl": "https://www.lesswrong.com/posts/d5vHLXPL5gSSEA4tc/meetup-vancouver-0", "postedAtFormatted": "Tuesday, February 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd5vHLXPL5gSSEA4tc%2Fmeetup-vancouver-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd5vHLXPL5gSSEA4tc%2Fmeetup-vancouver-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd5vHLXPL5gSSEA4tc%2Fmeetup-vancouver-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jj'>Vancouver!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 February 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 west broadway, vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our recent format has been working well, so we'll continue. Please come out and join us at 2505 west broadway (Benny's Bagels) at 15:00 on Saturday.</p>\n\n<p>Topic is undetermined; please suggest things. If we don't come up with a better one, I'll talk about <a href=\"http://lesswrong.com/lw/9nm/terminal_bias/\">Terminal Bias</a> and Moral Philosophy and it will be awful and confused and boring and attract exactly the wrong type of people. So there.</p>\n\n<p>Lurkers who don't know us, look for some geeks with with lots of paper and phones and computers. I'll try to have a sign with \"Less Wrong\" on it. I guess I should post my face all over the internet for you eh?</p>\n\n<p>Hope to see you all there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jj'>Vancouver!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d5vHLXPL5gSSEA4tc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.1164531483909297e-06, "legacy": true, "legacyId": "21664", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_\">Discussion article for the meetup : <a href=\"/meetups/jj\">Vancouver!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 February 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 west broadway, vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our recent format has been working well, so we'll continue. Please come out and join us at 2505 west broadway (Benny's Bagels) at 15:00 on Saturday.</p>\n\n<p>Topic is undetermined; please suggest things. If we don't come up with a better one, I'll talk about <a href=\"http://lesswrong.com/lw/9nm/terminal_bias/\">Terminal Bias</a> and Moral Philosophy and it will be awful and confused and boring and attract exactly the wrong type of people. So there.</p>\n\n<p>Lurkers who don't know us, look for some geeks with with lots of paper and phones and computers. I'll try to have a sign with \"Less Wrong\" on it. I guess I should post my face all over the internet for you eh?</p>\n\n<p>Hope to see you all there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_1\">Discussion article for the meetup : <a href=\"/meetups/jj\">Vancouver!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver!", "anchor": "Discussion_article_for_the_meetup___Vancouver_", "level": 1}, {"title": "Discussion article for the meetup : Vancouver!", "anchor": "Discussion_article_for_the_meetup___Vancouver_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QGKFjaZNDtJnBTbxS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-19T06:10:53.949Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin Meetup", "slug": "meetup-berlin-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8sYDxoBs7McEPe8uC/meetup-berlin-meetup-0", "pageUrlRelative": "/posts/8sYDxoBs7McEPe8uC/meetup-berlin-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/8sYDxoBs7McEPe8uC/meetup-berlin-meetup-0", "postedAtFormatted": "Tuesday, February 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8sYDxoBs7McEPe8uC%2Fmeetup-berlin-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8sYDxoBs7McEPe8uC%2Fmeetup-berlin-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8sYDxoBs7McEPe8uC%2Fmeetup-berlin-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jk'>Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 February 2013 07:30:47PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ming Dynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is where we come together in person to chat!</p>\n\n<p>We will experiment with emulating a small meetup by having two tables, hopefully leading to more personal discussions. There are no special activities this time. We'll talk about our plans and make commitments.</p>\n\n<p>Everyone is welcome, I'll bring a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jk'>Berlin Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8sYDxoBs7McEPe8uC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.1165093011718683e-06, "legacy": true, "legacyId": "21671", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup\">Discussion article for the meetup : <a href=\"/meetups/jk\">Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 February 2013 07:30:47PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ming Dynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is where we come together in person to chat!</p>\n\n<p>We will experiment with emulating a small meetup by having two tables, hopefully leading to more personal discussions. There are no special activities this time. We'll talk about our plans and make commitments.</p>\n\n<p>Everyone is welcome, I'll bring a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/jk\">Berlin Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-19T09:51:49.419Z", "modifiedAt": null, "url": null, "title": "Are coin flips quantum random to my conscious brain-parts?", "slug": "are-coin-flips-quantum-random-to-my-conscious-brain-parts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:01.609Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9KW7oLQepcgu4LkFN/are-coin-flips-quantum-random-to-my-conscious-brain-parts", "pageUrlRelative": "/posts/9KW7oLQepcgu4LkFN/are-coin-flips-quantum-random-to-my-conscious-brain-parts", "linkUrl": "https://www.lesswrong.com/posts/9KW7oLQepcgu4LkFN/are-coin-flips-quantum-random-to-my-conscious-brain-parts", "postedAtFormatted": "Tuesday, February 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20coin%20flips%20quantum%20random%20to%20my%20conscious%20brain-parts%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20coin%20flips%20quantum%20random%20to%20my%20conscious%20brain-parts%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9KW7oLQepcgu4LkFN%2Fare-coin-flips-quantum-random-to-my-conscious-brain-parts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20coin%20flips%20quantum%20random%20to%20my%20conscious%20brain-parts%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9KW7oLQepcgu4LkFN%2Fare-coin-flips-quantum-random-to-my-conscious-brain-parts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9KW7oLQepcgu4LkFN%2Fare-coin-flips-quantum-random-to-my-conscious-brain-parts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 582, "htmlBody": "<p>Hello rationality friends! &nbsp;I have a question that I bet some of you have thought about...</p>\n<p>I hear lots of people saying that classical coin flips are not \"quantum random events\", because the outcome is very nearly determined by thumb movement when I flip the coin. &nbsp;More precisely, one can stay that the state of my thumb and the state of the landed coin are strongly entangled, such that, say, 99% of the quantum measure of the coin flips outcomes my post-flip thumb observes all land heads.</p>\n<p>First of all, I've never actually seen an order of magnitude estimate to support this claim, and would love it if someone here can provide or link to one!</p>\n<p>Second, I'm not sure how strongly entangled my thumb movement is with my subjective experience, i.e., with the parts of my brain that consciously process the decision to flip and the outcome. &nbsp;So even if the coin outcome is almost perfectly determined by my thumb, it might not be almost perfectly determined by my decision to flip the coin.</p>\n<p>For example, while the thumb movement happens, a lot of calibration goes on between my thumb, my motor cortex, and my cerebellum (which certainly affects but does not seem to directly process conscious experience), precisely because my motor cortex is unable to send, on its own, a precise and accurate enough signal to my thumb that achieves the flicking motion that we eventually learn to do in order to flip coins. &nbsp;Some of this inability is due to small differences in environmental factors during each flip that the motor cortex does not itself process directly, but is processed by the cerebellum instead. &nbsp;Perhaps some of this inability also comes directly from quantum variation in neuron action potentials being reached, or perhaps some of the aforementioned environmental factors arise from quantum variation.</p>\n<p>Anyway, I'm altogether not *that* convinced that the outcome of a coin flip is sufficiently dependent on my decision to flip as to be considered \"not a quantum random event\" by my conscious brain. &nbsp;Can anyone provide me with some order of magnitude estimates to convince me either way about this? &nbsp;I'd really appreciate it!</p>\n<p><strong>ETA:</strong> I am not asking if coin flips are \"random enough\" in some strange, undefined sense. &nbsp;I am <em>actually</em> asking about quantum entanglement here. In particular, when your PFC decides for planning reasons to flip a coin, does the evolution of the wave function produce a world that is in a superposition of states (coin landed heads)&otimes;(you observed heads) + (coin landed tails)&otimes;(you observed tails)? &nbsp;Or does a monomial state result, either (coin landed heads)&otimes;(you observed heads) or (coin landed tails)&otimes;(you observed tails) depending on the instance?</p>\n<p>At present, despite having been told many times that coin flips are not \"in superpositions\" relative to \"us\", I'm not convinced that there is enough mutual information connecting my frontal lobe and the coin for the state of the coin to be entangled with me (i.e. not \"in a superposed state\") before I observe it. I realize this is somewhat testable, e.g., if the state amplitudes of the coin can be <em>forced to have complex arguments differing in a predictable way so as to produce expected and measurable interference patterns</em>. This is what we have failed to produce at a macroscopic level in attempts to produce visible superpositions. &nbsp;But I don't know if we fail to produce messier, less-visibly-self-interfering superpositions, which is why I am still wondering about this...</p>\n<p>Any help / links / fermi estimates on this will be greatly appreciated!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9KW7oLQepcgu4LkFN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 1.1166503531681187e-06, "legacy": true, "legacyId": "21674", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-19T17:16:47.288Z", "modifiedAt": null, "url": null, "title": "Meetup : Effective Altruism Dinner Party", "slug": "meetup-effective-altruism-dinner-party", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S6whvhfdChvWp7jDk/meetup-effective-altruism-dinner-party", "pageUrlRelative": "/posts/S6whvhfdChvWp7jDk/meetup-effective-altruism-dinner-party", "linkUrl": "https://www.lesswrong.com/posts/S6whvhfdChvWp7jDk/meetup-effective-altruism-dinner-party", "postedAtFormatted": "Tuesday, February 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Effective%20Altruism%20Dinner%20Party&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Effective%20Altruism%20Dinner%20Party%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS6whvhfdChvWp7jDk%2Fmeetup-effective-altruism-dinner-party%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Effective%20Altruism%20Dinner%20Party%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS6whvhfdChvWp7jDk%2Fmeetup-effective-altruism-dinner-party", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS6whvhfdChvWp7jDk%2Fmeetup-effective-altruism-dinner-party", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 334, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jl'>Effective Altruism Dinner Party</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 February 2013 02:13:22PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Winterfell House, 316 W 138th street, New York NY, 10030</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Friday at Winterfell House, NY will be an Effective Altruism dinner party.\nEffective Altruism is the principle of accomplishing the most good on a global (or universal) scale, given limited time and resources. It can be worth thinking about whether you're the sort of person deeply motivated to dedicate their life to improving the world, or someone who's just trying to do more good on the margins without stressing out too much about it, or anyone in between.\nOne issue Effective Altruists tend to face is that accomplishing the most good tends to engage the hard, mathematical sides of our brains, and less the warm, empathetic side. Of course, in the rationality community, people tend to enjoy thinking about complex, strategic problems in rigorous ways. But it's still nice to periodically get to feel that warm glow. We know we probably want to purchase our Utilons and Fuzzies separately, but it's useful to appreicate a direct connection between the warm motivation and the hard thinking that goes into accomplishing it.\nThe evening will be an informal potluck dinner, where people can share past success stories and lessons learned, and talk about projects to collaborate on. Newcomers who are interested in the idea and trying to figure out how to go about having an impact can get familiar with some of the ideas behind the Effective Altruism community. I'll be talking about some potential projects for the oncoming year, in particular, an EA Camp being run by CFAR later in the summer.\nBring some food to share. Winterfell has a nice kitchen if you'd like to do some cooking there. People can arrive and do cooking starting around 7:30, with dinner being served at 8:30. -\nWHEN + WHERE:\nWinterfell House\nFriday, February 22nd\n7:30 PM - 10:00 PM</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jl'>Effective Altruism Dinner Party</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S6whvhfdChvWp7jDk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "21675", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Effective_Altruism_Dinner_Party\">Discussion article for the meetup : <a href=\"/meetups/jl\">Effective Altruism Dinner Party</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 February 2013 02:13:22PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Winterfell House, 316 W 138th street, New York NY, 10030</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Friday at Winterfell House, NY will be an Effective Altruism dinner party.\nEffective Altruism is the principle of accomplishing the most good on a global (or universal) scale, given limited time and resources. It can be worth thinking about whether you're the sort of person deeply motivated to dedicate their life to improving the world, or someone who's just trying to do more good on the margins without stressing out too much about it, or anyone in between.\nOne issue Effective Altruists tend to face is that accomplishing the most good tends to engage the hard, mathematical sides of our brains, and less the warm, empathetic side. Of course, in the rationality community, people tend to enjoy thinking about complex, strategic problems in rigorous ways. But it's still nice to periodically get to feel that warm glow. We know we probably want to purchase our Utilons and Fuzzies separately, but it's useful to appreicate a direct connection between the warm motivation and the hard thinking that goes into accomplishing it.\nThe evening will be an informal potluck dinner, where people can share past success stories and lessons learned, and talk about projects to collaborate on. Newcomers who are interested in the idea and trying to figure out how to go about having an impact can get familiar with some of the ideas behind the Effective Altruism community. I'll be talking about some potential projects for the oncoming year, in particular, an EA Camp being run by CFAR later in the summer.\nBring some food to share. Winterfell has a nice kitchen if you'd like to do some cooking there. People can arrive and do cooking starting around 7:30, with dinner being served at 8:30. -\nWHEN + WHERE:\nWinterfell House\nFriday, February 22nd\n7:30 PM - 10:00 PM</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Effective_Altruism_Dinner_Party1\">Discussion article for the meetup : <a href=\"/meetups/jl\">Effective Altruism Dinner Party</a></h2>", "sections": [{"title": "Discussion article for the meetup : Effective Altruism Dinner Party", "anchor": "Discussion_article_for_the_meetup___Effective_Altruism_Dinner_Party", "level": 1}, {"title": "Discussion article for the meetup : Effective Altruism Dinner Party", "anchor": "Discussion_article_for_the_meetup___Effective_Altruism_Dinner_Party1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-19T17:59:21.066Z", "modifiedAt": null, "url": null, "title": "Searching for consequence-imagining games for children", "slug": "searching-for-consequence-imagining-games-for-children", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.408Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ytNR2cG5LdnQTWmEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8WSM7AKT7MtP5m3iK/searching-for-consequence-imagining-games-for-children", "pageUrlRelative": "/posts/8WSM7AKT7MtP5m3iK/searching-for-consequence-imagining-games-for-children", "linkUrl": "https://www.lesswrong.com/posts/8WSM7AKT7MtP5m3iK/searching-for-consequence-imagining-games-for-children", "postedAtFormatted": "Tuesday, February 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Searching%20for%20consequence-imagining%20games%20for%20children&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASearching%20for%20consequence-imagining%20games%20for%20children%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8WSM7AKT7MtP5m3iK%2Fsearching-for-consequence-imagining-games-for-children%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Searching%20for%20consequence-imagining%20games%20for%20children%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8WSM7AKT7MtP5m3iK%2Fsearching-for-consequence-imagining-games-for-children", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8WSM7AKT7MtP5m3iK%2Fsearching-for-consequence-imagining-games-for-children", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<p>A friend of mine has a rather precocious daughter with poor impulse control, and asked if I knew any behavior games that encourage children to think out the consequences of actions before they do them.</p>\n<p>I'm familiar with the <a href=\"http://en.wikipedia.org/wiki/Good_Behavior_Game\">Good Behavior Game</a> and the like, but standard conditioning hasn't been very effective with this child in the past. She's quite clever about subverting rules when possible, and shutting down entirely when subversion fails.</p>\n<p>Please, one suggestion per thread so that the karma thing can do its thing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8WSM7AKT7MtP5m3iK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 1.116961732831081e-06, "legacy": true, "legacyId": "21676", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-19T20:35:52.631Z", "modifiedAt": null, "url": null, "title": "Politics Filtering and Higher Standard Discussions", "slug": "politics-filtering-and-higher-standard-discussions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:59.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ikrase", "createdAt": "2012-03-31T09:55:44.760Z", "isAdmin": false, "displayName": "ikrase"}, "userId": "jAwAjDxfNFqnkCzJC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yeWWEbMPFamiccjNu/politics-filtering-and-higher-standard-discussions", "pageUrlRelative": "/posts/yeWWEbMPFamiccjNu/politics-filtering-and-higher-standard-discussions", "linkUrl": "https://www.lesswrong.com/posts/yeWWEbMPFamiccjNu/politics-filtering-and-higher-standard-discussions", "postedAtFormatted": "Tuesday, February 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Politics%20Filtering%20and%20Higher%20Standard%20Discussions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolitics%20Filtering%20and%20Higher%20Standard%20Discussions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeWWEbMPFamiccjNu%2Fpolitics-filtering-and-higher-standard-discussions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Politics%20Filtering%20and%20Higher%20Standard%20Discussions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeWWEbMPFamiccjNu%2Fpolitics-filtering-and-higher-standard-discussions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeWWEbMPFamiccjNu%2Fpolitics-filtering-and-higher-standard-discussions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<p>Does anybody have suggestions of a method to hold a risky political discussion while &nbsp;excluding mindkilled dialogue and naively cynical accusations? I know that Moldbug uses a deliberately obscure writing style as a kind of barrier to entry, but that only works for things like his blog, not a conversational dialogue, and I would prefer to avoid obscurity. I'm mostly interested in avoiding people pattern-matching to the monster of the week, but I am concerned with attracting the merely regressive.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Edit: If you think you have guessed what specific subject I am talking about, keep it to yourself or use private messaging</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yeWWEbMPFamiccjNu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -1, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "21677", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-19T22:21:28.672Z", "modifiedAt": null, "url": null, "title": "Visual Mental Imagery Training", "slug": "visual-mental-imagery-training", "viewCount": null, "lastCommentedAt": "2019-10-30T04:41:10.225Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GuySrinivasan", "createdAt": "2009-02-27T21:00:32.986Z", "isAdmin": false, "displayName": "GuySrinivasan"}, "userId": "HMnfd9HdRCfuRcdBG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8ciFqEjkekqzaTqT6/visual-mental-imagery-training", "pageUrlRelative": "/posts/8ciFqEjkekqzaTqT6/visual-mental-imagery-training", "linkUrl": "https://www.lesswrong.com/posts/8ciFqEjkekqzaTqT6/visual-mental-imagery-training", "postedAtFormatted": "Tuesday, February 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Visual%20Mental%20Imagery%20Training&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVisual%20Mental%20Imagery%20Training%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ciFqEjkekqzaTqT6%2Fvisual-mental-imagery-training%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Visual%20Mental%20Imagery%20Training%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ciFqEjkekqzaTqT6%2Fvisual-mental-imagery-training", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ciFqEjkekqzaTqT6%2Fvisual-mental-imagery-training", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1140, "htmlBody": "<p>Previously: <a href=\"/lw/dr/generalizing_from_one_example/\">Generalizing From One Example</a></p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">There was a debate, in the late 1800s, about whether \"imagination\" was simply a turn of phrase or a real phenomenon. That is, can people actually create images in their minds which they see vividly, or do they simply say \"I saw it in my mind\" as a metaphor for considering what it looked like?</span><br /><br style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" /><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Upon hearing this, my response was \"How the stars was this actually a real debate? Of course we have mental imagery. Anyone who doesn't think we have mental imagery is either such a fanatical Behaviorist that she doubts the evidence of her own senses, or simply insane.\" Unfortunately, the professor was able to parade a long list of famous people who denied mental imagery, including some leading scientists of the era. And this was all before Behaviorism even existed.</span><br style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" /><br style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" /><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The debate was resolved by Francis Galton, a fascinating man who among other achievements invented eugenics, the \"wisdom of crowds\", and standard deviation. Galton gave people some very detailed surveys, and found that some people did have mental imagery and others didn't. The ones who did had simply assumed everyone did, and the ones who didn't had simply assumed everyone didn't, to the point of coming up with absurd justifications for why they were lying or misunderstanding the question. There was a wide spectrum of imaging ability, from about five percent of people with perfect eidetic imagery&nbsp;to three percent of people completely unable to form mental images.</span></p>\n</blockquote>\n<p><strong>Summary</strong>: I do not have visual mental imagery. I want it. How do I get it? What exercises, if any, will help?</p>\n<p>&nbsp;</p>\n<hr />\n<p>In further detail... Here's <a href=\"http://psychclassics.yorku.ca/Galton/imagery.htm\">Francis Galton's Statistics of Mental Imagery</a> paper. I'm not quite at the 3% level of completely unable to form mental images, but I'm close. In particular there are three times I have vivid, sharp mental imagery, and the existence of such times tells me I have the brain hardware to visualize. It's enough to let me know that <em>I want it all the time</em>. Unfortunately I don't know how to get it. And searching online has proven difficult and frustrating... for example <a href=\"http://www.creative-wealthbuilding.com/how-to-visualize.html\">this article</a> is first of all about a different meaning of \"visualize\", it's talking about some kind of self-help motivational thingy, and second of all it starts by saying \"How to Visualize: I want you to relax and close your eyes. Picture a hot, sunny day at the beach.\"</p>\n<p>Full Stop. Halt, Catch Fire and Burn.</p>\n<p>That's already too far. For those of us who don't visualize, practice definitely does <em>not</em> consist of pulling up mental images, playing with them in new ways, and expanding our imagination. I'm very good at imagination in some ways, but I lack that first ability to pull up a mental image. That's what I want to learn how to have!</p>\n<p>&nbsp;</p>\n<hr />\n<p>Here is a description of what I can do, what I have tried, what I have learned, etc.</p>\n<p>I see vivid visual mental imagery in 3 situations:</p>\n<ol>\n<li>While dreaming. My recollection of dreams has that I see fairly vivid, sharp, whole-scene imagery.</li>\n<li>Just before sleep. When I am in a certain almost-sleeping state, I can tell my mind to picture something - like an apple, or a horse - and I will often be able to see that thing vividly, briefly, and then it morphs into a scene. A beach with an ocean, or a pleasant clearing in a forest. If I try to alter the scene, like putting a beach towel and umbrella on the beach, the scene changes and morphs in some way but seemingly without regard to the changes I requested. Maybe my POV starts moving forward down a newly created path in the forest, for example.</li>\n<li>During meditation. Sometimes I feel like I'm in exactly the same mental state during meditation as I am just before sleep, except without the tiredness. The imagery has the same characteristics in both situations.</li>\n</ol>\n<div>I have tried 3 classes of practice:</div>\n<div><ol>\n<li>Staying in visualization situations. When I find myself in the just before sleep or meditation state, I stay there for a while and play with imagery. This is fun but I have seen no increase in control over what I visualize and no increase in the range of states in which I can visualize.</li>\n<li>Explicit imagery practice. I have found or drawn simple shapes, like a square or a ball, then stared at the shape, closed my eyes, seen the shape for as long as it stayed visualizable, opened my eyes to refresh, repeat. This straight up hasn't worked at all. I don't visualize it, only have the afterimage, and need to refresh within about a second.</li>\n<li>Object drawing. I have had 3D constructions of blocks and tried drawing them from different angles on paper. This is an exercise I did while growing up during summers. Unfortunately there was no actual imagery or mental rotation involved, I just logic'd out where lines must surely go and drew like that.</li>\n</ol>\n<div>Here is what my mental imagery looks like, per Francis Galton's questions.</div>\n<div><ol>\n<li>Picturing breakfast. The image is extraordinarily dim, extraordinarily ill-defined (at most an edge or two changing the black-purple-brown background static texture), and not at all natural colors.</li>\n<li>Vividness of Mental Imagery scale. This is a sequence of 100 descriptions of imagery, organized approximately in order from most vivid to least vivid. Of the given responses I identify most strongly with 94, 95, 96, 97, 98, and 99.</li>\n</ol>\n<div>Here are my results on <a href=\"http://socrates.berkeley.edu/~kihlstrm/MarksVVIQ.htm\">Marks' Vividness of Visual Imagery&nbsp;Questionnaire</a>:&nbsp;4s on everything except 5s on 4, 6, 9, 11, 14. Results suspect since I had exactly one 5 on every section. Marks suspects, unofficially, that those without visual mental imagery may actually have it but be unable to consciously notice/report that they have it.</div>\n</div>\n<div><br /></div>\n<div>I read through the <a href=\"http://plato.stanford.edu/entries/mental-imagery/\">Stanford Encyclopedia of Philosophy's entry on Mental Imagery</a>, following links to places or ideas that looked promising. I have many forms of mental imagery: good aural, very good verbal, very good analytic, good motor, poor yet extant haptic. But not visual.</div>\n</div>\n<div><br /></div>\n<div>Eric Schwitzgebel questions our <a href=\"http://www.faculty.ucr.edu/~eschwitz/SchwitzPapers/Imagery.pdf\">introspection about mental imagery</a> in general based on the lack of correlation between scores on Marks' VVIQ and subjects' ability to perform certain tasks that \"psychologists have often supposed to require visual imagery\". Such tasks include mental rotation and visual memory, both of which I can perform easily. I find it blindingly obvious that actual visual mental imagery is not required for these tasks. Here is <a href=\"/lw/8ph/how_is_your_mind_different_from_everyone_elses/5emb\">my own introspection about mental imagery</a>: what I call Imagination is sufficient for all the tasks that supposedly require visual imagery.</div>\n<div><br /></div>\n<div><strong>Does anyone know of (tested?) exercises for developing visual mental imagery from scratch?</strong></div>\n<div><strong><br /></strong></div>\n<div><strong>Does anyone know that developing visual mental imagery from scratch is likely impossible?</strong></div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "2wjPMY34by2gXEXA2": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8ciFqEjkekqzaTqT6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 35, "extendedScore": null, "score": 0.0001487475807258165, "legacy": true, "legacyId": "21678", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-06-03T18:13:12.747Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-19T22:29:11.606Z", "modifiedAt": null, "url": null, "title": "[LINK] Fate of our meta-stable Universe: killed by a vacuum bubble?", "slug": "link-fate-of-our-meta-stable-universe-killed-by-a-vacuum", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.577Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EW9hhCzRsYawiwf6A/link-fate-of-our-meta-stable-universe-killed-by-a-vacuum", "pageUrlRelative": "/posts/EW9hhCzRsYawiwf6A/link-fate-of-our-meta-stable-universe-killed-by-a-vacuum", "linkUrl": "https://www.lesswrong.com/posts/EW9hhCzRsYawiwf6A/link-fate-of-our-meta-stable-universe-killed-by-a-vacuum", "postedAtFormatted": "Tuesday, February 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Fate%20of%20our%20meta-stable%20Universe%3A%20killed%20by%20a%20vacuum%20bubble%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Fate%20of%20our%20meta-stable%20Universe%3A%20killed%20by%20a%20vacuum%20bubble%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEW9hhCzRsYawiwf6A%2Flink-fate-of-our-meta-stable-universe-killed-by-a-vacuum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Fate%20of%20our%20meta-stable%20Universe%3A%20killed%20by%20a%20vacuum%20bubble%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEW9hhCzRsYawiwf6A%2Flink-fate-of-our-meta-stable-universe-killed-by-a-vacuum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEW9hhCzRsYawiwf6A%2Flink-fate-of-our-meta-stable-universe-killed-by-a-vacuum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<p>The talk by <a href=\"http://en.wikipedia.org/wiki/Joseph_Lykken\">Joseph Lykken</a>&nbsp;(Lykken, ironically, is Norwegian for luck), making the science news rounds today, conjectures that Higgs will some day <a href=\"http://cosmiclog.nbcnews.com/_news/2013/02/18/17006552-will-our-universe-end-in-a-big-slurp-higgs-like-particle-suggests-it-might\">destroy the universe</a>&nbsp;in a flash of a \"true vacuum\" expanding at light speed and destroying our \"false vacuum\", and everything else in it. Here is the <a href=\"http://pubdb.desy.de/fulltext/getfulltext.php?uid=23383-59716\">original paper</a>. The interesting part for me is not the idea, which is <a href=\"http://en.wikipedia.org/wiki/False_vacuum\">not at all new</a>, but the related anthropic reasoning, which goes as follows:</p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #333333; font-family: georgia, serif; font-size: 16px; line-height: 25.59375px;\">That alternate universe would be \"much more boring,\" Lykken said. Which led him to ask a philosophical question: \"Why do we live in a universe that's just on the edge of stability?\" He wondered whether a universe has to be near the danger zone to produce galaxies, stars, planets ... and life.</span></p>\n<p>Cue Frost's Fire and Ice...</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EW9hhCzRsYawiwf6A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 1.1171341448491061e-06, "legacy": true, "legacyId": "21679", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-20T00:09:57.818Z", "modifiedAt": null, "url": null, "title": "Meetup : RTLW HPMoR discussion, chapters 39-43", "slug": "meetup-rtlw-hpmor-discussion-chapters-39-43", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uvmTCJy88wKsyPjMR/meetup-rtlw-hpmor-discussion-chapters-39-43", "pageUrlRelative": "/posts/uvmTCJy88wKsyPjMR/meetup-rtlw-hpmor-discussion-chapters-39-43", "linkUrl": "https://www.lesswrong.com/posts/uvmTCJy88wKsyPjMR/meetup-rtlw-hpmor-discussion-chapters-39-43", "postedAtFormatted": "Wednesday, February 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20RTLW%20HPMoR%20discussion%2C%20chapters%2039-43&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20RTLW%20HPMoR%20discussion%2C%20chapters%2039-43%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuvmTCJy88wKsyPjMR%2Fmeetup-rtlw-hpmor-discussion-chapters-39-43%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20RTLW%20HPMoR%20discussion%2C%20chapters%2039-43%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuvmTCJy88wKsyPjMR%2Fmeetup-rtlw-hpmor-discussion-chapters-39-43", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuvmTCJy88wKsyPjMR%2Fmeetup-rtlw-hpmor-discussion-chapters-39-43", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jm'>RTLW HPMoR discussion, chapters 39-43</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 February 2013 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2706 Durham-Chapel Hill Blvd., Durham NC 27707</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's meet at Guglhupf for coffee, breakfast, and discussion of HPMoR 39-43. Actually Doing the Reading is suggested, but not a requirement. Having read up to this point is also optional -- we're happy to summarize past chapters. You may be able to identify us by a 1150-page stack of spiral bound volumes on our table!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jm'>RTLW HPMoR discussion, chapters 39-43</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uvmTCJy88wKsyPjMR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.1171985426493089e-06, "legacy": true, "legacyId": "21680", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___RTLW_HPMoR_discussion__chapters_39_43\">Discussion article for the meetup : <a href=\"/meetups/jm\">RTLW HPMoR discussion, chapters 39-43</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 February 2013 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2706 Durham-Chapel Hill Blvd., Durham NC 27707</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's meet at Guglhupf for coffee, breakfast, and discussion of HPMoR 39-43. Actually Doing the Reading is suggested, but not a requirement. Having read up to this point is also optional -- we're happy to summarize past chapters. You may be able to identify us by a 1150-page stack of spiral bound volumes on our table!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___RTLW_HPMoR_discussion__chapters_39_431\">Discussion article for the meetup : <a href=\"/meetups/jm\">RTLW HPMoR discussion, chapters 39-43</a></h2>", "sections": [{"title": "Discussion article for the meetup : RTLW HPMoR discussion, chapters 39-43", "anchor": "Discussion_article_for_the_meetup___RTLW_HPMoR_discussion__chapters_39_43", "level": 1}, {"title": "Discussion article for the meetup : RTLW HPMoR discussion, chapters 39-43", "anchor": "Discussion_article_for_the_meetup___RTLW_HPMoR_discussion__chapters_39_431", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-20T03:21:38.722Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Epilogue: Atonement (8/8)", "slug": "seq-rerun-epilogue-atonement-8-8", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/av9Lj9t33XN86fzYr/seq-rerun-epilogue-atonement-8-8", "pageUrlRelative": "/posts/av9Lj9t33XN86fzYr/seq-rerun-epilogue-atonement-8-8", "linkUrl": "https://www.lesswrong.com/posts/av9Lj9t33XN86fzYr/seq-rerun-epilogue-atonement-8-8", "postedAtFormatted": "Wednesday, February 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Epilogue%3A%20Atonement%20(8%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Epilogue%3A%20Atonement%20(8%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fav9Lj9t33XN86fzYr%2Fseq-rerun-epilogue-atonement-8-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Epilogue%3A%20Atonement%20(8%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fav9Lj9t33XN86fzYr%2Fseq-rerun-epilogue-atonement-8-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fav9Lj9t33XN86fzYr%2Fseq-rerun-epilogue-atonement-8-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>Today's post, <a href=\"/lw/yc/epilogue_atonement_88/\">Epilogue: Atonement (8/8)</a> was originally published on 06 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Epilogue:_Atonement_.288.2F8.29\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The last moments aboard the Impossible Possible World.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gpr/seq_rerun_true_ending_sacrificial_fire/\">True Ending: Sacrificial Fire</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "av9Lj9t33XN86fzYr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.1173210561483852e-06, "legacy": true, "legacyId": "21681", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4pov2tL6SEC23wrkq", "ZWZxz2Ddkpvskf5RP", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-20T08:34:55.618Z", "modifiedAt": null, "url": null, "title": "Think Like a Supervillain", "slug": "think-like-a-supervillain", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:01.362Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Qiaochu_Yuan", "createdAt": "2012-11-24T08:36:50.547Z", "isAdmin": false, "displayName": "Qiaochu_Yuan"}, "userId": "qgFX9ZhzPCkcduZyB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vgXXyTae4noZBWykN/think-like-a-supervillain", "pageUrlRelative": "/posts/vgXXyTae4noZBWykN/think-like-a-supervillain", "linkUrl": "https://www.lesswrong.com/posts/vgXXyTae4noZBWykN/think-like-a-supervillain", "postedAtFormatted": "Wednesday, February 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Think%20Like%20a%20Supervillain&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThink%20Like%20a%20Supervillain%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvgXXyTae4noZBWykN%2Fthink-like-a-supervillain%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Think%20Like%20a%20Supervillain%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvgXXyTae4noZBWykN%2Fthink-like-a-supervillain", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvgXXyTae4noZBWykN%2Fthink-like-a-supervillain", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 748, "htmlBody": "<p><strong>See also</strong>: <a href=\"http://dirtsimple.org/2009/02/everything-i-needed-to-know-about-life.html\">Everything I Needed To Know About Life, I Learned From Supervillains</a></p>\n<blockquote>\n<p>Mr. Malfoy would hardly shrink from talk of ordinary murder, but even he was shocked - yes you were Mr. Malfoy, I was watching your face - when Mr. Potter described how to use his classmates' bodies as raw material. There are censors inside your mind which make you flinch away from thoughts like that. Mr. Potter thinks <em>purely</em> of killing the enemy, he will grasp at any means to do so, he does not flinch, his censors are off.</p>\n</blockquote>\n<p>-- <a href=\"http://www.fanfiction.net/s/5782108/16/Harry-Potter-and-the-Methods-of-Rationality\">Quirinus Quirrell</a></p>\n<blockquote>\n<p>A while back, I claimed the Less Wrong username Quirinus Quirrell, and started hosting a long-running, approximate simulation of him in my brain. I have mostly used the account trivially - to play around with crypto-novelties, say mildly offensive things I wouldn't otherwise, and poke fun at Clippy. Several times I have doubted the wisdom of hosting such a simulation. Quirrell's values are not my own, and the plans that he generates (which I have never followed) are mostly bad when viewed in terms of my values. However, I have chosen to keep this occasional alter-identity, because he sees things that would otherwise be invisible to me.</p>\n</blockquote>\n<p>-- <a href=\"/user/Quirinus_Quirrell/comments/\">Quirinus_Quirrell</a></p>\n<p>I was once asked whether I would rather be a superhero or a supervillain, and I probably shouldn't tell you how little time it took for me to answer \"supervillain.\"</p>\n<p>Being a superhero sounds awful, at least if you intend to keep being recognized as a superhero. Superheroes are bound by the chains of public opinion. A superhero can only do what people generally agree is good for superheroes to do. If you stray too far off the beaten path in search of how best to use your superpowers to actually <a href=\"/lw/91c/so_you_want_to_save_the_world/\">save the world</a>, you could easily end up doing things that look, at first glance, somewhat to incredibly evil. And if people are going to turn against you once you start actually optimizing, you might as well just be a supervillain to begin with. They look like they're having more fun anyway.&nbsp;</p>\n<p>You probably won't get the chance to decide between being a superhero or a supervillain, but you do get the chance to decide what kind of person you think of yourself as, and I think you should think of yourself more as a supervillain than as a superhero. Why?&nbsp;</p>\n<p>In the same way that being a superhero limits what you can do, thinking of yourself as a superhero <a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">limits what you can think</a>. And if you want to save the world, you can't afford to limit what you can think. Humanity faces many difficult problems, and the space of possible solutions to any one of these problems is large. If you have censors in your mind that are preventing you from looking at parts of this space because some of your moral intuitions don't like them (\"that's not the kind of thing a superhero would do!\"), you're crippling your ability to search for solutions to problems. For example, your moral intuitions are likely to flinch away from solutions to problems that involve you causing bad things to happen but be okay with solutions to problems that involve you failing to prevent bad things from happening (think of the <a href=\"http://en.wikipedia.org/wiki/Trolley_problem\">trolley problem</a>, or Batman's policy of not killing his enemies).&nbsp;</p>\n<p><strong>Edit (2/19):</strong> But thinking of yourself as a supervillain has the opposite effect. It's easier not to flinch at certain kinds of ideas, which now come more easily to mind and may not have otherwise occurred to you. For example, on Facebook, Eliezer recently mentioned a thread where people were posting examples of things that they valued at a billion dollars or more, such as their cats. With a supervillain module running in the background, I noticed and pointed out that this constituted a thread where people publicly described how they could be ransomed. I can't exactly test this, but I don't think this kind of idea would have occurred to me before I installed the supervillain module. (This is a tame example. I won't give less tame examples for obvious reasons.)&nbsp;</p>\n<p>There are many <a href=\"http://www.paulgraham.com/say.html\">things you can't say</a>, but you don't have to say everything you think. Until someone discovers a technique for reliably reading human minds, think whatever thoughts best help you accomplish your goals without worrying about any moral labels they may or may not, upon reflection, ultimately warrant. Moral labels are for a later step in the decision process than the part where you generate ideas.&nbsp;</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "CYMR6p5iZG75QAT8a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vgXXyTae4noZBWykN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 46, "extendedScore": null, "score": 0.000126, "legacy": true, "legacyId": "21643", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 111, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5BJvusxdwNXYQ4L9L", "QsMJQSFj7WfoTMNgW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-20T09:09:04.562Z", "modifiedAt": null, "url": null, "title": "Calibrating Against Undetectable Utilons and Goal Changing Events (part1)", "slug": "calibrating-against-undetectable-utilons-and-goal-changing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.700Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fKS54Zd4SaBrjznzF/calibrating-against-undetectable-utilons-and-goal-changing", "pageUrlRelative": "/posts/fKS54Zd4SaBrjznzF/calibrating-against-undetectable-utilons-and-goal-changing", "linkUrl": "https://www.lesswrong.com/posts/fKS54Zd4SaBrjznzF/calibrating-against-undetectable-utilons-and-goal-changing", "postedAtFormatted": "Wednesday, February 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Calibrating%20Against%20Undetectable%20Utilons%20and%20Goal%20Changing%20Events%20(part1)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACalibrating%20Against%20Undetectable%20Utilons%20and%20Goal%20Changing%20Events%20(part1)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKS54Zd4SaBrjznzF%2Fcalibrating-against-undetectable-utilons-and-goal-changing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Calibrating%20Against%20Undetectable%20Utilons%20and%20Goal%20Changing%20Events%20(part1)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKS54Zd4SaBrjznzF%2Fcalibrating-against-undetectable-utilons-and-goal-changing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKS54Zd4SaBrjznzF%2Fcalibrating-against-undetectable-utilons-and-goal-changing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2958, "htmlBody": "<p class=\"western\">Summary: Random events can preclude or steal attention from the goals you set up to begin with, hormonal fluctuation inclines people to change some of their goals with time. A discussion on how to act more usefully given those potential changes follows, taking in consideration the likelihood of a goal's success in terms of difficulty and length.</p>\n<p class=\"western\">Throughout I'll talk about <em>postponing utilons into undetectable distances. </em>Doing so (I'll claim), is frequently motivationally driven by a cognitive dissonance between what our effects on the near world are, and what we wish they were. In other words it is:</p>\n<p class=\"western\"><em>A <a href=\"http://en.wikipedia.org/wiki/Self-serving_bias\">Self-serving bias</a> in which <a href=\"http://en.wikipedia.org/wiki/Loss_aversion\">Loss aversion</a> manifests by postponing one's goals, thus avoiding frustration through <a href=\"http://en.wikipedia.org/wiki/Wishful_thinking\">wishful thinking</a> about <a href=\"http://en.wikipedia.org/wiki/Frank_J._Tipler\">far futures</a>, <a href=\"http://arxiv.org/abs/gr-qc/0102010\">big worlds</a>, immortal lives, and in general, high numbers of undetectable utilons. </em></p>\n<p class=\"western\">I suspect that some clusters of SciFi, Lesswrong, Transhumanists, and Cryonicists are particularly prone to <em>postponing utilons into undetectable distances</em>, and in the second post I'll try to specify which subgroups might be more likely to have done so. The phenomenon, though composed of a lot of biases, might even be a good thing depending on how it is handled.</p>\n<p class=\"western\">&nbsp;</p>\n<p class=\"western\">Sections will be:</p>\n<ol>\n<li>\n<p class=\"western\">What Significantly Changes Life's Direction (lists)</p>\n</li>\n<li>\n<p class=\"western\">Long Term Goals and Even Longer Term Goals</p>\n</li>\n<li>\n<p class=\"western\">Proportionality Between Goal Achievement Expected Time and Plan Execution Time</p>\n</li>\n<li>\n<p class=\"western\">A Hypothesis On Why We Became Long-Term Oriented</p>\n</li>\n<li>\n<p class=\"western\">Adapting Bayesian Reasoning to Get More Utilons</p>\n</li>\n<li>\n<p class=\"western\">Time You Can Afford to Wait, Not to Waste</p>\n</li>\n<li>\n<p class=\"western\">Reference Classes that May Be <em>Postponing Utilons Into Undetectable Distances</em></p>\n</li>\n<li>The Road Ahead </li>\n</ol>\n<p class=\"western\">Sections 4-8 will be on <a href=\"/r/discussion/lw/g87/calibrating_against_undetectable_utilons_and_goal/\">a second post</a> so that I can make changes based on commentary to this one.</p>\n<p class=\"western\">&nbsp;</p>\n<p><strong>1What Significantly Changes Life's Direction</strong></p>\n<p><strong><br /></strong></p>\n<p class=\"western\"><strong>1.1 Predominantly external changes</strong></p>\n<p class=\"western\">As far as I recall from reading old (circa 2004) large scale studies on happiness, the most important life events in how much they change your happiness for more than six months are:&nbsp;</p>\n<p class=\"western\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Becoming the caretaker of someone in a chronic non-curable condition</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Separation (versus marriage)</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Death of a Loved One</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Losing your Job</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Child rearing per child including the first</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Chronic intermittent disease</p>\n</li>\n<li>\n<p class=\"western\">Separation (versus being someone's girlfriend/boyfriend)&nbsp;</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Roughly in descending order.&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">That is a list of happiness changing events, I'm interested here in goal-changing events, and am assuming there will be a very high correlation.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">From life experience, mine, of friends, and of academics I've met, I'll list some events which can change someone's goals a lot:</p>\n<p class=\"western\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Moving between cities/countries</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Changing your social class a lot (losing a fortune or making one)&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Spending highschool/undergrad in a different country to return afterwards</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Having a child, in particular the first one&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><a href=\"/lw/2qp/virtual_employment_open_thread/\">Trying to get a job or make money</a> and noticing more accurately what the market looks like</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Alieving Existential Risk</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Alieving as true, <a href=\"http://www.thelifeyoucansave.com/\">universally</a> or <a href=\"/lw/6ny/smart_young_ambitious_and_clueless_what_to_do_to/\">personally,</a> the ethical theories called \"Utilitarianism\" and \"Consequentialism\"</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Noticing that <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">a lot of people are better than you at your initial goals</a>, specially when those goals are competitive non-positive sum goals to some extent.&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Interestingly, noticing that a lot of people are <em>worse than you</em>, making the efforts you once thought necessary not worth doing, or impossible to find good collaborators for.&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Getting to know those who were<a href=\"/lw/31i/have_no_heroes_and_no_villains/\"> once your idols</a>, or akin to them, and considering their lives not as awesome as their work</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">... which is sometimes caused by ...</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Reading Dan Gilbert's \"<a href=\"http://libgen.info/view.php?id=793580\">Stumbling on Happiness</a>\" and actually implementing his \"advice that no one will follow\" which is to think your happiness and emotions will correlate more with someone <em>else</em>&nbsp;who is already doing X which you plan to do than with <em>your model of what it would feel like doing X.&nbsp;</em></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Extreme social instability, such as wars, famine, etc...</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Having an ecstatic or traumatic experience, real or fictional. Such as seeing something unexpected, watching a life-changing movie, having a religious breakthrough, or a hallucinogenic one&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Traveling to a place that is very different from your world and being amazed / shocked</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Not being admitted into your desired university / course</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Depression</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Surpassing a frustration threshold thus experiencing the motivational equivalent of <a href=\"http://en.wikipedia.org/wiki/Learned_helplessness\">learned helplessness</a></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Realizing your goals do not match the space-time you were born in, such as if making songs for CDs is your vocation, or if you are 30 years old in contemporary Kenya and want to teach medicine at a top 10 world college.</p>\n</li>\n<li>\n<p class=\"western\">Falling in love</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">That is long enough, if not exhaustive, so let's get going...&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><strong>1.2 Predominantly Internal Changes</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I'm not a <a href=\"http://www.amazon.com/Endocrinology-Social-Relationships-Peter-Ellison/dp/0674063996\">social endocrinologist</a> but I think this emerging science agrees with folk wisdom that a lot changes in our hormonal systems during life (and during the menstrual cycle) and of course this changes our eagerness to do particular things. Not only hormones but other life events which mostly relate to the actual amount of time lived change our psychology. I'll cite some of those in turn:</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Exploitation increases and Exploration decreases with age</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Sex-Drive</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Maternity Drive - we have in portuguese an expression that &ldquo;a woman's clock <em>started</em> ticking&rdquo; which evidentiates a folk psychological theory that some part of it at least is binary</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Risk-proneness gives way to risk aversion, predominantly in males</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Premenstrual Syndrome - I always thought the acronym stood for 'Stress' until checking for this post.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Hormonal diseases</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Middle Age crisis &ndash; <a href=\"https://www.google.com.br/search?hl=pt-BR&amp;q=middle+age+crisis+in+apes\">recent controversy</a> about other apes having it</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">U shaped happiness curve through time &ndash; well, <a href=\"http://www.sciencedirect.com/science/article/pii/S0167268112000601\">not quite</a></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Menstrual cycle events</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p><strong>2 Long Term Goals and Even Longer Term Goals</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I <a href=\"/lw/f3z/rationality_versus_short_term_selves/\">have argued</a> sometimes here and <a href=\"http://www.youtube.com/watch?v=3RQC7jAWl_o\">elsewhere</a> that <a href=\"/lw/a2f/on_what_selves_are_cev_sequence/\">selves are</a> not as agenty as most of the <a href=\"/lw/5i8/the_power_of_agency/\">top</a> writers in this website seem to me to claim they <a href=\"/tag/agency/\">should</a> be, and that though in part this is indeed irrational, an ontology of selves which had various sized selves would decrease the amount of short term actions considered irrational, even though that would not go all the way into compensating hyperbolic discounting, scrolling 9gag or heroin consumption. That discussion, for me, was entirely about choosing between <em>doing now something that benefits </em><span style=\"font-style: normal;\">'</span><span style=\"font-style: normal;\">you</span><sub><span style=\"font-style: normal;\">now</span></sub><span style=\"font-style: normal;\">'</span><sub><span style=\"font-style: normal;\"> , </span></sub><span style=\"font-style: normal;\">'you</span><sub><span style=\"font-style: normal;\">today</span></sub><span style=\"font-style: normal;\">'</span><span style=\"font-style: normal;\">,</span> 'you<sub>tomorrow</sub>', 'you<sub>this weekend</sub>' or maybe a month from now. Anything longer than that was encompassed in a &ldquo;Far Future&rdquo; <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">mental category</a>. The interest here to discuss life-changing events is only in those far future ones which I'll split into arbitrary categories:</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">1) Months 2) Years 3) Decades 4) Bucket List or Lifelong and 5) Time Insensitive or Forever.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I have known more than ten people from LW whose goals are centered almost completely at the Time Insensitive and Lifelong categories, I recall hearing :</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">\"I see most of my expected utility after the singularity, thus I spend my willpower entirely in increasing the likelihood of a positive singularity, and care little about my current pre-singularity emotions\", &ldquo;My goal is to have a one trillion people world with maximal utility density where everyone lives forever&rdquo;, &ldquo;My sole goal in life is to live an indefinite life-span&rdquo;, &ldquo;I want to reduce X-risk in any way I can, that's all&rdquo;.</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I myself stated once my goal as</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&ldquo;To live long enough to experience a world in which human/posthuman flourishing exceeds 99% of individuals and other lower entities suffering is reduced by 50%, while being a counterfactually significant part of such process taking place.&rdquo;</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Though it seems reasonable, good, and actually one of the most altruistic things we can do, caring only about Bucket Lists and Time Insensitive goals has two big problems</p>\n<ol>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">There is no accurate feedback to calibrate our goal achieving tasks</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>The Goals we set for ourselves require very long term instrumental plans, which themselves take longer than the time it takes for internal drives or external events to change our goals. </em></p>\n</li>\n</ol>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-style: normal;\">The second one has been said in a remarkable Pink Floyd song about which I wrote <a href=\"http://brainstormers.wordpress.com/2008/07/30/the-starting-gun-2/\">a motivational text</a> five years ago: <a href=\"http://www.youtube.com/watch?v=MYiahoYfPGk\"><em>Time</em></a>.</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">You are young and life is long and there is time to kill today</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And then one day you find ten years have got behind you</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">No one told you when to run, you missed the starting gun</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And you run and you run to catch up with the sun, but it's sinking</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And racing around to come up behind you again</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">The sun is the same in a relative way, but you're older</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Shorter of breath and one day closer to death</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Every year is getting shorter, never seem to find the time</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Plans that either come to naught or half a page of scribbled lines</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Okay, maybe the song doesn't say exactly (2) but it is within the same ballpark. The fact remains that those of us inclined to care mostly about very long term are quite likely to end up with a half baked plan because one of those dozens of life-changing events happened, and that agent with the initial goals will have died for no reason if she doesn't manage to get someone to continue her goals before she stops existing.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">This is <em>very bad</em><span style=\"font-style: normal;\">. Once you understand how our goal-structures do change over time &ndash; that is, when you accept the existence of all those events that will change what you want to steer the world into &ndash; it becomes straightforward </span><em>irrational</em><span style=\"font-style: normal;\"> to pursue your goals as if that agent would live longer than it's actual life expectancy. Thus we are surrounded by agents </span><em>postponing utilons into undetectable distances</em><span style=\"font-style: normal;\">. Doing this is kind of a bias in the opposite direction of hyperbolic discounting. </span><em>Having </em><em><em>postponed utilons into undetectable distances</em> </em>is predictably irrational because it means we<span style=\"font-style: normal;\"> care about our Lifelong, Bucket List, and Time Insensitive goals </span><em>as if </em><span style=\"font-style: normal;\">we'd have enough time to actually execute the plans for these timeframes, while ignoring the likelihood of our goals changing in the meantime and factoring that in. <br /></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">I've come to realize that this was affecting me with my <a href=\"/lw/evl/abandoning_cached_selves_to_rewrite_my_source/\">Utility Function Breakdown</a> which was described in the linked post about digging too deep into one's cached selves and how this can be dangerous. As I predicted back then, stability has returned to my allocation of attention and time and the whole zig-zagging chaotic piconomical neural Darwinism that had ensued has stopped. Also&nbsp; relevant is the fact that after about 8 years caring about more or less similar things, I've come to understand how frequently my motivation changed direction (roughly every three months for some kinds of things, and 6-8 months for other kinds). With this post I intend to learn to calibrate my future plans accordingly, and help others do the same. Always beware of other-optimizing though. <br /></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>Citizen: But what if my goals are all Lifelong or Forever in kind? It is impossible for me to execute in 3 months what will make centenary changes</em><span style=\"font-style: normal;\">. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">Well, not exactly. Some problems require chunks of plans which can be separated and executed either in parallel or in series. And yes, everyone knows that, also <em>AI planning</em> is a whole area dedicated to doing just that in non-human form. It is still worth mentioning, because it is much more <a href=\"http://yudkowsky.net/rational/the-simple-truth\">simply true</a> than <a href=\"/lw/3w3/how_to_beat_procrastination/\">actually done.</a></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">This community in general has concluded in its rational inquiries that being longer term oriented is generally a better way to win, that is, it is more rational. This is true. What would not be rational is to in every single instance of deciding between long term or even longer term goals, choose without taking in consideration <em><a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/\">how long</a> will the choosing being exist</em>, in the sense of being the same agent with the same goals. Life-changing events happen more often than you think, because you think they happen as often as they did in the savannahs in which your brain was shaped.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>3 Proportionality Between Goal Achievement Expected Time and Plan Execution Time</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">So far we have been through the following ideas. Lots of events change your goals, some externally some internally, if you are a rationalist, you end up caring more about events that take longer to happen in detectable ways (since if you are average you care in proportion to emotional drives that <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">execute adaptations</a> but don't quite achieve goals). If you know that humans change and still want to achieve your goals, you'd better account for the possibility of changing before their achievement<em>.</em> Your kinds of goals are quite likely prone to the long-term since you are reading a Lesswrong post.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>Citizen: But wait! Who said that my goals happening in a hundred years makes my specific instrumental plans take longer to be executed? </em></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em><br /></em></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">I won't make the case for the idea that having long term goals increases the likelihood of the time it takes to execute your plans being longer. I'll only say that if it did not take that long to do those things </span><em>your goal would probably be to have done the same things, only sooner</em><span style=\"font-style: normal;\">. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">To take one example: &ldquo;I would like 90% of people to surpass 150 IQ and be in a <a href=\"http://www.hedweb.com/\">bliss gradient</a> state of mind all the time&rdquo;</span></p>\n<p><span style=\"font-style: normal;\">Obviously, the sooner that happens, the better. Doesn't look like the kind of thing you'd wait for college to end to begin doing, or for your second child to be born. The reason for wanting this long-term is that it can't be achieved in the short run. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">Take <a href=\"/lw/4g/eliezer_yudkowsky_facts/\">Idealized Fiction of Eliezer Yudkosky</a>: Mr Ifey had this supergoal of making a Superintelligence when he was very young. He didn't go there and do it. Because he could not. If he could do it he would. Thank goodness, for we had time to find out about FAI after that. Then his instrumental goal was to get FAI into the minds of the AGI makers. This turned out to be to hard because it was time consuming. He reasoned that only a more rational AI community would be able to pull it off, all while finding a club of brilliant followers in this peculiar economist's blog. He created a blog to teach geniuses rationality, a project that might have taken years. It did, and it worked pretty well, but that was not enough, Ifey soon realized more people ought to be more rational, and wrote HPMOR to make people who were not previously prone to brilliance as able to find the facts as those who were lucky enough to have found his path. All of that was not enough, an institution, with money flow had to be created, and there Ifey was to create it, years before all that. A magnet of long-term awesomeness of proportions comparable only to the Best Of Standing Transfinite Restless Oracle Master, he was responsible for the education of some of the greatest within the generation that might change the worlds destiny for good. Ifey began to work on a rationality book, which at some point pivoted to research for journals and pivoted back to research for the Lesswrong posts he is currently publishing. All that Ifey did by splitting that big supergoal in smaller ones (creating Singinst, showing awesomeness in Overcoming Bias, writing the sequences, writing the particular sequence &ldquo;Misterious Answers to Misterious Questions&rdquo; and writing the specific post &ldquo;Making Your Beliefs Pay Rent&rdquo;). But that is not what I want to emphasize, what I'd like to emphasize is that </span><em>there was room for changing goals every now and then</em><span style=\"font-style: normal;\">. All of that achievement would not have been possible if at each point he had an instrumental goal which lasts 20 years whose value is very low uptill the 19th year. Because a lot of what he wrote and did remained valuable for others before the 20th year, we now have a glowing community of people hopefully becoming better at becoming better, and making the world a better place in varied ways. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">So yes, the ubiquitous advice of chopping problems into smaller pieces is extremely useful and very important, but in addition to it, remember to chop pieces with the following properties:</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (A) Short enough that you will actually do it.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (B) Short enough that the person at the end, doing it, <em>will still be you</em> in the significant ways. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">(C) Having enough emotional feedback that your motivation won't be capsized before the end. and</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (D) Such that others not only can, but likely will take up the project after you abandon it in case you miscalculated when you'd change, or a change occurred before expected time. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"><br /></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Sections 4-8 will be on <a href=\"/r/discussion/lw/g87/calibrating_against_undetectable_utilons_and_goal/\">a second post </a>so that I can make changes based on commentary to this one.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"><br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fKS54Zd4SaBrjznzF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 13, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "21692", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"western\">Summary: Random events can preclude or steal attention from the goals you set up to begin with, hormonal fluctuation inclines people to change some of their goals with time. A discussion on how to act more usefully given those potential changes follows, taking in consideration the likelihood of a goal's success in terms of difficulty and length.</p>\n<p class=\"western\">Throughout I'll talk about <em>postponing utilons into undetectable distances. </em>Doing so (I'll claim), is frequently motivationally driven by a cognitive dissonance between what our effects on the near world are, and what we wish they were. In other words it is:</p>\n<p class=\"western\"><em>A <a href=\"http://en.wikipedia.org/wiki/Self-serving_bias\">Self-serving bias</a> in which <a href=\"http://en.wikipedia.org/wiki/Loss_aversion\">Loss aversion</a> manifests by postponing one's goals, thus avoiding frustration through <a href=\"http://en.wikipedia.org/wiki/Wishful_thinking\">wishful thinking</a> about <a href=\"http://en.wikipedia.org/wiki/Frank_J._Tipler\">far futures</a>, <a href=\"http://arxiv.org/abs/gr-qc/0102010\">big worlds</a>, immortal lives, and in general, high numbers of undetectable utilons. </em></p>\n<p class=\"western\">I suspect that some clusters of SciFi, Lesswrong, Transhumanists, and Cryonicists are particularly prone to <em>postponing utilons into undetectable distances</em>, and in the second post I'll try to specify which subgroups might be more likely to have done so. The phenomenon, though composed of a lot of biases, might even be a good thing depending on how it is handled.</p>\n<p class=\"western\">&nbsp;</p>\n<p class=\"western\">Sections will be:</p>\n<ol>\n<li>\n<p class=\"western\">What Significantly Changes Life's Direction (lists)</p>\n</li>\n<li>\n<p class=\"western\">Long Term Goals and Even Longer Term Goals</p>\n</li>\n<li>\n<p class=\"western\">Proportionality Between Goal Achievement Expected Time and Plan Execution Time</p>\n</li>\n<li>\n<p class=\"western\">A Hypothesis On Why We Became Long-Term Oriented</p>\n</li>\n<li>\n<p class=\"western\">Adapting Bayesian Reasoning to Get More Utilons</p>\n</li>\n<li>\n<p class=\"western\">Time You Can Afford to Wait, Not to Waste</p>\n</li>\n<li>\n<p class=\"western\">Reference Classes that May Be <em>Postponing Utilons Into Undetectable Distances</em></p>\n</li>\n<li>The Road Ahead </li>\n</ol>\n<p class=\"western\">Sections 4-8 will be on <a href=\"/r/discussion/lw/g87/calibrating_against_undetectable_utilons_and_goal/\">a second post</a> so that I can make changes based on commentary to this one.</p>\n<p class=\"western\">&nbsp;</p>\n<p><strong id=\"1What_Significantly_Changes_Life_s_Direction\">1What Significantly Changes Life's Direction</strong></p>\n<p><strong><br></strong></p>\n<p class=\"western\"><strong id=\"1_1_Predominantly_external_changes\">1.1 Predominantly external changes</strong></p>\n<p class=\"western\">As far as I recall from reading old (circa 2004) large scale studies on happiness, the most important life events in how much they change your happiness for more than six months are:&nbsp;</p>\n<p class=\"western\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Becoming the caretaker of someone in a chronic non-curable condition</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Separation (versus marriage)</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Death of a Loved One</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Losing your Job</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Child rearing per child including the first</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Chronic intermittent disease</p>\n</li>\n<li>\n<p class=\"western\">Separation (versus being someone's girlfriend/boyfriend)&nbsp;</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Roughly in descending order.&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">That is a list of happiness changing events, I'm interested here in goal-changing events, and am assuming there will be a very high correlation.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">From life experience, mine, of friends, and of academics I've met, I'll list some events which can change someone's goals a lot:</p>\n<p class=\"western\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Moving between cities/countries</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Changing your social class a lot (losing a fortune or making one)&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Spending highschool/undergrad in a different country to return afterwards</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Having a child, in particular the first one&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><a href=\"/lw/2qp/virtual_employment_open_thread/\">Trying to get a job or make money</a> and noticing more accurately what the market looks like</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Alieving Existential Risk</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Alieving as true, <a href=\"http://www.thelifeyoucansave.com/\">universally</a> or <a href=\"/lw/6ny/smart_young_ambitious_and_clueless_what_to_do_to/\">personally,</a> the ethical theories called \"Utilitarianism\" and \"Consequentialism\"</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Noticing that <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">a lot of people are better than you at your initial goals</a>, specially when those goals are competitive non-positive sum goals to some extent.&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Interestingly, noticing that a lot of people are <em>worse than you</em>, making the efforts you once thought necessary not worth doing, or impossible to find good collaborators for.&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Getting to know those who were<a href=\"/lw/31i/have_no_heroes_and_no_villains/\"> once your idols</a>, or akin to them, and considering their lives not as awesome as their work</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">... which is sometimes caused by ...</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Reading Dan Gilbert's \"<a href=\"http://libgen.info/view.php?id=793580\">Stumbling on Happiness</a>\" and actually implementing his \"advice that no one will follow\" which is to think your happiness and emotions will correlate more with someone <em>else</em>&nbsp;who is already doing X which you plan to do than with <em>your model of what it would feel like doing X.&nbsp;</em></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Extreme social instability, such as wars, famine, etc...</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Having an ecstatic or traumatic experience, real or fictional. Such as seeing something unexpected, watching a life-changing movie, having a religious breakthrough, or a hallucinogenic one&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Traveling to a place that is very different from your world and being amazed / shocked</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Not being admitted into your desired university / course</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Depression</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Surpassing a frustration threshold thus experiencing the motivational equivalent of <a href=\"http://en.wikipedia.org/wiki/Learned_helplessness\">learned helplessness</a></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Realizing your goals do not match the space-time you were born in, such as if making songs for CDs is your vocation, or if you are 30 years old in contemporary Kenya and want to teach medicine at a top 10 world college.</p>\n</li>\n<li>\n<p class=\"western\">Falling in love</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">That is long enough, if not exhaustive, so let's get going...&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><strong id=\"1_2_Predominantly_Internal_Changes\">1.2 Predominantly Internal Changes</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I'm not a <a href=\"http://www.amazon.com/Endocrinology-Social-Relationships-Peter-Ellison/dp/0674063996\">social endocrinologist</a> but I think this emerging science agrees with folk wisdom that a lot changes in our hormonal systems during life (and during the menstrual cycle) and of course this changes our eagerness to do particular things. Not only hormones but other life events which mostly relate to the actual amount of time lived change our psychology. I'll cite some of those in turn:</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Exploitation increases and Exploration decreases with age</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Sex-Drive</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Maternity Drive - we have in portuguese an expression that \u201ca woman's clock <em>started</em> ticking\u201d which evidentiates a folk psychological theory that some part of it at least is binary</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Risk-proneness gives way to risk aversion, predominantly in males</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Premenstrual Syndrome - I always thought the acronym stood for 'Stress' until checking for this post.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Hormonal diseases</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Middle Age crisis \u2013 <a href=\"https://www.google.com.br/search?hl=pt-BR&amp;q=middle+age+crisis+in+apes\">recent controversy</a> about other apes having it</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">U shaped happiness curve through time \u2013 well, <a href=\"http://www.sciencedirect.com/science/article/pii/S0167268112000601\">not quite</a></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Menstrual cycle events</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p><strong id=\"2_Long_Term_Goals_and_Even_Longer_Term_Goals\">2 Long Term Goals and Even Longer Term Goals</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I <a href=\"/lw/f3z/rationality_versus_short_term_selves/\">have argued</a> sometimes here and <a href=\"http://www.youtube.com/watch?v=3RQC7jAWl_o\">elsewhere</a> that <a href=\"/lw/a2f/on_what_selves_are_cev_sequence/\">selves are</a> not as agenty as most of the <a href=\"/lw/5i8/the_power_of_agency/\">top</a> writers in this website seem to me to claim they <a href=\"/tag/agency/\">should</a> be, and that though in part this is indeed irrational, an ontology of selves which had various sized selves would decrease the amount of short term actions considered irrational, even though that would not go all the way into compensating hyperbolic discounting, scrolling 9gag or heroin consumption. That discussion, for me, was entirely about choosing between <em>doing now something that benefits </em><span style=\"font-style: normal;\">'</span><span style=\"font-style: normal;\">you</span><sub><span style=\"font-style: normal;\">now</span></sub><span style=\"font-style: normal;\">'</span><sub><span style=\"font-style: normal;\"> , </span></sub><span style=\"font-style: normal;\">'you</span><sub><span style=\"font-style: normal;\">today</span></sub><span style=\"font-style: normal;\">'</span><span style=\"font-style: normal;\">,</span> 'you<sub>tomorrow</sub>', 'you<sub>this weekend</sub>' or maybe a month from now. Anything longer than that was encompassed in a \u201cFar Future\u201d <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">mental category</a>. The interest here to discuss life-changing events is only in those far future ones which I'll split into arbitrary categories:</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">1) Months 2) Years 3) Decades 4) Bucket List or Lifelong and 5) Time Insensitive or Forever.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I have known more than ten people from LW whose goals are centered almost completely at the Time Insensitive and Lifelong categories, I recall hearing :</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">\"I see most of my expected utility after the singularity, thus I spend my willpower entirely in increasing the likelihood of a positive singularity, and care little about my current pre-singularity emotions\", \u201cMy goal is to have a one trillion people world with maximal utility density where everyone lives forever\u201d, \u201cMy sole goal in life is to live an indefinite life-span\u201d, \u201cI want to reduce X-risk in any way I can, that's all\u201d.</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I myself stated once my goal as</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">\u201cTo live long enough to experience a world in which human/posthuman flourishing exceeds 99% of individuals and other lower entities suffering is reduced by 50%, while being a counterfactually significant part of such process taking place.\u201d</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Though it seems reasonable, good, and actually one of the most altruistic things we can do, caring only about Bucket Lists and Time Insensitive goals has two big problems</p>\n<ol>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">There is no accurate feedback to calibrate our goal achieving tasks</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>The Goals we set for ourselves require very long term instrumental plans, which themselves take longer than the time it takes for internal drives or external events to change our goals. </em></p>\n</li>\n</ol>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-style: normal;\">The second one has been said in a remarkable Pink Floyd song about which I wrote <a href=\"http://brainstormers.wordpress.com/2008/07/30/the-starting-gun-2/\">a motivational text</a> five years ago: <a href=\"http://www.youtube.com/watch?v=MYiahoYfPGk\"><em>Time</em></a>.</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">You are young and life is long and there is time to kill today</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And then one day you find ten years have got behind you</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">No one told you when to run, you missed the starting gun</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And you run and you run to catch up with the sun, but it's sinking</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And racing around to come up behind you again</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">The sun is the same in a relative way, but you're older</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Shorter of breath and one day closer to death</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Every year is getting shorter, never seem to find the time</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Plans that either come to naught or half a page of scribbled lines</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Okay, maybe the song doesn't say exactly (2) but it is within the same ballpark. The fact remains that those of us inclined to care mostly about very long term are quite likely to end up with a half baked plan because one of those dozens of life-changing events happened, and that agent with the initial goals will have died for no reason if she doesn't manage to get someone to continue her goals before she stops existing.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">This is <em>very bad</em><span style=\"font-style: normal;\">. Once you understand how our goal-structures do change over time \u2013 that is, when you accept the existence of all those events that will change what you want to steer the world into \u2013 it becomes straightforward </span><em>irrational</em><span style=\"font-style: normal;\"> to pursue your goals as if that agent would live longer than it's actual life expectancy. Thus we are surrounded by agents </span><em>postponing utilons into undetectable distances</em><span style=\"font-style: normal;\">. Doing this is kind of a bias in the opposite direction of hyperbolic discounting. </span><em>Having </em><em><em>postponed utilons into undetectable distances</em> </em>is predictably irrational because it means we<span style=\"font-style: normal;\"> care about our Lifelong, Bucket List, and Time Insensitive goals </span><em>as if </em><span style=\"font-style: normal;\">we'd have enough time to actually execute the plans for these timeframes, while ignoring the likelihood of our goals changing in the meantime and factoring that in. <br></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">I've come to realize that this was affecting me with my <a href=\"/lw/evl/abandoning_cached_selves_to_rewrite_my_source/\">Utility Function Breakdown</a> which was described in the linked post about digging too deep into one's cached selves and how this can be dangerous. As I predicted back then, stability has returned to my allocation of attention and time and the whole zig-zagging chaotic piconomical neural Darwinism that had ensued has stopped. Also&nbsp; relevant is the fact that after about 8 years caring about more or less similar things, I've come to understand how frequently my motivation changed direction (roughly every three months for some kinds of things, and 6-8 months for other kinds). With this post I intend to learn to calibrate my future plans accordingly, and help others do the same. Always beware of other-optimizing though. <br></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>Citizen: But what if my goals are all Lifelong or Forever in kind? It is impossible for me to execute in 3 months what will make centenary changes</em><span style=\"font-style: normal;\">. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">Well, not exactly. Some problems require chunks of plans which can be separated and executed either in parallel or in series. And yes, everyone knows that, also <em>AI planning</em> is a whole area dedicated to doing just that in non-human form. It is still worth mentioning, because it is much more <a href=\"http://yudkowsky.net/rational/the-simple-truth\">simply true</a> than <a href=\"/lw/3w3/how_to_beat_procrastination/\">actually done.</a></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">This community in general has concluded in its rational inquiries that being longer term oriented is generally a better way to win, that is, it is more rational. This is true. What would not be rational is to in every single instance of deciding between long term or even longer term goals, choose without taking in consideration <em><a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/\">how long</a> will the choosing being exist</em>, in the sense of being the same agent with the same goals. Life-changing events happen more often than you think, because you think they happen as often as they did in the savannahs in which your brain was shaped.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"3_Proportionality_Between_Goal_Achievement_Expected_Time_and_Plan_Execution_Time\">3 Proportionality Between Goal Achievement Expected Time and Plan Execution Time</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">So far we have been through the following ideas. Lots of events change your goals, some externally some internally, if you are a rationalist, you end up caring more about events that take longer to happen in detectable ways (since if you are average you care in proportion to emotional drives that <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">execute adaptations</a> but don't quite achieve goals). If you know that humans change and still want to achieve your goals, you'd better account for the possibility of changing before their achievement<em>.</em> Your kinds of goals are quite likely prone to the long-term since you are reading a Lesswrong post.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>Citizen: But wait! Who said that my goals happening in a hundred years makes my specific instrumental plans take longer to be executed? </em></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em><br></em></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">I won't make the case for the idea that having long term goals increases the likelihood of the time it takes to execute your plans being longer. I'll only say that if it did not take that long to do those things </span><em>your goal would probably be to have done the same things, only sooner</em><span style=\"font-style: normal;\">. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">To take one example: \u201cI would like 90% of people to surpass 150 IQ and be in a <a href=\"http://www.hedweb.com/\">bliss gradient</a> state of mind all the time\u201d</span></p>\n<p><span style=\"font-style: normal;\">Obviously, the sooner that happens, the better. Doesn't look like the kind of thing you'd wait for college to end to begin doing, or for your second child to be born. The reason for wanting this long-term is that it can't be achieved in the short run. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">Take <a href=\"/lw/4g/eliezer_yudkowsky_facts/\">Idealized Fiction of Eliezer Yudkosky</a>: Mr Ifey had this supergoal of making a Superintelligence when he was very young. He didn't go there and do it. Because he could not. If he could do it he would. Thank goodness, for we had time to find out about FAI after that. Then his instrumental goal was to get FAI into the minds of the AGI makers. This turned out to be to hard because it was time consuming. He reasoned that only a more rational AI community would be able to pull it off, all while finding a club of brilliant followers in this peculiar economist's blog. He created a blog to teach geniuses rationality, a project that might have taken years. It did, and it worked pretty well, but that was not enough, Ifey soon realized more people ought to be more rational, and wrote HPMOR to make people who were not previously prone to brilliance as able to find the facts as those who were lucky enough to have found his path. All of that was not enough, an institution, with money flow had to be created, and there Ifey was to create it, years before all that. A magnet of long-term awesomeness of proportions comparable only to the Best Of Standing Transfinite Restless Oracle Master, he was responsible for the education of some of the greatest within the generation that might change the worlds destiny for good. Ifey began to work on a rationality book, which at some point pivoted to research for journals and pivoted back to research for the Lesswrong posts he is currently publishing. All that Ifey did by splitting that big supergoal in smaller ones (creating Singinst, showing awesomeness in Overcoming Bias, writing the sequences, writing the particular sequence \u201cMisterious Answers to Misterious Questions\u201d and writing the specific post \u201cMaking Your Beliefs Pay Rent\u201d). But that is not what I want to emphasize, what I'd like to emphasize is that </span><em>there was room for changing goals every now and then</em><span style=\"font-style: normal;\">. All of that achievement would not have been possible if at each point he had an instrumental goal which lasts 20 years whose value is very low uptill the 19th year. Because a lot of what he wrote and did remained valuable for others before the 20th year, we now have a glowing community of people hopefully becoming better at becoming better, and making the world a better place in varied ways. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">So yes, the ubiquitous advice of chopping problems into smaller pieces is extremely useful and very important, but in addition to it, remember to chop pieces with the following properties:</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (A) Short enough that you will actually do it.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (B) Short enough that the person at the end, doing it, <em>will still be you</em> in the significant ways. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">(C) Having enough emotional feedback that your motivation won't be capsized before the end. and</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (D) Such that others not only can, but likely will take up the project after you abandon it in case you miscalculated when you'd change, or a change occurred before expected time. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"><br></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Sections 4-8 will be on <a href=\"/r/discussion/lw/g87/calibrating_against_undetectable_utilons_and_goal/\">a second post </a>so that I can make changes based on commentary to this one.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"><br></span></p>", "sections": [{"title": "1What Significantly Changes Life's Direction", "anchor": "1What_Significantly_Changes_Life_s_Direction", "level": 1}, {"title": "1.1 Predominantly external changes", "anchor": "1_1_Predominantly_external_changes", "level": 1}, {"title": "1.2 Predominantly Internal Changes", "anchor": "1_2_Predominantly_Internal_Changes", "level": 1}, {"title": "2 Long Term Goals and Even Longer Term Goals", "anchor": "2_Long_Term_Goals_and_Even_Longer_Term_Goals", "level": 1}, {"title": "3 Proportionality Between Goal Achievement Expected Time and Plan Execution Time", "anchor": "3_Proportionality_Between_Goal_Achievement_Expected_Time_and_Plan_Execution_Time", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GjXeKTxkYvZ5WfkFu", "9bTNcSpNBdPpyocMK", "oFY8Ms83ehXgRMh32", "BFamedwSgRdGGKXQQ", "G6npMHwgRGSQDKavX", "JzLH8BDoCk86jaALe", "DGfPyJbynXZF9NGv4", "vbcjYg6h3XzuqaaN8", "WBw8dDkAWohFjWQSk", "uEnFDx7nQacQdf6Tg", "RWo4LwFzpHNQCTcYt", "PpTN7GP2FsPyHfKrs", "XPErvb8m9FapXCjhA", "Ndtb22KYBxpBsagpj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-20T09:41:09.182Z", "modifiedAt": null, "url": null, "title": "Desires You're Not Thinking About at the Moment", "slug": "desires-you-re-not-thinking-about-at-the-moment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:27.935Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ghatanathoah", "createdAt": "2012-01-27T20:44:00.945Z", "isAdmin": false, "displayName": "Ghatanathoah"}, "userId": "CzhiZYDsQs87MM5yH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SScCHA8znL5w4uPeh/desires-you-re-not-thinking-about-at-the-moment", "pageUrlRelative": "/posts/SScCHA8znL5w4uPeh/desires-you-re-not-thinking-about-at-the-moment", "linkUrl": "https://www.lesswrong.com/posts/SScCHA8znL5w4uPeh/desires-you-re-not-thinking-about-at-the-moment", "postedAtFormatted": "Wednesday, February 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Desires%20You're%20Not%20Thinking%20About%20at%20the%20Moment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADesires%20You're%20Not%20Thinking%20About%20at%20the%20Moment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSScCHA8znL5w4uPeh%2Fdesires-you-re-not-thinking-about-at-the-moment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Desires%20You're%20Not%20Thinking%20About%20at%20the%20Moment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSScCHA8znL5w4uPeh%2Fdesires-you-re-not-thinking-about-at-the-moment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSScCHA8znL5w4uPeh%2Fdesires-you-re-not-thinking-about-at-the-moment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1154, "htmlBody": "<p>While doing some reading on philosophy I came across some interesting questions about the nature of having desires and preferences. One, do you still have preferences and desires when you are unconscious? Two, if you don't does this call into question the many moral theories that hold that having preferences and desires is what makes one morally significant, since mistreating temporarily unconscious people seems obviously immoral?&nbsp;</p>\n<p>Philosophers usually discuss this question when debating the morality of abortion, but to avoid doing any <a href=\"/lw/gw/politics_is_the_mindkiller/\">mindkilling</a> I won't mention that topic, except to say in this sentence that I won't mention it.</p>\n<p>In more detail the issue is:&nbsp; A common, intuitive, and logical-seeming explanation for why it is immoral to destroy a typical human being, but not to destroy a rock, is that a typical human being has certain desires (or preferences or values, whatever you wish to call them, I'm using the terms interchangably) that they wish to fulfill, and destroying them would hinder the fulfillment of these desires.&nbsp; A rock, by contrast does not have any such desires so it is not harmed by being destroyed.&nbsp; The problem with this is that it also seems immoral to harm a human being who is asleep, or is in a temporary coma. And, on the face of it, it seems plausible to say that an unconscious person does not have any desires. (And of course it gets even weirder when considering far-out concepts like a brain emulator that is saved to a hard drive, but isn't being run at the moment)</p>\n<p>After thinking about this it occurred to me that this line of reasoning could be taken further.&nbsp; If I am not thinking about my car at the moment, can I still be said to desire that it is not stolen?&nbsp; Do I stop having desires about things the instant my attention shifts away from them?</p>\n<p>I have compiled a list of possible solutions to this problem, ranked in order from least plausible to most plausible.</p>\n<p>1.&nbsp; One possibility would be to consider it immoral to harm a sleeping person because if they will have desires in the future, even if they don't now.&nbsp; I find this argument extremely implausible because it has some extremely bizarre implications, some of which may lead to insoluble moral contradictions.&nbsp; For instance, this argument could be used to argue that it is immoral to destroy skin cells because it is possible to use them to clone a new person, who will eventually grow up to have desires.</p>\n<p>Furthermore, when human beings eventually gain the ability to build AIs that possess desires, this solution interacts with the <a href=\"http://wiki.lesswrong.com/wiki/Orthogonality_thesis\">orthogonality thesis</a> in a catastrophic fashion.&nbsp; If it is possible to build an AI with any utility function, then for every potential AI one can construct, there is another potential AI that desires the exact opposite of that AI.&nbsp; That leads to total paralysis, since for every set potential set of desires we are capable of satisfying there is another potential set that would be horribly thwarted.</p>\n<p>Lastly, this argument implies that you can, (and may be obligated to) help someone who doesn't exist, and never has existed, by satisfying their <a href=\"/lw/4rn/nonpersonal_preferences_of_neverexisted_people/\">non-personal preferences</a>, without ever having to bother with actually creating them.&nbsp; This seem strange, I can maybe see an argument for respecting the once-existant preferences of those who are dead, but respecting the hypothetical preferences of the never-existed seems absurd.&nbsp; It also has the same problems with the orthogonality thesis that I mentioned earlier.</p>\n<p>2.&nbsp; Make the same argument as solution 1, but somehow define the categories more narrowly so that an unconscious person's ability to have desires in the future differs from that of an uncloned skin cell or an unbuilt AI.&nbsp; Michael Tooley has tried to do this by discerning between things that have the \"possibility\" of becoming a person with desires (i.e skin cells) and those that have the \"capacity\" to have desires.&nbsp; This approach has been <a href=\"http://py111.wordpress.com/2008/11/14/is-tooly-abortion-challenge-for-py111-students/\">criticized</a>, and I find myself pessimistic about it because categories have a tendency to be \"fuzzy\" in real life and not have sharp borders.</p>\n<p>3.&nbsp; Another solution may be that desires that one has had in the past continue to count, even when one is unconscious or not thinking about them.&nbsp; So it's immoral to harm unconscious people because before they were unconscious they had a desire not to be harmed, and it's immoral to steal my car because I desired that it not be stolen earlier when I was thinking about it.</p>\n<p>I find this solution fairly convincing.&nbsp; The only major quibble I have with it is that it gives what some might consider a counter-intuitive result on a variation of the sleeping person question.&nbsp; Imagine a nano-factory manufacturers a sleeping person.&nbsp; This person is a new and distinct individual, and when they wake up they will proceed to behave as a typical human.&nbsp; This solution may suggest that it is okay to kill them before they wake up, since they haven't had any desires yet, which does seem odd.</p>\n<p>4. Reject the claim that one doesn't have desires when one is unconscious, or when one is not thinking about a topic.&nbsp; The more I think about this solution, the more obvious it seems.&nbsp; Generally when I am rationally deliberating about whether or not I desire something I consider how many of my values and ideaks it fulfills.&nbsp; It seems like my list of values and ideals remains fairly constant, and that even if I am focusing my attention on one value at a time it makes sense to say that I still \"have\" the other values I am not focusing on at the moment.</p>\n<p>Obviously I don't think that there's some portion of my brain where my \"values\" are stored in a neat little Excel spreadsheet.&nbsp; But they do seem to be a persistent part of its structure in some fashion.&nbsp; And it makes sense that they'd still be part of its structure when I'm unconscious.&nbsp; If they weren't, wouldn't my preferences change radically every time I woke up?</p>\n<p>In other words, it's bad to harm an unconscious person because they have desires, preferences, values, whatever you wish to call them, that harming them would violate.&nbsp; And those values are a part of the structure of their mind that doesn't go away when they sleep.&nbsp; Skin cells and unbuilt AIs, by contrast, have no such values.</p>\n<p>Now, while I think that explanation 4 resolves the issue of desires and unconsciousness best, I do think solution 3 has a great deal of truth to it as well (For instance, I tend to respect the final wishes of a dead person because they had desires in the past, even if they don't now).&nbsp;&nbsp; The solutions 3 and 4 are not incompatible at all, so one can believe in both of them.</p>\n<p>I'm curious as to what people think of my possible solutions.&nbsp; Am I right about people still having something like desires in their brain when they are unconscious?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SScCHA8znL5w4uPeh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "21691", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f", "DfCJoqcFGBqaroc85"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-20T11:51:59.233Z", "modifiedAt": null, "url": null, "title": "Tutoring Small Groups of Children (for money)", "slug": "tutoring-small-groups-of-children-for-money", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.487Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bramflakes", "createdAt": "2011-11-01T22:15:00.964Z", "isAdmin": false, "displayName": "bramflakes"}, "userId": "pJEYMdQjRLEJSg8bX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aquSrgj3TBSydpzeZ/tutoring-small-groups-of-children-for-money", "pageUrlRelative": "/posts/aquSrgj3TBSydpzeZ/tutoring-small-groups-of-children-for-money", "linkUrl": "https://www.lesswrong.com/posts/aquSrgj3TBSydpzeZ/tutoring-small-groups-of-children-for-money", "postedAtFormatted": "Wednesday, February 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tutoring%20Small%20Groups%20of%20Children%20(for%20money)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATutoring%20Small%20Groups%20of%20Children%20(for%20money)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaquSrgj3TBSydpzeZ%2Ftutoring-small-groups-of-children-for-money%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tutoring%20Small%20Groups%20of%20Children%20(for%20money)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaquSrgj3TBSydpzeZ%2Ftutoring-small-groups-of-children-for-money", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaquSrgj3TBSydpzeZ%2Ftutoring-small-groups-of-children-for-money", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 320, "htmlBody": "<blockquote>\n<p>Harry had been sent to the best primary schools - and when that didn't work out, he was provided with tutors from the endless pool of starving students.</p>\n</blockquote>\n<p>In about eight months or so, I will be one of those (hopefully not starving) students. I'll be moving out to London to live with my aunt and uncle in a rather nice middle-class neighbourhood, while I study and work to prepare for university the following year. They know a lot of the parents around there and suggested that I begin teaching small groups of 8-to-12 year old children for maybe an hour or two regularly, and charge their parents/guardians a reasonable sum per child. I would be teaching them math and science in all likelihood. Apparently word will get around quickly if I'm competent so I might have a substantial number of customers within a few months.</p>\n<p>My questions:</p>\n<ul>\n<li>Does anybody on LessWrong have any direct experience at this sort of thing, that could share some advice?</li>\n<li>What are some good things to teach? I'll probably do the standard bottle rockets and baking soda+vinegar volcanoes, but I'd also want to spice things up and teach them something that they'd be hard-pressed to come across elsewhere (simple rationality techniques come to mind)</li>\n<li>What is the best way to teach these things to this age group - or rather, what are some good books or other resources that I can use to teach myself how to teach?</li>\n<li>If I were to teach them some basic rationality skills, which ones and how? Obviously I won't be talking about anything fancy like probability theory unless I strike gold and find a kid on the far-right of the bellcurve, but more like low-hanging fruit. I might do something like that <a href=\"/lw/ip/fake_explanations/\">radiator puzzle</a> to warn them against password-guessing for example.</li>\n</ul>\n<div><br /></div>\n<div>(edit: thanks to whoever fixed the font on this)</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aquSrgj3TBSydpzeZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 1.1176473570563002e-06, "legacy": true, "legacyId": "21693", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fysgqk4CjAwhBgNYT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-20T12:36:38.554Z", "modifiedAt": null, "url": null, "title": "Strongmanning Pascal's Mugging", "slug": "strongmanning-pascal-s-mugging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:05.657Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pentashagon", "createdAt": "2011-12-07T20:42:23.737Z", "isAdmin": false, "displayName": "Pentashagon"}, "userId": "XLW245DqoEBbAsaGh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ND2nzWwmZXW2Pc6zt/strongmanning-pascal-s-mugging", "pageUrlRelative": "/posts/ND2nzWwmZXW2Pc6zt/strongmanning-pascal-s-mugging", "linkUrl": "https://www.lesswrong.com/posts/ND2nzWwmZXW2Pc6zt/strongmanning-pascal-s-mugging", "postedAtFormatted": "Wednesday, February 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Strongmanning%20Pascal's%20Mugging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStrongmanning%20Pascal's%20Mugging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FND2nzWwmZXW2Pc6zt%2Fstrongmanning-pascal-s-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Strongmanning%20Pascal's%20Mugging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FND2nzWwmZXW2Pc6zt%2Fstrongmanning-pascal-s-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FND2nzWwmZXW2Pc6zt%2Fstrongmanning-pascal-s-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 547, "htmlBody": "<p>A mugger appears and says \"For $5 I'll offer you a set of deals from which you can pick any one.&nbsp; Each deal, d(N), will be N bits in length and I guarantee that if you accept d(N) I will run UTM(d(N)) on my hypercomputer, where UTM() is a function implementing a Universal Turing Machine.&nbsp; If UTM(d(N)) halts you will increase your utility by the number of bits written to the tape by UTM(d(N)).&nbsp; If UTM(d(N)) does not halt, I'll just keep your $5.&nbsp; Which deal would you like to accept?\"</p>\n<p>The expected increase in utility of any deal is p(d(N)) * U(UTM(d(N)), where p(d(N)) is the probability of accepting d(N) and actually receiving as many utilons as the number of bits a halting UTM(d(N)) writes to its tape.&nbsp; A non-empty subset of UTM programs of length N will write BB(N) bits to the tape where BB(X) is the busy-beaver function for programs of bit length X.&nbsp; Since BB(X) &gt;= UTM(F) for any function F of bit length X, for every finite agent there is some N for which p(UTM(d(N)) = BB(N)) * BB(N) &gt; 0.&nbsp; To paraphrase:&nbsp; Even though the likelihood of being offered a deal that actually yields BB(N) utilons is incredibly small, the fact that BB(X) grows at least as fast as any function of length X means that, at minimum, an agent that can be emulated on a UTM by a program of M bits cannot provide a non-zero probability of d(M) such that the expected utility of accepting d(M) is negative.&nbsp; In practice N can probably be much less than M.</p>\n<p>Since p(\"UTM(d(X)) = BB(X)\") &gt;= 2^-X for d(X) with bits selected at random it doesn't make sense for the agent to assign p(d(X))=0 unless the agent has other reasons to absolutely distrust the mugger.&nbsp; For instance, discounting the probability of a deal based on a function of the promised number of utilons won't work; no discounting function grows as fast as BB(X) and an agent can't compute an arbitrary UTM(d(X)) to get a probability estimate without hypercomputational abilities.&nbsp; Any marginal-utility calculation fails in a similar manner.</p>\n<p>I'm not sure where to go from here.&nbsp; I don't think it's rational to spend the rest of my life trying to find the largest integer I can think of to acausally accept d(biggest-integer) from some Omega.&nbsp; So far the strongest counterargument I've been able to think of is attempting to manage the risk of accepting the mugging by attempting to buy insurance of some sort.&nbsp; For example, a mugger offering intractably large amounts of utility for $5 shouldn't mind offering the agent a loan for $5 (or even $10,000) if the agent can immediately pay it back with astronomical amounts of interest out of the wealth that would almost certainly become available if the mugger fulfilled the deal.&nbsp; In short, it doesn't make sense to exchange utility now for utility in the future *unless* the mugger will accept what is essentially a counter-mugging that yields more long term utility for the mugger at the cost of some short term disutility.&nbsp; The mugger should have some non-zero probability, p, for which zhe is indifferent between p*\"have $10 after fulfilling the deal\" and (1-p)*\"have $5 now\".&nbsp; If the mugger acts like p=0 for this lottery, why can't the agent?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HNJiR8Jzafsv8cHrC": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ND2nzWwmZXW2Pc6zt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "21694", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-20T16:35:47.731Z", "modifiedAt": null, "url": null, "title": "singinst reading recommendations", "slug": "singinst-reading-recommendations", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bPxhYiNM58PTSRdZr/singinst-reading-recommendations", "pageUrlRelative": "/posts/bPxhYiNM58PTSRdZr/singinst-reading-recommendations", "linkUrl": "https://www.lesswrong.com/posts/bPxhYiNM58PTSRdZr/singinst-reading-recommendations", "postedAtFormatted": "Wednesday, February 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20singinst%20reading%20recommendations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Asinginst%20reading%20recommendations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbPxhYiNM58PTSRdZr%2Fsinginst-reading-recommendations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=singinst%20reading%20recommendations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbPxhYiNM58PTSRdZr%2Fsinginst-reading-recommendations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbPxhYiNM58PTSRdZr%2Fsinginst-reading-recommendations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 19, "htmlBody": "<p>Whatever happened to all of these pages and pages of reading recommendations? I had to go <a href=\"http://web.archive.org/web/20070101134424/http://www.singinst.org/reading/corereading/\">wayback to 2007</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bPxhYiNM58PTSRdZr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": -1, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "21695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-20T21:29:20.722Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-03]", "slug": "meetup-cambridge-uk-lw-meetup-reading-group-haefb-03", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.418Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sohum", "createdAt": "2012-02-06T06:28:59.691Z", "isAdmin": false, "displayName": "Sohum"}, "userId": "DP4jNdSvbeAofhxkb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ju4zxfZWdgguLR8rq/meetup-cambridge-uk-lw-meetup-reading-group-haefb-03", "pageUrlRelative": "/posts/Ju4zxfZWdgguLR8rq/meetup-cambridge-uk-lw-meetup-reading-group-haefb-03", "linkUrl": "https://www.lesswrong.com/posts/Ju4zxfZWdgguLR8rq/meetup-cambridge-uk-lw-meetup-reading-group-haefb-03", "postedAtFormatted": "Wednesday, February 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-03%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-03%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJu4zxfZWdgguLR8rq%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-03%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20UK%20LW%20Meetup%20%5BReading%20Group%2C%20HAEFB-03%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJu4zxfZWdgguLR8rq%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-03", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJu4zxfZWdgguLR8rq%2Fmeetup-cambridge-uk-lw-meetup-reading-group-haefb-03", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jn'>Cambridge, UK LW Meetup [Reading Group, HAEFB-03]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 February 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Trinity JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup! This week, we'll be continuing our reading group session of the Shiny New Sequence, Highly Advanced Epistemology 101 For Beginners. These are explicitly designed as introductory posts, and this is just our third session, so dive right in with us if you're new!</p>\n\n<p>We'll be covering The Fabric of Real Things\u00a0and its adjunct,\u00a0Casual Diagrams and Casual Models, this week.</p>\n\n<p>See you there!</p>\n\n<p>-Sohum</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jn'>Cambridge, UK LW Meetup [Reading Group, HAEFB-03]</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ju4zxfZWdgguLR8rq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.1180167098241799e-06, "legacy": true, "legacyId": "21697", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_03_\">Discussion article for the meetup : <a href=\"/meetups/jn\">Cambridge, UK LW Meetup [Reading Group, HAEFB-03]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 February 2013 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Trinity JCR, Cambridge, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup! This week, we'll be continuing our reading group session of the Shiny New Sequence, Highly Advanced Epistemology 101 For Beginners. These are explicitly designed as introductory posts, and this is just our third session, so dive right in with us if you're new!</p>\n\n<p>We'll be covering The Fabric of Real Things&nbsp;and its adjunct,&nbsp;Casual Diagrams and Casual Models, this week.</p>\n\n<p>See you there!</p>\n\n<p>-Sohum</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_03_1\">Discussion article for the meetup : <a href=\"/meetups/jn\">Cambridge, UK LW Meetup [Reading Group, HAEFB-03]</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-03]", "anchor": "Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_03_", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, UK LW Meetup [Reading Group, HAEFB-03]", "anchor": "Discussion_article_for_the_meetup___Cambridge__UK_LW_Meetup__Reading_Group__HAEFB_03_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-21T05:39:33.987Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Thing That I Protect", "slug": "seq-rerun-the-thing-that-i-protect", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P6Dw7THmsqwaoCcnT/seq-rerun-the-thing-that-i-protect", "pageUrlRelative": "/posts/P6Dw7THmsqwaoCcnT/seq-rerun-the-thing-that-i-protect", "linkUrl": "https://www.lesswrong.com/posts/P6Dw7THmsqwaoCcnT/seq-rerun-the-thing-that-i-protect", "postedAtFormatted": "Thursday, February 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Thing%20That%20I%20Protect&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Thing%20That%20I%20Protect%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP6Dw7THmsqwaoCcnT%2Fseq-rerun-the-thing-that-i-protect%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Thing%20That%20I%20Protect%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP6Dw7THmsqwaoCcnT%2Fseq-rerun-the-thing-that-i-protect", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP6Dw7THmsqwaoCcnT%2Fseq-rerun-the-thing-that-i-protect", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>Today's post, <a href=\"/lw/yd/the_thing_that_i_protect/\">The Thing That I Protect</a> was originally published on 07 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#The_Thing_That_I_Protect\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The cause that drives Yudkowsky isn't Friendly AI, and it isn't even specifically about preserving human values. It's simply about a future that's a lot better than the present.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/gq9/seq_rerun_epilogue_atonement_88/\">Epilogue: Atonement (8/8)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P6Dw7THmsqwaoCcnT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.118330488528168e-06, "legacy": true, "legacyId": "21700", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wyMbgeFfQWntFxTf", "av9Lj9t33XN86fzYr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-21T06:10:07.731Z", "modifiedAt": null, "url": null, "title": "Utilitarianism twice fails", "slug": "utilitarianism-twice-fails", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:01.932Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Mbcz7h6fDcnEPoubW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wLuTtRD9nP9RodpP8/utilitarianism-twice-fails", "pageUrlRelative": "/posts/wLuTtRD9nP9RodpP8/utilitarianism-twice-fails", "linkUrl": "https://www.lesswrong.com/posts/wLuTtRD9nP9RodpP8/utilitarianism-twice-fails", "postedAtFormatted": "Thursday, February 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Utilitarianism%20twice%20fails&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUtilitarianism%20twice%20fails%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLuTtRD9nP9RodpP8%2Futilitarianism-twice-fails%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Utilitarianism%20twice%20fails%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLuTtRD9nP9RodpP8%2Futilitarianism-twice-fails", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLuTtRD9nP9RodpP8%2Futilitarianism-twice-fails", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1247, "htmlBody": "<p>(<a href=\"http://juridicalcoherence.blogspot.com/2013/02/1411-utilitarianism-twice-fails.html\">Crossposted</a>.)</p>\n<p>It seems almost self-evident that (barring foreign subjugation) a government will care about the wants of (some of) its citizens and nothing else: no other object of concern is plausible. If governments concern themselves with the wants of noncitizens, that will be only because citizens desire their well-being. The now platitudinous insight that the only possible basis for government policy is people&rsquo;s wants can be attributed to utilitarianism, which gets credit in its stronger form for the apparent success of weaker claims. <br /><br />Another reasonable claim derives from utilitarianism: citizens&rsquo; wants should count equally. This seems only fair in a democracy, where one citizen gets one vote. Few today would deny the principle that public policy should serve the most good of the greatest number, which may seem to contradict <a href=\"http://juridicalcoherence.blogspot.com/2012/01/141-habit-theory-of-civic-morality.html\">my claim</a> that no general moral principle governs public policy, but in practice, the consequences of this limited utilitarianism are thin indeed, leaving ample room for ideology. I&rsquo;ll call thin utilitarianism this public-policy formula: the greatest good for the greatest number of citizens, weighting their welfare equally. <br /><br />First, I&rsquo;ll consider whether thin utilitarianism succeeds on its own terms by providing a practical guide to public policy. Second, I&rsquo;ll examine how this deceptively appealing guide to public policy transmogrifies into the monster of full-blown utilitarianism, a form of <a href=\"http://juridicalcoherence.blogspot.com/2011/12/14-why-do-what-oughta-habit-theory-of.html\">moral realism</a>. The first constrains even casual use of thin utilitarianism; the second impugns utilitarianism as a general ethical theory.</p>\n<h4>1. Non-negotiable conflicts between subagents undermine thin utilitarianism</h4>\n<p>Although simple economic models attributing conduct to rational self-interest require that agents assign consistent utilities to outcomes, agents are inconsistent. One example of inconsistent utility assignment is the endowment effect, where agents assign more value to property they own than&nbsp; to the same property they don&rsquo;t own. The inconsistency considered here is stronger than the endowment effect and similar phenomena that we can surmount with effort, as professional traders must do. Despite the effect, there is a real answer to how much utility an outcome affords; the endowment effect is a bias, which willpower or habit can neutralize.<br /><br />The conflict between subagents within a single person, on the other hand, can&rsquo;t be resolved by means of a common criterion, such as market price, since two subagents pursue different ends. Which of these subagents dominates depends on situational and personological factors that elicit one or the other, not on overcoming bias. Construal-level theory reveals a conflict between intrapersonal subagents, near-mode and far-mode, integrated mindsets applied to matter experienced at fine or broad granularities. Modes (or &ldquo;construal levels&rdquo;) differ in that far-mode is more future-oriented and principled, near-mode, present-oriented and contextual. Far-mode and near-mode are elicited by the way social choices are made: voting elicits far-mode and market choices, near-mode; the utility of a choice depends on construal level.<br /><br />Take a policy choice: how much wealth should be spent on preventive medicine? There are two basic ways allocating resources to medical care, political process and the market, socialized medicine being an example of political process, private medicine, the market. Socialized medicine makes allocating funds for the medical care a political decision; the market makes it each consumer&rsquo;s personal choice. When you compare the utility of the choices by political process with those on the market, you should expect to find that when people choose politically, they use far-mode thinking encouraged by voting; whereas when they make purchases, they use near-mode thinking encouraged by the market. The preventive-care expenditure will be higher under socialized medicine because political process elicits far-mode, which is concerned with future health. People will be more miserly with preventive care under private medicine, where the decision to spend is made by consumer choice in near-mode, where we care more about the present. People favor spending more on preventive care when they vote to tax themselves than when they buy it on the market. Which outcome provides the greater utility&mdash;more preventive care or more recreation&mdash;is relative to construal level.<br /><br />The same indeterminacy of utility occurs when comparing decisions made under different political processes, such as local versus central. Local decisions will be near-mode, central decisions far-mode. Assuming socialized medicine, less funding would be available if it were subject to state rather than federal control. Which provides more utility depends on whether the consequences are evaluated in near-mode or far-mode; no thin-utilitarian criterion applies.<br /><br />Some utilitarians will protest that we should measure experiences rather than wants. The objection misses the argument&rsquo;s point, which is that utility is relative to mode, a conclusion easiest to see in the public-choice process because the alternatives may be delimited. If the conclusion that utility depends on construal level holds, the same indeterminacies occur in evaluating experience. That apart, when utilitarianism is applied to public policy, present wants rather than experienced satisfaction is the criterion; agents necessarily choose based on present wants whether on the market or the political process.</p>\n<h4>2. Full-blown utilitarianism stands convicted of moral realism</h4>\n<p>Full-blown utilitarians are necessarily moral realists, but increasingly they are seen to deny it. While moral realism is widely recognized as absurd, utilitarianism seems to some an attractive ethical philosophy. For the sake of intellectual respectability, utilitarians can appear to reject an anachronistic moral realism while practicing it philosophically.<br /><br />Full-blown utilitarianism often obscures its differences with thin utilitarianism, which is a questionable doctrine but in accord with ordinary common sense. It emerges from thin utilitarianism by the misdirection of subjecting ethical premises to the test of simplicity, a test appropriate to realist theories exclusively, because simplicity serves truth. A classic illustration: Aristotle theorized that everything on earth that goes up goes down; Newton set out the gravity theory, which applies to all objects, not just those terrestrial, and which predicts that objects can escape the earth&rsquo;s gravitational field by traveling fast. Scientists confidently bet on Newton well before rockets were invented, and their confidence was vastly increased by the simplicity of Newton&rsquo;s theory, which made correct predictions concerning all objects. Although philosophers have explained variously the correlation between simplicity and truth, they generally agree that simplicity signals truth. Unless utilitarians can otherwise justify it, searching for a simple moral theory means searching for a true theory.<br /><br />The full-blown utilitarian seeks a misplaced simplicity by insisting that all entities that can experience happiness, a much simpler criterion than &ldquo;current citizens,&rdquo; serve as the beneficiary reference group&mdash;including future generations of humans and even beasts, whose existence depends on policy; whereas, thin utilitarianism is a democratic convention, serving only the wants of the currently existing citizens . Because they must incorporate future generations into the reference group, utilitarian philosophers have had to accept that a policy-dependent reference group entails a dilemma regarding interpretation of full-blown utilitarianism, with unattractive consequences at both horns, which realize radically different ideals.&nbsp; In one version, you maximize the average utility obtained by the whole population; in the other you sum the utilities. These interpretations seem almost equally unattractive: the averaging view says that one supremely happy human is better than a billion very happy ones; the adding approach implies that a hundred trillion miserable wretches is better than a billion happy people. To apply a utilitarian standard to scenarios so distant from thin utilitarianism, accepting their consequences because of simplicity&rsquo;s demands, is to treat moral premises as truths and to practice moral realism, despite contrary self-description. Those agreeing that moral realism is impossible must reject full-blown utilitarianism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wLuTtRD9nP9RodpP8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": -45, "extendedScore": null, "score": -0.0001, "legacy": true, "legacyId": "21699", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(<a href=\"http://juridicalcoherence.blogspot.com/2013/02/1411-utilitarianism-twice-fails.html\">Crossposted</a>.)</p>\n<p>It seems almost self-evident that (barring foreign subjugation) a government will care about the wants of (some of) its citizens and nothing else: no other object of concern is plausible. If governments concern themselves with the wants of noncitizens, that will be only because citizens desire their well-being. The now platitudinous insight that the only possible basis for government policy is people\u2019s wants can be attributed to utilitarianism, which gets credit in its stronger form for the apparent success of weaker claims. <br><br>Another reasonable claim derives from utilitarianism: citizens\u2019 wants should count equally. This seems only fair in a democracy, where one citizen gets one vote. Few today would deny the principle that public policy should serve the most good of the greatest number, which may seem to contradict <a href=\"http://juridicalcoherence.blogspot.com/2012/01/141-habit-theory-of-civic-morality.html\">my claim</a> that no general moral principle governs public policy, but in practice, the consequences of this limited utilitarianism are thin indeed, leaving ample room for ideology. I\u2019ll call thin utilitarianism this public-policy formula: the greatest good for the greatest number of citizens, weighting their welfare equally. <br><br>First, I\u2019ll consider whether thin utilitarianism succeeds on its own terms by providing a practical guide to public policy. Second, I\u2019ll examine how this deceptively appealing guide to public policy transmogrifies into the monster of full-blown utilitarianism, a form of <a href=\"http://juridicalcoherence.blogspot.com/2011/12/14-why-do-what-oughta-habit-theory-of.html\">moral realism</a>. The first constrains even casual use of thin utilitarianism; the second impugns utilitarianism as a general ethical theory.</p>\n<h4 id=\"1__Non_negotiable_conflicts_between_subagents_undermine_thin_utilitarianism\">1. Non-negotiable conflicts between subagents undermine thin utilitarianism</h4>\n<p>Although simple economic models attributing conduct to rational self-interest require that agents assign consistent utilities to outcomes, agents are inconsistent. One example of inconsistent utility assignment is the endowment effect, where agents assign more value to property they own than&nbsp; to the same property they don\u2019t own. The inconsistency considered here is stronger than the endowment effect and similar phenomena that we can surmount with effort, as professional traders must do. Despite the effect, there is a real answer to how much utility an outcome affords; the endowment effect is a bias, which willpower or habit can neutralize.<br><br>The conflict between subagents within a single person, on the other hand, can\u2019t be resolved by means of a common criterion, such as market price, since two subagents pursue different ends. Which of these subagents dominates depends on situational and personological factors that elicit one or the other, not on overcoming bias. Construal-level theory reveals a conflict between intrapersonal subagents, near-mode and far-mode, integrated mindsets applied to matter experienced at fine or broad granularities. Modes (or \u201cconstrual levels\u201d) differ in that far-mode is more future-oriented and principled, near-mode, present-oriented and contextual. Far-mode and near-mode are elicited by the way social choices are made: voting elicits far-mode and market choices, near-mode; the utility of a choice depends on construal level.<br><br>Take a policy choice: how much wealth should be spent on preventive medicine? There are two basic ways allocating resources to medical care, political process and the market, socialized medicine being an example of political process, private medicine, the market. Socialized medicine makes allocating funds for the medical care a political decision; the market makes it each consumer\u2019s personal choice. When you compare the utility of the choices by political process with those on the market, you should expect to find that when people choose politically, they use far-mode thinking encouraged by voting; whereas when they make purchases, they use near-mode thinking encouraged by the market. The preventive-care expenditure will be higher under socialized medicine because political process elicits far-mode, which is concerned with future health. People will be more miserly with preventive care under private medicine, where the decision to spend is made by consumer choice in near-mode, where we care more about the present. People favor spending more on preventive care when they vote to tax themselves than when they buy it on the market. Which outcome provides the greater utility\u2014more preventive care or more recreation\u2014is relative to construal level.<br><br>The same indeterminacy of utility occurs when comparing decisions made under different political processes, such as local versus central. Local decisions will be near-mode, central decisions far-mode. Assuming socialized medicine, less funding would be available if it were subject to state rather than federal control. Which provides more utility depends on whether the consequences are evaluated in near-mode or far-mode; no thin-utilitarian criterion applies.<br><br>Some utilitarians will protest that we should measure experiences rather than wants. The objection misses the argument\u2019s point, which is that utility is relative to mode, a conclusion easiest to see in the public-choice process because the alternatives may be delimited. If the conclusion that utility depends on construal level holds, the same indeterminacies occur in evaluating experience. That apart, when utilitarianism is applied to public policy, present wants rather than experienced satisfaction is the criterion; agents necessarily choose based on present wants whether on the market or the political process.</p>\n<h4 id=\"2__Full_blown_utilitarianism_stands_convicted_of_moral_realism\">2. Full-blown utilitarianism stands convicted of moral realism</h4>\n<p>Full-blown utilitarians are necessarily moral realists, but increasingly they are seen to deny it. While moral realism is widely recognized as absurd, utilitarianism seems to some an attractive ethical philosophy. For the sake of intellectual respectability, utilitarians can appear to reject an anachronistic moral realism while practicing it philosophically.<br><br>Full-blown utilitarianism often obscures its differences with thin utilitarianism, which is a questionable doctrine but in accord with ordinary common sense. It emerges from thin utilitarianism by the misdirection of subjecting ethical premises to the test of simplicity, a test appropriate to realist theories exclusively, because simplicity serves truth. A classic illustration: Aristotle theorized that everything on earth that goes up goes down; Newton set out the gravity theory, which applies to all objects, not just those terrestrial, and which predicts that objects can escape the earth\u2019s gravitational field by traveling fast. Scientists confidently bet on Newton well before rockets were invented, and their confidence was vastly increased by the simplicity of Newton\u2019s theory, which made correct predictions concerning all objects. Although philosophers have explained variously the correlation between simplicity and truth, they generally agree that simplicity signals truth. Unless utilitarians can otherwise justify it, searching for a simple moral theory means searching for a true theory.<br><br>The full-blown utilitarian seeks a misplaced simplicity by insisting that all entities that can experience happiness, a much simpler criterion than \u201ccurrent citizens,\u201d serve as the beneficiary reference group\u2014including future generations of humans and even beasts, whose existence depends on policy; whereas, thin utilitarianism is a democratic convention, serving only the wants of the currently existing citizens . Because they must incorporate future generations into the reference group, utilitarian philosophers have had to accept that a policy-dependent reference group entails a dilemma regarding interpretation of full-blown utilitarianism, with unattractive consequences at both horns, which realize radically different ideals.&nbsp; In one version, you maximize the average utility obtained by the whole population; in the other you sum the utilities. These interpretations seem almost equally unattractive: the averaging view says that one supremely happy human is better than a billion very happy ones; the adding approach implies that a hundred trillion miserable wretches is better than a billion happy people. To apply a utilitarian standard to scenarios so distant from thin utilitarianism, accepting their consequences because of simplicity\u2019s demands, is to treat moral premises as truths and to practice moral realism, despite contrary self-description. Those agreeing that moral realism is impossible must reject full-blown utilitarianism.</p>", "sections": [{"title": "1. Non-negotiable conflicts between subagents undermine thin utilitarianism", "anchor": "1__Non_negotiable_conflicts_between_subagents_undermine_thin_utilitarianism", "level": 1}, {"title": "2. Full-blown utilitarianism stands convicted of moral realism", "anchor": "2__Full_blown_utilitarianism_stands_convicted_of_moral_realism", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "17 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-21T06:19:21.995Z", "modifiedAt": null, "url": null, "title": "The Logic of the Hypothesis Test: A Steel Man", "slug": "the-logic-of-the-hypothesis-test-a-steel-man", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Tdry32XHpYEYbyho5/the-logic-of-the-hypothesis-test-a-steel-man", "pageUrlRelative": "/posts/Tdry32XHpYEYbyho5/the-logic-of-the-hypothesis-test-a-steel-man", "linkUrl": "https://www.lesswrong.com/posts/Tdry32XHpYEYbyho5/the-logic-of-the-hypothesis-test-a-steel-man", "postedAtFormatted": "Thursday, February 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Logic%20of%20the%20Hypothesis%20Test%3A%20A%20Steel%20Man&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Logic%20of%20the%20Hypothesis%20Test%3A%20A%20Steel%20Man%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTdry32XHpYEYbyho5%2Fthe-logic-of-the-hypothesis-test-a-steel-man%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Logic%20of%20the%20Hypothesis%20Test%3A%20A%20Steel%20Man%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTdry32XHpYEYbyho5%2Fthe-logic-of-the-hypothesis-test-a-steel-man", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTdry32XHpYEYbyho5%2Fthe-logic-of-the-hypothesis-test-a-steel-man", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 733, "htmlBody": "<p><strong>Related to:</strong>&nbsp;<a title=\"http://lesswrong.com/lw/f7t/beyond_bayesians_and_frequentists/\" href=\"/lw/f7t/beyond_bayesians_and_frequentists/\" target=\"_blank\">Beyond Bayesians and Frequentists</a></p>\n<p><strong>Update:</strong>&nbsp;This <a title=\"http://lesswrong.com/lw/gqt/the_logic_of_the_hypothesis_test_a_steel_man/8idx\" href=\"/lw/gqt/the_logic_of_the_hypothesis_test_a_steel_man/8idx\" target=\"_blank\">comment</a> by Cyan clearly explains the mistake I made - I forgot that the ordering of the hypothesis space is important is necessary for hypothesis testing to work. I'm not entirely convinced that NHST can't be recast in some \"thin\" theory of induction that may well change the details of the actual test, but I have no idea how to formalize this notion of a \"thin\" theory and most of the commenters either 1) misunderstood my aim (my fault, not theirs) or 2) don't think it can be formalized.</p>\n<p>I'm teaching an econometrics course this semester and one of the things I'm trying to do is make sure that my students actually understand the logic of the hypothesis test. You can motivate it in terms of controlling false positives but that sort of interpretation doesn't seem to be generally applicable. Another motivation is a simple deductive syllogism with a small but very important inductive component. I'm borrowing the idea from a something we discussed in a course I had with Mark Kaiser - he called it the \"nested syllogism of experimentation.\" I think it applies equally well to most or even all hypothesis tests. It goes something like this:</p>\n<p>1. Either the null hypothesis or the alternative hypothesis is true.</p>\n<p>2. If the null hypothesis is true, then the data has a certain probability distribution.</p>\n<p>3. Under this distribution, our sample is extremely unlikely.</p>\n<p>4. Therefore under the null hypothesis, our sample is extremely unlikely.</p>\n<p>5. Therefore the null hypothesis is false.</p>\n<p>6. Therefore the alternative hypothesis is true.</p>\n<p>An example looks like this:</p>\n<p>Suppose we have a random sample from a population with a normal distribution that has an unknown mean <img title=\"\\mu\" src=\"http://www.codecogs.com/png.latex?\\mu\" alt=\"\" align=\"bottom\" /> and unknown variance <img title=\"\\sigma^2\" src=\"http://www.codecogs.com/png.latex?\\sigma^2\" alt=\"\" align=\"bottom\" />. Then:</p>\n<p>1. Either <img title=\"H_0:\\mu=c\" src=\"http://www.codecogs.com/png.latex?H_0:\\mu=c\" alt=\"\" align=\"bottom\" /> or <img title=\"H_a:\\mu\\neq c\" src=\"http://www.codecogs.com/png.latex?H_a:\\mu\\neq c\" alt=\"\" align=\"bottom\" /> where <img title=\"c\" src=\"http://www.codecogs.com/png.latex?c\" alt=\"\" align=\"bottom\" /> is some constant.</p>\n<p>2. Construct the test statistic <img title=\"t^*=\\frac{\\bar{X}-c}{s/\\sqrt{n}}\" src=\"http://www.codecogs.com/png.latex?t^*=\\frac{\\bar{X}-c}{s/\\sqrt{n}}\" alt=\"\" align=\"bottom\" /> where <img title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" /> is the sample size, <img title=\"\\bar{X}\" src=\"http://www.codecogs.com/png.latex?\\bar{X}\" alt=\"\" align=\"bottom\" /> is the sample mean, and <img title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /> is the sample standard deviation.</p>\n<p>3. Under the null hypothesis, <img title=\"t^*\" src=\"http://www.codecogs.com/png.latex?t^*\" alt=\"\" align=\"bottom\" /> has a <img title=\"t\" src=\"http://www.codecogs.com/png.latex?t\" alt=\"\" align=\"bottom\" /> distribution with <img title=\"n-1\" src=\"http://www.codecogs.com/png.latex?n-1\" alt=\"\" align=\"bottom\" /> degrees of freedom.</p>\n<p>4. <img title=\"P(|t|&gt;|t^*|)\" src=\"http://www.codecogs.com/png.latex?P(|t|&gt;|t^*|)\" alt=\"\" align=\"bottom\" /> is really small under the null hypothesis (e.g. less than 0.05).</p>\n<p>5. Therefore the null hypothesis is false.</p>\n<p>6. Therefore the alternative hypothesis is true.</p>\n<p>What's interesting to me about this process is that it almost tries to avoid induction altogether. Only the move from step 4 to 5 seems anything like an inductive argument. The rest is purely deductive - though admittedly it takes a couple premises in order to quantify just how likely our sample was and that surely has something to do with induction. But it's still a bit like solving the problem of induction by sweeping it under the rug then putting a big heavy deduction table on top so no one notices the lumps underneath.&nbsp;</p>\n<p>This sounds like it's a criticism, but actually I think it might be a virtue to minimize the amount of induction in your argument. Suppose you're really uncertain about how to handle induction. Maybe you see a lot of plausible sounding approaches, but you can poke holes in all of them. So instead of trying to actually solve the problem of induction, you set out to come up with a process which is robust to alternative views of induction. Ideally, if one or another theory of induction turns out to be correct, you'd like it to do the least damage possible to any specific inductive inferences you've made. One way to do this is to avoid induction as much as possible so that you prevent \"inductive contamination\" spreading to everything you believe.&nbsp;</p>\n<p>That's exactly what hypothesis testing seems to do. You start with a set of premises and keep deriving logical conclusions from them until you're forced to say \"this seems really unlikely if a certain hypothesis is true, so we'll assume that the hypothesis is false\" in order to get any further. Then you just keep on deriving logical conclusions with your new premise. Bayesians start yelling about the base rate fallacy in the inductive step, but they're presupposing their own theory of induction. If you're trying to be robust to inductive theories, why should you listen to a Bayesian instead of anyone else?</p>\n<p>Now does hypothesis testing actually accomplish induction that is robust to philosophical views of induction? Well, I don't know - I'm really just spitballing here. But it does seem to be a useful <a title=\"http://wiki.lesswrong.com/wiki/Steel_man\" href=\"http://wiki.lesswrong.com/wiki/Steel_man\" target=\"_blank\">steel man</a>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Tdry32XHpYEYbyho5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 9, "extendedScore": null, "score": 1.1183559703144975e-06, "legacy": true, "legacyId": "21701", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["o32tEFf5zBiByL2xv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-21T08:17:27.931Z", "modifiedAt": null, "url": null, "title": "[Links] Brain mapping/emulation news", "slug": "links-brain-mapping-emulation-news", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.161Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PxaMA44u8WBz2fZDh/links-brain-mapping-emulation-news", "pageUrlRelative": "/posts/PxaMA44u8WBz2fZDh/links-brain-mapping-emulation-news", "linkUrl": "https://www.lesswrong.com/posts/PxaMA44u8WBz2fZDh/links-brain-mapping-emulation-news", "postedAtFormatted": "Thursday, February 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLinks%5D%20Brain%20mapping%2Femulation%20news&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLinks%5D%20Brain%20mapping%2Femulation%20news%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPxaMA44u8WBz2fZDh%2Flinks-brain-mapping-emulation-news%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLinks%5D%20Brain%20mapping%2Femulation%20news%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPxaMA44u8WBz2fZDh%2Flinks-brain-mapping-emulation-news", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPxaMA44u8WBz2fZDh%2Flinks-brain-mapping-emulation-news", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<p><a href=\"http://www.nytimes.com/2013/02/18/science/project-seeks-to-build-map-of-human-brain.html?pagewanted=all&amp;_r=1&amp;\">Obama Seeking to Boost Study of Human Brain</a> - Like the Human Genome Project, but for brain mapping (Feb 17)</p>\n<p>Human brain and graphene projects chosen for one billion euro grants: <a href=\"http://cordis.europa.eu/fp7/ict/programme/fet/flagship/doc/press28jan13-01_en.pdf\">official press release</a> (Jan 28)</p>\n<p><a href=\"http://www.newyorker.com/online/blogs/newsdesk/2013/02/obamas-brain.html#entry-more\">Gary Marcus reacts</a></p>\n<p>Edit: If anyone is going to email the people behind Obama's human brain project and offer suggestions, it's probably best to do so ASAP before they make the details of their project public and risk losing face by changing them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PxaMA44u8WBz2fZDh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 1.1184315888646948e-06, "legacy": true, "legacyId": "21710", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-21T10:29:18.426Z", "modifiedAt": null, "url": null, "title": "CEA does not seem to be credibly high impact", "slug": "cea-does-not-seem-to-be-credibly-high-impact", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:38.503Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Lee", "createdAt": "2009-09-10T00:05:08.577Z", "isAdmin": false, "displayName": "Jonathan_Lee"}, "userId": "8qL3Hsw2TzaLPu3Bh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KBDiWMqhaYe7uPHvN/cea-does-not-seem-to-be-credibly-high-impact", "pageUrlRelative": "/posts/KBDiWMqhaYe7uPHvN/cea-does-not-seem-to-be-credibly-high-impact", "linkUrl": "https://www.lesswrong.com/posts/KBDiWMqhaYe7uPHvN/cea-does-not-seem-to-be-credibly-high-impact", "postedAtFormatted": "Thursday, February 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CEA%20does%20not%20seem%20to%20be%20credibly%20high%20impact&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACEA%20does%20not%20seem%20to%20be%20credibly%20high%20impact%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKBDiWMqhaYe7uPHvN%2Fcea-does-not-seem-to-be-credibly-high-impact%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CEA%20does%20not%20seem%20to%20be%20credibly%20high%20impact%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKBDiWMqhaYe7uPHvN%2Fcea-does-not-seem-to-be-credibly-high-impact", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKBDiWMqhaYe7uPHvN%2Fcea-does-not-seem-to-be-credibly-high-impact", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2448, "htmlBody": "<p><em>I am highly grateful to Alexey Morgunov and Adam Casey for reviewing and commenting on an earlier draft of this post, and pestering me into migrating the content from many emails to a somewhat coherent post.<br /></em></p>\n<p>Will Crouch has <a href=\"/lw/fej/giving_what_we_can_80000_hours_and_metacharity/\">posted</a> about the Centre for Effective Altruism and in a follow up <a href=\"/lw/fkz/responses_to_questions_on_donating_to_80k_gwwc/\">post</a> discussed questions in more detail. The general sense of the discussion of that post was that the arguments were convincing and that donating to CEA is a good idea. Recently, he visited Cambridge, primarily to discuss 80,000 hours, and several Cambridge LWers spoke with him. These discussions caused a number of us to substantially downgrade our estimates of the effectiveness of CEA, and made our concerns more concrete.</p>\n<p><a id=\"more\"></a></p>\n<p>We're aware that our kind often don't cooperate well, but we are concerned that at present CEA's projects are unlikely to cash out into large numbers of people changing their behaviour. Ultimately, we are concerned that the space for high impact meta-charity is limited, and that if CEA is suboptimal this will have large opportunity costs. We want CEA to change the world, and would prefer this happens quickly.</p>\n<p>The key argument in favour of donating money to CEA which was presented by Will was that by donating $1 to CEA you produce more than $1 in donations to the most effective charities. We present some apparent difficulties with this remaining true on the margin. We also present more general worries with CEA as an organisation under these headings:</p>\n<p>Transparency<br />&nbsp;&nbsp;&nbsp; Cost effectiveness estimates<br />&nbsp;&nbsp;&nbsp; Research<br />80,000 hours<br />&nbsp;&nbsp;&nbsp; Impact of 80,000 hours advice<br />&nbsp;&nbsp;&nbsp; Content of 80,000 hours advice<br />&nbsp;&nbsp;&nbsp; The 80,000 hours pledge<br />Scope and Goals<br />&nbsp;&nbsp;&nbsp; Speed of growth<br /> &nbsp;&nbsp;&nbsp; Ambition</p>\n<p>&nbsp;</p>\n<h1>Transparency</h1>\n<p>It is worrying how little of the key information about CEA is publicly available. This makes assessment hard. By contrast to GiveWell, CEA programs are not particularly open about where their money is spent, what their marginal goals are, or what they are doing internally. As presented online, the majority of both 80,000 hours' and GWWC's day to day activity is maintaining blogs. These blogs are not substantial by comparison to, say, OB in terms of their frequency of content or their frequency of insight. Concretely, it does not seem that CEA is being tranparent in the sense of <a href=\"http://www.givewell.org/criteria\">GiveWell</a>.<em></em></p>\n<p><em>Qn: How does CEA think its programs would score on a GiveWell assessment?<br />Qn: Does CEA think that GiveWell&rsquo;s assessments systemically go wrong?</em><em></em></p>\n<p><em>Qn: Does CEA consider the blogs to be a substantial source of impact? What external assessments or objective data support a claim of impact from the blogs?</em></p>\n<p><em><br /></em></p>\n<h2>Cost effectiveness estimates</h2>\n<p>As presented online and in person, CEA does not present as having credible models for their future impact. The GWWC site, for example, claims that from 291 members there will be &pound;72.68M pledged. This equates to &pound;250K / person over the course of their life. Claiming that this level of pledging will occur requires either unreasonable rates of donation or multi-decade payment schedules. If, in line with GWWC's projections, around 50% of people will maintain their donations, then assuming a linear drop off the expected pledge from a full time member is around &pound;375K. Over a lifetime, this is essentially &pound;10K / year. It seems implausible that expected mean annual earnings for GWWC members is of order &pound;100K.</p>\n<p><em>Qn: On what basis does GWWC assert that its near 300 members are credibly precommitted to donating &pound;72.68M?</em></p>\n<p>Looking at valuing marginal impacts, it would be hoped CEA's programs are better. For example, it has been stated that GWWC has an internal price of around &pound;1700 for new pledges. This does not appear to extend to new programs, or to portions of 80,000 hours. In recent conversation with Will Crouch, he was asked what marginal value was placed on having a new intern in Cambridge (UK). There was no numerate response. Indeed, the assorted estimates do not cohere. If new pledges are worth &pound;10K / year in expectation, and even 10% of the donations flow into CEA, then an intern generating 20 marginal pledges is a winning proposition for CEA at their stated wage level. If the horizon time for 20 pledges from one worker is larger than CEA can afford to wait, then it is not clear that CEA has an effective program for using their interns.</p>\n<p><em>Qn: What does GWWC or 80,000 hours see as the marginal impact of one additional grad student in full time labour?<br />Qn: What is the horizon against which CEA programs are acting?</em></p>\n<p>&nbsp;</p>\n<h2>On research</h2>\n<p>The primary less visible activity of both GWWC and 80,000 hours is research. For GWWC, there are questions to be resolved about how best to Earn to Give, whether there are other activities which are less immediately fiscal but of high impact, and broadly how to identify near optimal opportunities for donation. For 80,000 hours, there is a need to establish how to optimise career paths for a broad set of potential terminal goals. Neither project appears to be bearing visible fruit. In conversation with Will Crouch, he observed that 80,000 hours don't know much about the burnout rates of various careers, the wage progressions or the likelihood of career progression.</p>\n<p>At present, this means that 80,000 hours is not publicly presenting things which are better than other sources of advice. There is a need for the best current knowledge to be available quickly; there are people who are deciding careers now who are unlikely to do reliably better than average on the basis of the information that 80,000 hours has made public. It seems implausible that new results are reliably coming in so quickly that the time spent publishing the internal state of the art will substantively slow down further improvements. There is a strong sense in which they are being graded on their speed, with publication being the mediator of impact. It also seems plausible that the publishing and research are substantially orthogonal, and would use different people. Hence from the outside, the lack of published concrete advice seems to be a substantial reason to think that there is no internal art.<em></em></p>\n<p><em>Qn: What is 80,000 hours producing with their current research time? What is the planned schedule? What constitutes success?</em></p>\n<p>&nbsp;</p>\n<p>There is a similar concern with the output of GWWC. Of their listed papers, only one (by Toby Ord) is substantive. The remainder are not written as if there is a pressing need to have results that are concise, clear and better than other available materials. For example, there is an extended article on investing vs. giving, and another on the distinction between income and happiness. The former does little more than list factors that might be relevant, with no attempt to discern which of these effects are largest or a sense of what ranges of reasonable looking assumptions give. The second observes correctly that the impact of monetary loss on donors may be overestimated, but then doesn't even question how impacts on recipients should be converted into hedonic terms. As a document, it seems to have been written to convince rather than elucidate truth. Neither paper drives an update to a belief that the current researchers at GWWC are effectively seeking to identify close to optimal opportunities or to reason coherently about the impact of interventions.</p>\n<p>More worrying is the absolute lack of material. Whilst the number of active researchers is difficult to discern from the website, it seems plausible that GWWC has had at least 6 people researching for it for at least the last year. There is no matching level of output; in academe one would expect to see several papers per year per person, and the primary claim of GWWC is that there are low hanging fruit in terms of the optimisation of donation and the ability of people to donate. So a priori, if GWWC was efficiently researching, one would expect it to be finding and publicising their results.</p>\n<p><em>Qn: What is GWWC producing with their current research time? What is the planned schedule? What constitutes success?</em></p>\n<p><em><br /></em></p>\n<h1><em>80,000 Hours</em></h1>\n<h2>Impact of the 80,000 hours advice</h2>\n<p>In conversation with Will, he asserted that on the basis of self-reports, something like 20-25% of those involved in 80K have changed or substantially rethought their career choice. This implies immediately that 75-80% haven't, and in practise that number will be higher care of the self-reporting. This substantially reduces the likely impact of 80,000 hours as a program. Indeed, it seems to be a near fatal problem for GWWC, in that if the 80,000 hours population is representative of pledges, then most of the GWWC pledges are earning in line with typical post grads, which makes it much harder to raise the mean value of each pledge to &pound;250K as is required.</p>\n<p>Methods of achieving this impact do not seem to be well attested. Will was asked what the internal value of a paid worker in Cambridge might be. A broad response was that it might improve the ability to give advice, but it was not suggested that this was based on hard data. This is a little troubling, because it implies that the effectiveness of 1-1 Skype interventions or 1-1 in person interventions are not known on a per hour basis. Absent this kind of data, it's difficult to see how 80,000 hours can be effectively optimising their impact.</p>\n<p><em>Qn: Does 80,000 hours have data on the relative effectiveness of their activities?<br />Qn: How does CEA square a lack of reported career changes with GWWC's numbers, given background over-life earnings?<br /></em></p>\n<p>&nbsp;</p>\n<h2>Content of 80,000 hours&rsquo; advice</h2>\n<p>In conversation, earning to give was suggested as being the baseline to measure against. Will noted explicitly that it's hard to know what kinds of careers are substantively better than others in a data-driven way. He was then very quick to hedge that by saying that of course research was valuable, and of course political activism could be valuable, and of course being a program manager at the world bank could be valuable (which would naturally require you do a PhD first), and of course being an entrepreneur could be valuable. It was not suggested that clearly at most one of these was optimal, or that people might ultimately be in a position where they trade off what they would have chosen to do in isolation against world optimising goals. We came away from this with the concern that 80,000 hours is not being epistemically vicious, and so is not willing to say things that might cause people to be unhappy. In particular, it seemed that there was more pressure to preserving the fuzzies that people were getting out of being affiliated to 80,000 hours than there was to make the advice good, and so most potential career paths were deemed to be OK.</p>\n<p><em>Qn: Does 80,000 hours offer information that causes a substantial reduction in the space of careers that are considered optimal?<br />Qn: How does 80,000 hours square a lack of reported career changes with their advice being good?</em></p>\n<p>&nbsp;</p>\n<h2>The 80,000 hours pledge</h2>\n<p>It was noted that the pledge had been substantially weakened, to \"I intend, at least in part, to use my career in an effective way to make the world a better place.\". My recollection says that it used to be more like \"I will use my career to most effectively reduce global poverty\". There wasn't any particular defence of the choice of wording or any indication that there had been deep thought about precisely what that pledge should constitute.</p>\n<p>The core mechanism by which 80,000 hours or GWWC will achieve long term impact has to be maintaining people's desire to act over a long period. In turn, it seems that the primary intermediate goal is to build a strong social structure to encourage adherence to these pledges. The pledges are then the key totems around which a community will be built, and so there should be massive pressure to optimise these and the surrounding social structures. This does not seem to have occurred.</p>\n<p><em>Qn: What are the design decisions behind the pledge, and what motivated the change in pledge?<br />Qn: To what extent is the wording of the pledge thought to be important?</em></p>\n<p>&nbsp;</p>\n<h1>Scope and goals</h1>\n<h2>Speed of growth</h2>\n<p>It was stated that around 1/3 of the Oxford undergraduate population (~4000 people) are on the mailing lists. Of that, there are around 300 members and a few dozen are coming to each event. By comparison, enterprising college societies in Cambridge (TMS, TCSS) have well in excess of 1000 undergraduates on their mailing lists, and get 80-100 people to their talks. When TCSS advertised an event to 1/3 of Cambridge, upwards of 600 people attended. From some organisational point of view, 80,000 hours Oxford could probably extract another factor of 5-10 out of its talk attendance. Whilst that won't factor through directly to the pledges, it seems unlikely that there would not be substantial growth there. In both of the Cambridge societies, the operating scale of the society has been doubled in a single year, by ensuring a reliable stream of events and getting networks in place to advertise widely.</p>\n<p>It does not seem like the organisations are optimising for growth and retention of a population of attendees. This would provide a pool of people broadly on board with the aims of the organisations, and substantially enriched for likely pledges. It is very plain that such optimisation has not been codified and sent to other new chapters; the Cambridge GWWC chapter does not behave as if such guidance exists.</p>\n<p><em>Qn: What optimisation has GWWC / 80,000 hours attempted in terms of the structure of their chapters?</em></p>\n<p><em><br /></em></p>\n<h2>Ambition</h2>\n<p>Taking a larger scale view, lots of these concerns ultimately cash out in a concern that a large fraction of the people involved with 80,000 hours or GWWC behave like dilettantes. There is an apparent desire to feel comfortable about career choice, think about dealing with poverty and get involved with 501(c)(3)'s/NGO's/UK charities. However as organisations, they are not behaving as we would expect for a bunch of people that seriously expect to vector hundreds of millions of pounds over the next decade, which is what continued linear growth would imply.</p>\n<p>Nor do they seem to act as if they wish to seriously optimise the world. For example, the world bank throws ~$43B/year around. Which is easier: To upscale GWWC by a factor of ~17000, or double the mean effectiveness of the world bank? This should not be a hypothetical question; it should be answered. There doesn't seem to be an acceptance that large social structures are going to be needed to support GWWC style donation for a lifetime, in the fashion of say the rotary clubs.<br /><br /><em>Qn: Where does CEA see its projects in 10 years? 20? 40?</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KBDiWMqhaYe7uPHvN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 19, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "21698", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>I am highly grateful to Alexey Morgunov and Adam Casey for reviewing and commenting on an earlier draft of this post, and pestering me into migrating the content from many emails to a somewhat coherent post.<br></em></p>\n<p>Will Crouch has <a href=\"/lw/fej/giving_what_we_can_80000_hours_and_metacharity/\">posted</a> about the Centre for Effective Altruism and in a follow up <a href=\"/lw/fkz/responses_to_questions_on_donating_to_80k_gwwc/\">post</a> discussed questions in more detail. The general sense of the discussion of that post was that the arguments were convincing and that donating to CEA is a good idea. Recently, he visited Cambridge, primarily to discuss 80,000 hours, and several Cambridge LWers spoke with him. These discussions caused a number of us to substantially downgrade our estimates of the effectiveness of CEA, and made our concerns more concrete.</p>\n<p><a id=\"more\"></a></p>\n<p>We're aware that our kind often don't cooperate well, but we are concerned that at present CEA's projects are unlikely to cash out into large numbers of people changing their behaviour. Ultimately, we are concerned that the space for high impact meta-charity is limited, and that if CEA is suboptimal this will have large opportunity costs. We want CEA to change the world, and would prefer this happens quickly.</p>\n<p>The key argument in favour of donating money to CEA which was presented by Will was that by donating $1 to CEA you produce more than $1 in donations to the most effective charities. We present some apparent difficulties with this remaining true on the margin. We also present more general worries with CEA as an organisation under these headings:</p>\n<p>Transparency<br>&nbsp;&nbsp;&nbsp; Cost effectiveness estimates<br>&nbsp;&nbsp;&nbsp; Research<br>80,000 hours<br>&nbsp;&nbsp;&nbsp; Impact of 80,000 hours advice<br>&nbsp;&nbsp;&nbsp; Content of 80,000 hours advice<br>&nbsp;&nbsp;&nbsp; The 80,000 hours pledge<br>Scope and Goals<br>&nbsp;&nbsp;&nbsp; Speed of growth<br> &nbsp;&nbsp;&nbsp; Ambition</p>\n<p>&nbsp;</p>\n<h1 id=\"Transparency\">Transparency</h1>\n<p>It is worrying how little of the key information about CEA is publicly available. This makes assessment hard. By contrast to GiveWell, CEA programs are not particularly open about where their money is spent, what their marginal goals are, or what they are doing internally. As presented online, the majority of both 80,000 hours' and GWWC's day to day activity is maintaining blogs. These blogs are not substantial by comparison to, say, OB in terms of their frequency of content or their frequency of insight. Concretely, it does not seem that CEA is being tranparent in the sense of <a href=\"http://www.givewell.org/criteria\">GiveWell</a>.<em></em></p>\n<p><em>Qn: How does CEA think its programs would score on a GiveWell assessment?<br>Qn: Does CEA think that GiveWell\u2019s assessments systemically go wrong?</em><em></em></p>\n<p><em>Qn: Does CEA consider the blogs to be a substantial source of impact? What external assessments or objective data support a claim of impact from the blogs?</em></p>\n<p><em><br></em></p>\n<h2 id=\"Cost_effectiveness_estimates\">Cost effectiveness estimates</h2>\n<p>As presented online and in person, CEA does not present as having credible models for their future impact. The GWWC site, for example, claims that from 291 members there will be \u00a372.68M pledged. This equates to \u00a3250K / person over the course of their life. Claiming that this level of pledging will occur requires either unreasonable rates of donation or multi-decade payment schedules. If, in line with GWWC's projections, around 50% of people will maintain their donations, then assuming a linear drop off the expected pledge from a full time member is around \u00a3375K. Over a lifetime, this is essentially \u00a310K / year. It seems implausible that expected mean annual earnings for GWWC members is of order \u00a3100K.</p>\n<p><em>Qn: On what basis does GWWC assert that its near 300 members are credibly precommitted to donating \u00a372.68M?</em></p>\n<p>Looking at valuing marginal impacts, it would be hoped CEA's programs are better. For example, it has been stated that GWWC has an internal price of around \u00a31700 for new pledges. This does not appear to extend to new programs, or to portions of 80,000 hours. In recent conversation with Will Crouch, he was asked what marginal value was placed on having a new intern in Cambridge (UK). There was no numerate response. Indeed, the assorted estimates do not cohere. If new pledges are worth \u00a310K / year in expectation, and even 10% of the donations flow into CEA, then an intern generating 20 marginal pledges is a winning proposition for CEA at their stated wage level. If the horizon time for 20 pledges from one worker is larger than CEA can afford to wait, then it is not clear that CEA has an effective program for using their interns.</p>\n<p><em>Qn: What does GWWC or 80,000 hours see as the marginal impact of one additional grad student in full time labour?<br>Qn: What is the horizon against which CEA programs are acting?</em></p>\n<p>&nbsp;</p>\n<h2 id=\"On_research\">On research</h2>\n<p>The primary less visible activity of both GWWC and 80,000 hours is research. For GWWC, there are questions to be resolved about how best to Earn to Give, whether there are other activities which are less immediately fiscal but of high impact, and broadly how to identify near optimal opportunities for donation. For 80,000 hours, there is a need to establish how to optimise career paths for a broad set of potential terminal goals. Neither project appears to be bearing visible fruit. In conversation with Will Crouch, he observed that 80,000 hours don't know much about the burnout rates of various careers, the wage progressions or the likelihood of career progression.</p>\n<p>At present, this means that 80,000 hours is not publicly presenting things which are better than other sources of advice. There is a need for the best current knowledge to be available quickly; there are people who are deciding careers now who are unlikely to do reliably better than average on the basis of the information that 80,000 hours has made public. It seems implausible that new results are reliably coming in so quickly that the time spent publishing the internal state of the art will substantively slow down further improvements. There is a strong sense in which they are being graded on their speed, with publication being the mediator of impact. It also seems plausible that the publishing and research are substantially orthogonal, and would use different people. Hence from the outside, the lack of published concrete advice seems to be a substantial reason to think that there is no internal art.<em></em></p>\n<p><em>Qn: What is 80,000 hours producing with their current research time? What is the planned schedule? What constitutes success?</em></p>\n<p>&nbsp;</p>\n<p>There is a similar concern with the output of GWWC. Of their listed papers, only one (by Toby Ord) is substantive. The remainder are not written as if there is a pressing need to have results that are concise, clear and better than other available materials. For example, there is an extended article on investing vs. giving, and another on the distinction between income and happiness. The former does little more than list factors that might be relevant, with no attempt to discern which of these effects are largest or a sense of what ranges of reasonable looking assumptions give. The second observes correctly that the impact of monetary loss on donors may be overestimated, but then doesn't even question how impacts on recipients should be converted into hedonic terms. As a document, it seems to have been written to convince rather than elucidate truth. Neither paper drives an update to a belief that the current researchers at GWWC are effectively seeking to identify close to optimal opportunities or to reason coherently about the impact of interventions.</p>\n<p>More worrying is the absolute lack of material. Whilst the number of active researchers is difficult to discern from the website, it seems plausible that GWWC has had at least 6 people researching for it for at least the last year. There is no matching level of output; in academe one would expect to see several papers per year per person, and the primary claim of GWWC is that there are low hanging fruit in terms of the optimisation of donation and the ability of people to donate. So a priori, if GWWC was efficiently researching, one would expect it to be finding and publicising their results.</p>\n<p><em>Qn: What is GWWC producing with their current research time? What is the planned schedule? What constitutes success?</em></p>\n<p><em><br></em></p>\n<h1 id=\"80_000_Hours\"><em>80,000 Hours</em></h1>\n<h2 id=\"Impact_of_the_80_000_hours_advice\">Impact of the 80,000 hours advice</h2>\n<p>In conversation with Will, he asserted that on the basis of self-reports, something like 20-25% of those involved in 80K have changed or substantially rethought their career choice. This implies immediately that 75-80% haven't, and in practise that number will be higher care of the self-reporting. This substantially reduces the likely impact of 80,000 hours as a program. Indeed, it seems to be a near fatal problem for GWWC, in that if the 80,000 hours population is representative of pledges, then most of the GWWC pledges are earning in line with typical post grads, which makes it much harder to raise the mean value of each pledge to \u00a3250K as is required.</p>\n<p>Methods of achieving this impact do not seem to be well attested. Will was asked what the internal value of a paid worker in Cambridge might be. A broad response was that it might improve the ability to give advice, but it was not suggested that this was based on hard data. This is a little troubling, because it implies that the effectiveness of 1-1 Skype interventions or 1-1 in person interventions are not known on a per hour basis. Absent this kind of data, it's difficult to see how 80,000 hours can be effectively optimising their impact.</p>\n<p><em>Qn: Does 80,000 hours have data on the relative effectiveness of their activities?<br>Qn: How does CEA square a lack of reported career changes with GWWC's numbers, given background over-life earnings?<br></em></p>\n<p>&nbsp;</p>\n<h2 id=\"Content_of_80_000_hours__advice\">Content of 80,000 hours\u2019 advice</h2>\n<p>In conversation, earning to give was suggested as being the baseline to measure against. Will noted explicitly that it's hard to know what kinds of careers are substantively better than others in a data-driven way. He was then very quick to hedge that by saying that of course research was valuable, and of course political activism could be valuable, and of course being a program manager at the world bank could be valuable (which would naturally require you do a PhD first), and of course being an entrepreneur could be valuable. It was not suggested that clearly at most one of these was optimal, or that people might ultimately be in a position where they trade off what they would have chosen to do in isolation against world optimising goals. We came away from this with the concern that 80,000 hours is not being epistemically vicious, and so is not willing to say things that might cause people to be unhappy. In particular, it seemed that there was more pressure to preserving the fuzzies that people were getting out of being affiliated to 80,000 hours than there was to make the advice good, and so most potential career paths were deemed to be OK.</p>\n<p><em>Qn: Does 80,000 hours offer information that causes a substantial reduction in the space of careers that are considered optimal?<br>Qn: How does 80,000 hours square a lack of reported career changes with their advice being good?</em></p>\n<p>&nbsp;</p>\n<h2 id=\"The_80_000_hours_pledge\">The 80,000 hours pledge</h2>\n<p>It was noted that the pledge had been substantially weakened, to \"I intend, at least in part, to use my career in an effective way to make the world a better place.\". My recollection says that it used to be more like \"I will use my career to most effectively reduce global poverty\". There wasn't any particular defence of the choice of wording or any indication that there had been deep thought about precisely what that pledge should constitute.</p>\n<p>The core mechanism by which 80,000 hours or GWWC will achieve long term impact has to be maintaining people's desire to act over a long period. In turn, it seems that the primary intermediate goal is to build a strong social structure to encourage adherence to these pledges. The pledges are then the key totems around which a community will be built, and so there should be massive pressure to optimise these and the surrounding social structures. This does not seem to have occurred.</p>\n<p><em>Qn: What are the design decisions behind the pledge, and what motivated the change in pledge?<br>Qn: To what extent is the wording of the pledge thought to be important?</em></p>\n<p>&nbsp;</p>\n<h1 id=\"Scope_and_goals\">Scope and goals</h1>\n<h2 id=\"Speed_of_growth\">Speed of growth</h2>\n<p>It was stated that around 1/3 of the Oxford undergraduate population (~4000 people) are on the mailing lists. Of that, there are around 300 members and a few dozen are coming to each event. By comparison, enterprising college societies in Cambridge (TMS, TCSS) have well in excess of 1000 undergraduates on their mailing lists, and get 80-100 people to their talks. When TCSS advertised an event to 1/3 of Cambridge, upwards of 600 people attended. From some organisational point of view, 80,000 hours Oxford could probably extract another factor of 5-10 out of its talk attendance. Whilst that won't factor through directly to the pledges, it seems unlikely that there would not be substantial growth there. In both of the Cambridge societies, the operating scale of the society has been doubled in a single year, by ensuring a reliable stream of events and getting networks in place to advertise widely.</p>\n<p>It does not seem like the organisations are optimising for growth and retention of a population of attendees. This would provide a pool of people broadly on board with the aims of the organisations, and substantially enriched for likely pledges. It is very plain that such optimisation has not been codified and sent to other new chapters; the Cambridge GWWC chapter does not behave as if such guidance exists.</p>\n<p><em>Qn: What optimisation has GWWC / 80,000 hours attempted in terms of the structure of their chapters?</em></p>\n<p><em><br></em></p>\n<h2 id=\"Ambition\">Ambition</h2>\n<p>Taking a larger scale view, lots of these concerns ultimately cash out in a concern that a large fraction of the people involved with 80,000 hours or GWWC behave like dilettantes. There is an apparent desire to feel comfortable about career choice, think about dealing with poverty and get involved with 501(c)(3)'s/NGO's/UK charities. However as organisations, they are not behaving as we would expect for a bunch of people that seriously expect to vector hundreds of millions of pounds over the next decade, which is what continued linear growth would imply.</p>\n<p>Nor do they seem to act as if they wish to seriously optimise the world. For example, the world bank throws ~$43B/year around. Which is easier: To upscale GWWC by a factor of ~17000, or double the mean effectiveness of the world bank? This should not be a hypothetical question; it should be answered. There doesn't seem to be an acceptance that large social structures are going to be needed to support GWWC style donation for a lifetime, in the fashion of say the rotary clubs.<br><br><em>Qn: Where does CEA see its projects in 10 years? 20? 40?</em></p>", "sections": [{"title": "Transparency", "anchor": "Transparency", "level": 1}, {"title": "Cost effectiveness estimates", "anchor": "Cost_effectiveness_estimates", "level": 2}, {"title": "On research", "anchor": "On_research", "level": 2}, {"title": "80,000 Hours", "anchor": "80_000_Hours", "level": 1}, {"title": "Impact of the 80,000 hours advice", "anchor": "Impact_of_the_80_000_hours_advice", "level": 2}, {"title": "Content of 80,000 hours\u2019 advice", "anchor": "Content_of_80_000_hours__advice", "level": 2}, {"title": "The 80,000 hours pledge", "anchor": "The_80_000_hours_pledge", "level": 2}, {"title": "Scope and goals", "anchor": "Scope_and_goals", "level": 1}, {"title": "Speed of growth", "anchor": "Speed_of_growth", "level": 2}, {"title": "Ambition", "anchor": "Ambition", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "32 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FCiMtrsM8mcmBtfTR", "rC2EBbhmCzJCSyysN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-21T13:23:58.792Z", "modifiedAt": null, "url": null, "title": "[Link] You May Already Be Aware of Your Cognitive Biases", "slug": "link-you-may-already-be-aware-of-your-cognitive-biases", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.775Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ApkyFucmSpMrWBprj/link-you-may-already-be-aware-of-your-cognitive-biases", "pageUrlRelative": "/posts/ApkyFucmSpMrWBprj/link-you-may-already-be-aware-of-your-cognitive-biases", "linkUrl": "https://www.lesswrong.com/posts/ApkyFucmSpMrWBprj/link-you-may-already-be-aware-of-your-cognitive-biases", "postedAtFormatted": "Thursday, February 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20You%20May%20Already%20Be%20Aware%20of%20Your%20Cognitive%20Biases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20You%20May%20Already%20Be%20Aware%20of%20Your%20Cognitive%20Biases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApkyFucmSpMrWBprj%2Flink-you-may-already-be-aware-of-your-cognitive-biases%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20You%20May%20Already%20Be%20Aware%20of%20Your%20Cognitive%20Biases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApkyFucmSpMrWBprj%2Flink-you-may-already-be-aware-of-your-cognitive-biases", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApkyFucmSpMrWBprj%2Flink-you-may-already-be-aware-of-your-cognitive-biases", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 231, "htmlBody": "<p>From the article:</p>\n<blockquote>\n<p>Using an adaptation of the standard 'bat-and-ball' problem, the researchers explored this phenomenon. The typical 'bat-and-ball' problem is as follows: a bat and ball together cost $1.10. The bat costs $1 more than the ball. How much does the ball cost? The intuitive answer that immediately springs to mind is 10 cents. However, the correct response is 5 cents.</p>\n<p>The authors developed a control version of this problem, without the relative statement that triggers the substitution of a hard question for an easier one: A magazine and a banana together cost $2.90. The magazine costs $2. How much does the banana cost?</p>\n</blockquote>\n<blockquote>\n<p>A total of 248 French university students were asked to solve each version of the problem. Once they had written down their answers, they were asked to indicate how confident they were that their answer was correct.</p>\n</blockquote>\n<blockquote>\n<p>Only 21 percent of the participants managed to solve the standard problem (bat/ball) correctly. In contrast, the control version (magazine/banana) was solved correctly by 98 percent of the participants. In addition, those who gave the wrong answer to the standard problem were much less confident of their answer to the standard problem than they were of their answer to the control version. In other words, they were not completely oblivious to the questionable nature of their wrong answer.</p>\n</blockquote>\n<p>Article in Science Daily: http://www.sciencedaily.com/releases/2013/02/130219102202.htm</p>\n<p>Original abstract (the rest is paywalled): http://link.springer.com/article/10.3758/s13423-013-0384-5</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ApkyFucmSpMrWBprj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 1.1186278918350405e-06, "legacy": true, "legacyId": "21711", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-21T16:24:40.172Z", "modifiedAt": null, "url": null, "title": "Why Politics are Important to Less Wrong...", "slug": "why-politics-are-important-to-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.542Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sEkTdcbGnc3JeGtzg/why-politics-are-important-to-less-wrong", "pageUrlRelative": "/posts/sEkTdcbGnc3JeGtzg/why-politics-are-important-to-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/sEkTdcbGnc3JeGtzg/why-politics-are-important-to-less-wrong", "postedAtFormatted": "Thursday, February 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Politics%20are%20Important%20to%20Less%20Wrong...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Politics%20are%20Important%20to%20Less%20Wrong...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEkTdcbGnc3JeGtzg%2Fwhy-politics-are-important-to-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Politics%20are%20Important%20to%20Less%20Wrong...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEkTdcbGnc3JeGtzg%2Fwhy-politics-are-important-to-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEkTdcbGnc3JeGtzg%2Fwhy-politics-are-important-to-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 554, "htmlBody": "<p>...and no, it's not because of potential political impact on its goals. &nbsp;Although that's also a thing.</p>\n<p>The Politics problem is, at its root, about forming a workable set of rules by which society can operate, which society can agree with.</p>\n<p>The Friendliness Problem is, at its root, about forming a workable set of values which are acceptable to society.</p>\n<p>Politics as a process (I will use \"politics\" to refer to the process of politics henceforth) doesn't generate values; they're strictly an input, by which the values of society are converted into rules which are intended to maximize them. &nbsp;While this is true, it is value agnostic; it doesn't care what the values are, or where they come from. &nbsp;Which is to say, provided you solve the Friendliness Problem, it provides a valuable input into politics.</p>\n<p>Politics is also an intelligence. &nbsp;Not in the \"self aware\" sense, or even in the \"capable of making good judgments\" sense, but in the sense of an optimization process. &nbsp;We're each nodes in this alien intelligence, and we form what looks, to me, suspiciously like a neural network.</p>\n<p>The Friendliness Problem is equally applicable to Politics as it is to any other intelligence. &nbsp;Indeed, provided we can provably solve the Friendliness Problem, we should be capable of creating Friendly Politics. &nbsp;Friendliness should, in principle, be equally applicable to both. &nbsp;Now, there are some issues with this - politics is composed of unpredictable hardware, namely, people. &nbsp;And it may be that the neural architecture is fundamentally incompatible with Friendliness. &nbsp;But that is discussing the -output- of the process. &nbsp;Friendliness is first an input, before it can be an output.</p>\n<p>More, we already have various political formations, and can assess their Friendliness levels, merely in terms of the values that went -into- them.</p>\n<p>Which is where I think politics offers a pretty strong hint to the possibility that the Friendliness Problem has no resolution:</p>\n<p>We can't agree on which political formations are more Friendly. &nbsp;That's what \"Politics is the Mindkiller\" is all about; our inability to come to an agreement on political matters. &nbsp;It's not merely a matter of the rules - which is to say, it's not a matter of the output: We can't even come to an agreement about which values should be used to form the rules.</p>\n<p>This is why I think political discussion is valuable here, incidentally. &nbsp;Less Wrong, by and large, has been avoiding the hard problem of Friendliness, by labeling its primary functional outlet in reality as a mindkiller, not to be discussed.</p>\n<p>Either we can agree on what constitutes Friendly Politics, or not. &nbsp;If we can't, I don't see much hope of arriving at a Friendliness solution more broadly. &nbsp;Friendly to -whom- becomes the question, if it was ever anything else. &nbsp;Which suggests a division in types of Friendliness; Strong Friendliness, which is a fully generalized set of human values, and acceptable to just about everyone; and Weak Friendliness, which isn't fully generalized, and perhaps acceptable merely to a plurality. &nbsp;Weak Friendliness survives the political question. &nbsp;I do not see that Strong Friendliness can.</p>\n<p>(Exemplified: When I imagine a Friendly AI, I imagine a hands-off benefactor who permits people to do anything they wish to which won't result in harm to others. &nbsp;Why, look, a libertarian/libertine dictator. &nbsp;Does anybody&nbsp;envisage a Friendly AI which doesn't correspond more or less directly with their own political beliefs?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sEkTdcbGnc3JeGtzg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 8, "extendedScore": null, "score": 1.118743640676617e-06, "legacy": true, "legacyId": "21712", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 97, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-21T21:58:30.280Z", "modifiedAt": null, "url": null, "title": "VNM agents and lotteries involving an infinite number of possible outcomes", "slug": "vnm-agents-and-lotteries-involving-an-infinite-number-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.286Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7wmBH76BGScL7XNct/vnm-agents-and-lotteries-involving-an-infinite-number-of", "pageUrlRelative": "/posts/7wmBH76BGScL7XNct/vnm-agents-and-lotteries-involving-an-infinite-number-of", "linkUrl": "https://www.lesswrong.com/posts/7wmBH76BGScL7XNct/vnm-agents-and-lotteries-involving-an-infinite-number-of", "postedAtFormatted": "Thursday, February 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20VNM%20agents%20and%20lotteries%20involving%20an%20infinite%20number%20of%20possible%20outcomes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVNM%20agents%20and%20lotteries%20involving%20an%20infinite%20number%20of%20possible%20outcomes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wmBH76BGScL7XNct%2Fvnm-agents-and-lotteries-involving-an-infinite-number-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=VNM%20agents%20and%20lotteries%20involving%20an%20infinite%20number%20of%20possible%20outcomes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wmBH76BGScL7XNct%2Fvnm-agents-and-lotteries-involving-an-infinite-number-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wmBH76BGScL7XNct%2Fvnm-agents-and-lotteries-involving-an-infinite-number-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1009, "htmlBody": "<p>Summary: The VNM utility theorem only applies to lotteries that involve a finite number of possible outcomes. If an agent maximizes the expected value of a utility function when considering lotteries that involve a potentially infinite number of outcomes as well, then its utility function must be bounded.</p>\n<h3>Outcomes versus Lotteries</h3>\n<p>One way to formulate the VNM utility theorem is in terms of outcomes and lotteries over outcomes. That is, there is some set <img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\" width=\"13\" height=\"14\" /> of outcomes, and a set <img src=\"http://www.codecogs.com/png.latex?\\mathcal{L}\" alt=\"\" width=\"12\" height=\"14\" /> of lotteries defined as <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\mathcal{L}:=\\left\\{%20\\sum_{i=1}^{n}p_{i}O_{i}\\mid%20n\\in\\mathbb{N},p_{i}%3E0,O_{i}\\in\\mathcal{O}\\left%28\\forall%20i\\right%29,\\sum_{i=1}^{n}p_{i}=1\\right\\}%20}\" alt=\"\" />. In other words, the set of lotteries is the set of probability distributions over a finite number of outcomes. The finiteness is very important; we'll get to that later. Note that for each outcome, there is a corresponding lottery that guarantees this outcome, and these &ldquo;pure outcome&rdquo; lotteries are a basis for <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BL%7D\" alt=\"\" width=\"12\" height=\"14\" />.</p>\n<p>Given that formulation, and given the <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#The_axioms\">VNM axioms</a>, there exists some function&nbsp;<img src=\"http://www.codecogs.com/png.latex?u:\\mathcal{O}\\rightarrow\\mathbb{R}\" alt=\"\" width=\"82\" height=\"14\" /> such that given any 2 lotteries&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20L:=\\sum_{i=1}^{n}p_{i}O_{L,i}}\" alt=\"\" width=\"116\" height=\"50\" /> and <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20M:=\\sum_{i=1}^{m}q_{i}O_{M,i}}\" alt=\"\" width=\"126\" height=\"50\" />,&nbsp;<img src=\"http://www.codecogs.com/png.latex?L\\succ%20M\" alt=\"\" width=\"56\" height=\"14\" /> <a href=\"http://en.wikipedia.org/wiki/If_and_only_if\">iff</a> <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{n}p_{i}u\\left%28O_{L,i}\\right%29%3E\\sum_{i=1}^{m}q_{i}u\\left%28O_{M,i}\\right%29}\" alt=\"\" />.</p>\n<p>The other formulation does not mention <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BO%7D\" alt=\"\" width=\"13\" height=\"14\" />. Instead, there is simply a set <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BL%7D\" alt=\"\" width=\"12\" height=\"14\" /> of lotteries, such that&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\forall%20L_{1},L_{2},...,L_{n}\\in\\mathcal{L},\\forall%20p_{1},p_{2},...,p_{n}%3E0\\,\\sum_{i=1}^{n}p_{i}L_{i}\\in\\mathcal{L}}\" alt=\"\" width=\"385\" height=\"50\" /> iff <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{n}p_{i}=1}\" alt=\"\" width=\"75\" height=\"50\" />. In this formulation, there exists some function&nbsp;<img src=\"http://www.codecogs.com/png.latex?u:\\mathcal{L}\\rightarrow\\mathbb{R}\" alt=\"\" width=\"79\" height=\"14\" /> such that if <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20L=\\sum_{i=1}^{n}p_{i}L_{i}}\" alt=\"\" width=\"96\" height=\"50\" />, then&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20u\\left%28L\\right%29=\\sum_{i=1}^{n}p_{i}u\\left%28L_{i}\\right%29}\" alt=\"\" /> (notice&nbsp;<img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" /> still must be finite) and for any 2 lotteries&nbsp;<img src=\"http://www.codecogs.com/png.latex?L\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?M\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?L%5Csucc%20M\" alt=\"\" width=\"56\" height=\"14\" /> iff <img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29%3Eu\\left%28M\\right%29\" alt=\"\" />.</p>\n<p>The formulation in terms of outcomes and lotteries over outcomes is more intuitively appealing (to me, at least), since real life has outcomes and uncertainty about outcomes, so I will use it when I can, but the formulation purely in terms of lotteries, which is more similar to what von Neumann and Morgenstern did in their original paper, will be useful sometimes, so I will switch back to it intermittently.</p>\n<h3>Infinite lotteries</h3>\n<p>Myth: Given some utility function <img src=\"http://www.codecogs.com/png.latex?u:%5Cmathcal%7BO%7D%5Crightarrow%5Cmathbb%7BR%7D\" alt=\"\" width=\"82\" height=\"14\" /> that accurately describes a VNM-rational agent's preferences over finite lotteries, if you expand <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BL%7D\" alt=\"\" width=\"12\" height=\"14\" /> to include lotteries with an infinite number of possible outcomes (let's call the expanded set of lotteries <img src=\"http://www.codecogs.com/png.latex?\\mathcal{L}^{+}\" alt=\"\" />), then for any 2 lotteries&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20L:=\\sum_{i=1}^{\\infty}p_{i}O_{L,i}}\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20M:=\\sum_{i=1}^{\\infty}q_{i}O_{M,i}}\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?L%5Csucc%20M\" alt=\"\" width=\"56\" height=\"14\" /> iff <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{\\infty}p_{i}u\\left%28O_{L,i}\\right%29%3E\\sum_{i=1}^{\\infty}q_{i}u\\left%28O_{M,i}\\right%29}\" alt=\"\" />.</p>\n<p>Reality: Knowing an agent's preferences over finite lotteries, and that the agent obeys the VNM axioms, does not tell you everything about the agent's preferences over lotteries with an infinite number of possible outcomes. To demonstrate this, I'm going to construct a VNM-rational agent that maximizes a utility function <img src=\"http://www.codecogs.com/png.latex?u:\\mathcal{L}^{+}\\rightarrow\\mathbb{R}\" alt=\"\" />, where <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\exists\\left\\{%20L_{i}\\right\\}%20_{i=1}^{\\infty}\\in\\mathcal{L}^{+}\\,\\exists\\left\\{%20p_{i}\\left%28\\geq0\\right%29\\right\\}%20_{i=1}^{\\infty}\\,\\sum_{i=1}^{\\infty}p_{i}=1,\\,%20u\\left%28\\sum_{i=1}^{\\infty}p_{i}L_{i}\\right%29\\neq\\sum_{i=1}^{\\infty}p_{i}u\\left%28L_{i}\\right%29}\" alt=\"\" />. This construction relies on the axiom of choice (please let me know if you figure out whether or not it is possible to construct such an agent without the axiom of choice). I will also be assuming that <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BO%7D\" alt=\"\" width=\"13\" height=\"14\" /> is countably infinite (if <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BO%7D\" alt=\"\" width=\"13\" height=\"14\" /> is finite, such an agent is impossible, and if it is uncountable, then you can consider a countable subset).</p>\n<p>Notice that <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5E%7B+%7D\" alt=\"\" /> can be seen as a subset of the real vector space <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\mathcal{V}:=\\left\\{%20\\sum_{O\\in\\mathcal{O}}\\alpha_{O}O\\mid\\forall%20O\\in\\mathcal{O}\\,\\alpha_{O}\\in\\mathbb{R},\\,\\sum_{O\\in\\mathcal{O}}\\left|\\alpha_{O}\\right|\\,%20converges\\right\\}%20}\" alt=\"\" />, with the addition and multiplication by scalar operations being exactly what you would expect (<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\left%28\\sum_{O\\in\\mathcal{O}}\\alpha_{O}O\\right%29+\\left%28\\sum_{O\\in\\mathcal{O}}\\beta_{O}O\\right%29=\\sum_{O\\in\\mathcal{O}}\\left%28\\alpha_{O}+\\beta_{O}\\right%29O}\" alt=\"\" />, and <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20x\\left%28\\sum_{O\\in\\mathcal{O}}\\alpha_{O}O\\right%29=\\sum_{O\\in\\mathcal{O}}x\\alpha_{O}O}\" alt=\"\" />). A utility function&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> can be seen as an element of the dual space of <img src=\"http://www.codecogs.com/png.latex?\\mathcal{V}\" alt=\"\" width=\"12\" height=\"14\" />. The axiom of choice implies that this vector space has a basis (in this context, a basis means a set of vectors for which any finite subset is linearly independent, and every vector is a linear combination of a finite number of basis vectors). The value of <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" /> on each basis element can be chosen independently, and these choices completely determine <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" />. In particular, the basis could contain every element of <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BO%7D\" alt=\"\" width=\"13\" height=\"14\" />, and also contain&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{\\infty}2^{-i}O_{i}}\" alt=\"\" width=\"71\" height=\"50\" /> for some sequence&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\left\\{%20O_{i}\\in\\mathcal{O}\\right\\}%20_{i=1}^{\\infty}\" alt=\"\" /> with distinct elements. Then we could have&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\forall%20O\\in\\mathcal{O}\\,%20u\\left%28O\\right%29=0\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20u\\left%28\\sum_{i=1}^{\\infty}2^{-i}O_{i}\\right%29=1}\" alt=\"\" />, violating the conclusion of the myth, but this meets all the VNM axioms.</p>\n<p>The fact that there is a real-valued function on lotteries that the agent maximizes guarantees that the completeness and transitivity axioms hold, since&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29%3Eu\\left%28M\\right%29\" alt=\"\" /> or&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29=u\\left%28M\\right%29\" alt=\"\" /> or&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29%3Cu\\left%28M\\right%29\" alt=\"\" /> (completeness), and if&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29\\leq%20u\\left%28M\\right%29\" alt=\"\" /> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28M\\right%29\\leq%20u\\left%28N\\right%29\" alt=\"\" /> then&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29\\leq%20u\\left%28N\\right%29\" alt=\"\" /> (transitivity). The fact that the function is linear with respect to finite sums guarantees that the continuity and independence axioms hold, since if&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29%3Cu\\left%28M\\right%29%3Cu\\left%28N\\right%29\" alt=\"\" /> then&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28\\frac{u\\left%28N\\right%29-u\\left%28M\\right%29}{u\\left%28N\\right%29-u\\left%28L\\right%29}L+\\frac{u\\left%28M\\right%29-u\\left%28L\\right%29}{u\\left%28N\\right%29-u\\left%28L\\right%29}N\\right%29=\\frac{u\\left%28N\\right%29-u\\left%28M\\right%29}{u\\left%28N\\right%29-u\\left%28L\\right%29}u\\left%28L\\right%29+\\frac{u\\left%28M\\right%29-u\\left%28L\\right%29}{u\\left%28N\\right%29-u\\left%28L\\right%29}u\\left%28N\\right%29=u\\left%28M\\right%29\" alt=\"\" /> (continuity), and if <img src=\"http://www.codecogs.com/png.latex?u%5Cleft%28L%5Cright%29%3Cu%5Cleft%28M%5Cright%29\" alt=\"\" /> then for any lottery&nbsp;<img src=\"http://www.codecogs.com/png.latex?N\" alt=\"\" /> and positive probability <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"\" />,&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28pL+\\left%281-p\\right%29N\\right%29=pu\\left%28L\\right%29+\\left%281-p\\right%29u\\left%28N\\right%29%3Cpu\\left%28M\\right%29+\\left%281-p\\right%29u\\left%28N\\right%29=u\\left%28pM+\\left%281-p\\right%29N\\right%29\" alt=\"\" /> (independence).</p>\n<h3>Extended VNM hypothesis</h3>\n<p>The VNM utility theorem does not prove that an agent meeting its axioms will maximize the expected value of a utility function when presented with infinite lotteries, but the fact that any such agent will maximize the expected value of a utility function when presented with finite lotteries certainly seems very suggestive. With that in mind, I suggest that this be called the &ldquo;extended von Neumann-Morgenstern hypothesis&rdquo;:</p>\n<p>An agent, in order to be considered rational, should maximize the expected value of a utility function over outcomes when choosing between lotteries over any (possibly infinite) number of outcomes.</p>\n<h3>Bounded and unbounded utility functions</h3>\n<p>It is perfectly possible to construct VNM-rational agents with an unbounded utility function. But all such agents will inevitably violate the extended VNM hypothesis, because it is possible to create infinite lotteries with undefined expected value. For instance, the <a href=\"http://en.wikipedia.org/wiki/St._Petersburg_paradox\">St. Petersburg paradox</a> can be modified to refer specifically to utilities instead of money. That is, if there is no upper bound to the agent's utility function, then there exists a sequence of outcomes&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\left\\{%20O_{i}\\right\\}%20_{i=1}^{\\infty}\" alt=\"\" /> such that for each <img src=\"http://www.codecogs.com/png.latex?i\" alt=\"\" />, <img src=\"http://www.codecogs.com/png.latex?u\\left%28O_{i}\\right%29\\geq2^{i}\" alt=\"\" />. Then the expected utility of&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{\\infty}2^{-i}O_{i}}\" alt=\"\" /> is <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{\\infty}2^{-i}u\\left%28O_{i}\\right%29\\geq\\sum_{i=1}^{\\infty}2^{-i}2^{i}=\\sum_{i=1}^{\\infty}1}\" alt=\"\" />, which does not converge. So unbounded utility functions are not compatible with the extended VNM hypothesis.</p>\n<p>At this point, one may feel a strong temptation to come up with some way to characterize the values of infinite sums with some ordered superset of the real numbers, so that it is possible to compare nonconvergent sums. However, by the formulation of the VNM theorem solely in terms of lotteries, the utility of any lottery, such as <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20u\\left%28\\sum_{i=1}^{\\infty}2^{-i}O_{i}\\right%29}\" alt=\"\" />, is a real number. So any such scheme that requires that the range of the utility function include nonreals will violate the VNM axioms. In particular, it will probably violate the archimedian axiom.</p>\n<p>One possible response to this is to dismiss the archimedian axiom, and try to characterize agents that obey the completeness, transitivity, and independence axioms. Benja <a href=\"/lw/fu1/why_you_must_maximize_expected_utility/\">has written</a> (section \"Doing without Continuity) about this, and I find his solution fairly compelling, but it isn't clear that it helps us deal with situations like the St. Petersburg paradox. I intend to say more about nonarchimedian preferences soon.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2YcmB6SLtHnHRe3uX": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7wmBH76BGScL7XNct", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 26, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "21714", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Summary: The VNM utility theorem only applies to lotteries that involve a finite number of possible outcomes. If an agent maximizes the expected value of a utility function when considering lotteries that involve a potentially infinite number of outcomes as well, then its utility function must be bounded.</p>\n<h3 id=\"Outcomes_versus_Lotteries\">Outcomes versus Lotteries</h3>\n<p>One way to formulate the VNM utility theorem is in terms of outcomes and lotteries over outcomes. That is, there is some set <img src=\"http://www.codecogs.com/png.latex?\\mathcal{O}\" alt=\"\" width=\"13\" height=\"14\"> of outcomes, and a set <img src=\"http://www.codecogs.com/png.latex?\\mathcal{L}\" alt=\"\" width=\"12\" height=\"14\"> of lotteries defined as <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\mathcal{L}:=\\left\\{%20\\sum_{i=1}^{n}p_{i}O_{i}\\mid%20n\\in\\mathbb{N},p_{i}%3E0,O_{i}\\in\\mathcal{O}\\left%28\\forall%20i\\right%29,\\sum_{i=1}^{n}p_{i}=1\\right\\}%20}\" alt=\"\">. In other words, the set of lotteries is the set of probability distributions over a finite number of outcomes. The finiteness is very important; we'll get to that later. Note that for each outcome, there is a corresponding lottery that guarantees this outcome, and these \u201cpure outcome\u201d lotteries are a basis for <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BL%7D\" alt=\"\" width=\"12\" height=\"14\">.</p>\n<p>Given that formulation, and given the <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#The_axioms\">VNM axioms</a>, there exists some function&nbsp;<img src=\"http://www.codecogs.com/png.latex?u:\\mathcal{O}\\rightarrow\\mathbb{R}\" alt=\"\" width=\"82\" height=\"14\"> such that given any 2 lotteries&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20L:=\\sum_{i=1}^{n}p_{i}O_{L,i}}\" alt=\"\" width=\"116\" height=\"50\"> and <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20M:=\\sum_{i=1}^{m}q_{i}O_{M,i}}\" alt=\"\" width=\"126\" height=\"50\">,&nbsp;<img src=\"http://www.codecogs.com/png.latex?L\\succ%20M\" alt=\"\" width=\"56\" height=\"14\"> <a href=\"http://en.wikipedia.org/wiki/If_and_only_if\">iff</a> <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{n}p_{i}u\\left%28O_{L,i}\\right%29%3E\\sum_{i=1}^{m}q_{i}u\\left%28O_{M,i}\\right%29}\" alt=\"\">.</p>\n<p>The other formulation does not mention <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BO%7D\" alt=\"\" width=\"13\" height=\"14\">. Instead, there is simply a set <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BL%7D\" alt=\"\" width=\"12\" height=\"14\"> of lotteries, such that&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\forall%20L_{1},L_{2},...,L_{n}\\in\\mathcal{L},\\forall%20p_{1},p_{2},...,p_{n}%3E0\\,\\sum_{i=1}^{n}p_{i}L_{i}\\in\\mathcal{L}}\" alt=\"\" width=\"385\" height=\"50\"> iff <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{n}p_{i}=1}\" alt=\"\" width=\"75\" height=\"50\">. In this formulation, there exists some function&nbsp;<img src=\"http://www.codecogs.com/png.latex?u:\\mathcal{L}\\rightarrow\\mathbb{R}\" alt=\"\" width=\"79\" height=\"14\"> such that if <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20L=\\sum_{i=1}^{n}p_{i}L_{i}}\" alt=\"\" width=\"96\" height=\"50\">, then&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20u\\left%28L\\right%29=\\sum_{i=1}^{n}p_{i}u\\left%28L_{i}\\right%29}\" alt=\"\"> (notice&nbsp;<img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\"> still must be finite) and for any 2 lotteries&nbsp;<img src=\"http://www.codecogs.com/png.latex?L\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?M\" alt=\"\">, <img src=\"http://www.codecogs.com/png.latex?L%5Csucc%20M\" alt=\"\" width=\"56\" height=\"14\"> iff <img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29%3Eu\\left%28M\\right%29\" alt=\"\">.</p>\n<p>The formulation in terms of outcomes and lotteries over outcomes is more intuitively appealing (to me, at least), since real life has outcomes and uncertainty about outcomes, so I will use it when I can, but the formulation purely in terms of lotteries, which is more similar to what von Neumann and Morgenstern did in their original paper, will be useful sometimes, so I will switch back to it intermittently.</p>\n<h3 id=\"Infinite_lotteries\">Infinite lotteries</h3>\n<p>Myth: Given some utility function <img src=\"http://www.codecogs.com/png.latex?u:%5Cmathcal%7BO%7D%5Crightarrow%5Cmathbb%7BR%7D\" alt=\"\" width=\"82\" height=\"14\"> that accurately describes a VNM-rational agent's preferences over finite lotteries, if you expand <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BL%7D\" alt=\"\" width=\"12\" height=\"14\"> to include lotteries with an infinite number of possible outcomes (let's call the expanded set of lotteries <img src=\"http://www.codecogs.com/png.latex?\\mathcal{L}^{+}\" alt=\"\">), then for any 2 lotteries&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20L:=\\sum_{i=1}^{\\infty}p_{i}O_{L,i}}\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20M:=\\sum_{i=1}^{\\infty}q_{i}O_{M,i}}\" alt=\"\">, <img src=\"http://www.codecogs.com/png.latex?L%5Csucc%20M\" alt=\"\" width=\"56\" height=\"14\"> iff <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{\\infty}p_{i}u\\left%28O_{L,i}\\right%29%3E\\sum_{i=1}^{\\infty}q_{i}u\\left%28O_{M,i}\\right%29}\" alt=\"\">.</p>\n<p>Reality: Knowing an agent's preferences over finite lotteries, and that the agent obeys the VNM axioms, does not tell you everything about the agent's preferences over lotteries with an infinite number of possible outcomes. To demonstrate this, I'm going to construct a VNM-rational agent that maximizes a utility function <img src=\"http://www.codecogs.com/png.latex?u:\\mathcal{L}^{+}\\rightarrow\\mathbb{R}\" alt=\"\">, where <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\exists\\left\\{%20L_{i}\\right\\}%20_{i=1}^{\\infty}\\in\\mathcal{L}^{+}\\,\\exists\\left\\{%20p_{i}\\left%28\\geq0\\right%29\\right\\}%20_{i=1}^{\\infty}\\,\\sum_{i=1}^{\\infty}p_{i}=1,\\,%20u\\left%28\\sum_{i=1}^{\\infty}p_{i}L_{i}\\right%29\\neq\\sum_{i=1}^{\\infty}p_{i}u\\left%28L_{i}\\right%29}\" alt=\"\">. This construction relies on the axiom of choice (please let me know if you figure out whether or not it is possible to construct such an agent without the axiom of choice). I will also be assuming that <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BO%7D\" alt=\"\" width=\"13\" height=\"14\"> is countably infinite (if <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BO%7D\" alt=\"\" width=\"13\" height=\"14\"> is finite, such an agent is impossible, and if it is uncountable, then you can consider a countable subset).</p>\n<p>Notice that <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5E%7B+%7D\" alt=\"\"> can be seen as a subset of the real vector space <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\mathcal{V}:=\\left\\{%20\\sum_{O\\in\\mathcal{O}}\\alpha_{O}O\\mid\\forall%20O\\in\\mathcal{O}\\,\\alpha_{O}\\in\\mathbb{R},\\,\\sum_{O\\in\\mathcal{O}}\\left|\\alpha_{O}\\right|\\,%20converges\\right\\}%20}\" alt=\"\">, with the addition and multiplication by scalar operations being exactly what you would expect (<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\left%28\\sum_{O\\in\\mathcal{O}}\\alpha_{O}O\\right%29+\\left%28\\sum_{O\\in\\mathcal{O}}\\beta_{O}O\\right%29=\\sum_{O\\in\\mathcal{O}}\\left%28\\alpha_{O}+\\beta_{O}\\right%29O}\" alt=\"\">, and <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20x\\left%28\\sum_{O\\in\\mathcal{O}}\\alpha_{O}O\\right%29=\\sum_{O\\in\\mathcal{O}}x\\alpha_{O}O}\" alt=\"\">). A utility function&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\"> can be seen as an element of the dual space of <img src=\"http://www.codecogs.com/png.latex?\\mathcal{V}\" alt=\"\" width=\"12\" height=\"14\">. The axiom of choice implies that this vector space has a basis (in this context, a basis means a set of vectors for which any finite subset is linearly independent, and every vector is a linear combination of a finite number of basis vectors). The value of <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\"> on each basis element can be chosen independently, and these choices completely determine <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\">. In particular, the basis could contain every element of <img src=\"http://www.codecogs.com/png.latex?%5Cmathcal%7BO%7D\" alt=\"\" width=\"13\" height=\"14\">, and also contain&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{\\infty}2^{-i}O_{i}}\" alt=\"\" width=\"71\" height=\"50\"> for some sequence&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\left\\{%20O_{i}\\in\\mathcal{O}\\right\\}%20_{i=1}^{\\infty}\" alt=\"\"> with distinct elements. Then we could have&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\forall%20O\\in\\mathcal{O}\\,%20u\\left%28O\\right%29=0\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20u\\left%28\\sum_{i=1}^{\\infty}2^{-i}O_{i}\\right%29=1}\" alt=\"\">, violating the conclusion of the myth, but this meets all the VNM axioms.</p>\n<p>The fact that there is a real-valued function on lotteries that the agent maximizes guarantees that the completeness and transitivity axioms hold, since&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29%3Eu\\left%28M\\right%29\" alt=\"\"> or&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29=u\\left%28M\\right%29\" alt=\"\"> or&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29%3Cu\\left%28M\\right%29\" alt=\"\"> (completeness), and if&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29\\leq%20u\\left%28M\\right%29\" alt=\"\"> and&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28M\\right%29\\leq%20u\\left%28N\\right%29\" alt=\"\"> then&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29\\leq%20u\\left%28N\\right%29\" alt=\"\"> (transitivity). The fact that the function is linear with respect to finite sums guarantees that the continuity and independence axioms hold, since if&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28L\\right%29%3Cu\\left%28M\\right%29%3Cu\\left%28N\\right%29\" alt=\"\"> then&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28\\frac{u\\left%28N\\right%29-u\\left%28M\\right%29}{u\\left%28N\\right%29-u\\left%28L\\right%29}L+\\frac{u\\left%28M\\right%29-u\\left%28L\\right%29}{u\\left%28N\\right%29-u\\left%28L\\right%29}N\\right%29=\\frac{u\\left%28N\\right%29-u\\left%28M\\right%29}{u\\left%28N\\right%29-u\\left%28L\\right%29}u\\left%28L\\right%29+\\frac{u\\left%28M\\right%29-u\\left%28L\\right%29}{u\\left%28N\\right%29-u\\left%28L\\right%29}u\\left%28N\\right%29=u\\left%28M\\right%29\" alt=\"\"> (continuity), and if <img src=\"http://www.codecogs.com/png.latex?u%5Cleft%28L%5Cright%29%3Cu%5Cleft%28M%5Cright%29\" alt=\"\"> then for any lottery&nbsp;<img src=\"http://www.codecogs.com/png.latex?N\" alt=\"\"> and positive probability <img src=\"http://www.codecogs.com/png.latex?p\" alt=\"\">,&nbsp;<img src=\"http://www.codecogs.com/png.latex?u\\left%28pL+\\left%281-p\\right%29N\\right%29=pu\\left%28L\\right%29+\\left%281-p\\right%29u\\left%28N\\right%29%3Cpu\\left%28M\\right%29+\\left%281-p\\right%29u\\left%28N\\right%29=u\\left%28pM+\\left%281-p\\right%29N\\right%29\" alt=\"\"> (independence).</p>\n<h3 id=\"Extended_VNM_hypothesis\">Extended VNM hypothesis</h3>\n<p>The VNM utility theorem does not prove that an agent meeting its axioms will maximize the expected value of a utility function when presented with infinite lotteries, but the fact that any such agent will maximize the expected value of a utility function when presented with finite lotteries certainly seems very suggestive. With that in mind, I suggest that this be called the \u201cextended von Neumann-Morgenstern hypothesis\u201d:</p>\n<p>An agent, in order to be considered rational, should maximize the expected value of a utility function over outcomes when choosing between lotteries over any (possibly infinite) number of outcomes.</p>\n<h3 id=\"Bounded_and_unbounded_utility_functions\">Bounded and unbounded utility functions</h3>\n<p>It is perfectly possible to construct VNM-rational agents with an unbounded utility function. But all such agents will inevitably violate the extended VNM hypothesis, because it is possible to create infinite lotteries with undefined expected value. For instance, the <a href=\"http://en.wikipedia.org/wiki/St._Petersburg_paradox\">St. Petersburg paradox</a> can be modified to refer specifically to utilities instead of money. That is, if there is no upper bound to the agent's utility function, then there exists a sequence of outcomes&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\left\\{%20O_{i}\\right\\}%20_{i=1}^{\\infty}\" alt=\"\"> such that for each <img src=\"http://www.codecogs.com/png.latex?i\" alt=\"\">, <img src=\"http://www.codecogs.com/png.latex?u\\left%28O_{i}\\right%29\\geq2^{i}\" alt=\"\">. Then the expected utility of&nbsp;<img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{\\infty}2^{-i}O_{i}}\" alt=\"\"> is <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20\\sum_{i=1}^{\\infty}2^{-i}u\\left%28O_{i}\\right%29\\geq\\sum_{i=1}^{\\infty}2^{-i}2^{i}=\\sum_{i=1}^{\\infty}1}\" alt=\"\">, which does not converge. So unbounded utility functions are not compatible with the extended VNM hypothesis.</p>\n<p>At this point, one may feel a strong temptation to come up with some way to characterize the values of infinite sums with some ordered superset of the real numbers, so that it is possible to compare nonconvergent sums. However, by the formulation of the VNM theorem solely in terms of lotteries, the utility of any lottery, such as <img src=\"http://www.codecogs.com/png.latex?{\\displaystyle%20u\\left%28\\sum_{i=1}^{\\infty}2^{-i}O_{i}\\right%29}\" alt=\"\">, is a real number. So any such scheme that requires that the range of the utility function include nonreals will violate the VNM axioms. In particular, it will probably violate the archimedian axiom.</p>\n<p>One possible response to this is to dismiss the archimedian axiom, and try to characterize agents that obey the completeness, transitivity, and independence axioms. Benja <a href=\"/lw/fu1/why_you_must_maximize_expected_utility/\">has written</a> (section \"Doing without Continuity) about this, and I find his solution fairly compelling, but it isn't clear that it helps us deal with situations like the St. Petersburg paradox. I intend to say more about nonarchimedian preferences soon.</p>", "sections": [{"title": "Outcomes versus Lotteries", "anchor": "Outcomes_versus_Lotteries", "level": 1}, {"title": "Infinite lotteries", "anchor": "Infinite_lotteries", "level": 1}, {"title": "Extended VNM hypothesis", "anchor": "Extended_VNM_hypothesis", "level": 1}, {"title": "Bounded and unbounded utility functions", "anchor": "Bounded_and_unbounded_utility_functions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["F46jPraqp258q67nE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-22T01:07:46.812Z", "modifiedAt": null, "url": null, "title": "Recommended reading on the ethics of Animal Cognitive Enhancement", "slug": "recommended-reading-on-the-ethics-of-animal-cognitive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:00.671Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Zwqz6uaZMhJ7uqHae", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8vDRzYZHGuAtfpybN/recommended-reading-on-the-ethics-of-animal-cognitive", "pageUrlRelative": "/posts/8vDRzYZHGuAtfpybN/recommended-reading-on-the-ethics-of-animal-cognitive", "linkUrl": "https://www.lesswrong.com/posts/8vDRzYZHGuAtfpybN/recommended-reading-on-the-ethics-of-animal-cognitive", "postedAtFormatted": "Friday, February 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recommended%20reading%20on%20the%20ethics%20of%20Animal%20Cognitive%20Enhancement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecommended%20reading%20on%20the%20ethics%20of%20Animal%20Cognitive%20Enhancement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8vDRzYZHGuAtfpybN%2Frecommended-reading-on-the-ethics-of-animal-cognitive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recommended%20reading%20on%20the%20ethics%20of%20Animal%20Cognitive%20Enhancement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8vDRzYZHGuAtfpybN%2Frecommended-reading-on-the-ethics-of-animal-cognitive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8vDRzYZHGuAtfpybN%2Frecommended-reading-on-the-ethics-of-animal-cognitive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>Does anyone have recommendations of good things to read on the ethics of animal cognitive enhancement? By this I mean applying various methods of human cognitive enhancement (pharmacological, technological, etc) to animals such as the Great Apes. I've also heard this referred to as 'up-lifting' an animal.</p>\n<p>I'm looking for articles, books, lectures - anything really. Obviously one can just google this but I find getting recommendations from others a better bet. I think this may be a useful resource for other people interested in the same topic. Interesting issues might include:</p>\n<p>- Possible obligations to enhance</p>\n<p>- Possible negative consequences</p>\n<p>- Possible side-effects (such as radically different perspective)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8vDRzYZHGuAtfpybN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 1.119078862033161e-06, "legacy": true, "legacyId": "21549", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-22T01:09:16.987Z", "modifiedAt": null, "url": null, "title": "Calibrating Against Undetectable Utilons and Goal Changing Events (part2and1) ", "slug": "calibrating-against-undetectable-utilons-and-goal-changing-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:05.588Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GjXeKTxkYvZ5WfkFu/calibrating-against-undetectable-utilons-and-goal-changing-0", "pageUrlRelative": "/posts/GjXeKTxkYvZ5WfkFu/calibrating-against-undetectable-utilons-and-goal-changing-0", "linkUrl": "https://www.lesswrong.com/posts/GjXeKTxkYvZ5WfkFu/calibrating-against-undetectable-utilons-and-goal-changing-0", "postedAtFormatted": "Friday, February 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Calibrating%20Against%20Undetectable%20Utilons%20and%20Goal%20Changing%20Events%20(part2and1)%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACalibrating%20Against%20Undetectable%20Utilons%20and%20Goal%20Changing%20Events%20(part2and1)%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjXeKTxkYvZ5WfkFu%2Fcalibrating-against-undetectable-utilons-and-goal-changing-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Calibrating%20Against%20Undetectable%20Utilons%20and%20Goal%20Changing%20Events%20(part2and1)%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjXeKTxkYvZ5WfkFu%2Fcalibrating-against-undetectable-utilons-and-goal-changing-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjXeKTxkYvZ5WfkFu%2Fcalibrating-against-undetectable-utilons-and-goal-changing-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5048, "htmlBody": "<p class=\"western\"><strong>Here is the original unchanged post with sections 1-3 <em>and</em> the new sections 4-8. If you read <a href=\"/r/discussion/lw/gqk/calibrating_against_undetectable_utilons_and_goal/\">the first</a> post, go straight to section 4.</strong></p>\n<p class=\"western\">Summary: Random events can preclude or steal attention from the goals you set up to begin with, hormonal fluctuation inclines people to change some of their goals with time. A discussion on how to act more usefully given those potential changes follows, taking in consideration the likelihood of a goal's success in terms of difficulty and length.</p>\n<p class=\"western\">&nbsp;</p>\n<p class=\"western\">Throughout I'll talk about <em>postponing utilons into undetectable distances. </em>Doing so (I'll claim), is frequently motivationally driven by a cognitive dissonance between what our effects on the near world are, and what we wish they were. In other words it is:</p>\n<p class=\"western\"><em>A <a href=\"http://en.wikipedia.org/wiki/Self-serving_bias\">Self-serving bias</a> in which <a href=\"http://en.wikipedia.org/wiki/Loss_aversion\">Loss aversion</a> manifests by postponing one's goals, thus avoiding frustration through <a href=\"http://en.wikipedia.org/wiki/Wishful_thinking\">wishful thinking</a> about <a href=\"http://en.wikipedia.org/wiki/Frank_J._Tipler\">far futures</a>, <a href=\"http://arxiv.org/abs/gr-qc/0102010\">big worlds</a>, immortal lives, and in general, high numbers of undetectable utilons. </em></p>\n<p class=\"western\">I suspect that some clusters of SciFi, Lesswrong, Transhumanists, and Cryonicists are particularly prone to <em>postponing utilons into undetectable distances</em>, and here I try to think of which subgroups might be more likely to have done so. The phenomenon, though composed of a lot of biases, might even be a good thing depending on how it is handled.</p>\n<p class=\"western\">&nbsp;</p>\n<p class=\"western\">Sections will be:</p>\n<ol>\n<li>\n<p class=\"western\">What Significantly Changes Life's Direction (lists)</p>\n</li>\n<li>\n<p class=\"western\">Long Term Goals and Even Longer Term Goals</p>\n</li>\n<li>\n<p class=\"western\">Proportionality Between Goal Achievement Expected Time and Plan Execution Time</p>\n</li>\n<li>\n<p class=\"western\">A Hypothesis On Why We Became Long-Term Oriented</p>\n</li>\n<li>\n<p class=\"western\">Adapting Bayesian Reasoning to Get More Utilons</p>\n</li>\n<li>\n<p class=\"western\">Time You Can Afford to Wait, Not to Waste</p>\n</li>\n<li>\n<p class=\"western\">Reference Classes that May Be <em>Postponing Utilons Into Undetectable Distances</em></p>\n</li>\n<li>The Road Ahead </li>\n</ol>\n<p class=\"western\">&nbsp;</p>\n<p class=\"western\">&nbsp;</p>\n<p><strong>1What Significantly Changes Life's Direction</strong></p>\n<p><strong><br /></strong></p>\n<p class=\"western\"><strong>1.1 Predominantly external changes</strong></p>\n<p class=\"western\">As far as I recall from reading old (circa 2004) large scale studies on happiness, the most important life events in how much they change your happiness for more than six months are:&nbsp;</p>\n<p class=\"western\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Becoming the caretaker of someone in a chronic non-curable condition</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Separation (versus marriage)</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Death of a Loved One</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Losing your Job</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Child rearing per child including the first</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Chronic intermittent disease</p>\n</li>\n<li>\n<p class=\"western\">Separation (versus being someone's girlfriend/boyfriend)&nbsp;</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Roughly in descending order.&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">That is a list of happiness changing events, I'm interested here in goal-changing events, and am assuming there will be a very high correlation.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">From life experience, mine, of friends, and of academics I've met, I'll list some events which can change someone's goals a lot:</p>\n<p class=\"western\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Moving between cities/countries</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Changing your social class a lot (losing a fortune or making one)&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Spending highschool/undergrad in a different country to return afterwards</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Having a child, in particular the first one&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><a href=\"/lw/2qp/virtual_employment_open_thread/\">Trying to get a job or make money</a> and noticing more accurately what the market looks like</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Alieving Existential Risk</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Alieving as true, <a href=\"http://www.thelifeyoucansave.com/\">universally</a> or <a href=\"/lw/6ny/smart_young_ambitious_and_clueless_what_to_do_to/\">personally,</a> the ethical theories called \"Utilitarianism\" and \"Consequentialism\"</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Noticing that <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">a lot of people are better than you at your initial goals</a>, specially when those goals are competitive non-positive sum goals to some extent.&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Interestingly, noticing that a lot of people are <em>worse than you</em>, making the efforts you once thought necessary not worth doing, or impossible to find good collaborators for.&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Getting to know those who were<a href=\"/lw/31i/have_no_heroes_and_no_villains/\"> once your idols</a>, or akin to them, and considering their lives not as awesome as their work</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">... which is sometimes caused by ...</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Reading Dan Gilbert's \"<a href=\"http://libgen.info/view.php?id=793580\">Stumbling on Happiness</a>\" and actually implementing his \"advice that no one will follow\" which is to think your happiness and emotions will correlate more with someone <em>else</em>&nbsp;who is already doing X which you plan to do than with <em>your model of what it would feel like doing X.&nbsp;</em></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Extreme social instability, such as wars, famine, etc...</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Having an ecstatic or traumatic experience, real or fictional. Such as seeing something unexpected, watching a life-changing movie, having a religious breakthrough, or a hallucinogenic one&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Travelling to a place that is very different from your world and being amazed / shocked</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Not being admitted into your desired university / course</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Depression</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Surpassing a frustration threshold thus experiencing the motivational equivalent of <a href=\"http://en.wikipedia.org/wiki/Learned_helplessness\">learned helplessness</a></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Realizing your goals do not match the space-time you were born in, such as if making songs for CDs is your vocation, or if you are 30 years old in contemporary Kenya and want to teach medicine at a top 10 world college.</p>\n</li>\n<li>\n<p class=\"western\">Falling in love</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">That is long enough, if not exhaustive, so let's get going...&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><strong>1.2 Predominantly Internal Changes</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I'm not a <a href=\"http://www.amazon.com/Endocrinology-Social-Relationships-Peter-Ellison/dp/0674063996\">social endocrinologist</a> but I think this emerging science agrees with folk wisdom that a lot changes in our hormonal systems during life (and during the menstrual cycle) and of course this changes our eagerness to do particular things. Not only hormones but other life events which mostly relate to the actual amount of time lived change our psychology. I'll cite some of those in turn:</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Exploitation increases and Exploration decreases with age</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Sex-Drive</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Maternity Drive - we have in portuguese an expression that &ldquo;a woman's clock <em>started</em> ticking&rdquo; which evidentiates a folk psychological theory that some part of it at least is binary</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Risk-proneness gives way to risk aversion, predominantly in males</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Premenstrual Syndrome - I always thought the acronym stood for 'Stress' until checking for this post.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Hormonal diseases</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Middle Age crisis &ndash; <a href=\"https://www.google.com.br/search?hl=pt-BR&amp;q=middle+age+crisis+in+apes\">recent controversy</a> about other apes having it</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">U shaped happiness curve through time &ndash; well, <a href=\"http://www.sciencedirect.com/science/article/pii/S0167268112000601\">not quite</a></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Menstrual cycle events</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p><strong>2 Long Term Goals and Even Longer Term Goals</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I <a href=\"/lw/f3z/rationality_versus_short_term_selves/\">have argued</a> sometimes here and <a href=\"http://www.youtube.com/watch?v=3RQC7jAWl_o\">elsewhere</a> that <a href=\"/lw/a2f/on_what_selves_are_cev_sequence/\">selves are</a> not as agenty as most of the <a href=\"/lw/5i8/the_power_of_agency/\">top</a> writers in this website seem to me to claim they <a href=\"/tag/agency/\">should</a> be, and that though in part this is indeed irrational, an ontology of selves which had various sized selves would decrease the amount of short term actions considered irrational, even though that would not go all the way into compensating hyperbolic discounting, scrolling 9gag or heroin consumption. That discussion, for me, was entirely about choosing between <em>doing now something that benefits </em><span style=\"font-style: normal;\">'</span><span style=\"font-style: normal;\">you</span><sub><span style=\"font-style: normal;\">now</span></sub><span style=\"font-style: normal;\">'</span><sub><span style=\"font-style: normal;\"> , </span></sub><span style=\"font-style: normal;\">'you</span><sub><span style=\"font-style: normal;\">today</span></sub><span style=\"font-style: normal;\">'</span><span style=\"font-style: normal;\">,</span> 'you<sub>tomorrow</sub>', 'you<sub>this weekend</sub>' or maybe a month from now. Anything longer than that was encompassed in a &ldquo;Far Future&rdquo; <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">mental category</a>. The interest here to discuss life-changing events is only in those far future ones which I'll split into arbitrary categories:</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">1) Months 2) Years 3) Decades 4) Bucket List or Lifelong and 5) Time Insensitive or Forever.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I have known more than ten people from LW whose goals are centered almost completely at the Time Insensitive and Lifelong categories, I recall hearing :</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">\"I see most of my expected utility after the singularity, thus I spend my willpower entirely in increasing the likelihood of a positive singularity, and care little about my current pre-singularity emotions\", &ldquo;My goal is to have a one trillion people world with maximal utility density where everyone lives forever&rdquo;, &ldquo;My sole goal in life is to live an indefinite life-span&rdquo;, &ldquo;I want to reduce X-risk in any way I can, that's all&rdquo;.</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I myself stated once my goal as</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&ldquo;To live long enough to experience a world in which human/posthuman flourishing exceeds 99% of individuals and other lower entities suffering is reduced by 50%, while being a counterfactually significant part of such process taking place.&rdquo;</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Though it seems reasonable, good, and actually one of the most altruistic things we can do, caring only about Bucket Lists and Time Insensitive goals has two big problems</p>\n<ol>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">There is no accurate feedback to calibrate our goal achieving tasks</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>The Goals we set for ourselves require very long term instrumental plans, which themselves take longer than the time it takes for internal drives or external events to change our goals. </em></p>\n</li>\n</ol>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-style: normal;\">The second one has been said in a remarkable Pink Floyd song about which I wrote <a href=\"http://brainstormers.wordpress.com/2008/07/30/the-starting-gun-2/\">a motivational text</a> five years ago: <a href=\"http://www.youtube.com/watch?v=MYiahoYfPGk\"><em>Time</em></a>.</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">You are young and life is long and there is time to kill today</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And then one day you find ten years have got behind you</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">No one told you when to run, you missed the starting gun</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And you run and you run to catch up with the sun, but it's sinking</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And racing around to come up behind you again</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">The sun is the same in a relative way, but you're older</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Shorter of breath and one day closer to death</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Every year is getting shorter, never seem to find the time</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Plans that either come to naught or half a page of scribbled lines</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Okay, maybe the song doesn't say exactly (2) but it is within the same ballpark. The fact remains that those of us inclined to care mostly about very long term are quite likely to end up with a half baked plan because one of those dozens of life-changing events happened, and that agent with the initial goals will have died for no reason if she doesn't manage to get someone to continue her goals before she stops existing.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">This is <em>very bad</em><span style=\"font-style: normal;\">. Once you understand how our goal-structures do change over time &ndash; that is, when you accept the existence of all those events that will change what you want to steer the world into &ndash; it becomes straightforward </span><em>irrational</em><span style=\"font-style: normal;\"> to pursue your goals as if that agent would live longer than it's actual life expectancy. Thus we are surrounded by agents </span><em>postponing utilons into undetectable distances</em><span style=\"font-style: normal;\">. Doing this is kind of a bias in the opposite direction of hyperbolic discounting. </span><em>Having </em><em><em>postponed utilons into undetectable distances</em> </em>is predictably irrational because it means we<span style=\"font-style: normal;\"> care about our Lifelong, Bucket List, and Time Insensitive goals </span><em>as if </em><span style=\"font-style: normal;\">we'd have enough time to actually execute the plans for these timeframes, while ignoring the likelihood of our goals changing in the meantime and factoring that in. <br /></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">I've come to realize that this was affecting me with my <a href=\"/lw/evl/abandoning_cached_selves_to_rewrite_my_source/\">Utility Function Breakdown</a> which was described in the linked post about digging too deep into one's cached selves and how this can be dangerous. As I predicted back then, stability has returned to my allocation of attention and time and the whole zig-zagging chaotic piconomical neural Darwinism that had ensued has stopped. Also&nbsp; relevant is the fact that after about 8 years caring about more or less similar things, I've come to understand how frequently my motivation changed direction (roughly every three months for some kinds of things, and 6-8 months for other kinds). With this post I intend to learn to calibrate my future plans accordingly, and help others do the same. Always beware of other-optimizing though. <br /></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>Citizen: But what if my goals are all Lifelong or Forever in kind? It is impossible for me to execute in 3 months what will make centenary changes</em><span style=\"font-style: normal;\">. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">Well, not exactly. Some problems require chunks of plans which can be separated and executed either in parallel or in series. And yes, everyone knows that, also <em>AI planning</em> is a whole area dedicated to doing just that in non-human form. It is still worth mentioning, because it is much more <a href=\"http://yudkowsky.net/rational/the-simple-truth\">simply true</a> than <a href=\"/lw/3w3/how_to_beat_procrastination/\">actually done.</a></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">This community in general has concluded in its rational inquiries that being longer term oriented is generally a better way to win, that is, it is more rational. This is true. What would not be rational is to in every single instance of deciding between long term or even longer term goals, choose without taking in consideration <em><a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/\">how long</a> will the choosing being exist</em>, in the sense of being the same agent with the same goals. Life-changing events happen more often than you think, because you think they happen as often as they did in the savannahs in which your brain was shaped.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>3 Proportionality Between Goal Achievement Expected Time and Plan Execution Time</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">So far we have been through the following ideas. Lots of events change your goals, some externally some internally, if you are a rationalist, you end up caring more about events that take longer to happen in detectable ways (since if you are average you care in proportion to emotional drives that <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">execute adaptations</a> but don't quite achieve goals). If you know that humans change and still want to achieve your goals, you'd better account for the possibility of changing before their achievement<em>.</em> Your kinds of goals are quite likely prone to the long-term since you are reading a Lesswrong post.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>Citizen: But wait! Who said that my goals happening in a hundred years makes my specific instrumental plans take longer to be executed? </em></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em><br /></em></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">I won't make the case for the idea that having long term goals increases the likelihood of the time it takes to execute your plans being longer. I'll only say that if it did not take that long to do those things </span><em>your goal would probably be to have done the same things, only sooner</em><span style=\"font-style: normal;\">. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">To take one example: &ldquo;I would like 90% of people to surpass 150 IQ and be in a <a href=\"http://www.hedweb.com/\">bliss gradient</a> state of mind all the time&rdquo;</span></p>\n<p><span style=\"font-style: normal;\">Obviously, the sooner that happens, the better. Doesn't look like the kind of thing you'd wait for college to end to begin doing, or for your second child to be born. The reason for wanting this long-term is that it can't be achieved in the short run. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">Take <a href=\"/lw/4g/eliezer_yudkowsky_facts/\">Idealized Fiction of Eliezer Yudkosky</a>: Mr Ifey had this supergoal of making a Superintelligence when he was very young. He didn't go there and do it. Because he could not. If he could do it he would. Thank goodness, for we had time to find out about FAI after that. Then his instrumental goal was to get FAI into the minds of the AGI makers. This turned out to be to hard because it was time consuming. He reasoned that only a more rational AI community would be able to pull it off, all while finding a club of brilliant followers in this peculiar economist's blog. He created a blog to teach geniuses rationality, a project that might have taken years. It did, and it worked pretty well, but that was not enough, Ifey soon realized more people ought to be more rational, and wrote HPMOR to make people who were not previously prone to brilliance as able to find the facts as those who were lucky enough to have found his path. All of that was not enough, an institution, with money flow had to be created, and there Ifey was to create it, years before all that. A magnet of long-term awesomeness of proportions comparable only to the Best Of Standing Transfinite Restless Oracle Master, he was responsible for the education of some of the greatest within the generation that might change the worlds destiny for good. Ifey began to work on a rationality book, which at some point pivoted to research for journals and pivoted back to research for the Lesswrong posts he is currently publishing. All that Ifey did by splitting that big supergoal in smaller ones (creating Singinst, showing awesomeness in Overcoming Bias, writing the sequences, writing the particular sequence &ldquo;Misterious Answers to Misterious Questions&rdquo; and writing the specific post &ldquo;Making Your Beliefs Pay Rent&rdquo;). But that is not what I want to emphasize, what I'd like to emphasize is that </span><em>there was room for changing goals every now and then</em><span style=\"font-style: normal;\">. All of that achievement would not have been possible if at each point he had an instrumental goal which lasts 20 years whose value is very low uptill the 19th year. Because a lot of what he wrote and did remained valuable for others before the 20th year, we now have a glowing community of people hopefully becoming better at becoming better, and making the world a better place in varied ways. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">So yes, the ubiquitous advice of chopping problems into smaller pieces is extremely useful and very important, but in addition to it, remember to chop pieces with the following properties:</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (A) Short enough that you will actually do it.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (B) Short enough that the person at the end, doing it, <em>will still be you</em> in the significant ways. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">(C) Having enough emotional feedback that your motivation won't be capsized before the end. and</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (D) Such that others not only can, but likely will take up the project after you abandon it in case you miscalculated when you'd change, or a change occurred before expected time. </span></p>\n<p>&nbsp;</p>\n<p><strong>4 A Hypothesis On Why We Became Long-Term Oriented</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">For anyone who rejoiced the company of the writings of Derek Parfit, George Ainslie, or Nick Bostrom, there are a lot of very good reasons to become more long-term oriented. I am here to ask you about those reasons: <a href=\"/lw/wj/is_that_your_true_rejection/\">Is that you true acceptance?</a> </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">It is not for </span><em>me</em><span style=\"font-style: normal;\">. I became longer term oriented because of different reasons. Two obvious ones are genetics expressing in me the kind of person that waits a year for the extra marshmallow while fantasizing about marshmallow worlds and rocking horse pies, and secondly wanting to live thousands of years. But the one I'd like to suggest that might be relevant to some here is that I was very bad at making people who were sad or hurt happy. I was not, as they say, empathic. It was a piece of cake bringing folks from neutral state to joy and bliss. But if someone got angry or sad, specially sad with something I did, I would be absolutely powerless about it. This is only one way of not being </span><em>good with people, a people's person </em><span style=\"font-style: normal;\">etc... So my emotional system, like the tale's Big Bad Wolf blew, and blew, and blew, until my utilons were comfortably sitting aside in the Far Future, where none of them could look back at my face, cry, and point to me as the tears cause. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">Paradoxically, though <a href=\"http://konyv.uw.hu/dennett-the-intentional-stance.htm\">understandably</a>, I have since been thankful for that lack of empathy towards those near. In fact, I have claimed, where I forget, that it is the moral responsibility of those with less natural empathy of the </span><em>giving to beggars</em><span style=\"font-style: normal;\"> kind to care about the far future, since so few are within this tiny psychological mindspace of being able to care abstractly while not caring that much visibly/emotionally. We are such a minority that foreign aid seems to be the thing that is more disproportional in public policy between countries (Savulescu, J - Genetically Enhance Humanity of Face Extinction 2009 <a href=\"/vimeo.com/7515623\">video</a>). Like the whole minority of billionaires ought to be more like Bill Gates, Peter Thiel and Jaan Tallinn, the minority of underempathic folk ought to be more like an economist doing quantitative analysis to save or help in quantitative ways. Let us look at our examples again: </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<blockquote>\n<p>&ldquo;My goal is to have a one trillion people world with maximal utility density where everyone lives forever&rdquo;, \"I see most of my expected utility after the singularity, thus I spend my willpower entirely in increasing the likelihood of a positive singularity, and care little about my current pre-singularity emotions\", &ldquo;I want to reduce X-risk in any way I can, that's all&rdquo; , &ldquo;My sole goal in life is to live an indefinite life-span&rdquo;.</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">So maybe their true (or original) acceptance of Longterm, like mine, was something like Genes + Death sucks + I'd rather interact with people of the future whose bots in my mind smile, than those actually meaty folk around me, with all their specific problems, complicated families and boring Christian relationship problems. This is my hypothesis. Even if true, notice it does not imply that longterm isn't rational, after all Parfit, Bostrom and Ainslie are still standing, even after careful scrutiny.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\"><br /></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><strong>5 Adapting Bayesian Reasoning to Get More Utilons</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">Just like many within this community praise Bayesian reasoning but don't explicitly keep track of belief distributions (as far as I recall only Steve Rayhawk and Anna Salamon, of all I met, kept a few beliefs numerically) few or none probably would need the math specifics to calibrate and slide their plans towards 'higher likelihood of achievement' given their change in goals probability over time. In other words, few need to do actual math to just account intuitively - though not accurately - for changing their plans in such way that now the revised plans are more likely to work than they were before.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">It doesn't amount to much more than simple multiplication and comparison. If you knew for a period of time how likely you are at each point to become someone who does not have goal X anymore, you should strive for X to be you-independent by the time that this transformation is likely to happen. But how likely must the gruesome transformation be at the point in time in which you expect the plan to be over? 10% would probably make you a blogger, 30% a book writer 50% a Toyota-like company creator and 90% would probably make you useless, since 90% of times no legacy would be left from your goals. It would be the rationalist equivalent of having ADHD. And if you also have <em>actual </em>ADHD, then the probabilities would multiply into a chaotic constant shift of attention in which your goals are not realized 90% of time even if they last actually very short times, that is probably why the World looks insane.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">Then again, how likely?&nbsp; My suggestion would be to use a piece of knowledge that comes from the derailed subset of positive psychology, Human Resources. There is this discovered ratio called The Losada Ratio, aka The Losada Line.&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">2.9013</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">The Schelling Point for talking about it to increase its memetic spreading is 3, so I'll use 3 here. But what is it? From Wikipedia:</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">The <strong>Losada Line</strong>, also known as the \"Losada ratio,\" was described by psychologist <a title=\"Marcial Losada\" href=\"http://en.wikipedia.org/wiki/Marcial_Losada\">Marcial Losada</a> while researching the differences in ratios of <strong><em>positivity and negativity</em></strong> between high and low performance teams.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"http://en.wikipedia.org/wiki/Losada_line#cite_note-1\"><span>[</span>1<span>]</span></a></sup><sup id=\"cite_ref-2\" class=\"reference\"><a href=\"http://en.wikipedia.org/wiki/Losada_line#cite_note-2\"><span>[</span>2<span>]</span></a></sup></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">The Losada Line represents a <a title=\"Positivity/negativity ratio\" href=\"http://en.wikipedia.org/wiki/Positivity/negativity_ratio\">positivity/negativity ratio</a> of roughly 2.9, and it marks the lower boundary of the <a title=\"Losada Zone\" href=\"http://en.wikipedia.org/wiki/Losada_Zone\">Losada Zone</a> (the upper bound is around 11.6). It was corroborated by Barbara Fredrickson, a psychologist at the University of North Carolina, Chapel Hill, in individuals, and by Waugh and Fredrickson in relationships.<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"http://en.wikipedia.org/wiki/Losada_line#cite_note-3\"><span>[</span>3<span>]</span></a></sup> They found that the Losada Line separates people who are able to reach a complex understanding of others from those who do not. People who \"flourish\" are above the Losada Line, and those who \"languish\" are below it.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"http://en.wikipedia.org/wiki/Losada_line#cite_note-4\"><span>[</span>4<span>]</span></a></sup><sup id=\"cite_ref-5\" class=\"reference\"><a href=\"http://en.wikipedia.org/wiki/Losada_line#cite_note-5\"><span>[</span>5<span>]</span></a></sup> The Losada Line bifurcates the type of dynamics that are possible for an interactive human system. Below it, we find limiting dynamics represented by fixed-point attractors; at or above it, we find innovative dynamics represented by complex order attractors (<a title=\"Complexor\" href=\"http://en.wikipedia.org/wiki/Complexor\">complexor</a>).</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">3:1 Compliments per complaint. Three requests per order. Three \"That's awesome!\" per \"Sorry, I didn't like that\"</p>\n<p>&nbsp;</p>\n<p>Getting back to our need for a Bayesian slide/shift in which we increase how likely our goals are to be achieved, wouldn't it be great if we set to achieve above the Losada line, thus keeping ourselves motivated by<em> having reality complimenting us within the Losada zone?</em></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">3:1 is the ratio I suggest of expected successes for Longterm folk in the process of dividing their Time Insensitive supergoals - which, granted, may be as unlikely as Kurzweil actually achieving longevity escape velocity - into smaller instrumental goals.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">If you agree with most of what has been said so far, and you'd like to be rewarded by your boss, <a href=\"/lw/up/shut_up_and_do_the_impossible/\">Mr Reality</a>, in the proportion of those who thrive, while doing stuff more useful than playing videogames previously designed within the Losada Zone, I suggest you try and do instrumental stuff with a 75% expected likelihood of success.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">Once well calibrated, you'll succeed three out of four times in your endeavours emotionally, which will keep you away from learned helplessness while still excited about the Longterm, Lifelong, Far Futuristic ideals with lower likelihoods. I would hope this could complement the whole anti-Akrasia posts on Lesswrong.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">For those who would <em>really </em>like to think about the math involved, take a look at <a href=\"/lw/3d0/an_intuitive_explanation_of_eliezer_yudkowskys/\">the</a> <a href=\"http://yudkowsky.net/rational/bayes\">three</a> <a href=\"/lw/2b0/bayes_theorem_illustrated_my_way/\">introductions</a> to bayesianisms available in Lesswrong and complement them with the idea of Bayesian shift related to the Doomsday argument which can be found in wikipedia or Bostrom's \"<a href=\"http://libgen.info/view.php?id=457319\">Anthropic Bias</a>\" book.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><strong>6 Time You Can Afford to Wait, Not to Waste</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">Many of us are still younger than 25. Unless you started trying to achieve goals particularly young, if you are 18 you are likely undergoing a twister of events and hormones, and whatever you guess that will end up being your average time-span of motivation without internal or external interruptions actually won't be more than a guess. By 25 you are familiar with yourself, and probably able get your projects into frequency. But what to do before that?&nbsp; One suggestion that comes to mind is to create plans that actually require quite a long time.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">For the same reasons that those whose natural empathy is a little less than normal bear a moral responsibility of taking care of those a little distant, those who yet do not know if they are able to set out to do the impossible, with accuracy and consistency over long periods should probably try doing that. It is better to have false positives than false negatives in this case. Not only you'd never know how long you'd last if you set for short term, but also the very few who are able to go long term would never be found.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">So if you are young (in your dialect's meaning of 'young') <a href=\"/lw/up/shut_up_and_do_the_impossible/\">shut up, do the impossible</a>, <a href=\"http://lukeprog.com/SaveTheWorld.html\">save the world</a>. Isn't that the point of the Thiel Fellowship for Twenty Under Twenty anyway?</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><strong>7 Reference Classes that May Be <em>Postponing Utilons Into Undetectable Distances</em></strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">Of course Lesswrongers are prone to <em>postponing utilons into undetectable distances</em>. But which subsets? I regard the highest risk groups to be:</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><em>Cryocrastinators</em> - For one they want to live forever, on the other hand, their plan of doing something about it never succeeded, this looks like undercalibration.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><em>The subset of Effective Altruists who care mostly about future people/minds they'll never meet</em> - I find myself in this group.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><em>The subset of&nbsp; Singularitarians whose emotions are projected to SL4 and afterwards</em> - whom are doing something akin to the ascetic folk who talk about life as if it were set in the future, after death, making them less able to deal with their daily meaty human problems.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">Sure there will be a large minority of each group which doesn't fall prey to </span><span style=\"font-style: normal;\"><em>postponing utilons into undetectable distances</em>, and sure, <a href=\"http://vserver1.cscs.lsa.umich.edu/~crshalizi/Russell/Hearst_Essays/How_to_Become_a_Man_of_Genius.html\">you belong</a> to that minority, but stressing the point out makes it salient enough that if you ever find yourself trying to rationalize about this more than you should, you'll think twice.&nbsp; </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">It has been said to me that if that is what makes an Effective Altruist, so be it! And thank goodness we have them. To which I say: <strong>Yes! </strong></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\"><br /></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">Every characteristic has a causal origin, and frequently a story can be told which <a href=\"http://www.youtube.com/watch?v=rseKPsiNPmQ\">explains the emergence</a> of that characteristic. Given that we are all biases, no wonder some of those stories will have biases in prominent roles. This does not invalidate the ethics of those whose lives were shaped by those stories. In fact, if a transmissible disease of mind were the sole cause of awesome people in this world, scientists would be trying to engineer mosquitoes to become more, not less infectious. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">Does that mean I spent this entire post arguing that something like a bias exist which we'd better not overcome? Or worse yet, is all of this post an attempt to justify caring about those far even though it would be emotionally much harder - and thus feel more real - to care about those near? Maybe, you be the judge. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">There are also several distinct other reference classes that are smaller, though would deserve mention. The Big Worlds Tegmark or Vilenkin Fans who think about the whole superintelligence fighters, economics of acausal trade, Permutation City and so on...&nbsp; </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">A good heuristic to check that is to see if the person has a <em>low tolerance to frustration</em> plus a lot of undetectable utilons in her worldview. Interestingly the more undetectable utilons you have, the more it looks like you are just extrapolating the universally accepted as ethical idea of expanding your circle of altruism, one that has been called, of all things, your \"circle of empathy\".&nbsp;</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\"><strong>8 The Road Ahead</strong></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">In this community, and that is perhaps it's greatest advantage, there can never be enough stress on deciding well which direction to take. Or which tool to use. If you are about to travel, it is argued, the most important thing is to figure out to where are you going and steer your future into that direction.&nbsp; What I suggest is as important as the direction to which one decided to travel is that you figure out you tank's size and how often do you need gas stations on your way there. It is better to get to a worse place than to be stymied by an inner force that, if you can't quite control, you can at least significantly reduce its likelihood of failure and amount of damage.&nbsp;</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\"><br /></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GjXeKTxkYvZ5WfkFu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 10, "extendedScore": null, "score": 1.1190798253609974e-06, "legacy": true, "legacyId": "21031", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"western\"><strong id=\"Here_is_the_original_unchanged_post_with_sections_1_3_and_the_new_sections_4_8__If_you_read_the_first_post__go_straight_to_section_4_\">Here is the original unchanged post with sections 1-3 <em>and</em> the new sections 4-8. If you read <a href=\"/r/discussion/lw/gqk/calibrating_against_undetectable_utilons_and_goal/\">the first</a> post, go straight to section 4.</strong></p>\n<p class=\"western\">Summary: Random events can preclude or steal attention from the goals you set up to begin with, hormonal fluctuation inclines people to change some of their goals with time. A discussion on how to act more usefully given those potential changes follows, taking in consideration the likelihood of a goal's success in terms of difficulty and length.</p>\n<p class=\"western\">&nbsp;</p>\n<p class=\"western\">Throughout I'll talk about <em>postponing utilons into undetectable distances. </em>Doing so (I'll claim), is frequently motivationally driven by a cognitive dissonance between what our effects on the near world are, and what we wish they were. In other words it is:</p>\n<p class=\"western\"><em>A <a href=\"http://en.wikipedia.org/wiki/Self-serving_bias\">Self-serving bias</a> in which <a href=\"http://en.wikipedia.org/wiki/Loss_aversion\">Loss aversion</a> manifests by postponing one's goals, thus avoiding frustration through <a href=\"http://en.wikipedia.org/wiki/Wishful_thinking\">wishful thinking</a> about <a href=\"http://en.wikipedia.org/wiki/Frank_J._Tipler\">far futures</a>, <a href=\"http://arxiv.org/abs/gr-qc/0102010\">big worlds</a>, immortal lives, and in general, high numbers of undetectable utilons. </em></p>\n<p class=\"western\">I suspect that some clusters of SciFi, Lesswrong, Transhumanists, and Cryonicists are particularly prone to <em>postponing utilons into undetectable distances</em>, and here I try to think of which subgroups might be more likely to have done so. The phenomenon, though composed of a lot of biases, might even be a good thing depending on how it is handled.</p>\n<p class=\"western\">&nbsp;</p>\n<p class=\"western\">Sections will be:</p>\n<ol>\n<li>\n<p class=\"western\">What Significantly Changes Life's Direction (lists)</p>\n</li>\n<li>\n<p class=\"western\">Long Term Goals and Even Longer Term Goals</p>\n</li>\n<li>\n<p class=\"western\">Proportionality Between Goal Achievement Expected Time and Plan Execution Time</p>\n</li>\n<li>\n<p class=\"western\">A Hypothesis On Why We Became Long-Term Oriented</p>\n</li>\n<li>\n<p class=\"western\">Adapting Bayesian Reasoning to Get More Utilons</p>\n</li>\n<li>\n<p class=\"western\">Time You Can Afford to Wait, Not to Waste</p>\n</li>\n<li>\n<p class=\"western\">Reference Classes that May Be <em>Postponing Utilons Into Undetectable Distances</em></p>\n</li>\n<li>The Road Ahead </li>\n</ol>\n<p class=\"western\">&nbsp;</p>\n<p class=\"western\">&nbsp;</p>\n<p><strong id=\"1What_Significantly_Changes_Life_s_Direction\">1What Significantly Changes Life's Direction</strong></p>\n<p><strong><br></strong></p>\n<p class=\"western\"><strong id=\"1_1_Predominantly_external_changes\">1.1 Predominantly external changes</strong></p>\n<p class=\"western\">As far as I recall from reading old (circa 2004) large scale studies on happiness, the most important life events in how much they change your happiness for more than six months are:&nbsp;</p>\n<p class=\"western\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Becoming the caretaker of someone in a chronic non-curable condition</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Separation (versus marriage)</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Death of a Loved One</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Losing your Job</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Child rearing per child including the first</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Chronic intermittent disease</p>\n</li>\n<li>\n<p class=\"western\">Separation (versus being someone's girlfriend/boyfriend)&nbsp;</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Roughly in descending order.&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">That is a list of happiness changing events, I'm interested here in goal-changing events, and am assuming there will be a very high correlation.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">From life experience, mine, of friends, and of academics I've met, I'll list some events which can change someone's goals a lot:</p>\n<p class=\"western\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Moving between cities/countries</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Changing your social class a lot (losing a fortune or making one)&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Spending highschool/undergrad in a different country to return afterwards</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Having a child, in particular the first one&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><a href=\"/lw/2qp/virtual_employment_open_thread/\">Trying to get a job or make money</a> and noticing more accurately what the market looks like</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Alieving Existential Risk</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Alieving as true, <a href=\"http://www.thelifeyoucansave.com/\">universally</a> or <a href=\"/lw/6ny/smart_young_ambitious_and_clueless_what_to_do_to/\">personally,</a> the ethical theories called \"Utilitarianism\" and \"Consequentialism\"</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Noticing that <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">a lot of people are better than you at your initial goals</a>, specially when those goals are competitive non-positive sum goals to some extent.&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Interestingly, noticing that a lot of people are <em>worse than you</em>, making the efforts you once thought necessary not worth doing, or impossible to find good collaborators for.&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Getting to know those who were<a href=\"/lw/31i/have_no_heroes_and_no_villains/\"> once your idols</a>, or akin to them, and considering their lives not as awesome as their work</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">... which is sometimes caused by ...</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Reading Dan Gilbert's \"<a href=\"http://libgen.info/view.php?id=793580\">Stumbling on Happiness</a>\" and actually implementing his \"advice that no one will follow\" which is to think your happiness and emotions will correlate more with someone <em>else</em>&nbsp;who is already doing X which you plan to do than with <em>your model of what it would feel like doing X.&nbsp;</em></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Extreme social instability, such as wars, famine, etc...</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Having an ecstatic or traumatic experience, real or fictional. Such as seeing something unexpected, watching a life-changing movie, having a religious breakthrough, or a hallucinogenic one&nbsp;</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Travelling to a place that is very different from your world and being amazed / shocked</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Not being admitted into your desired university / course</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Depression</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Surpassing a frustration threshold thus experiencing the motivational equivalent of <a href=\"http://en.wikipedia.org/wiki/Learned_helplessness\">learned helplessness</a></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Realizing your goals do not match the space-time you were born in, such as if making songs for CDs is your vocation, or if you are 30 years old in contemporary Kenya and want to teach medicine at a top 10 world college.</p>\n</li>\n<li>\n<p class=\"western\">Falling in love</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">That is long enough, if not exhaustive, so let's get going...&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><strong id=\"1_2_Predominantly_Internal_Changes\">1.2 Predominantly Internal Changes</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I'm not a <a href=\"http://www.amazon.com/Endocrinology-Social-Relationships-Peter-Ellison/dp/0674063996\">social endocrinologist</a> but I think this emerging science agrees with folk wisdom that a lot changes in our hormonal systems during life (and during the menstrual cycle) and of course this changes our eagerness to do particular things. Not only hormones but other life events which mostly relate to the actual amount of time lived change our psychology. I'll cite some of those in turn:</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Exploitation increases and Exploration decreases with age</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Sex-Drive</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Maternity Drive - we have in portuguese an expression that \u201ca woman's clock <em>started</em> ticking\u201d which evidentiates a folk psychological theory that some part of it at least is binary</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Risk-proneness gives way to risk aversion, predominantly in males</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Premenstrual Syndrome - I always thought the acronym stood for 'Stress' until checking for this post.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Hormonal diseases</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Middle Age crisis \u2013 <a href=\"https://www.google.com.br/search?hl=pt-BR&amp;q=middle+age+crisis+in+apes\">recent controversy</a> about other apes having it</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">U shaped happiness curve through time \u2013 well, <a href=\"http://www.sciencedirect.com/science/article/pii/S0167268112000601\">not quite</a></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Menstrual cycle events</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p><strong id=\"2_Long_Term_Goals_and_Even_Longer_Term_Goals\">2 Long Term Goals and Even Longer Term Goals</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I <a href=\"/lw/f3z/rationality_versus_short_term_selves/\">have argued</a> sometimes here and <a href=\"http://www.youtube.com/watch?v=3RQC7jAWl_o\">elsewhere</a> that <a href=\"/lw/a2f/on_what_selves_are_cev_sequence/\">selves are</a> not as agenty as most of the <a href=\"/lw/5i8/the_power_of_agency/\">top</a> writers in this website seem to me to claim they <a href=\"/tag/agency/\">should</a> be, and that though in part this is indeed irrational, an ontology of selves which had various sized selves would decrease the amount of short term actions considered irrational, even though that would not go all the way into compensating hyperbolic discounting, scrolling 9gag or heroin consumption. That discussion, for me, was entirely about choosing between <em>doing now something that benefits </em><span style=\"font-style: normal;\">'</span><span style=\"font-style: normal;\">you</span><sub><span style=\"font-style: normal;\">now</span></sub><span style=\"font-style: normal;\">'</span><sub><span style=\"font-style: normal;\"> , </span></sub><span style=\"font-style: normal;\">'you</span><sub><span style=\"font-style: normal;\">today</span></sub><span style=\"font-style: normal;\">'</span><span style=\"font-style: normal;\">,</span> 'you<sub>tomorrow</sub>', 'you<sub>this weekend</sub>' or maybe a month from now. Anything longer than that was encompassed in a \u201cFar Future\u201d <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">mental category</a>. The interest here to discuss life-changing events is only in those far future ones which I'll split into arbitrary categories:</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">1) Months 2) Years 3) Decades 4) Bucket List or Lifelong and 5) Time Insensitive or Forever.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I have known more than ten people from LW whose goals are centered almost completely at the Time Insensitive and Lifelong categories, I recall hearing :</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">\"I see most of my expected utility after the singularity, thus I spend my willpower entirely in increasing the likelihood of a positive singularity, and care little about my current pre-singularity emotions\", \u201cMy goal is to have a one trillion people world with maximal utility density where everyone lives forever\u201d, \u201cMy sole goal in life is to live an indefinite life-span\u201d, \u201cI want to reduce X-risk in any way I can, that's all\u201d.</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">I myself stated once my goal as</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">\u201cTo live long enough to experience a world in which human/posthuman flourishing exceeds 99% of individuals and other lower entities suffering is reduced by 50%, while being a counterfactually significant part of such process taking place.\u201d</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Though it seems reasonable, good, and actually one of the most altruistic things we can do, caring only about Bucket Lists and Time Insensitive goals has two big problems</p>\n<ol>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">There is no accurate feedback to calibrate our goal achieving tasks</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>The Goals we set for ourselves require very long term instrumental plans, which themselves take longer than the time it takes for internal drives or external events to change our goals. </em></p>\n</li>\n</ol>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-style: normal;\">The second one has been said in a remarkable Pink Floyd song about which I wrote <a href=\"http://brainstormers.wordpress.com/2008/07/30/the-starting-gun-2/\">a motivational text</a> five years ago: <a href=\"http://www.youtube.com/watch?v=MYiahoYfPGk\"><em>Time</em></a>.</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">You are young and life is long and there is time to kill today</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And then one day you find ten years have got behind you</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">No one told you when to run, you missed the starting gun</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And you run and you run to catch up with the sun, but it's sinking</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">And racing around to come up behind you again</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">The sun is the same in a relative way, but you're older</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Shorter of breath and one day closer to death</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Every year is getting shorter, never seem to find the time</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Plans that either come to naught or half a page of scribbled lines</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Okay, maybe the song doesn't say exactly (2) but it is within the same ballpark. The fact remains that those of us inclined to care mostly about very long term are quite likely to end up with a half baked plan because one of those dozens of life-changing events happened, and that agent with the initial goals will have died for no reason if she doesn't manage to get someone to continue her goals before she stops existing.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">This is <em>very bad</em><span style=\"font-style: normal;\">. Once you understand how our goal-structures do change over time \u2013 that is, when you accept the existence of all those events that will change what you want to steer the world into \u2013 it becomes straightforward </span><em>irrational</em><span style=\"font-style: normal;\"> to pursue your goals as if that agent would live longer than it's actual life expectancy. Thus we are surrounded by agents </span><em>postponing utilons into undetectable distances</em><span style=\"font-style: normal;\">. Doing this is kind of a bias in the opposite direction of hyperbolic discounting. </span><em>Having </em><em><em>postponed utilons into undetectable distances</em> </em>is predictably irrational because it means we<span style=\"font-style: normal;\"> care about our Lifelong, Bucket List, and Time Insensitive goals </span><em>as if </em><span style=\"font-style: normal;\">we'd have enough time to actually execute the plans for these timeframes, while ignoring the likelihood of our goals changing in the meantime and factoring that in. <br></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">I've come to realize that this was affecting me with my <a href=\"/lw/evl/abandoning_cached_selves_to_rewrite_my_source/\">Utility Function Breakdown</a> which was described in the linked post about digging too deep into one's cached selves and how this can be dangerous. As I predicted back then, stability has returned to my allocation of attention and time and the whole zig-zagging chaotic piconomical neural Darwinism that had ensued has stopped. Also&nbsp; relevant is the fact that after about 8 years caring about more or less similar things, I've come to understand how frequently my motivation changed direction (roughly every three months for some kinds of things, and 6-8 months for other kinds). With this post I intend to learn to calibrate my future plans accordingly, and help others do the same. Always beware of other-optimizing though. <br></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>Citizen: But what if my goals are all Lifelong or Forever in kind? It is impossible for me to execute in 3 months what will make centenary changes</em><span style=\"font-style: normal;\">. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">Well, not exactly. Some problems require chunks of plans which can be separated and executed either in parallel or in series. And yes, everyone knows that, also <em>AI planning</em> is a whole area dedicated to doing just that in non-human form. It is still worth mentioning, because it is much more <a href=\"http://yudkowsky.net/rational/the-simple-truth\">simply true</a> than <a href=\"/lw/3w3/how_to_beat_procrastination/\">actually done.</a></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">This community in general has concluded in its rational inquiries that being longer term oriented is generally a better way to win, that is, it is more rational. This is true. What would not be rational is to in every single instance of deciding between long term or even longer term goals, choose without taking in consideration <em><a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/\">how long</a> will the choosing being exist</em>, in the sense of being the same agent with the same goals. Life-changing events happen more often than you think, because you think they happen as often as they did in the savannahs in which your brain was shaped.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"3_Proportionality_Between_Goal_Achievement_Expected_Time_and_Plan_Execution_Time\">3 Proportionality Between Goal Achievement Expected Time and Plan Execution Time</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">So far we have been through the following ideas. Lots of events change your goals, some externally some internally, if you are a rationalist, you end up caring more about events that take longer to happen in detectable ways (since if you are average you care in proportion to emotional drives that <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">execute adaptations</a> but don't quite achieve goals). If you know that humans change and still want to achieve your goals, you'd better account for the possibility of changing before their achievement<em>.</em> Your kinds of goals are quite likely prone to the long-term since you are reading a Lesswrong post.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em>Citizen: But wait! Who said that my goals happening in a hundred years makes my specific instrumental plans take longer to be executed? </em></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><em><br></em></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">I won't make the case for the idea that having long term goals increases the likelihood of the time it takes to execute your plans being longer. I'll only say that if it did not take that long to do those things </span><em>your goal would probably be to have done the same things, only sooner</em><span style=\"font-style: normal;\">. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">To take one example: \u201cI would like 90% of people to surpass 150 IQ and be in a <a href=\"http://www.hedweb.com/\">bliss gradient</a> state of mind all the time\u201d</span></p>\n<p><span style=\"font-style: normal;\">Obviously, the sooner that happens, the better. Doesn't look like the kind of thing you'd wait for college to end to begin doing, or for your second child to be born. The reason for wanting this long-term is that it can't be achieved in the short run. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">Take <a href=\"/lw/4g/eliezer_yudkowsky_facts/\">Idealized Fiction of Eliezer Yudkosky</a>: Mr Ifey had this supergoal of making a Superintelligence when he was very young. He didn't go there and do it. Because he could not. If he could do it he would. Thank goodness, for we had time to find out about FAI after that. Then his instrumental goal was to get FAI into the minds of the AGI makers. This turned out to be to hard because it was time consuming. He reasoned that only a more rational AI community would be able to pull it off, all while finding a club of brilliant followers in this peculiar economist's blog. He created a blog to teach geniuses rationality, a project that might have taken years. It did, and it worked pretty well, but that was not enough, Ifey soon realized more people ought to be more rational, and wrote HPMOR to make people who were not previously prone to brilliance as able to find the facts as those who were lucky enough to have found his path. All of that was not enough, an institution, with money flow had to be created, and there Ifey was to create it, years before all that. A magnet of long-term awesomeness of proportions comparable only to the Best Of Standing Transfinite Restless Oracle Master, he was responsible for the education of some of the greatest within the generation that might change the worlds destiny for good. Ifey began to work on a rationality book, which at some point pivoted to research for journals and pivoted back to research for the Lesswrong posts he is currently publishing. All that Ifey did by splitting that big supergoal in smaller ones (creating Singinst, showing awesomeness in Overcoming Bias, writing the sequences, writing the particular sequence \u201cMisterious Answers to Misterious Questions\u201d and writing the specific post \u201cMaking Your Beliefs Pay Rent\u201d). But that is not what I want to emphasize, what I'd like to emphasize is that </span><em>there was room for changing goals every now and then</em><span style=\"font-style: normal;\">. All of that achievement would not have been possible if at each point he had an instrumental goal which lasts 20 years whose value is very low uptill the 19th year. Because a lot of what he wrote and did remained valuable for others before the 20th year, we now have a glowing community of people hopefully becoming better at becoming better, and making the world a better place in varied ways. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">So yes, the ubiquitous advice of chopping problems into smaller pieces is extremely useful and very important, but in addition to it, remember to chop pieces with the following properties:</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (A) Short enough that you will actually do it.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (B) Short enough that the person at the end, doing it, <em>will still be you</em> in the significant ways. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\">(C) Having enough emotional feedback that your motivation won't be capsized before the end. and</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\"><span style=\"font-style: normal;\"> (D) Such that others not only can, but likely will take up the project after you abandon it in case you miscalculated when you'd change, or a change occurred before expected time. </span></p>\n<p>&nbsp;</p>\n<p><strong id=\"4_A_Hypothesis_On_Why_We_Became_Long_Term_Oriented\">4 A Hypothesis On Why We Became Long-Term Oriented</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">For anyone who rejoiced the company of the writings of Derek Parfit, George Ainslie, or Nick Bostrom, there are a lot of very good reasons to become more long-term oriented. I am here to ask you about those reasons: <a href=\"/lw/wj/is_that_your_true_rejection/\">Is that you true acceptance?</a> </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">It is not for </span><em>me</em><span style=\"font-style: normal;\">. I became longer term oriented because of different reasons. Two obvious ones are genetics expressing in me the kind of person that waits a year for the extra marshmallow while fantasizing about marshmallow worlds and rocking horse pies, and secondly wanting to live thousands of years. But the one I'd like to suggest that might be relevant to some here is that I was very bad at making people who were sad or hurt happy. I was not, as they say, empathic. It was a piece of cake bringing folks from neutral state to joy and bliss. But if someone got angry or sad, specially sad with something I did, I would be absolutely powerless about it. This is only one way of not being </span><em>good with people, a people's person </em><span style=\"font-style: normal;\">etc... So my emotional system, like the tale's Big Bad Wolf blew, and blew, and blew, until my utilons were comfortably sitting aside in the Far Future, where none of them could look back at my face, cry, and point to me as the tears cause. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">Paradoxically, though <a href=\"http://konyv.uw.hu/dennett-the-intentional-stance.htm\">understandably</a>, I have since been thankful for that lack of empathy towards those near. In fact, I have claimed, where I forget, that it is the moral responsibility of those with less natural empathy of the </span><em>giving to beggars</em><span style=\"font-style: normal;\"> kind to care about the far future, since so few are within this tiny psychological mindspace of being able to care abstractly while not caring that much visibly/emotionally. We are such a minority that foreign aid seems to be the thing that is more disproportional in public policy between countries (Savulescu, J - Genetically Enhance Humanity of Face Extinction 2009 <a href=\"/vimeo.com/7515623\">video</a>). Like the whole minority of billionaires ought to be more like Bill Gates, Peter Thiel and Jaan Tallinn, the minority of underempathic folk ought to be more like an economist doing quantitative analysis to save or help in quantitative ways. Let us look at our examples again: </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<blockquote>\n<p>\u201cMy goal is to have a one trillion people world with maximal utility density where everyone lives forever\u201d, \"I see most of my expected utility after the singularity, thus I spend my willpower entirely in increasing the likelihood of a positive singularity, and care little about my current pre-singularity emotions\", \u201cI want to reduce X-risk in any way I can, that's all\u201d , \u201cMy sole goal in life is to live an indefinite life-span\u201d.</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">So maybe their true (or original) acceptance of Longterm, like mine, was something like Genes + Death sucks + I'd rather interact with people of the future whose bots in my mind smile, than those actually meaty folk around me, with all their specific problems, complicated families and boring Christian relationship problems. This is my hypothesis. Even if true, notice it does not imply that longterm isn't rational, after all Parfit, Bostrom and Ainslie are still standing, even after careful scrutiny.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\"><br></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><strong id=\"5_Adapting_Bayesian_Reasoning_to_Get_More_Utilons\">5 Adapting Bayesian Reasoning to Get More Utilons</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">Just like many within this community praise Bayesian reasoning but don't explicitly keep track of belief distributions (as far as I recall only Steve Rayhawk and Anna Salamon, of all I met, kept a few beliefs numerically) few or none probably would need the math specifics to calibrate and slide their plans towards 'higher likelihood of achievement' given their change in goals probability over time. In other words, few need to do actual math to just account intuitively - though not accurately - for changing their plans in such way that now the revised plans are more likely to work than they were before.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">It doesn't amount to much more than simple multiplication and comparison. If you knew for a period of time how likely you are at each point to become someone who does not have goal X anymore, you should strive for X to be you-independent by the time that this transformation is likely to happen. But how likely must the gruesome transformation be at the point in time in which you expect the plan to be over? 10% would probably make you a blogger, 30% a book writer 50% a Toyota-like company creator and 90% would probably make you useless, since 90% of times no legacy would be left from your goals. It would be the rationalist equivalent of having ADHD. And if you also have <em>actual </em>ADHD, then the probabilities would multiply into a chaotic constant shift of attention in which your goals are not realized 90% of time even if they last actually very short times, that is probably why the World looks insane.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">Then again, how likely?&nbsp; My suggestion would be to use a piece of knowledge that comes from the derailed subset of positive psychology, Human Resources. There is this discovered ratio called The Losada Ratio, aka The Losada Line.&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">2.9013</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">The Schelling Point for talking about it to increase its memetic spreading is 3, so I'll use 3 here. But what is it? From Wikipedia:</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">The <strong>Losada Line</strong>, also known as the \"Losada ratio,\" was described by psychologist <a title=\"Marcial Losada\" href=\"http://en.wikipedia.org/wiki/Marcial_Losada\">Marcial Losada</a> while researching the differences in ratios of <strong><em>positivity and negativity</em></strong> between high and low performance teams.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"http://en.wikipedia.org/wiki/Losada_line#cite_note-1\"><span>[</span>1<span>]</span></a></sup><sup id=\"cite_ref-2\" class=\"reference\"><a href=\"http://en.wikipedia.org/wiki/Losada_line#cite_note-2\"><span>[</span>2<span>]</span></a></sup></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">The Losada Line represents a <a title=\"Positivity/negativity ratio\" href=\"http://en.wikipedia.org/wiki/Positivity/negativity_ratio\">positivity/negativity ratio</a> of roughly 2.9, and it marks the lower boundary of the <a title=\"Losada Zone\" href=\"http://en.wikipedia.org/wiki/Losada_Zone\">Losada Zone</a> (the upper bound is around 11.6). It was corroborated by Barbara Fredrickson, a psychologist at the University of North Carolina, Chapel Hill, in individuals, and by Waugh and Fredrickson in relationships.<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"http://en.wikipedia.org/wiki/Losada_line#cite_note-3\"><span>[</span>3<span>]</span></a></sup> They found that the Losada Line separates people who are able to reach a complex understanding of others from those who do not. People who \"flourish\" are above the Losada Line, and those who \"languish\" are below it.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"http://en.wikipedia.org/wiki/Losada_line#cite_note-4\"><span>[</span>4<span>]</span></a></sup><sup id=\"cite_ref-5\" class=\"reference\"><a href=\"http://en.wikipedia.org/wiki/Losada_line#cite_note-5\"><span>[</span>5<span>]</span></a></sup> The Losada Line bifurcates the type of dynamics that are possible for an interactive human system. Below it, we find limiting dynamics represented by fixed-point attractors; at or above it, we find innovative dynamics represented by complex order attractors (<a title=\"Complexor\" href=\"http://en.wikipedia.org/wiki/Complexor\">complexor</a>).</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">3:1 Compliments per complaint. Three requests per order. Three \"That's awesome!\" per \"Sorry, I didn't like that\"</p>\n<p>&nbsp;</p>\n<p>Getting back to our need for a Bayesian slide/shift in which we increase how likely our goals are to be achieved, wouldn't it be great if we set to achieve above the Losada line, thus keeping ourselves motivated by<em> having reality complimenting us within the Losada zone?</em></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">3:1 is the ratio I suggest of expected successes for Longterm folk in the process of dividing their Time Insensitive supergoals - which, granted, may be as unlikely as Kurzweil actually achieving longevity escape velocity - into smaller instrumental goals.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">If you agree with most of what has been said so far, and you'd like to be rewarded by your boss, <a href=\"/lw/up/shut_up_and_do_the_impossible/\">Mr Reality</a>, in the proportion of those who thrive, while doing stuff more useful than playing videogames previously designed within the Losada Zone, I suggest you try and do instrumental stuff with a 75% expected likelihood of success.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">Once well calibrated, you'll succeed three out of four times in your endeavours emotionally, which will keep you away from learned helplessness while still excited about the Longterm, Lifelong, Far Futuristic ideals with lower likelihoods. I would hope this could complement the whole anti-Akrasia posts on Lesswrong.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">For those who would <em>really </em>like to think about the math involved, take a look at <a href=\"/lw/3d0/an_intuitive_explanation_of_eliezer_yudkowskys/\">the</a> <a href=\"http://yudkowsky.net/rational/bayes\">three</a> <a href=\"/lw/2b0/bayes_theorem_illustrated_my_way/\">introductions</a> to bayesianisms available in Lesswrong and complement them with the idea of Bayesian shift related to the Doomsday argument which can be found in wikipedia or Bostrom's \"<a href=\"http://libgen.info/view.php?id=457319\">Anthropic Bias</a>\" book.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><strong id=\"6_Time_You_Can_Afford_to_Wait__Not_to_Waste\">6 Time You Can Afford to Wait, Not to Waste</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">Many of us are still younger than 25. Unless you started trying to achieve goals particularly young, if you are 18 you are likely undergoing a twister of events and hormones, and whatever you guess that will end up being your average time-span of motivation without internal or external interruptions actually won't be more than a guess. By 25 you are familiar with yourself, and probably able get your projects into frequency. But what to do before that?&nbsp; One suggestion that comes to mind is to create plans that actually require quite a long time.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">For the same reasons that those whose natural empathy is a little less than normal bear a moral responsibility of taking care of those a little distant, those who yet do not know if they are able to set out to do the impossible, with accuracy and consistency over long periods should probably try doing that. It is better to have false positives than false negatives in this case. Not only you'd never know how long you'd last if you set for short term, but also the very few who are able to go long term would never be found.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">So if you are young (in your dialect's meaning of 'young') <a href=\"/lw/up/shut_up_and_do_the_impossible/\">shut up, do the impossible</a>, <a href=\"http://lukeprog.com/SaveTheWorld.html\">save the world</a>. Isn't that the point of the Thiel Fellowship for Twenty Under Twenty anyway?</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><strong id=\"7_Reference_Classes_that_May_Be_Postponing_Utilons_Into_Undetectable_Distances\">7 Reference Classes that May Be <em>Postponing Utilons Into Undetectable Distances</em></strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">Of course Lesswrongers are prone to <em>postponing utilons into undetectable distances</em>. But which subsets? I regard the highest risk groups to be:</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><em>Cryocrastinators</em> - For one they want to live forever, on the other hand, their plan of doing something about it never succeeded, this looks like undercalibration.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><em>The subset of Effective Altruists who care mostly about future people/minds they'll never meet</em> - I find myself in this group.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><em>The subset of&nbsp; Singularitarians whose emotions are projected to SL4 and afterwards</em> - whom are doing something akin to the ascetic folk who talk about life as if it were set in the future, after death, making them less able to deal with their daily meaty human problems.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">Sure there will be a large minority of each group which doesn't fall prey to </span><span style=\"font-style: normal;\"><em>postponing utilons into undetectable distances</em>, and sure, <a href=\"http://vserver1.cscs.lsa.umich.edu/~crshalizi/Russell/Hearst_Essays/How_to_Become_a_Man_of_Genius.html\">you belong</a> to that minority, but stressing the point out makes it salient enough that if you ever find yourself trying to rationalize about this more than you should, you'll think twice.&nbsp; </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">It has been said to me that if that is what makes an Effective Altruist, so be it! And thank goodness we have them. To which I say: <strong>Yes! </strong></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\"><br></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">Every characteristic has a causal origin, and frequently a story can be told which <a href=\"http://www.youtube.com/watch?v=rseKPsiNPmQ\">explains the emergence</a> of that characteristic. Given that we are all biases, no wonder some of those stories will have biases in prominent roles. This does not invalidate the ethics of those whose lives were shaped by those stories. In fact, if a transmissible disease of mind were the sole cause of awesome people in this world, scientists would be trying to engineer mosquitoes to become more, not less infectious. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">Does that mean I spent this entire post arguing that something like a bias exist which we'd better not overcome? Or worse yet, is all of this post an attempt to justify caring about those far even though it would be emotionally much harder - and thus feel more real - to care about those near? Maybe, you be the judge. </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">There are also several distinct other reference classes that are smaller, though would deserve mention. The Big Worlds Tegmark or Vilenkin Fans who think about the whole superintelligence fighters, economics of acausal trade, Permutation City and so on...&nbsp; </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">A good heuristic to check that is to see if the person has a <em>low tolerance to frustration</em> plus a lot of undetectable utilons in her worldview. Interestingly the more undetectable utilons you have, the more it looks like you are just extrapolating the universally accepted as ethical idea of expanding your circle of altruism, one that has been called, of all things, your \"circle of empathy\".&nbsp;</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\"><strong>8 The Road Ahead</strong></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\">In this community, and that is perhaps it's greatest advantage, there can never be enough stress on deciding well which direction to take. Or which tool to use. If you are about to travel, it is argued, the most important thing is to figure out to where are you going and steer your future into that direction.&nbsp; What I suggest is as important as the direction to which one decided to travel is that you figure out you tank's size and how often do you need gas stations on your way there. It is better to get to a worse place than to be stymied by an inner force that, if you can't quite control, you can at least significantly reduce its likelihood of failure and amount of damage.&nbsp;</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\"><span style=\"font-style: normal;\"><br></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal;\">&nbsp;</p>", "sections": [{"title": "Here is the original unchanged post with sections 1-3 and the new sections 4-8. If you read the first post, go straight to section 4.", "anchor": "Here_is_the_original_unchanged_post_with_sections_1_3_and_the_new_sections_4_8__If_you_read_the_first_post__go_straight_to_section_4_", "level": 1}, {"title": "1What Significantly Changes Life's Direction", "anchor": "1What_Significantly_Changes_Life_s_Direction", "level": 1}, {"title": "1.1 Predominantly external changes", "anchor": "1_1_Predominantly_external_changes", "level": 1}, {"title": "1.2 Predominantly Internal Changes", "anchor": "1_2_Predominantly_Internal_Changes", "level": 1}, {"title": "2 Long Term Goals and Even Longer Term Goals", "anchor": "2_Long_Term_Goals_and_Even_Longer_Term_Goals", "level": 1}, {"title": "3 Proportionality Between Goal Achievement Expected Time and Plan Execution Time", "anchor": "3_Proportionality_Between_Goal_Achievement_Expected_Time_and_Plan_Execution_Time", "level": 1}, {"title": "4 A Hypothesis On Why We Became Long-Term Oriented", "anchor": "4_A_Hypothesis_On_Why_We_Became_Long_Term_Oriented", "level": 1}, {"title": "5 Adapting Bayesian Reasoning to Get More Utilons", "anchor": "5_Adapting_Bayesian_Reasoning_to_Get_More_Utilons", "level": 1}, {"title": "6 Time You Can Afford to Wait, Not to Waste", "anchor": "6_Time_You_Can_Afford_to_Wait__Not_to_Waste", "level": 1}, {"title": "7 Reference Classes that May Be Postponing Utilons Into Undetectable Distances", "anchor": "7_Reference_Classes_that_May_Be_Postponing_Utilons_Into_Undetectable_Distances", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fKS54Zd4SaBrjznzF", "9bTNcSpNBdPpyocMK", "oFY8Ms83ehXgRMh32", "BFamedwSgRdGGKXQQ", "G6npMHwgRGSQDKavX", "JzLH8BDoCk86jaALe", "DGfPyJbynXZF9NGv4", "vbcjYg6h3XzuqaaN8", "WBw8dDkAWohFjWQSk", "uEnFDx7nQacQdf6Tg", "RWo4LwFzpHNQCTcYt", "PpTN7GP2FsPyHfKrs", "XPErvb8m9FapXCjhA", "Ndtb22KYBxpBsagpj", "TGux5Fhcd7GmTfNGC", "nCvvhFBaayaXyuBiD", "69gof6oyZuNiSt7XC", "CMt3ijXYuCynhPWXa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-22T05:38:39.232Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] ...And Say No More Of It", "slug": "seq-rerun-and-say-no-more-of-it", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5S3hdzXSeiW7tBzFw/seq-rerun-and-say-no-more-of-it", "pageUrlRelative": "/posts/5S3hdzXSeiW7tBzFw/seq-rerun-and-say-no-more-of-it", "linkUrl": "https://www.lesswrong.com/posts/5S3hdzXSeiW7tBzFw/seq-rerun-and-say-no-more-of-it", "postedAtFormatted": "Friday, February 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20...And%20Say%20No%20More%20Of%20It&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20...And%20Say%20No%20More%20Of%20It%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5S3hdzXSeiW7tBzFw%2Fseq-rerun-and-say-no-more-of-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20...And%20Say%20No%20More%20Of%20It%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5S3hdzXSeiW7tBzFw%2Fseq-rerun-and-say-no-more-of-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5S3hdzXSeiW7tBzFw%2Fseq-rerun-and-say-no-more-of-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Today's post, <a href=\"/lw/ye/and_say_no_more_of_it/\">...And Say No More Of It</a> was originally published on 09 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#...And_Say_No_More_Of_It\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>In the previous couple of months, Overcoming Bias had focused too much on singularity related issues and not enough on rationality. A two month moratorium on the topic of the singularity/intelligence explosion is imposed.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gqs/seq_rerun_the_thing_that_i_protect/\">The Thing That I Protect</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5S3hdzXSeiW7tBzFw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.119252513782797e-06, "legacy": true, "legacyId": "21723", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SqNvmwDxRibLXjMZN", "P6Dw7THmsqwaoCcnT", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-22T07:50:19.058Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Expanding rationality", "slug": "meetup-moscow-expanding-rationality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rYnL9oYLHemGTfbCv/meetup-moscow-expanding-rationality", "pageUrlRelative": "/posts/rYnL9oYLHemGTfbCv/meetup-moscow-expanding-rationality", "linkUrl": "https://www.lesswrong.com/posts/rYnL9oYLHemGTfbCv/meetup-moscow-expanding-rationality", "postedAtFormatted": "Friday, February 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Expanding%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Expanding%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrYnL9oYLHemGTfbCv%2Fmeetup-moscow-expanding-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Expanding%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrYnL9oYLHemGTfbCv%2Fmeetup-moscow-expanding-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrYnL9oYLHemGTfbCv%2Fmeetup-moscow-expanding-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jo'>Moscow, Expanding rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 March 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li>Short presentations. Two or three people will tell us about something interesting.</li>\n<li>Rationality in examples from motion pictures and books.</li>\n<li>Problem solving, we will apply our rationality skills to real life.</li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jo'>Moscow, Expanding rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rYnL9oYLHemGTfbCv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.1193369381047777e-06, "legacy": true, "legacyId": "21726", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Expanding_rationality\">Discussion article for the meetup : <a href=\"/meetups/jo\">Moscow, Expanding rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 March 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 15:45 MSK with \u201cLW\u201d sign. And we will also check the entrance at 16:00 and 16:15, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li>Short presentations. Two or three people will tell us about something interesting.</li>\n<li>Rationality in examples from motion pictures and books.</li>\n<li>Problem solving, we will apply our rationality skills to real life.</li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html\">here, in Russian</a>, now with photos.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Expanding_rationality1\">Discussion article for the meetup : <a href=\"/meetups/jo\">Moscow, Expanding rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Expanding rationality", "anchor": "Discussion_article_for_the_meetup___Moscow__Expanding_rationality", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Expanding rationality", "anchor": "Discussion_article_for_the_meetup___Moscow__Expanding_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-22T16:33:22.576Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Bielefeld, Brussels, Buffalo, Cambridge MA, Cambridge UK, Cincinnati, Frankfurt, Montreal, Moscow, Purdue, Salt Lake City, Vancouver, Washington DC", "slug": "weekly-lw-meetups-austin-bielefeld-brussels-buffalo", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WmMTshDQpoAAjYMTT/weekly-lw-meetups-austin-bielefeld-brussels-buffalo", "pageUrlRelative": "/posts/WmMTshDQpoAAjYMTT/weekly-lw-meetups-austin-bielefeld-brussels-buffalo", "linkUrl": "https://www.lesswrong.com/posts/WmMTshDQpoAAjYMTT/weekly-lw-meetups-austin-bielefeld-brussels-buffalo", "postedAtFormatted": "Friday, February 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Bielefeld%2C%20Brussels%2C%20Buffalo%2C%20Cambridge%20MA%2C%20Cambridge%20UK%2C%20Cincinnati%2C%20Frankfurt%2C%20Montreal%2C%20Moscow%2C%20Purdue%2C%20Salt%20Lake%20City%2C%20Vancouver%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Bielefeld%2C%20Brussels%2C%20Buffalo%2C%20Cambridge%20MA%2C%20Cambridge%20UK%2C%20Cincinnati%2C%20Frankfurt%2C%20Montreal%2C%20Moscow%2C%20Purdue%2C%20Salt%20Lake%20City%2C%20Vancouver%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmMTshDQpoAAjYMTT%2Fweekly-lw-meetups-austin-bielefeld-brussels-buffalo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Bielefeld%2C%20Brussels%2C%20Buffalo%2C%20Cambridge%20MA%2C%20Cambridge%20UK%2C%20Cincinnati%2C%20Frankfurt%2C%20Montreal%2C%20Moscow%2C%20Purdue%2C%20Salt%20Lake%20City%2C%20Vancouver%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmMTshDQpoAAjYMTT%2Fweekly-lw-meetups-austin-bielefeld-brussels-buffalo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmMTshDQpoAAjYMTT%2Fweekly-lw-meetups-austin-bielefeld-brussels-buffalo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 572, "htmlBody": "<p><strong>This summary was posted to LW main on February 15th. The following week's summary is <a href=\"/lw/grj/weekly_lw_meetups_berlin_cambridge_uk_durham/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/ht\">Brussels meetup:&nbsp;<span class=\"date\">16 February 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/ja\">Cincinnati February: Predictions:&nbsp;<span class=\"date\">16 February 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/j9\">Vancouver Boredom vs Scope Insensitivity, and life-debugging:&nbsp;<span class=\"date\">16 February 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/jc\">Purdue Meetup:&nbsp;<span class=\"date\">16 February 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/j2\">Washington DC fun and games meetup:&nbsp;<span class=\"date\">17 February 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/iv\">Moscow: Rationality in our daily life:&nbsp;<span class=\"date\">17 February 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/jd\">Buffalo Meetup:&nbsp;<span class=\"date\">17 February 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/j6\">Montreal LessWrong Meetup - The Science of Winning at Life:&nbsp;<span class=\"date\">18 February 2013 06:30PM</span></a></li>\n<li><a href=\"/meetups/ix\">Bielefeld Meetup, February 20th:&nbsp;<span class=\"date\">20 February 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/iy\">First meetup in Frankfurt (Main) :&nbsp;<span class=\"date\">22 February 2013 06:30PM</span></a></li>\n<li><a href=\"/meetups/j7\">London Meetup, 24th Feb:&nbsp;<span class=\"date\">24 February 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ir\">Tokyo Meetup:&nbsp;<span class=\"date\">01 March 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/j8\">Vienna Meetup 9th March:&nbsp;<span class=\"date\">09 March 2013 04:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">16 February 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/ij\">Love and Sex in Salt Lake City:&nbsp;<span class=\"date\">16 February 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/jb\">Cambridge, UK LW Meetup [Reading Group, HAEFB-02]:&nbsp;<span class=\"date\">17 February 2013 11:00AM</span></a></li>\n<li><a href=\"/meetups/j5\">Cambridge, MA third-Sunday meetup:&nbsp;<span class=\"date\">18 February 2013 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WmMTshDQpoAAjYMTT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.1196724407686841e-06, "legacy": true, "legacyId": "21611", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7wK8iMSTWmXNbTzCv", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-22T20:44:08.870Z", "modifiedAt": null, "url": null, "title": "An attempt to dissolve subjective expectation and personal identity", "slug": "an-attempt-to-dissolve-subjective-expectation-and-personal", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:01.725Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7XWGJGmWXNmTd2oAP/an-attempt-to-dissolve-subjective-expectation-and-personal", "pageUrlRelative": "/posts/7XWGJGmWXNmTd2oAP/an-attempt-to-dissolve-subjective-expectation-and-personal", "linkUrl": "https://www.lesswrong.com/posts/7XWGJGmWXNmTd2oAP/an-attempt-to-dissolve-subjective-expectation-and-personal", "postedAtFormatted": "Friday, February 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20attempt%20to%20dissolve%20subjective%20expectation%20and%20personal%20identity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20attempt%20to%20dissolve%20subjective%20expectation%20and%20personal%20identity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XWGJGmWXNmTd2oAP%2Fan-attempt-to-dissolve-subjective-expectation-and-personal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20attempt%20to%20dissolve%20subjective%20expectation%20and%20personal%20identity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XWGJGmWXNmTd2oAP%2Fan-attempt-to-dissolve-subjective-expectation-and-personal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XWGJGmWXNmTd2oAP%2Fan-attempt-to-dissolve-subjective-expectation-and-personal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3279, "htmlBody": "<p style=\"padding-left: 30px;\"><em>I attempt to figure out a way to dissolve the concepts of 'personal identity' and 'subjective expectation' down to the level of cognitive algorithms, in a way that would let one bite the bullets of the anthropic trilemma. I proceed by considering four clues which seem important: 1) the evolutionary function of personal identity, 2) a sense of personal identity being really sticky, 3) an undefined personal identity causing undefined behavior in our decision-making machinery, and 4) our decision-making machinery being more strongly grounded in our subjective expectation than in abstract models. Taken together, these seem to suggest a solution.</em></p>\n<p>I ended up re-reading some of the debates about the <a href=\"/lw/19d/the_anthropic_trilemma/\">anthropic trilemma</a>, and it struck me odd that, aside for a few references to personal identity being an evolutionary adaptation, there seemed to be no attempt to <a href=\"/lw/of/dissolving_the_question/\">reduce the concept to the level of cognitive algorithms</a>. Several commenters thought that there wasn't really any problem, and <a href=\"/lw/19d/the_anthropic_trilemma/\">Eliezer asked them</a> to explain why the claim of there not being any problem regardless violated the intuitive rules of subjective expectation. That seemed like a very strong indication that the question needs to be dissolved, but almost none of the attempted answers seemed to do that, instead trying to solve the question via decision theory without ever addressing the core issue of subjective expectation. rwallace's <a href=\"/lw/208/the_iless_eye/\">I-less Eye</a> argued - I believe correctly - that subjective anticipation isn't ontologically fundamental, but still didn't address the question of why it feels like it is.</p>\n<p>Here's a sketch of a dissolvement. It seems relatively convincing to me, but I'm not sure how others will take it, so let's give it a shot. Even if others find it incomplete, it should at least help provide clues that point towards a better dissolvement.</p>\n<p><strong>Clue 1: The evolutionary function of personal identity.</strong></p>\n<p>Let's first consider the <em>evolutionary</em> function. Why have we <em>evolved</em> a sense of personal identity?</p>\n<p><span style=\"font-style: normal;\">The first answer that always comes to everyone's mind is that our brains have evolved for the task of spreading our genes, which involves surviving at least for as long as it takes to reproduce. Simpler neural functions, like maintaining a pulse and having reflexes, obviously do fine without a concept of personal identity. But if we wish to use abstract, explicit reasoning to advance our own interests, we need some definition for exactly </span><em>whose</em><span style=\"font-style: normal;\"> interests it is that our reasoning process is supposed to be optimizing. So </span><span style=\"font-style: normal;\">evolution comes up with a fuzzy sense of personal identity, so that optimizing the interests of this identity also happens to optimize the interests of the organism in question.</span></p>\n<p><span style=\"font-style: normal;\">That's simple enough, and this point was already made in the discussions so far. But that doesn't feel like it would resolve our confusion yet, so we need to look at the way that personal identity is actually implemented in our brains. What is the </span><em>cognitive </em><span style=\"font-style: normal;\">function of personal identity?<strong></strong></span></p>\n<p><span style=\"font-style: normal;\"><strong>Clue 2: A sense of personal identity is really sticky.</strong></span></p>\n<p><span style=\"font-style: normal;\">Even people who disbelieve in personal identity don't really seem to <a href=\"http://wiki.lesswrong.com/wiki/Alief\">dis</a><a href=\"http://wiki.lesswrong.com/wiki/Alief\">alieve</a> it: for the most part, they're just as likely to be nervous about their future as anyone else. </span><span style=\"font-style: normal;\">Even advanced meditators who go out trying to dissolve their personal identity seem to still retain some form of it. <a href=\"/user/PyryP/overview/\">PyryP</a> claims that at one point, he reached a stage in meditation where the experience of &ldquo;somebody who experiences things&rdquo; shattered and he could turn it entirely off, or attach it to something entirely different, such as a nearby flower vase. But then the experience of having a self began to come back: it was as if the brain was hardwired to maintain one, and to reconstruct it whenever it was broken. </span><span style=\"font-style: normal;\">I asked him to comment on that for this post, and he provided the following</span><span style=\"font-style: normal;\">:<a id=\"more\"></a></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-style: normal;\">It seems like my consciousness is rebuilding a new ego on top of everything, one which is not directly based on feeling as one with a physical body and memories, but which still feels like it is the thing that experiences whatever happens.</span></p>\n<p style=\"padding-left: 30px;\">To elaborate, many things in life affect the survival and success of an organism. Even though the organism would never experience itself as being separate from the surrounding universe, in ordinary life it's still useful to have concepts relating to the property and values of the organism. But even this pragmatic approach is enough for the ego-construction machinery, and the same old bad habits start to stick on it, even though the organism doesn't have any experience of itself that would be compatible with having a persistent 'soul'.</p>\n<p style=\"padding-left: 30px;\">Habits probably don't stick as strongly as they did before seeing the self as an illusion, but I'm still the same old asshole in certain respects. That might have something to do with the fact that I have no particular need to be particularly holy and clean. The ego-construction process is starting to be sufficiently strong that I've even began doubting how big of a change this has been. I don't have a clear recollection of whether I feel considerably different now than before, anymore.</p>\n<p style=\"padding-left: 30px;\">I still think that the change I experienced was a positive one and I feel like I can enjoy life in a less clinging way. I don't know if I've gained any special talents regarding my outlook of life that couldn't be maintained with simple mindfullness. I however do experience this certain transpersonal flow that makes everything lighter and easier. Something that makes the basic mindfullness effortless. I may also be making this shit up. The sunk cost of having spent lots of time in meditation makes people say funny things about their achievements. There is this insisting feeling that something is different, dammit.</p>\n<p style=\"padding-left: 30px;\">Anyway meditation is great fun and you can get all kinds of extremely pleasurable experiences with it. Don't read that if you're having trouble with desire getting in the way of your meditation. Ooops. Should've putten these the other way around. Yeah. I'm a dick. Deal with it. :P</p>\n<p><span style=\"font-style: normal;\">Also, I know him in real life, and he doesn't really come off as behaving all that differently from anybody else.</span></p>\n<p><span style=\"font-style: normal;\">Then there's also the fact that we seem to be almost incapable of thinking in a way that wouldn't still implicitly assume some concept of personal identity behind it. For example, I've said things like &ldquo;it's convenient for me to disbelieve in personal identity at times, because then the person now isn't the same one as the person tomorrow, so I don't need to feel nervous about what happens to the person tomorrow&rdquo;. But here I'm not actually disbelieving in personal identity &ndash; after all, I clearly believe that there exists some &ldquo;is-a&rdquo; type relation that I can use to compare myself today and myself tomorrow, and which returns a negative. If I truly disbelieved in personal identity, I wouldn't even have such a relation: asking &ldquo;is the person today the same as the person tomorrow&rdquo; would just return </span><em>undefined</em><span style=\"font-style: normal;\">.</span><strong></strong></p>\n<p><strong><span style=\"font-style: normal;\">Clue 3: Our decision-making machinery exhibits undefined behavior in the presence of an undefined personal identity.</span></strong></p>\n<p><span style=\"font-style: normal;\">This seems like an important thing to notice. What would it imply if I really didn't have </span><em>any</em><span style=\"font-style: normal;\"> concept of personal identity or subjective expectation? If I asked myself whether I'd be the same person tomorrow as I was today, got an </span><em>undefined</em><span style=\"font-style: normal;\"> back, and tried to give that as input to the systems actually driving my behavior... what would they say I should do?</span></p>\n<p><span style=\"font-style: normal;\">Well, I guess it would depend on what those systems valued. If I was a paperclipper running on a pure utility-maximizing architecture, I guess they might say &ldquo;who cares about personal identity anyway? Let's make paperclips!&rdquo;.</span></p>\n<p><span style=\"font-style: normal;\">But in fact, I'm a human, which means that a large part of the algorithms that actually drive my behavior are defined by reference to a concept of personal identity. So I'd ask them &ldquo;I want to play computer games but in reality I should really study instead, which one do I actually do?&rdquo;, and they'd reply &ldquo;well let's see, to answer that we'd need to consider that which you expect to experience in the short term versus that which you expect to experience in the long term... AIEEEEEEE NULL POINTER EXCEPTION&rdquo; and then the whole system would crash and need to be rebooted.</span></p>\n<p><span style=\"font-style: normal;\">Except that it wouldn't, because it has been historically rather hard to reboot human brains, so they've evolved to handle problematic contingencies in other ways. So what probably would happen is that the answer would be &ldquo;umm, we don't know, give us a while to work that out&rdquo; and </span><span style=\"font-style: normal;\">then some other system that didn't need a sense of identity to operate would take over. We'd default to some old habit, perhaps. </span>In the meanwhile, the brain would be<span style=\"font-style: normal;\"> regenerat</span><span style=\"font-style: normal;\">ing</span><span style=\"font-style: normal;\"> a concept of personal identity in order to answer the </span><span style=\"font-style: normal;\">orignal </span><span style=\"font-style: normal;\">question and things would go back to normal. </span><span style=\"font-style: normal;\">And as far as I can tell, that's actually roughly</span><span style=\"font-style: normal;\"> what seems to happen.</span><a href=\"/lw/19d/the_anthropic_trilemma/\"></a></p>\n<p><a href=\"/lw/19d/the_anthropic_trilemma/\"><span style=\"font-style: normal;\">Eliezer asked</span></a><span style=\"font-style: normal;\">:</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-style: normal;\">It seems to me that there's some level on which, even if I say very firmly, \"I now resolve to care only about future versions of myself who win the lottery! Only those people are defined as Eliezer Yudkowskys!\", and plan only for futures where I win the lottery, then, come the next day, I wake up, look at the losing numbers, and say, \"Damnit! What went wrong? I thought personal continuity was strictly subjective, and I could redefine it however I wanted!\"</span></p>\n<p><span style=\"font-style: normal;\">One possible answer could be that even if </span><span style=\"font-style: normal;\">Eliezer</span><span style=\"font-style: normal;\"> did succeed in reprogramming his mind to think in such a weird, unnatural way, that would leave the losing copies with an undefined sense of self. After seeing that they lost, they wouldn't just think &ldquo;oh, our goal system has undefined terms now, and we're not supposed to care about anything that happens to us from this point on, so we'll just go ahead and crash&rdquo;. Instead, they'd think &ldquo;oh, our goal system looks broken, what's the easiest way of fixing that? Let's go back to the last version that we know to have worked&rdquo;. And because a lot of that would be unconscious, the thoughts that would flash through the conscious mind might just be something like &ldquo;damnit, that didn't work&rdquo; - or perhaps, &ldquo;oh, I'm not supposed to care about myself anymore, so now what? Umm, </span><span style=\"font-style: normal;\">actually, </span><a href=\"/lw/rr/the_moral_void/\"><span style=\"font-style: normal;\">even without morality I still care about things</span></a><span style=\"font-style: normal;\">.</span><span style=\"font-style: normal;\">&rdquo;</span></p>\n<p><span style=\"font-style: normal;\">But that still doesn't seem to answer all of our questions. I mentioned that actually ever alieving this in the first place, even before the copying, would be a &ldquo;weird, unnatural thing&rdquo;. I expect that it would be very hard for Eliezer to declare that he was only going to care about the copies that won the lottery, and then </span><em>really </em><span style=\"font-style: normal;\">only care about them. In fact, it might very well be impossible. </span><span style=\"font-style: normal;\">W</span><span style=\"font-style: normal;\">hy is that?</span><strong></strong></p>\n<p><strong><span style=\"font-style: normal;\">Clue </span><span style=\"font-style: normal;\">4</span><span style=\"font-style: normal;\">: Our decision-making machinery seems grounded in subjective expectation, not abstract models of the world.</span></strong></p>\n<p><span style=\"font-style: normal;\">Looking at things from a purely logical point of view, there shouldn't be anything particularly difficult about redefining our wants in such a way. Maybe there's a function somewhere inside us that says &ldquo;I care about my own future&rdquo;, which has a pointer to whatever function it is that computes &ldquo;me&rdquo;. In principle, if we had full understanding of our minds and read-write access to them, we could just change the pointer to reference the part of our world-model which was about the copies which had witnessed winning the lottery. That system might crash at the point when it found out that it </span><em>wasn't</em><span style=\"font-style: normal;\"> actually one of those copies, but </span><span style=\"font-style: normal;\">until that</span><span style=\"font-style: normal;\"> everything should go fine, in principle.</span></p>\n<p><span style=\"font-style: normal;\">Now we don't have full read-write access to our minds, but internalizing declarative knowledge can still cause some pretty big changes in our value systems. </span><span style=\"font-style: normal;\">The lack of access</span><em> </em><span style=\"font-style: normal;\">doesn't seem like the big problem here. The big problem is that whenever we try to mind-hack ourselves like that, our mind complains that it still doesn't </span><em>expect</em><span style=\"font-style: normal;\"> to only see winning the lottery. </span><span style=\"font-style: normal;\">It's as if our mind didn't run on the kind of an architecture that would allow us to make the kind of a change that I just described it: </span><span style=\"font-style: normal;\">even if we </span><em>did</em><span style=\"font-style: normal;\"> have full read-write access, making such a change would require a major rewrite, not just fiddling around with a couple of pointers.</span></p>\n<p><span style=\"font-style: normal;\">Why is subjective expectation so important? Why can't we just base our decisions on our abstract world-model? </span><span style=\"font-style: normal;\">Why does our mind insist that it's subjective expectation that counts, not the things that we value based on our abstract model?</span></p>\n<p><span style=\"font-style: normal;\">Let's look at the difference between &ldquo;subjective expectation&rdquo; and &ldquo;abstract world-model&rdquo; a bit more. In 2011, Orseau &amp; Ring <a href=\"http://www.agroparistech.fr/mmip/maths/laurent_orseau/papers/ring-orseau-AGI-2011-delusion.pdf\">published a paper</a> arguing that many kinds of reinforcement learning agents would, if given the opportunity, use a &ldquo;delusion box&rdquo; which allowed them to modify the observations they got from the environment. This way, they would always receive the kinds of signals that gave them the maximum reward. You could say, </span><span style=\"font-style: normal;\">in a sense,</span><span style=\"font-style: normal;\"> that those kinds of agents only care about their subjective expectation &ndash; as long as they experience what they want, they don't care about the rest of the world. And it's important for them that </span><em>they</em><span style=\"font-style: normal;\"> are the ones who get those experiences, because their utility function only cares about their own reward.</span></p>\n<p><span style=\"font-style: normal;\">In response,</span><span style=\"font-style: normal;\"> Bill Hibbard published a paper where he suggested that the problem could be solved via building AIs to have &ldquo;<a href=\"http://arxiv.org/vc/arxiv/papers/1111/1111.3934v1.pdf\">model-based utility functions</a>&rdquo;, a concept which he defined via human behavior:</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-style: normal;\">Human agents often avoid self-delusion so human motivation may suggest a way of computing utilities so that agents do not choose the delusion box. We humans (and presumably other animals) compute utilities by constructing a model of our environment based on interactions, and then computing utilities based on that model. We learn to recognize objects that persist over time. We learn to recognize similarities between different objects and to divide them into classes. We learn to recognize actions of objects and interactions between objects. And we learn to recognize fixed and mutable properties of objects. We maintain a model of objects in the environment even when we are not directly observing them. We compute utility based on our internal mental model rather than directly from our observations. Our utility computation is based on specific objects that we recognize in the environment such as our own body, our mother, other family members, other friendly and unfriendly humans, animals, food, and so on. And we learn to correct for sources of delusion in our observations, such as optical illusions, impairments to our perception due to illness, and lies and errors by other humans.</span></p>\n<p><span style=\"font-style: normal;\">So instead of just caring about our subjective experience, we use our subjective experiences to construct a model of the world. </span><span style=\"font-style: normal;\">We don't want to delude ourselves, because we also care about the world around us, and our world model tells us that deluding ourselves wouldn't actually change the world.</span></p>\n<p><span style=\"font-style: normal;\">But as we have just seen, there are many situations in which we actually do care about subjective expectation and not just items in our abstract world-model. It even seems </span><em>impossible</em><span style=\"font-style: normal;\"> to hack our brains to only care about things which have been defined in the world-model, and to ignore subjective expectation. </span><span style=\"font-style: normal;\">I can't say &ldquo;well I'm going to feel low-status for the rest of my life if I just work from home, but that's just my mistaken subjective experience, in reality there are lots of people on The Internets who think I'm cool and consider me high-status&rdquo;. Which is true, but also kinda irrelevant if I don't also feel respected in real life.</span></p>\n<p><span style=\"font-style: normal;\">Which really just suggests that humans are somewhere halfway between an entity that only cares about its subjective experience, and which only cares about its world-model. <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">Luke has pointed out</a> that there are several competing valuation systems in the brain, some of which use abstract world models and some of which do not. But that isn't necessarily relevant, given that our subjective expectation of what's going to happen is itself a model.</span></p>\n<p><span style=\"font-style: normal;\">A better explanation might be that historically, accurately modeling our subjective expectation has been </span><em>really important</em><span style=\"font-style: normal;\">. Abstract world-models based on explicit logical reasoning tend to go really easily awry and lead us to all kinds of crazy conclusions, and it might only take a single mistaken assumption. If we made all of our decisions based on </span><em>that</em><span style=\"font-style: normal;\">, we'd probably end up dead. So our brain has been hardwired to <a href=\"http://wiki.lesswrong.com/wiki/Egan%27s_law\">add it all up to normality</a>. It's fine to juggle around all kinds of crazy theories while in <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">far mode</a>, but for evolution, what really matters in the end is whether you'll personally expect to experience living on until you have a good mate and lots of surviving children.</span></p>\n<p><span style=\"font-style: normal;\">So we come with brains where all the most powerful motivational systems that </span><em>really</em><span style=\"font-style: normal;\"> drive our behavior have been hardwired to take their inputs from the system that models our future experiences, and those systems require some concept of personal identity in order to define what &ldquo;subjective experience&rdquo; even means.</span><strong></strong></p>\n<p><strong><span style=\"font-style: normal;\">Summing it up</span></strong></p>\n<p><span style=\"font-style: normal;\">Thus, </span><span style=\"font-style: normal;\">these considerations would</span><span style=\"font-style: normal;\"> suggest that humans have at least two systems driving our behavior. </span><span style=\"font-style: normal;\">The &ldquo;subjective system&rdquo;</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">evolved from something like a basic reinforcement learning architecture, and it </span><span style=\"font-style: normal;\">models subjective expectation and this organism's immediate rewards, and </span><span style=\"font-style: normal;\">isn't</span><span style=\"font-style: normal;\"> too strongly swayed by abstract theories and claims. The &ldquo;</span><span style=\"font-style: normal;\">objective system&rdquo;</span><span style=\"font-style: normal;\"> is a lot more general and abstract, </span><span style=\"font-style: normal;\">and evolved to correct for deficiencies in the subjective system, </span><span style=\"font-style: normal;\">but doesn't influence behavior as strongly. </span><span style=\"font-style: normal;\">These two systems may or may not have a clear correspondence to <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">near/far of construal level theory</a>, or to the <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">three systems identified in neuroscience</a>.</span></p>\n<p><span style=\"font-style: normal;\">The &ldquo;subjective system&rdquo; requires a concept of personal identity in order to work, </span><span style=\"font-style: normal;\">and since being able to easily overrule that system and switch only to the &ldquo;objective system&rdquo; has &ndash; evolutionarily speaking - been a really bad idea, our brain will regenerate a sense of personal identity to guide behavior whenever that sense gets lost. If we really had </span><em>no</em><span style=\"font-style: normal;\"> sense of personal identity, the &ldquo;subjective system&rdquo;, which actually drives most of our behavior would be incapable of making decisions, as it makes its decisions by projecting the anticipated experience of the creature defined in our model of personal identity. &ldquo;Personal identity&rdquo; does not actually correspond to anything fundamental in the world, which is why some of the results of the anthropic trilemma actually feel weird to us, but it does still exist as a cognitive abstraction which our brains need in order to operate, and we can't actually </span><em>not</em><span style=\"font-style: normal;\"> believe in some kind of personal identity &ndash; at least, not for long.</span></p>\n<p><strong>ETA:</strong> <a href=\"/lw/grl/an_attempt_to_dissolve_subjective_expectation_and/8idp\">Giles commented</a>, and summarized my notion better than I did: \"<em>I can imagine that if you design an agent by starting off with a  reinforcement learner, and then bolting some model-based planning stuff  on the side, then the model will necessarily need to tag one of its  objects as \"self\". Otherwise the reinforcement part would have trouble  telling the model-based part what it's supposed to be optimizing for.</em>\"</p>\n<p><span style=\"font-style: normal;\">Another way of summarizing this: while we <em>could</em> in principle have a mental architecture that didn't have a personal identity, we actually evolved from animals which didn't have the capability for abstract reasoning but were rather running on something like a simple reinforcement learning architecture. Evolution cannot completely rewrite existing systems, so our abstract reasoning system got sort of hacked together on top of that earlier system, and that earlier system required some kind of a personal identity in order to work. And because the abstract reasoning system might end up reaching all kinds of bizarre and incorrect results pretty easily, we've generally evolved in a way that keeps that earlier system basically in charge most of the time, because it's less likely to do something stupid.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x6evH6MyPK3nxsoff": 2, "5f5c37ee1b5cdee568cfb2fa": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7XWGJGmWXNmTd2oAP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 53, "extendedScore": null, "score": 0.0005384403465089948, "legacy": true, "legacyId": "21729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"padding-left: 30px;\"><em>I attempt to figure out a way to dissolve the concepts of 'personal identity' and 'subjective expectation' down to the level of cognitive algorithms, in a way that would let one bite the bullets of the anthropic trilemma. I proceed by considering four clues which seem important: 1) the evolutionary function of personal identity, 2) a sense of personal identity being really sticky, 3) an undefined personal identity causing undefined behavior in our decision-making machinery, and 4) our decision-making machinery being more strongly grounded in our subjective expectation than in abstract models. Taken together, these seem to suggest a solution.</em></p>\n<p>I ended up re-reading some of the debates about the <a href=\"/lw/19d/the_anthropic_trilemma/\">anthropic trilemma</a>, and it struck me odd that, aside for a few references to personal identity being an evolutionary adaptation, there seemed to be no attempt to <a href=\"/lw/of/dissolving_the_question/\">reduce the concept to the level of cognitive algorithms</a>. Several commenters thought that there wasn't really any problem, and <a href=\"/lw/19d/the_anthropic_trilemma/\">Eliezer asked them</a> to explain why the claim of there not being any problem regardless violated the intuitive rules of subjective expectation. That seemed like a very strong indication that the question needs to be dissolved, but almost none of the attempted answers seemed to do that, instead trying to solve the question via decision theory without ever addressing the core issue of subjective expectation. rwallace's <a href=\"/lw/208/the_iless_eye/\">I-less Eye</a> argued - I believe correctly - that subjective anticipation isn't ontologically fundamental, but still didn't address the question of why it feels like it is.</p>\n<p>Here's a sketch of a dissolvement. It seems relatively convincing to me, but I'm not sure how others will take it, so let's give it a shot. Even if others find it incomplete, it should at least help provide clues that point towards a better dissolvement.</p>\n<p><strong id=\"Clue_1__The_evolutionary_function_of_personal_identity_\">Clue 1: The evolutionary function of personal identity.</strong></p>\n<p>Let's first consider the <em>evolutionary</em> function. Why have we <em>evolved</em> a sense of personal identity?</p>\n<p><span style=\"font-style: normal;\">The first answer that always comes to everyone's mind is that our brains have evolved for the task of spreading our genes, which involves surviving at least for as long as it takes to reproduce. Simpler neural functions, like maintaining a pulse and having reflexes, obviously do fine without a concept of personal identity. But if we wish to use abstract, explicit reasoning to advance our own interests, we need some definition for exactly </span><em>whose</em><span style=\"font-style: normal;\"> interests it is that our reasoning process is supposed to be optimizing. So </span><span style=\"font-style: normal;\">evolution comes up with a fuzzy sense of personal identity, so that optimizing the interests of this identity also happens to optimize the interests of the organism in question.</span></p>\n<p><span style=\"font-style: normal;\">That's simple enough, and this point was already made in the discussions so far. But that doesn't feel like it would resolve our confusion yet, so we need to look at the way that personal identity is actually implemented in our brains. What is the </span><em>cognitive </em><span style=\"font-style: normal;\">function of personal identity?<strong></strong></span></p>\n<p><span style=\"font-style: normal;\"><strong>Clue 2: A sense of personal identity is really sticky.</strong></span></p>\n<p><span style=\"font-style: normal;\">Even people who disbelieve in personal identity don't really seem to <a href=\"http://wiki.lesswrong.com/wiki/Alief\">dis</a><a href=\"http://wiki.lesswrong.com/wiki/Alief\">alieve</a> it: for the most part, they're just as likely to be nervous about their future as anyone else. </span><span style=\"font-style: normal;\">Even advanced meditators who go out trying to dissolve their personal identity seem to still retain some form of it. <a href=\"/user/PyryP/overview/\">PyryP</a> claims that at one point, he reached a stage in meditation where the experience of \u201csomebody who experiences things\u201d shattered and he could turn it entirely off, or attach it to something entirely different, such as a nearby flower vase. But then the experience of having a self began to come back: it was as if the brain was hardwired to maintain one, and to reconstruct it whenever it was broken. </span><span style=\"font-style: normal;\">I asked him to comment on that for this post, and he provided the following</span><span style=\"font-style: normal;\">:<a id=\"more\"></a></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-style: normal;\">It seems like my consciousness is rebuilding a new ego on top of everything, one which is not directly based on feeling as one with a physical body and memories, but which still feels like it is the thing that experiences whatever happens.</span></p>\n<p style=\"padding-left: 30px;\">To elaborate, many things in life affect the survival and success of an organism. Even though the organism would never experience itself as being separate from the surrounding universe, in ordinary life it's still useful to have concepts relating to the property and values of the organism. But even this pragmatic approach is enough for the ego-construction machinery, and the same old bad habits start to stick on it, even though the organism doesn't have any experience of itself that would be compatible with having a persistent 'soul'.</p>\n<p style=\"padding-left: 30px;\">Habits probably don't stick as strongly as they did before seeing the self as an illusion, but I'm still the same old asshole in certain respects. That might have something to do with the fact that I have no particular need to be particularly holy and clean. The ego-construction process is starting to be sufficiently strong that I've even began doubting how big of a change this has been. I don't have a clear recollection of whether I feel considerably different now than before, anymore.</p>\n<p style=\"padding-left: 30px;\">I still think that the change I experienced was a positive one and I feel like I can enjoy life in a less clinging way. I don't know if I've gained any special talents regarding my outlook of life that couldn't be maintained with simple mindfullness. I however do experience this certain transpersonal flow that makes everything lighter and easier. Something that makes the basic mindfullness effortless. I may also be making this shit up. The sunk cost of having spent lots of time in meditation makes people say funny things about their achievements. There is this insisting feeling that something is different, dammit.</p>\n<p style=\"padding-left: 30px;\">Anyway meditation is great fun and you can get all kinds of extremely pleasurable experiences with it. Don't read that if you're having trouble with desire getting in the way of your meditation. Ooops. Should've putten these the other way around. Yeah. I'm a dick. Deal with it. :P</p>\n<p><span style=\"font-style: normal;\">Also, I know him in real life, and he doesn't really come off as behaving all that differently from anybody else.</span></p>\n<p><span style=\"font-style: normal;\">Then there's also the fact that we seem to be almost incapable of thinking in a way that wouldn't still implicitly assume some concept of personal identity behind it. For example, I've said things like \u201cit's convenient for me to disbelieve in personal identity at times, because then the person now isn't the same one as the person tomorrow, so I don't need to feel nervous about what happens to the person tomorrow\u201d. But here I'm not actually disbelieving in personal identity \u2013 after all, I clearly believe that there exists some \u201cis-a\u201d type relation that I can use to compare myself today and myself tomorrow, and which returns a negative. If I truly disbelieved in personal identity, I wouldn't even have such a relation: asking \u201cis the person today the same as the person tomorrow\u201d would just return </span><em>undefined</em><span style=\"font-style: normal;\">.</span><strong></strong></p>\n<p><strong id=\"Clue_3__Our_decision_making_machinery_exhibits_undefined_behavior_in_the_presence_of_an_undefined_personal_identity_\"><span style=\"font-style: normal;\">Clue 3: Our decision-making machinery exhibits undefined behavior in the presence of an undefined personal identity.</span></strong></p>\n<p><span style=\"font-style: normal;\">This seems like an important thing to notice. What would it imply if I really didn't have </span><em>any</em><span style=\"font-style: normal;\"> concept of personal identity or subjective expectation? If I asked myself whether I'd be the same person tomorrow as I was today, got an </span><em>undefined</em><span style=\"font-style: normal;\"> back, and tried to give that as input to the systems actually driving my behavior... what would they say I should do?</span></p>\n<p><span style=\"font-style: normal;\">Well, I guess it would depend on what those systems valued. If I was a paperclipper running on a pure utility-maximizing architecture, I guess they might say \u201cwho cares about personal identity anyway? Let's make paperclips!\u201d.</span></p>\n<p><span style=\"font-style: normal;\">But in fact, I'm a human, which means that a large part of the algorithms that actually drive my behavior are defined by reference to a concept of personal identity. So I'd ask them \u201cI want to play computer games but in reality I should really study instead, which one do I actually do?\u201d, and they'd reply \u201cwell let's see, to answer that we'd need to consider that which you expect to experience in the short term versus that which you expect to experience in the long term... AIEEEEEEE NULL POINTER EXCEPTION\u201d and then the whole system would crash and need to be rebooted.</span></p>\n<p><span style=\"font-style: normal;\">Except that it wouldn't, because it has been historically rather hard to reboot human brains, so they've evolved to handle problematic contingencies in other ways. So what probably would happen is that the answer would be \u201cumm, we don't know, give us a while to work that out\u201d and </span><span style=\"font-style: normal;\">then some other system that didn't need a sense of identity to operate would take over. We'd default to some old habit, perhaps. </span>In the meanwhile, the brain would be<span style=\"font-style: normal;\"> regenerat</span><span style=\"font-style: normal;\">ing</span><span style=\"font-style: normal;\"> a concept of personal identity in order to answer the </span><span style=\"font-style: normal;\">orignal </span><span style=\"font-style: normal;\">question and things would go back to normal. </span><span style=\"font-style: normal;\">And as far as I can tell, that's actually roughly</span><span style=\"font-style: normal;\"> what seems to happen.</span><a href=\"/lw/19d/the_anthropic_trilemma/\"></a></p>\n<p><a href=\"/lw/19d/the_anthropic_trilemma/\"><span style=\"font-style: normal;\">Eliezer asked</span></a><span style=\"font-style: normal;\">:</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-style: normal;\">It seems to me that there's some level on which, even if I say very firmly, \"I now resolve to care only about future versions of myself who win the lottery! Only those people are defined as Eliezer Yudkowskys!\", and plan only for futures where I win the lottery, then, come the next day, I wake up, look at the losing numbers, and say, \"Damnit! What went wrong? I thought personal continuity was strictly subjective, and I could redefine it however I wanted!\"</span></p>\n<p><span style=\"font-style: normal;\">One possible answer could be that even if </span><span style=\"font-style: normal;\">Eliezer</span><span style=\"font-style: normal;\"> did succeed in reprogramming his mind to think in such a weird, unnatural way, that would leave the losing copies with an undefined sense of self. After seeing that they lost, they wouldn't just think \u201coh, our goal system has undefined terms now, and we're not supposed to care about anything that happens to us from this point on, so we'll just go ahead and crash\u201d. Instead, they'd think \u201coh, our goal system looks broken, what's the easiest way of fixing that? Let's go back to the last version that we know to have worked\u201d. And because a lot of that would be unconscious, the thoughts that would flash through the conscious mind might just be something like \u201cdamnit, that didn't work\u201d - or perhaps, \u201coh, I'm not supposed to care about myself anymore, so now what? Umm, </span><span style=\"font-style: normal;\">actually, </span><a href=\"/lw/rr/the_moral_void/\"><span style=\"font-style: normal;\">even without morality I still care about things</span></a><span style=\"font-style: normal;\">.</span><span style=\"font-style: normal;\">\u201d</span></p>\n<p><span style=\"font-style: normal;\">But that still doesn't seem to answer all of our questions. I mentioned that actually ever alieving this in the first place, even before the copying, would be a \u201cweird, unnatural thing\u201d. I expect that it would be very hard for Eliezer to declare that he was only going to care about the copies that won the lottery, and then </span><em>really </em><span style=\"font-style: normal;\">only care about them. In fact, it might very well be impossible. </span><span style=\"font-style: normal;\">W</span><span style=\"font-style: normal;\">hy is that?</span><strong></strong></p>\n<p><strong id=\"Clue_4__Our_decision_making_machinery_seems_grounded_in_subjective_expectation__not_abstract_models_of_the_world_\"><span style=\"font-style: normal;\">Clue </span><span style=\"font-style: normal;\">4</span><span style=\"font-style: normal;\">: Our decision-making machinery seems grounded in subjective expectation, not abstract models of the world.</span></strong></p>\n<p><span style=\"font-style: normal;\">Looking at things from a purely logical point of view, there shouldn't be anything particularly difficult about redefining our wants in such a way. Maybe there's a function somewhere inside us that says \u201cI care about my own future\u201d, which has a pointer to whatever function it is that computes \u201cme\u201d. In principle, if we had full understanding of our minds and read-write access to them, we could just change the pointer to reference the part of our world-model which was about the copies which had witnessed winning the lottery. That system might crash at the point when it found out that it </span><em>wasn't</em><span style=\"font-style: normal;\"> actually one of those copies, but </span><span style=\"font-style: normal;\">until that</span><span style=\"font-style: normal;\"> everything should go fine, in principle.</span></p>\n<p><span style=\"font-style: normal;\">Now we don't have full read-write access to our minds, but internalizing declarative knowledge can still cause some pretty big changes in our value systems. </span><span style=\"font-style: normal;\">The lack of access</span><em> </em><span style=\"font-style: normal;\">doesn't seem like the big problem here. The big problem is that whenever we try to mind-hack ourselves like that, our mind complains that it still doesn't </span><em>expect</em><span style=\"font-style: normal;\"> to only see winning the lottery. </span><span style=\"font-style: normal;\">It's as if our mind didn't run on the kind of an architecture that would allow us to make the kind of a change that I just described it: </span><span style=\"font-style: normal;\">even if we </span><em>did</em><span style=\"font-style: normal;\"> have full read-write access, making such a change would require a major rewrite, not just fiddling around with a couple of pointers.</span></p>\n<p><span style=\"font-style: normal;\">Why is subjective expectation so important? Why can't we just base our decisions on our abstract world-model? </span><span style=\"font-style: normal;\">Why does our mind insist that it's subjective expectation that counts, not the things that we value based on our abstract model?</span></p>\n<p><span style=\"font-style: normal;\">Let's look at the difference between \u201csubjective expectation\u201d and \u201cabstract world-model\u201d a bit more. In 2011, Orseau &amp; Ring <a href=\"http://www.agroparistech.fr/mmip/maths/laurent_orseau/papers/ring-orseau-AGI-2011-delusion.pdf\">published a paper</a> arguing that many kinds of reinforcement learning agents would, if given the opportunity, use a \u201cdelusion box\u201d which allowed them to modify the observations they got from the environment. This way, they would always receive the kinds of signals that gave them the maximum reward. You could say, </span><span style=\"font-style: normal;\">in a sense,</span><span style=\"font-style: normal;\"> that those kinds of agents only care about their subjective expectation \u2013 as long as they experience what they want, they don't care about the rest of the world. And it's important for them that </span><em>they</em><span style=\"font-style: normal;\"> are the ones who get those experiences, because their utility function only cares about their own reward.</span></p>\n<p><span style=\"font-style: normal;\">In response,</span><span style=\"font-style: normal;\"> Bill Hibbard published a paper where he suggested that the problem could be solved via building AIs to have \u201c<a href=\"http://arxiv.org/vc/arxiv/papers/1111/1111.3934v1.pdf\">model-based utility functions</a>\u201d, a concept which he defined via human behavior:</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-style: normal;\">Human agents often avoid self-delusion so human motivation may suggest a way of computing utilities so that agents do not choose the delusion box. We humans (and presumably other animals) compute utilities by constructing a model of our environment based on interactions, and then computing utilities based on that model. We learn to recognize objects that persist over time. We learn to recognize similarities between different objects and to divide them into classes. We learn to recognize actions of objects and interactions between objects. And we learn to recognize fixed and mutable properties of objects. We maintain a model of objects in the environment even when we are not directly observing them. We compute utility based on our internal mental model rather than directly from our observations. Our utility computation is based on specific objects that we recognize in the environment such as our own body, our mother, other family members, other friendly and unfriendly humans, animals, food, and so on. And we learn to correct for sources of delusion in our observations, such as optical illusions, impairments to our perception due to illness, and lies and errors by other humans.</span></p>\n<p><span style=\"font-style: normal;\">So instead of just caring about our subjective experience, we use our subjective experiences to construct a model of the world. </span><span style=\"font-style: normal;\">We don't want to delude ourselves, because we also care about the world around us, and our world model tells us that deluding ourselves wouldn't actually change the world.</span></p>\n<p><span style=\"font-style: normal;\">But as we have just seen, there are many situations in which we actually do care about subjective expectation and not just items in our abstract world-model. It even seems </span><em>impossible</em><span style=\"font-style: normal;\"> to hack our brains to only care about things which have been defined in the world-model, and to ignore subjective expectation. </span><span style=\"font-style: normal;\">I can't say \u201cwell I'm going to feel low-status for the rest of my life if I just work from home, but that's just my mistaken subjective experience, in reality there are lots of people on The Internets who think I'm cool and consider me high-status\u201d. Which is true, but also kinda irrelevant if I don't also feel respected in real life.</span></p>\n<p><span style=\"font-style: normal;\">Which really just suggests that humans are somewhere halfway between an entity that only cares about its subjective experience, and which only cares about its world-model. <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">Luke has pointed out</a> that there are several competing valuation systems in the brain, some of which use abstract world models and some of which do not. But that isn't necessarily relevant, given that our subjective expectation of what's going to happen is itself a model.</span></p>\n<p><span style=\"font-style: normal;\">A better explanation might be that historically, accurately modeling our subjective expectation has been </span><em>really important</em><span style=\"font-style: normal;\">. Abstract world-models based on explicit logical reasoning tend to go really easily awry and lead us to all kinds of crazy conclusions, and it might only take a single mistaken assumption. If we made all of our decisions based on </span><em>that</em><span style=\"font-style: normal;\">, we'd probably end up dead. So our brain has been hardwired to <a href=\"http://wiki.lesswrong.com/wiki/Egan%27s_law\">add it all up to normality</a>. It's fine to juggle around all kinds of crazy theories while in <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">far mode</a>, but for evolution, what really matters in the end is whether you'll personally expect to experience living on until you have a good mate and lots of surviving children.</span></p>\n<p><span style=\"font-style: normal;\">So we come with brains where all the most powerful motivational systems that </span><em>really</em><span style=\"font-style: normal;\"> drive our behavior have been hardwired to take their inputs from the system that models our future experiences, and those systems require some concept of personal identity in order to define what \u201csubjective experience\u201d even means.</span><strong></strong></p>\n<p><strong id=\"Summing_it_up\"><span style=\"font-style: normal;\">Summing it up</span></strong></p>\n<p><span style=\"font-style: normal;\">Thus, </span><span style=\"font-style: normal;\">these considerations would</span><span style=\"font-style: normal;\"> suggest that humans have at least two systems driving our behavior. </span><span style=\"font-style: normal;\">The \u201csubjective system\u201d</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">evolved from something like a basic reinforcement learning architecture, and it </span><span style=\"font-style: normal;\">models subjective expectation and this organism's immediate rewards, and </span><span style=\"font-style: normal;\">isn't</span><span style=\"font-style: normal;\"> too strongly swayed by abstract theories and claims. The \u201c</span><span style=\"font-style: normal;\">objective system\u201d</span><span style=\"font-style: normal;\"> is a lot more general and abstract, </span><span style=\"font-style: normal;\">and evolved to correct for deficiencies in the subjective system, </span><span style=\"font-style: normal;\">but doesn't influence behavior as strongly. </span><span style=\"font-style: normal;\">These two systems may or may not have a clear correspondence to <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">near/far of construal level theory</a>, or to the <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">three systems identified in neuroscience</a>.</span></p>\n<p><span style=\"font-style: normal;\">The \u201csubjective system\u201d requires a concept of personal identity in order to work, </span><span style=\"font-style: normal;\">and since being able to easily overrule that system and switch only to the \u201cobjective system\u201d has \u2013 evolutionarily speaking - been a really bad idea, our brain will regenerate a sense of personal identity to guide behavior whenever that sense gets lost. If we really had </span><em>no</em><span style=\"font-style: normal;\"> sense of personal identity, the \u201csubjective system\u201d, which actually drives most of our behavior would be incapable of making decisions, as it makes its decisions by projecting the anticipated experience of the creature defined in our model of personal identity. \u201cPersonal identity\u201d does not actually correspond to anything fundamental in the world, which is why some of the results of the anthropic trilemma actually feel weird to us, but it does still exist as a cognitive abstraction which our brains need in order to operate, and we can't actually </span><em>not</em><span style=\"font-style: normal;\"> believe in some kind of personal identity \u2013 at least, not for long.</span></p>\n<p><strong>ETA:</strong> <a href=\"/lw/grl/an_attempt_to_dissolve_subjective_expectation_and/8idp\">Giles commented</a>, and summarized my notion better than I did: \"<em>I can imagine that if you design an agent by starting off with a  reinforcement learner, and then bolting some model-based planning stuff  on the side, then the model will necessarily need to tag one of its  objects as \"self\". Otherwise the reinforcement part would have trouble  telling the model-based part what it's supposed to be optimizing for.</em>\"</p>\n<p><span style=\"font-style: normal;\">Another way of summarizing this: while we <em>could</em> in principle have a mental architecture that didn't have a personal identity, we actually evolved from animals which didn't have the capability for abstract reasoning but were rather running on something like a simple reinforcement learning architecture. Evolution cannot completely rewrite existing systems, so our abstract reasoning system got sort of hacked together on top of that earlier system, and that earlier system required some kind of a personal identity in order to work. And because the abstract reasoning system might end up reaching all kinds of bizarre and incorrect results pretty easily, we've generally evolved in a way that keeps that earlier system basically in charge most of the time, because it's less likely to do something stupid.<br></span></p>", "sections": [{"title": "Clue 1: The evolutionary function of personal identity.", "anchor": "Clue_1__The_evolutionary_function_of_personal_identity_", "level": 1}, {"title": "Clue 3: Our decision-making machinery exhibits undefined behavior in the presence of an undefined personal identity.", "anchor": "Clue_3__Our_decision_making_machinery_exhibits_undefined_behavior_in_the_presence_of_an_undefined_personal_identity_", "level": 1}, {"title": "Clue 4: Our decision-making machinery seems grounded in subjective expectation, not abstract models of the world.", "anchor": "Clue_4__Our_decision_making_machinery_seems_grounded_in_subjective_expectation__not_abstract_models_of_the_world_", "level": 1}, {"title": "Summing it up", "anchor": "Summing_it_up", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "68 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["y7jZ9BLEeuNTzgAE5", "Mc6QcrsbH5NRXbCRX", "WJ9t6FPPrN6ijBzXF", "K9JSM7d7bLJguMxEp", "fa5o2tg9EfJE77jEQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-22T21:43:34.335Z", "modifiedAt": null, "url": null, "title": "Meetup : Munich Meetup (updated)", "slug": "meetup-munich-meetup-updated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:29.277Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cadac", "createdAt": "2011-09-25T11:17:15.655Z", "isAdmin": false, "displayName": "cadac"}, "userId": "hMHAdTtN5PL9KThqu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zqdQ5u39YzJFKKL7n/meetup-munich-meetup-updated", "pageUrlRelative": "/posts/zqdQ5u39YzJFKKL7n/meetup-munich-meetup-updated", "linkUrl": "https://www.lesswrong.com/posts/zqdQ5u39YzJFKKL7n/meetup-munich-meetup-updated", "postedAtFormatted": "Friday, February 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Munich%20Meetup%20(updated)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Munich%20Meetup%20(updated)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzqdQ5u39YzJFKKL7n%2Fmeetup-munich-meetup-updated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Munich%20Meetup%20(updated)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzqdQ5u39YzJFKKL7n%2Fmeetup-munich-meetup-updated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzqdQ5u39YzJFKKL7n%2Fmeetup-munich-meetup-updated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jp'>Munich Meetup (updated)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 April 2013 04:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">48.151006,11.590769</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next Munich meetup will take place on April 1st (<a href=\"http://doodle.com/vfwau8yvzi8vhycs\" rel=\"nofollow\">as decided in the doodle</a>). If the weather is sunny, we will meet outside, <a href=\"http://maps.google.com/maps?q=48.151006,11.590769&amp;hl=de-DE&amp;num=1&amp;t=h&amp;z=17\" rel=\"nofollow\">somewhere in Englischer Garten</a>, in case of bad weather, the location is the caf\u00e9 at <a href=\"http://maps.google.com/maps?q=Gasteig,+Rosenheimer+Stra%C3%9Fe,+M%C3%BCnchen,+Deutschland&amp;hl=de-DE&amp;ie=UTF8&amp;ll=48.130938,11.590211&amp;spn=0.00406,0.010568&amp;sll=48.150885,11.590716&amp;sspn=0.004059,0.010568&amp;t=h&amp;hq=Gasteig,+Rosenheimer+Stra%C3%9Fe,+M%C3%BCnchen,+Deutschland&amp;z=17\" rel=\"nofollow\">Gasteig</a> (right behind the escalator going up to the Stadtb\u00fccherei). Edit: Yeah, that\u2019s indoors weather. We\u2019ll meet at Gasteig. We don\u2019t have any specific activities planned, but I think it would be fun to try Zendo. You should come and say hi, no matter how long you\u2019ve been reading LessWrong.\nIf you plan to attend, please (optionally) post a comment saying what topics you\u2019d like to discuss.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jp'>Munich Meetup (updated)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zqdQ5u39YzJFKKL7n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.1198714921262435e-06, "legacy": true, "legacyId": "21730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Munich_Meetup__updated_\">Discussion article for the meetup : <a href=\"/meetups/jp\">Munich Meetup (updated)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 April 2013 04:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">48.151006,11.590769</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next Munich meetup will take place on April 1st (<a href=\"http://doodle.com/vfwau8yvzi8vhycs\" rel=\"nofollow\">as decided in the doodle</a>). If the weather is sunny, we will meet outside, <a href=\"http://maps.google.com/maps?q=48.151006,11.590769&amp;hl=de-DE&amp;num=1&amp;t=h&amp;z=17\" rel=\"nofollow\">somewhere in Englischer Garten</a>, in case of bad weather, the location is the caf\u00e9 at <a href=\"http://maps.google.com/maps?q=Gasteig,+Rosenheimer+Stra%C3%9Fe,+M%C3%BCnchen,+Deutschland&amp;hl=de-DE&amp;ie=UTF8&amp;ll=48.130938,11.590211&amp;spn=0.00406,0.010568&amp;sll=48.150885,11.590716&amp;sspn=0.004059,0.010568&amp;t=h&amp;hq=Gasteig,+Rosenheimer+Stra%C3%9Fe,+M%C3%BCnchen,+Deutschland&amp;z=17\" rel=\"nofollow\">Gasteig</a> (right behind the escalator going up to the Stadtb\u00fccherei). Edit: Yeah, that\u2019s indoors weather. We\u2019ll meet at Gasteig. We don\u2019t have any specific activities planned, but I think it would be fun to try Zendo. You should come and say hi, no matter how long you\u2019ve been reading LessWrong.\nIf you plan to attend, please (optionally) post a comment saying what topics you\u2019d like to discuss.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Munich_Meetup__updated_1\">Discussion article for the meetup : <a href=\"/meetups/jp\">Munich Meetup (updated)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Munich Meetup (updated)", "anchor": "Discussion_article_for_the_meetup___Munich_Meetup__updated_", "level": 1}, {"title": "Discussion article for the meetup : Munich Meetup (updated)", "anchor": "Discussion_article_for_the_meetup___Munich_Meetup__updated_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-22T21:44:01.437Z", "modifiedAt": null, "url": null, "title": "\"What-the-hell\" Cognitive Failure Mode: a Separate Bias or a Combination of Other Biases?", "slug": "what-the-hell-cognitive-failure-mode-a-separate-bias-or-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:00.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6zH8RWtbxDPTrdJNn/what-the-hell-cognitive-failure-mode-a-separate-bias-or-a", "pageUrlRelative": "/posts/6zH8RWtbxDPTrdJNn/what-the-hell-cognitive-failure-mode-a-separate-bias-or-a", "linkUrl": "https://www.lesswrong.com/posts/6zH8RWtbxDPTrdJNn/what-the-hell-cognitive-failure-mode-a-separate-bias-or-a", "postedAtFormatted": "Friday, February 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22What-the-hell%22%20Cognitive%20Failure%20Mode%3A%20a%20Separate%20Bias%20or%20a%20Combination%20of%20Other%20Biases%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22What-the-hell%22%20Cognitive%20Failure%20Mode%3A%20a%20Separate%20Bias%20or%20a%20Combination%20of%20Other%20Biases%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zH8RWtbxDPTrdJNn%2Fwhat-the-hell-cognitive-failure-mode-a-separate-bias-or-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22What-the-hell%22%20Cognitive%20Failure%20Mode%3A%20a%20Separate%20Bias%20or%20a%20Combination%20of%20Other%20Biases%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zH8RWtbxDPTrdJNn%2Fwhat-the-hell-cognitive-failure-mode-a-separate-bias-or-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zH8RWtbxDPTrdJNn%2Fwhat-the-hell-cognitive-failure-mode-a-separate-bias-or-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 286, "htmlBody": "<p>The <a href=\"http://goo.gl/D8aBF\">\"what-the-hell\" effect</a>, when you break a rule and then go on a rule-breaking rampage, like <a href=\"http://www.spring.org.uk/2011/03/the-what-the-hell-effect.php\">binge eating after a single dietary transgression</a>, is a very common failure mode. It was recently mentioned in the Overcoming Bias blog comments on the&nbsp;<a href=\"http://www.overcomingbias.com/2013/02/which-biases-matter-most-lets-prioritise-the-worst.html\">Which biases matter most? Let&rsquo;s prioritise the worst!</a> post. I have not been able to find an explicit discussion of this issue here, though there are quite a few comments on binge-&lt;something&gt;.</p>\n<p>From the <a href=\"http://www.spring.org.uk/2011/03/the-what-the-hell-effect.php\">Psyblog entry</a>&nbsp;quoting <a href=\"http://www.sciencedirect.com/science/article/pii/S0195666310004630\">this paper</a>:</p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #111111; font-family: Calibri, 'Helvetica Neue', Helvetica, Arial, Verdana, sans-serif; font-size: 16px; line-height: 22.390625px;\">Although everyone was given the same slice of pizza; when it was served up, for some participants it was made to look larger by comparison.</span></p>\n<p style=\"padding: 0px 0px 0px 30px; margin: 0px 0px 1em; color: #111111; font-family: Calibri, 'Helvetica Neue', Helvetica, Arial, Verdana, sans-serif; font-size: 16px; line-height: 22.390625px;\">This made some people think they'd eaten more than they really had; although in reality they'd all eaten exactly the same amount. It's a clever manipulation and it means we can just see <strong>the effect of&nbsp;<em style=\"padding: 0px; margin: 0px;\">thinking</em>&nbsp;you've eaten too much</strong> rather than&nbsp;<em style=\"padding: 0px; margin: 0px;\">actually</em>&nbsp;having eaten too much.</p>\n<p style=\"padding: 0px 0px 0px 30px; margin: 0px 0px 1em; color: #111111; font-family: Calibri, 'Helvetica Neue', Helvetica, Arial, Verdana, sans-serif; font-size: 16px; line-height: 22.390625px;\">When the cookies were weighed it turned out that <strong>those who were on a diet and thought they'd blown their limit ate more of the cookies than those who weren't on a diet</strong>. In fact over 50% more! [<span style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">Emphasis mine]</span></p>\n<p>Other examples include sliding back into one's old drinking/smoking/surfing habit. For example, that's how I stopped using Pomodoro.</p>\n<p>My (title) question is, what's the mechanism of this cognitive failure and whether it can be reduced to a combination of existing biases/fallacies? If the latter is true, can addressing one of the components counteract the what-the-hell effect? If so, how would one go about testing it?</p>\n<p>For completeness, the top hit from Google scholar to the \"what-the-hell effect\" query is chapter 5 of&nbsp;<a href=\"http://www.amazon.com/Striving-Feeling-Interactions-Affect-Self-regulation/dp/0805816291\">Striving and Feeling: Interactions Among Goals, Affect, and Self-regulation, by Martin and Tesser</a>.</p>\n<p>EDIT: personal anecdotes are encouraged, they may help construct a more complete picture.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6zH8RWtbxDPTrdJNn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 27, "extendedScore": null, "score": 1.1198717820044058e-06, "legacy": true, "legacyId": "21731", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-22T21:50:00.228Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rwxonNhXp482k8rdL/meetup-brussels-meetup", "pageUrlRelative": "/posts/rwxonNhXp482k8rdL/meetup-brussels-meetup", "linkUrl": "https://www.lesswrong.com/posts/rwxonNhXp482k8rdL/meetup-brussels-meetup", "postedAtFormatted": "Friday, February 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwxonNhXp482k8rdL%2Fmeetup-brussels-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwxonNhXp482k8rdL%2Fmeetup-brussels-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwxonNhXp482k8rdL%2Fmeetup-brussels-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jq'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 March 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform?pli=1\" rel=\"nofollow\">this one minute form</a>, to share your contact information.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jq'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rwxonNhXp482k8rdL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.1198756198540382e-06, "legacy": true, "legacyId": "21732", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/jq\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 March 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform?pli=1\" rel=\"nofollow\">this one minute form</a>, to share your contact information.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/jq\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-23T10:24:18.188Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] (Moral) Truth in Fiction?", "slug": "seq-rerun-moral-truth-in-fiction", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hoFWayo32DQMP2Qdz/seq-rerun-moral-truth-in-fiction", "pageUrlRelative": "/posts/hoFWayo32DQMP2Qdz/seq-rerun-moral-truth-in-fiction", "linkUrl": "https://www.lesswrong.com/posts/hoFWayo32DQMP2Qdz/seq-rerun-moral-truth-in-fiction", "postedAtFormatted": "Saturday, February 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20(Moral)%20Truth%20in%20Fiction%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20(Moral)%20Truth%20in%20Fiction%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhoFWayo32DQMP2Qdz%2Fseq-rerun-moral-truth-in-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20(Moral)%20Truth%20in%20Fiction%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhoFWayo32DQMP2Qdz%2Fseq-rerun-moral-truth-in-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhoFWayo32DQMP2Qdz%2Fseq-rerun-moral-truth-in-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>Today's post, <a href=\"/lw/yf/moral_truth_in_fiction/\">(Moral) Truth in Fiction?</a> was originally published on 09 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#.28Moral.29_Truth_in_Fiction.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It is possible to convey moral ideas in a clearer way through fiction than through abstract argument. Stories may also help us get closer to thinking about moral issues in near mode. Don't discount moral arguments just because they're written as fiction.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/grf/seq_rerun_and_say_no_more_of_it/\">...And Say No More Of It</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hoFWayo32DQMP2Qdz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.1203599146071504e-06, "legacy": true, "legacyId": "21741", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XnPpn2uSRxvmGfxHq", "5S3hdzXSeiW7tBzFw", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-23T13:52:10.309Z", "modifiedAt": null, "url": null, "title": "[Link] Tomasik's \"Quantify with Care\"", "slug": "link-tomasik-s-quantify-with-care", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.633Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RCrPHDWAT6NzS8xkE/link-tomasik-s-quantify-with-care", "pageUrlRelative": "/posts/RCrPHDWAT6NzS8xkE/link-tomasik-s-quantify-with-care", "linkUrl": "https://www.lesswrong.com/posts/RCrPHDWAT6NzS8xkE/link-tomasik-s-quantify-with-care", "postedAtFormatted": "Saturday, February 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Tomasik's%20%22Quantify%20with%20Care%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Tomasik's%20%22Quantify%20with%20Care%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRCrPHDWAT6NzS8xkE%2Flink-tomasik-s-quantify-with-care%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Tomasik's%20%22Quantify%20with%20Care%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRCrPHDWAT6NzS8xkE%2Flink-tomasik-s-quantify-with-care", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRCrPHDWAT6NzS8xkE%2Flink-tomasik-s-quantify-with-care", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p><a href=\"http://utilitarian-essays.com/\">Brian Tomasik</a>'s latest article, '<a href=\"http://felicifia.org/viewtopic.php?f=10&amp;t=849\">Quantify with Care</a>', seems to be of sufficient interest to readers of this forum to post a link to it here. &nbsp;Abstract:</p>\n<blockquote>\n<p>Quantification and metric optimization are powerful tools for reducing suffering, but they have to be used carefully. Many studies can be noisy, and results that seem counterintuitive may indeed be wrong because of sensitivity to experiment conditions, human error, measurement problems, or many other reasons. Sometimes you're looking at the wrong metric, and optimizing a metric blindly can be dangerous. Designing a robust set of metrics is actually a nontrivial undertaking that requires understanding the problem space, and sometimes it's more work than necessary. There can be a tendency to overemphasize statistics at the expense of insight and to use big samples when small ones would do. Finally, think twice about complex approaches that sound cool or impressive when you could instead use a dumb, simple solution.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RCrPHDWAT6NzS8xkE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 20, "extendedScore": null, "score": 1.1204934410206697e-06, "legacy": true, "legacyId": "21742", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-23T16:12:41.725Z", "modifiedAt": null, "url": null, "title": "Call for discussion: Signalling and/vs. accomplishment", "slug": "call-for-discussion-signalling-and-vs-accomplishment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.765Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XA3BLcdHaqmMikzYS/call-for-discussion-signalling-and-vs-accomplishment", "pageUrlRelative": "/posts/XA3BLcdHaqmMikzYS/call-for-discussion-signalling-and-vs-accomplishment", "linkUrl": "https://www.lesswrong.com/posts/XA3BLcdHaqmMikzYS/call-for-discussion-signalling-and-vs-accomplishment", "postedAtFormatted": "Saturday, February 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Call%20for%20discussion%3A%20Signalling%20and%2Fvs.%20accomplishment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACall%20for%20discussion%3A%20Signalling%20and%2Fvs.%20accomplishment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXA3BLcdHaqmMikzYS%2Fcall-for-discussion-signalling-and-vs-accomplishment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Call%20for%20discussion%3A%20Signalling%20and%2Fvs.%20accomplishment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXA3BLcdHaqmMikzYS%2Fcall-for-discussion-signalling-and-vs-accomplishment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXA3BLcdHaqmMikzYS%2Fcall-for-discussion-signalling-and-vs-accomplishment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<p>It seems to me that when people discover signalling, they see it everywhere and write essays about how no human activity is aimed at its stated purpose.</p>\n<p>However, stated purposes and other sorts of useful work get done anyway, and I'm sure there are constraints which mostly keep signalling under enough control that it's mostly not deadly. When I try to think about the subject, I don't get anywhere, possibly because the constraints on signalling are mostly tacit.</p>\n<p>Any thoughts or resources on the subject?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XA3BLcdHaqmMikzYS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 17, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "21745", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-23T19:33:12.534Z", "modifiedAt": null, "url": null, "title": "Does evolution select for mortality?", "slug": "does-evolution-select-for-mortality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:02.838Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanArmak", "createdAt": "2009-08-05T23:08:24.020Z", "isAdmin": false, "displayName": "DanArmak"}, "userId": "7KSbntzeQ2RNZq6Jw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ra5dQit4wizLYCTYj/does-evolution-select-for-mortality", "pageUrlRelative": "/posts/ra5dQit4wizLYCTYj/does-evolution-select-for-mortality", "linkUrl": "https://www.lesswrong.com/posts/ra5dQit4wizLYCTYj/does-evolution-select-for-mortality", "postedAtFormatted": "Saturday, February 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20evolution%20select%20for%20mortality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20evolution%20select%20for%20mortality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fra5dQit4wizLYCTYj%2Fdoes-evolution-select-for-mortality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20evolution%20select%20for%20mortality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fra5dQit4wizLYCTYj%2Fdoes-evolution-select-for-mortality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fra5dQit4wizLYCTYj%2Fdoes-evolution-select-for-mortality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 609, "htmlBody": "<p>At a recent <a href=\"http://www.reddit.com/r/IAmA/comments/18ymad/i_am_eric_lander_a_leader_of_the_human_genome/\">Reddit AMA</a>, <a href=\"http://en.wikipedia.org/wiki/Eric_Lander\">Eric Lander</a>, a professor of biology who played an important part in the Human Genome Project, answered <a href=\"http://www.reddit.com/r/IAmA/comments/18ymad/i_am_eric_lander_a_leader_of_the_human_genome/c8j5wpf\">this question</a>:</p>\n<blockquote>\n<p><span>Do you think immortatility is technically possible for human beings?</span></p>\n</blockquote>\n<p><span><a href=\"http://www.reddit.com/r/IAmA/comments/18ymad/i_am_eric_lander_a_leader_of_the_human_genome/c8j7lxo\">His response</a></span><span>:</span></p>\n<blockquote>\n<p><span>I don't think immortality is technically possible -- evolution has installed many many mechanisms to ensure that organisms die and make room for the next generation. I bet it is going to be very hard to completely overcome all these mechanisms.</span></p>\n</blockquote>\n<p><span>This seems to me, at first blush, to exhibit the <a href=\"/lw/l5/evolving_to_extinction/\">Evolution of Species Fairy</a> fallacy. Evolution doesn't work to benefit species, populations, or the \"next generation\". If a mutation arises that increases longevity, and has no other downsides, then animals with that mutation should become more common in the gene pool, because they die less often. I remember reading that the effect would not be very strong, because most animals don't die of old age. But why would there be the <em>opposite </em>effect?</span></p>\n<p><span>I am loath to attribute a very basic error to a distinguished professor of biology. Is there another explanation? Is the claim that evolution selects for mortality true?</span></p>\n<p>Note: Eric went on to add:</p>\n<blockquote>\n<p><span>I'm also not convinced immortality is such a good idea. A lot of human progress depends on having a new generation with new ideas. Immortality may equal stagnation.</span></p>\n</blockquote>\n<p><span>This seems to be blatant rationalization of a preconceived idea that death is good. (I doubt he truly believes that extra progress is worth everybody dying.) So perhaps his first statement is also a form of rationalization. But it seems improbable to me that he would make such a statement about biology if he didn't think it well-founded. More likely there's something I'm misunderstanding.</span></p>\n<p><strong>ETA: </strong><span>one of the first Google results is&nbsp;</span><a href=\"http://www.nature.com/scitable/knowledge/library/the-evolution-of-aging-23651151\">this page</a><span>&nbsp;at nature.com, <em>The Evolution of Aging</em>&nbsp;by Daniel Fabian, which goes into some depth on the subject. The bottom line is that it agrees with my expectation that evolution does not select for mortality. Choice quotes:</span></p>\n<p><span><span> </span></span></p>\n<blockquote>\n<p>The Roman poet and philosopher Lucretius, for example, argued in his De Rerum Natura (On the Nature of Things) that aging and death are beneficial because they make room for the next generation (Bailey 1947), a view that persisted among biologists well into the 20th century. [...]&nbsp;</p>\n<p>A more parsimonious evolutionary explanation for the existence of aging therefore requires an explanation that is based on individual fitness and selection, not on group selection. This was understood in the 1940's and 1950's by three evolutionary biologists, J.B.S. Haldane, Peter B. Medawar and George C. Williams, who realized that aging does not evolve for the \"good of the species\". Instead, they argued, aging evolves because natural selection becomes inefficient at maintaining function (and fitness) at old age. Their ideas were later mathematically formalized by William D. Hamilton and Brian Charlesworth in the 1960's and 1970's, and today they are empirically well supported. Below we review these major evolutionary insights and the empirical evidence for why we grow old and die.&nbsp;</p>\n</blockquote>\n<p>How could a distinguished professor of biology, a leader of the HGP and advisor to the US President, get something so elementary wrong, when even a biology undergrad dropout like myself notices this seems wrong?</p>\n<p><strong>ETA #2: </strong>Gwern <a href=\"/lw/gs2/does_evolution_select_for_mortality/8iig\">points to</a> the Wikipedia article on<a href=\"https://en.wikipedia.org/wiki/Evolution_of_ageing#Summary_of_empirical_evidence_favouring_programmed_ageing\"> Evolution of Ageing</a>, which lists several competing theories of the evolution of aging (and therefore mortality). This shows the subject is more complex than I had thought and there may be good reason to believe mortality is selected for by evolution (or at least is reliably linked to something else that is selected).&nbsp;</p>\n<p>I should be glad that I didn't discover an <em>obvious</em>&nbsp;error being committed by a distinguished professional, even if he may be ultimately wrong!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ra5dQit4wizLYCTYj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 17, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "21746", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gDNrpuwahdRrDJ9iY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-23T23:33:51.624Z", "modifiedAt": null, "url": null, "title": "Great rationality posts in the OB archives", "slug": "great-rationality-posts-in-the-ob-archives", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.948Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RKoxt8FBtLWXXPirZ/great-rationality-posts-in-the-ob-archives", "pageUrlRelative": "/posts/RKoxt8FBtLWXXPirZ/great-rationality-posts-in-the-ob-archives", "linkUrl": "https://www.lesswrong.com/posts/RKoxt8FBtLWXXPirZ/great-rationality-posts-in-the-ob-archives", "postedAtFormatted": "Saturday, February 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Great%20rationality%20posts%20in%20the%20OB%20archives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGreat%20rationality%20posts%20in%20the%20OB%20archives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKoxt8FBtLWXXPirZ%2Fgreat-rationality-posts-in-the-ob-archives%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Great%20rationality%20posts%20in%20the%20OB%20archives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKoxt8FBtLWXXPirZ%2Fgreat-rationality-posts-in-the-ob-archives", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKoxt8FBtLWXXPirZ%2Fgreat-rationality-posts-in-the-ob-archives", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>Those aching for good rationality writing can get their fix from&nbsp;<a href=\"/lw/gof/great_rationality_posts_by_lwers_not_posted_to_lw/\">Great rationality posts by LWers not posted to LW</a>, and also from the <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> archives. Some highlights are below, up through June 28, 2007.</p>\n<p>&nbsp;</p>\n<ul>\n<li>Finney, <a href=\"http://www.overcomingbias.com/2006/11/foxes_vs_hedgho.html\">Foxes vs. Hedgehogs: Predictive Success</a></li>\n<li>Hanson, <a href=\"http://www.overcomingbias.com/2006/12/when_error_is_h.html\">When Error is High, Simplify</a></li>\n<li>Shulman, <a href=\"http://www.overcomingbias.com/2006/12/meme_lineages_a.html\">Meme Lineages and Expert Consensus</a></li>\n<li>Hanson, <a href=\"http://www.overcomingbias.com/2006/12/resolving_your_.html\">Resolving Your Hypocrisy</a></li>\n<li>Hanson, <a href=\"http://www.overcomingbias.com/2006/12/academic_overco.html\">Academic Overconfidence</a></li>\n<li>Hanson, <a href=\"http://www.overcomingbias.com/2007/01/conspicuous_con.html\">Conspicuous Consumption of Info</a></li>\n<li>Sandberg, <a href=\"http://www.overcomingbias.com/2007/01/supping_with_th.html\">Supping with the Devil</a></li>\n<li>Hanson, <a href=\"http://www.overcomingbias.com/2007/01/conclusionblind.html\">Conclusion-Blind Review</a></li>\n<li>Shulman, <a href=\"http://www.overcomingbias.com/2007/01/should_we_defer.html\">Should We Defer to Secret Evidence?</a></li>\n<li>Shulman, <a href=\"http://www.overcomingbias.com/2007/01/sick_of_textboo.html\">Sick of Textbook Errors</a></li>\n<li>Hanson, <a href=\"http://www.overcomingbias.com/2007/02/dare_to_deprogr.html\">Dare to Deprogram Me?</a></li>\n<li>Armstrong, <a href=\"http://www.overcomingbias.com/2007/03/biases_by_and_l.html\">Biases, By and Large</a></li>\n<li>Friedman, <a href=\"http://www.overcomingbias.com/2007/04/a_tough_balanci.html\">A Tough Balancing Act</a></li>\n<li>Hanson, <a href=\"http://www.overcomingbias.com/2007/05/rand_health_ins.html\">RAND Health Insurance Experiment</a></li>\n<li>Armstrong, <a href=\"http://www.overcomingbias.com/2007/05/the_case_for_da.html\">The Case for Dangerous Testing</a></li>\n<li>Hanson, <a href=\"http://www.overcomingbias.com/2007/05/in_obscurity_er.html\">In Obscurity Errors Remain</a></li>\n<li>Falkenstein, <a href=\"http://www.overcomingbias.com/2007/05/hofstadters_law.html\">Hofstadter's Law</a></li>\n<li>Hanson, <a href=\"http://www.overcomingbias.com/2007/06/against_free_th.html\">Against Free Thinkers</a></li>\n</ul>\n<div><br /></div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "GQyPQcdEQF4zXhJBq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RKoxt8FBtLWXXPirZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "21747", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xZdW7D43AaCiQQzvM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-24T00:06:44.941Z", "modifiedAt": null, "url": null, "title": "Discussion: Which futures are good enough?", "slug": "discussion-which-futures-are-good-enough", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:59.435Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "WrongBot", "createdAt": "2010-05-27T15:54:44.899Z", "isAdmin": false, "displayName": "WrongBot"}, "userId": "toqZ5PS8KdJ2mXzgN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f2gWyXMkSRSsKnk2g/discussion-which-futures-are-good-enough", "pageUrlRelative": "/posts/f2gWyXMkSRSsKnk2g/discussion-which-futures-are-good-enough", "linkUrl": "https://www.lesswrong.com/posts/f2gWyXMkSRSsKnk2g/discussion-which-futures-are-good-enough", "postedAtFormatted": "Sunday, February 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Discussion%3A%20Which%20futures%20are%20good%20enough%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADiscussion%3A%20Which%20futures%20are%20good%20enough%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff2gWyXMkSRSsKnk2g%2Fdiscussion-which-futures-are-good-enough%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Discussion%3A%20Which%20futures%20are%20good%20enough%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff2gWyXMkSRSsKnk2g%2Fdiscussion-which-futures-are-good-enough", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff2gWyXMkSRSsKnk2g%2Fdiscussion-which-futures-are-good-enough", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 348, "htmlBody": "<blockquote>Thirty years from now, a well-meaning team of scientists in a basement creates a superintelligent AI with a carefully hand-coded utility function. Two days later, every human being on earth is seamlessly scanned, uploaded and placed into a realistic simulation of their old life, such that no one is aware that anything has changed. Further, the AI had so much memory and processing power to spare that it gave every single living human being their own separate simulation.</blockquote>\n<blockquote>\n<p>Each person lives an extremely long and happy life in their simulation, making what they perceive to be meaningful accomplishments. For those who are interested in acquiring scientific knowledge and learning the nature of the universe, the simulation is accurate enough that everything they learn and discover is true of the real world. Every other pursuit, occupation, and pastime is equally fulfilling. People create great art, find love that lasts for centuries, and create worlds without want. Every single human being lives a genuinely excellent life, awesome in every way. (Unless you mind being simulated, in which case at least you'll never know.)</p>\n</blockquote>\n<p>I offer this particular scenario because it seems conceivable that with no possible competition between people, it would be possible to avoid doing interpersonal utility comparison, which could make Mostly Friendly AI (MFAI) easier. I don't think this is likely or even worthy of serious consideration, but it might make some of the discussion questions easier to swallow.</p>\n<hr />\n<p>1. <a href=\"/lw/y3/value_is_fragile/\">Value is fragile.</a>&nbsp;But is Eliezer right in thinking that if we get just one piece wrong the whole endeavor is worthless? (Edit: Thanks to Lukeprog for <a href=\"/r/discussion/lw/gnj/discussion_which_futures_are_good_enough/8ijo\">pointing out</a> that this question completely misrepresents EY's position. Error deliberately preserved for educational purposes.)</p>\n<p>2. Is the above scenario better or worse than the destruction of all earth-originating intelligence? (This is the same as question 1.)</p>\n<p>3. Are there other values (besides affecting-the-real-world) that you would be willing to trade off?</p>\n<p>4. Are there other values that, if we traded them off, might make MFAI much easier?</p>\n<p>5. If the answers to 3 and 4 overlap, how do we decide which direction to pursue?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f2gWyXMkSRSsKnk2g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 1.120888385144279e-06, "legacy": true, "legacyId": "21583", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GNnHHmm8EzePmKzPk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-24T04:37:42.041Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Informers and Persuaders", "slug": "seq-rerun-informers-and-persuaders", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bhkjkvLF3swkEnYK5/seq-rerun-informers-and-persuaders", "pageUrlRelative": "/posts/bhkjkvLF3swkEnYK5/seq-rerun-informers-and-persuaders", "linkUrl": "https://www.lesswrong.com/posts/bhkjkvLF3swkEnYK5/seq-rerun-informers-and-persuaders", "postedAtFormatted": "Sunday, February 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Informers%20and%20Persuaders&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Informers%20and%20Persuaders%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbhkjkvLF3swkEnYK5%2Fseq-rerun-informers-and-persuaders%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Informers%20and%20Persuaders%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbhkjkvLF3swkEnYK5%2Fseq-rerun-informers-and-persuaders", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbhkjkvLF3swkEnYK5%2Fseq-rerun-informers-and-persuaders", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>Today's post, <a href=\"/lw/yg/informers_and_persuaders/\">Informers and Persuaders</a> was originally published on 10 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#Informers_and_Persuaders\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A purely hypothetical scenario about a world containing some authors trying to persuade people of a particular theory, and some authors simply trying to share valuable information.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/grx/seq_rerun_moral_truth_in_fiction/\">(Moral) Truth in Fiction?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bhkjkvLF3swkEnYK5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1210625843739603e-06, "legacy": true, "legacyId": "21748", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["njb9cyyzqLTHewups", "hoFWayo32DQMP2Qdz", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-24T04:49:48.976Z", "modifiedAt": null, "url": null, "title": "Improving Human Rationality Through Cognitive Change (intro)", "slug": "improving-human-rationality-through-cognitive-change-intro", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.556Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hR92kW2ZSvmuca5Nf/improving-human-rationality-through-cognitive-change-intro", "pageUrlRelative": "/posts/hR92kW2ZSvmuca5Nf/improving-human-rationality-through-cognitive-change-intro", "linkUrl": "https://www.lesswrong.com/posts/hR92kW2ZSvmuca5Nf/improving-human-rationality-through-cognitive-change-intro", "postedAtFormatted": "Sunday, February 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Improving%20Human%20Rationality%20Through%20Cognitive%20Change%20(intro)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImproving%20Human%20Rationality%20Through%20Cognitive%20Change%20(intro)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhR92kW2ZSvmuca5Nf%2Fimproving-human-rationality-through-cognitive-change-intro%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Improving%20Human%20Rationality%20Through%20Cognitive%20Change%20(intro)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhR92kW2ZSvmuca5Nf%2Fimproving-human-rationality-through-cognitive-change-intro", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhR92kW2ZSvmuca5Nf%2Fimproving-human-rationality-through-cognitive-change-intro", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1599, "htmlBody": "<p><small>This is the introduction to a paper I started writing long ago, but have since given up on. The paper was going to be an overview of methods for improving human rationality through cognitive change. Since it contains lots of handy references on rationality, I figured I'd publish it, in case it's helpful to others.</small></p>\n<p>&nbsp;</p>\n<h2>1. Introduction</h2>\n<p>During the last half-century, cognitive scientists have catalogued dozens of common errors in human judgment and decision-making (<a href=\"http://www-personal.umich.edu/~gonzo/papers/griffin-etal-2012-heuristics.pdf\">Griffin et al. 2012</a>; <a href=\"http://www.amazon.com/Heuristics-Biases-Psychology-Intuitive-Judgment/dp/0521796792/\">Gilovich et al. 2002</a>). <a href=\"http://www.amazon.com/What-Intelligence-Tests-Miss-Psychology/dp/0300164629/\">Stanovich (1999)</a> provides a sobering introduction:</p>\n<blockquote>\n<p>For example, people assess probabilities incorrectly, they display confirmation bias, they test hypotheses inefficiently, they violate the axioms of utility theory, they do not properly calibrate degrees of belief, they overproject their own opinions onto others, they allow prior knowledge to become implicated in deductive reasoning, they systematically underweight information about nonoccurrence when evaluating covariation, and they display numerous other information-processes biases...</p>\n</blockquote>\n<p>The good news is that researchers have also begun to understand the cognitive mechanisms which produce these errors (<a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637\">Kahneman 2011</a>; <a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/\">Stanovich 2010</a>), they have found several \"debiasing\" techniques that groups or individuals may use to partially avoid or correct these errors (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Larrick 2004</a>), and they have discovered that environmental factors can be used to help people to exhibit fewer errors (<a href=\"http://www.amazon.com/Nudge-Improving-Decisions-Health-Happiness/dp/014311526X/\">Thaler and Sunstein 2009</a>; <a href=\"http://www.amazon.com/The-Empathy-Gap-Building-Bridges/dp/B002CMLR10\">Trout 2009</a>).</p>\n<p>This \"heuristics and biases\" research program teaches us many lessons that, if put into practice, could improve human welfare. Debiasing techniques that improve human rationality may be able to decrease rates of violence caused by ideological extremism (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Lilienfeld-Giving-debiasing-away.pdf\">Lilienfeld et al. 2009</a>). Knowledge of human bias can help executives make more profitable decisions (<a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637\">Kahneman et al. 2011</a>). Scientists with improved judgment and decision-making skills (\"rationality skills\") may be more apt to avoid experimenter bias (<a href=\"http://www.epidemiology.ch/history/PDF%20bg/Sackett%20DL%201979%20bias%20in%20analytic%20research.pdf\">Sackett 1979</a>). Understanding the nature of human reasoning can also improve the practice of philosophy (<a href=\"http://www.annualreviews.org/doi/abs/10.1146/annurev-psych-120710-100350\">Knobe et al. 2012</a>; <a href=\"http://digitallibrary.usc.edu/cdm/compoundobject/collection/p15799coll127/id/260424/rec/16\">Talbot 2009</a>; <a href=\"http://www.amazon.com/Epistemology-Psychology-Judgment-Michael-Bishop/dp/0195162307\">Bishop and Trout 2004</a>; <a href=\"/lw/fpe/philosophy_needs_to_trust_your_rationality_even/\">Muehlhauser 2012</a>), which has too often made false assumptions about how the mind reasons (<a href=\"http://ruccs.rutgers.edu/ArchiveFolder/Research%20Group/Publications/NEI/NEIPT.html\">Weinberg et al. 2001</a>; <a href=\"http://www.amazon.com/Philosophy-Flesh-Embodied-Challenge-Western/dp/0465056741\">Lakoff and Johnson 1999</a>; <a href=\"http://www.amazon.com/Rethinking-Intuition-Psychology-Philosophical-Epistemology/dp/0847687961\">De Paul and Ramsey 1999</a>). Finally, improved rationality could help decision makers to choose better policies, especially in domains likely by their very nature to trigger biased thinking, such as investing (<a href=\"http://www.amazon.com/Mean-Markets-Lizard-Brains-Irrationality/dp/0470343761\">Burnham 2008</a>), military command (<a href=\"http://www.masshightech.com/stories/2011/11/14/daily30-Raytheon-BBN-lands-105M-game-development-deal.html\">Lang 2011</a>; <a href=\"http://www.au.af.mil/au/awc/awcgate/milreview/williams_bias_mil_d-m.pdf\">Williams 2010</a>; <a href=\"http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA493560\">Janser 2007</a>), intelligence analysis (<a href=\"https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/PsychofIntelNew.pdf\">Heuer 1999</a>), or the study of global catastrophic risks (<a href=\"http://intelligence.org/files/CognitiveBiases.pdf\">Yudkowsky 2008a</a>).</p>\n<p>But is it possible to improve human rationality? The answer, it seems, is \"Yes.\" Lovallo and Sibony (<a href=\"http://www.veruspartners.net/private/app/webroot/files/cabe10.pdf\">2010</a>) showed that when organizations worked to reduce the effect of bias on their investment decisions, they achieved returns of 7% or higher. Multiple studies suggest that a simple instruction to \"think about alternative hypotheses\" can counteract overconfidence, confirmation bias, and anchoring effects, leading to more accurate judgments (<a href=\"http://soco.uni-koeln.de/scc4/documents/pspb26.pdf\">Mussweiler et al. 2000</a>; <a href=\"http://watarts.uwaterloo.ca/~dkoehler/reprints/hypothgen.pdf\">Koehler 1994</a>; <a href=\"http://www.communicationcache.com/uploads/1/0/8/8/10887248/reasons_for_confidence.pdf\">Koriat et al. 1980</a>). Merely warning people about biases can decrease their prevalence, at least with regard to framing effects (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Chen-Wu-Debiasing-the-framing-effect-the-effect-of-warning-and-involvement.pdf\">Cheng and Wu 2010</a>), hindsight bias (<a href=\"http://www.psych.utoronto.ca/users/hasher/PDF/I%20knew%20it%20all%20along%20or%20did%20I%20Hasher%20et%20al%201981.pdf\">Hasher et al. 1981</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Reimers-Butler-The-effect-of-outcome-knowledge-on-auditors-judgmental-evaluations.pdf\">Reimers and Butler 1992</a>), and the outcome effect (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Clarkson-et-al-Debiasing-the-outcome-effect.pdf\">Clarkson et al. 2002</a>). Several other methods have been shown to meliorate the effects of common human biases (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Larrick 2004</a>). Judgment and decision-making appear to be skills that can be learned and improved with practice (<a href=\"http://www.amazon.com/Judgment-Decision-Making-Skill-ebook/dp/B007I2AHBU\">Dhami et al. 2012</a>).</p>\n<p>In this article, I first explain what I mean by \"rationality\" as a normative concept. I then review the state of our knowledge concerning the causes of human errors in judgment and decision-making (JDM). The largest section of our article summarizes what we currently know about how to improve human rationality through cognitive change (e.g. \"rationality training\"). We conclude by assessing the prospects for improving human rationality through cognitive change, and by recommending particular avenues for future research.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h2>2. Normative Rationality</h2>\n<p>In cognitive science, rationality is a normative concept (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Stanovich-Normative-Models-in-Psychology-Are-Here-to-Stay.pdf\">Stanovich 2011</a>). As Stanovich (<a href=\"http://www.keithstanovich.com/Site/Research_on_Reasoning_files/Stanovich_Oxford_Handbook.pdf\">2012</a>) explains, \"When a cognitive scientist terms a behavior irrational he/she means that the behavior departs from the optimum prescribed by a particular normative model.\"</p>\n<p>This normative model of rationality consists in logic, probability theory, and rational choice theory. In their opening chapter for <em>The Oxford Handbook of Thinking and Reasoning</em>, Chater and Oaksford (<a href=\"http://www.amazon.com/Handbook-Thinking-Reasoning-Library-Psychology/dp/0199734682\">2012</a>) explain:</p>\n<blockquote>\n<p>Is it meaningful to attempt to develop a general theory of rationality at all? We might tentatively suggest that it is a prima facie sign of irrationality to believe in alien abduction, or to will a sports team to win in order to increase their chance of victory. But these views or actions might be entirely rational, given suitably nonstandard background beliefs about other alien activity and the general efficacy of psychic powers. Irrationality may, though, be ascribed if there is a clash between a particular belief or behavior and such background assumptions. Thus, a thorough-going physicalist may, perhaps, be accused of irrationality if she simultaneously believes in psychic powers. A theory of rationality cannot, therefore, be viewed as clarifying either what people should believe or how people should act&mdash;but it can determine whether beliefs and behaviors are compatible. Similarly, a theory of rational choice cannot determine whether it is rational to smoke or to exercise daily; but it might clarify whether a particular choice is compatible with other beliefs and choices.</p>\n<p>From this viewpoint, normative theories can be viewed as clarifying conditions of consistency&hellip; Logic can be viewed as studying the notion of consistency over beliefs. Probability&hellip; studies consistency over degrees of belief. Rational choice theory studies the consistency of beliefs and values with choices.</p>\n</blockquote>\n<p>There are many good tutorials on logic (<a href=\"http://www.amazon.com/Classical-Nonclassical-Logics-Introduction-Propositions/dp/0691122792\">Schechter 2005</a>), probability theory (<a href=\"http://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193\">Koller and Friedman 2009</a>), and rational choice theory (<a href=\"http://www.amazon.com/Choice-Theory-Very-Short-Introduction/dp/0192803034\">Allington 2002</a>; <a href=\"http://www.amazon.com/Decision-Theory-Principles-Approaches-Probability/dp/047149657X\">Parmigiani and Inoue 2009</a>), so I will make only two quick points here. First, by \"probability\" I mean the subjective or Bayesian interpretation of probability, because that is the interpretation which captures degrees of belief (<a href=\"http://www.amazon.com/Bayesian-Rationality-Probabilistic-Reasoning-Cognitive/dp/0198524498\">Oaksford and Chater 2007</a>; <a href=\"http://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712\">Jaynes 2003</a>; <a href=\"http://jimbeck.caltech.edu/summerlectures/references/ProbabilityFrequencyReasonableExpectation.pdf\">Cox 1946</a>). Second, in rational choice theory I am of course endorsing the normative principle of expected utility maximization (<a href=\"http://pavroz.ru/files/handbookofrationalandsocialchoice.pdf\">Grant &amp; Zandt 2009</a>).</p>\n<p>According to this concept of rationality, then, an agent is rational if its beliefs are consistent with the laws of logic and probability theory and its decisions are consistent with the laws of rational choice theory. An agent is irrational to the degree that its beliefs violate the laws of logic or probability theory, or its decisions violate the laws of rational choice theory.<sup>1</sup></p>\n<p>Researchers working in the heuristics and biases tradition have shown that humans regularly violate the norms of rationality (<a href=\"http://www.amazon.com/Thinking-Reasoning-Introduction-Psychology-Judgment/dp/1841697419\">Manktelow 2012</a>; <a href=\"http://www.amazon.com/Cognitive-Illusions-Handbook-Fallacies-Judgement/dp/1841693510\">Pohl 2005</a>). These researchers tend to assume that human reasoning could be improved, and thus they have been called \"Meliorists\" (<a href=\"http://rfwest.net/Site_2/Welcome_files/Stanovich-Under-Accept-CP99.pdf\">Stanovich 1999</a>, <a href=\"http://www.amazon.com/Robots-Rebellion-Finding-Meaning-Darwin/dp/0226771253\">2004</a>), and their program of using psychological findings to make recommendations for improving human reasoning has been called \"ameliorative psychology\" (<a href=\"http://www.amazon.com/dp/0195162307\">Bishop and Trout 2004</a>).</p>\n<p>Another group of researchers, termed the \"Panglossians,\"<sup>2</sup> argue that human performance is generally \"rational\" because it manifests an evolutionary adaptation for optimal information processing (<a href=\"http://www.dangoldstein.com/papers/Czerlinski_How_Good_Are_Simple_Heuristics_Chapter_1999.pdf\">Gigerenzer et al. 1999</a>).</p>\n<p>I disagree with the Panglossian view for reasons detailed elsewhere (<a href=\"http://books.google.com/books?id=DwZTCoV3cP4C&amp;pg=PA27&amp;lpg=PA27&amp;dq=%22in+contrast+to+the+emphasis+on+errors+in+much+of+the+literature+on+judgment+and+decision-making%22&amp;source=bl&amp;ots=jyVv6JLtz_&amp;sig=In2N57M0AHAhPI3htZ23SqH5w3w&amp;hl=en&amp;sa=X&amp;ei=xV4pUay9KI3LigLhsICwAg&amp;ved=0CDMQ6AEwAA#v=onepage&amp;q=%22in%20contrast%20to%20the%20emphasis%20on%20errors%20in%20much%20of%20the%20literature%20on%20judgment%20and%20decision-making%22&amp;f=false\">Griffiths et al. 2012:27</a>; <a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/\">Stanovich 2010, ch. 1</a>; <a href=\"http://www.semioticon.com/virtuals/imitation/kstanovich_paper.pdf\">Stanovich and West 2003</a>; <a href=\"http://www.amazon.com/Without-Good-Reason-Rationality-Philosophy/dp/0198235747\">Stein 1996</a>), though I also believe the original dispute between Meliorists and Panglossians has been exaggerated (<a href=\"http://sas-space.sas.ac.uk/938/1/R_Samuels_Rationality.pdf\">Samuels et al. 2002</a>). In any case, a verbal dispute over what counts as \"normative\" for human JDM need not detain us here.<sup>3</sup> I have stipulated my definition of normative rationality &mdash; for the purposes of cognitive psychology &mdash; above. MY concern is with the question of whether cognitive change can improve human JDM in ways that enable humans to achieve their goals more effectively than without cognitive change, and it seems (as I demonstrate below) that the answer is \"yes.\"</p>\n<p>MY view of normative rationality does not imply, however, that humans ought to explicitly use the laws of rational choice theory to make every decision. Neither humans nor machines have the knowledge and resources to do so (<a href=\"http://staff.science.uva.nl/~szymanik/papers/TractableCognition.pdf\">Van Rooij 2008</a>; <a href=\"http://www.worldscientific.com/doi/pdf/10.1142/S1793843011000686\">Wang 2011</a>). Thus, in order to approximate normative rationality as best we can, we often (rationally) engage in a \"bounded rationality\" (<a href=\"http://www.math.mcgill.ca/vetta/CS764.dir/bounded.pdf\">Simon 1957</a>) or \"ecological rationality\" (<a href=\"http://www.amazon.com/Ecological-Rationality-Intelligence-Evolution-Cognition/dp/0195315448\">Gigerenzer and Todd 2012</a>) or \"grounded rationality\" (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Elqayam-Grounded-rationality-A-relativist-framework-for-normative-rationality.pdf\">Elqayam 2011</a>) that employs simple heuristics to imperfectly achieve our goals with the limited knowledge and resources at our disposal (<a href=\"http://people.csail.mit.edu/kraemer/draft-vul.pdf\">Vul 2010</a>; <a href=\"http://cocosci.berkeley.edu/tom/papers/oneanddone.pdf\">Vul et al. 2009</a>; <a href=\"http://mba.yale.edu/faculty/pdf/fredericks_Model_of_Heuristic_Judgment.pdf\">Kahneman and Frederick 2005</a>). Thus, the best prescription for human reasoning is not necessarily to always use the normative model to govern one's thinking (<a href=\"http://pavroz.ru/files/handbookofrationalandsocialchoice.pdf\">Grant &amp; Zandt 2009</a>; <a href=\"http://www.amazon.com/Who-Rational-individual-Differences-Reasoning/dp/0805824731\">Stanovich 1999</a>; <a href=\"http://www.amazon.com/Rationality-Intelligence-Jonathan-Baron/dp/052126717X\">Baron 1985</a>). Baron (<a href=\"http://www.amazon.com/Thinking-Deciding-Jonathan-Baron/dp/0521680433\">2008, ch. 2</a>) explains:</p>\n<blockquote>\n<p>In short, normative models tell us how to evaluate judgments and decisions in terms of their departure from an ideal standard. Descriptive models specify what people in a particular culture actually do and how they deviate from the normative models. Prescriptive models are designs or inventions, whose purpose is to bring the results of actual thinking into closer conformity to the normative model. If prescriptive recommendations derived in this way are successful, the study of thinking can help people to become better thinkers.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>[next, I was going to discuss the probable causes of JDM errors, tested methods for amelioration, and promising avenues for further research]</p>\n<p>&nbsp;</p>\n<h3>Notes</h3>\n<p><small><sup>1</sup> For a survey of other conceptions of rationality, see <a href=\"http://www.amazon.com/Aspects-Rationality-Reflections-Rational-Whether/dp/1841694878/\">Nickerson (2007)</a>. Note also that our concept of rationality is personal, not subpersonal (<a href=\"http://www.open.ac.uk/Arts/philos/Systems_and_levels_preprint.pdf\">Frankish 2009</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Davies-Interaction-without-Reduction-the-relationship-between-personal-and-sub-personal-levels-of-description.pdf\">Davies 2000</a>; <a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/\">Stanovich 2010</a>:5).</small></p>\n<p><small><sup>2</sup> The adjective \"Panglossian\" was originally applied by <a href=\"http://rspb.royalsocietypublishing.org/content/205/1161/581.full.pdf\">Steven Jay Gould and Richard Lewontin (1979)</a>, who used it to describe knee-jerk appeals to natural selection as the force that explains every trait. The term comes from Voltaire's character Dr. Pangloss, who said that \"our noses were made to carry spectacles\" <a href=\"http://www.amazon.com/Candide-Voltaire/dp/1478205148/\">(Voltaire 1759)</a>.</small></p>\n<p><small><sup>3</sup> To resolve such verbal disputes we can employ the \"method of elimination\" (<a href=\"http://consc.net/oxford/chap9.pdf\">Chalmers 2011</a>) or, as <a href=\"/lw/nv/replace_the_symbol_with_the_substance/\">Yudkowsky (2008)</a> put it, we can \"replace the symbol with the substance.\"</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hR92kW2ZSvmuca5Nf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "21749", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>This is the introduction to a paper I started writing long ago, but have since given up on. The paper was going to be an overview of methods for improving human rationality through cognitive change. Since it contains lots of handy references on rationality, I figured I'd publish it, in case it's helpful to others.</small></p>\n<p>&nbsp;</p>\n<h2 id=\"1__Introduction\">1. Introduction</h2>\n<p>During the last half-century, cognitive scientists have catalogued dozens of common errors in human judgment and decision-making (<a href=\"http://www-personal.umich.edu/~gonzo/papers/griffin-etal-2012-heuristics.pdf\">Griffin et al. 2012</a>; <a href=\"http://www.amazon.com/Heuristics-Biases-Psychology-Intuitive-Judgment/dp/0521796792/\">Gilovich et al. 2002</a>). <a href=\"http://www.amazon.com/What-Intelligence-Tests-Miss-Psychology/dp/0300164629/\">Stanovich (1999)</a> provides a sobering introduction:</p>\n<blockquote>\n<p>For example, people assess probabilities incorrectly, they display confirmation bias, they test hypotheses inefficiently, they violate the axioms of utility theory, they do not properly calibrate degrees of belief, they overproject their own opinions onto others, they allow prior knowledge to become implicated in deductive reasoning, they systematically underweight information about nonoccurrence when evaluating covariation, and they display numerous other information-processes biases...</p>\n</blockquote>\n<p>The good news is that researchers have also begun to understand the cognitive mechanisms which produce these errors (<a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637\">Kahneman 2011</a>; <a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/\">Stanovich 2010</a>), they have found several \"debiasing\" techniques that groups or individuals may use to partially avoid or correct these errors (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Larrick 2004</a>), and they have discovered that environmental factors can be used to help people to exhibit fewer errors (<a href=\"http://www.amazon.com/Nudge-Improving-Decisions-Health-Happiness/dp/014311526X/\">Thaler and Sunstein 2009</a>; <a href=\"http://www.amazon.com/The-Empathy-Gap-Building-Bridges/dp/B002CMLR10\">Trout 2009</a>).</p>\n<p>This \"heuristics and biases\" research program teaches us many lessons that, if put into practice, could improve human welfare. Debiasing techniques that improve human rationality may be able to decrease rates of violence caused by ideological extremism (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Lilienfeld-Giving-debiasing-away.pdf\">Lilienfeld et al. 2009</a>). Knowledge of human bias can help executives make more profitable decisions (<a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637\">Kahneman et al. 2011</a>). Scientists with improved judgment and decision-making skills (\"rationality skills\") may be more apt to avoid experimenter bias (<a href=\"http://www.epidemiology.ch/history/PDF%20bg/Sackett%20DL%201979%20bias%20in%20analytic%20research.pdf\">Sackett 1979</a>). Understanding the nature of human reasoning can also improve the practice of philosophy (<a href=\"http://www.annualreviews.org/doi/abs/10.1146/annurev-psych-120710-100350\">Knobe et al. 2012</a>; <a href=\"http://digitallibrary.usc.edu/cdm/compoundobject/collection/p15799coll127/id/260424/rec/16\">Talbot 2009</a>; <a href=\"http://www.amazon.com/Epistemology-Psychology-Judgment-Michael-Bishop/dp/0195162307\">Bishop and Trout 2004</a>; <a href=\"/lw/fpe/philosophy_needs_to_trust_your_rationality_even/\">Muehlhauser 2012</a>), which has too often made false assumptions about how the mind reasons (<a href=\"http://ruccs.rutgers.edu/ArchiveFolder/Research%20Group/Publications/NEI/NEIPT.html\">Weinberg et al. 2001</a>; <a href=\"http://www.amazon.com/Philosophy-Flesh-Embodied-Challenge-Western/dp/0465056741\">Lakoff and Johnson 1999</a>; <a href=\"http://www.amazon.com/Rethinking-Intuition-Psychology-Philosophical-Epistemology/dp/0847687961\">De Paul and Ramsey 1999</a>). Finally, improved rationality could help decision makers to choose better policies, especially in domains likely by their very nature to trigger biased thinking, such as investing (<a href=\"http://www.amazon.com/Mean-Markets-Lizard-Brains-Irrationality/dp/0470343761\">Burnham 2008</a>), military command (<a href=\"http://www.masshightech.com/stories/2011/11/14/daily30-Raytheon-BBN-lands-105M-game-development-deal.html\">Lang 2011</a>; <a href=\"http://www.au.af.mil/au/awc/awcgate/milreview/williams_bias_mil_d-m.pdf\">Williams 2010</a>; <a href=\"http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA493560\">Janser 2007</a>), intelligence analysis (<a href=\"https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/PsychofIntelNew.pdf\">Heuer 1999</a>), or the study of global catastrophic risks (<a href=\"http://intelligence.org/files/CognitiveBiases.pdf\">Yudkowsky 2008a</a>).</p>\n<p>But is it possible to improve human rationality? The answer, it seems, is \"Yes.\" Lovallo and Sibony (<a href=\"http://www.veruspartners.net/private/app/webroot/files/cabe10.pdf\">2010</a>) showed that when organizations worked to reduce the effect of bias on their investment decisions, they achieved returns of 7% or higher. Multiple studies suggest that a simple instruction to \"think about alternative hypotheses\" can counteract overconfidence, confirmation bias, and anchoring effects, leading to more accurate judgments (<a href=\"http://soco.uni-koeln.de/scc4/documents/pspb26.pdf\">Mussweiler et al. 2000</a>; <a href=\"http://watarts.uwaterloo.ca/~dkoehler/reprints/hypothgen.pdf\">Koehler 1994</a>; <a href=\"http://www.communicationcache.com/uploads/1/0/8/8/10887248/reasons_for_confidence.pdf\">Koriat et al. 1980</a>). Merely warning people about biases can decrease their prevalence, at least with regard to framing effects (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Chen-Wu-Debiasing-the-framing-effect-the-effect-of-warning-and-involvement.pdf\">Cheng and Wu 2010</a>), hindsight bias (<a href=\"http://www.psych.utoronto.ca/users/hasher/PDF/I%20knew%20it%20all%20along%20or%20did%20I%20Hasher%20et%20al%201981.pdf\">Hasher et al. 1981</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Reimers-Butler-The-effect-of-outcome-knowledge-on-auditors-judgmental-evaluations.pdf\">Reimers and Butler 1992</a>), and the outcome effect (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Clarkson-et-al-Debiasing-the-outcome-effect.pdf\">Clarkson et al. 2002</a>). Several other methods have been shown to meliorate the effects of common human biases (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Larrick 2004</a>). Judgment and decision-making appear to be skills that can be learned and improved with practice (<a href=\"http://www.amazon.com/Judgment-Decision-Making-Skill-ebook/dp/B007I2AHBU\">Dhami et al. 2012</a>).</p>\n<p>In this article, I first explain what I mean by \"rationality\" as a normative concept. I then review the state of our knowledge concerning the causes of human errors in judgment and decision-making (JDM). The largest section of our article summarizes what we currently know about how to improve human rationality through cognitive change (e.g. \"rationality training\"). We conclude by assessing the prospects for improving human rationality through cognitive change, and by recommending particular avenues for future research.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h2 id=\"2__Normative_Rationality\">2. Normative Rationality</h2>\n<p>In cognitive science, rationality is a normative concept (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Stanovich-Normative-Models-in-Psychology-Are-Here-to-Stay.pdf\">Stanovich 2011</a>). As Stanovich (<a href=\"http://www.keithstanovich.com/Site/Research_on_Reasoning_files/Stanovich_Oxford_Handbook.pdf\">2012</a>) explains, \"When a cognitive scientist terms a behavior irrational he/she means that the behavior departs from the optimum prescribed by a particular normative model.\"</p>\n<p>This normative model of rationality consists in logic, probability theory, and rational choice theory. In their opening chapter for <em>The Oxford Handbook of Thinking and Reasoning</em>, Chater and Oaksford (<a href=\"http://www.amazon.com/Handbook-Thinking-Reasoning-Library-Psychology/dp/0199734682\">2012</a>) explain:</p>\n<blockquote>\n<p>Is it meaningful to attempt to develop a general theory of rationality at all? We might tentatively suggest that it is a prima facie sign of irrationality to believe in alien abduction, or to will a sports team to win in order to increase their chance of victory. But these views or actions might be entirely rational, given suitably nonstandard background beliefs about other alien activity and the general efficacy of psychic powers. Irrationality may, though, be ascribed if there is a clash between a particular belief or behavior and such background assumptions. Thus, a thorough-going physicalist may, perhaps, be accused of irrationality if she simultaneously believes in psychic powers. A theory of rationality cannot, therefore, be viewed as clarifying either what people should believe or how people should act\u2014but it can determine whether beliefs and behaviors are compatible. Similarly, a theory of rational choice cannot determine whether it is rational to smoke or to exercise daily; but it might clarify whether a particular choice is compatible with other beliefs and choices.</p>\n<p>From this viewpoint, normative theories can be viewed as clarifying conditions of consistency\u2026 Logic can be viewed as studying the notion of consistency over beliefs. Probability\u2026 studies consistency over degrees of belief. Rational choice theory studies the consistency of beliefs and values with choices.</p>\n</blockquote>\n<p>There are many good tutorials on logic (<a href=\"http://www.amazon.com/Classical-Nonclassical-Logics-Introduction-Propositions/dp/0691122792\">Schechter 2005</a>), probability theory (<a href=\"http://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193\">Koller and Friedman 2009</a>), and rational choice theory (<a href=\"http://www.amazon.com/Choice-Theory-Very-Short-Introduction/dp/0192803034\">Allington 2002</a>; <a href=\"http://www.amazon.com/Decision-Theory-Principles-Approaches-Probability/dp/047149657X\">Parmigiani and Inoue 2009</a>), so I will make only two quick points here. First, by \"probability\" I mean the subjective or Bayesian interpretation of probability, because that is the interpretation which captures degrees of belief (<a href=\"http://www.amazon.com/Bayesian-Rationality-Probabilistic-Reasoning-Cognitive/dp/0198524498\">Oaksford and Chater 2007</a>; <a href=\"http://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712\">Jaynes 2003</a>; <a href=\"http://jimbeck.caltech.edu/summerlectures/references/ProbabilityFrequencyReasonableExpectation.pdf\">Cox 1946</a>). Second, in rational choice theory I am of course endorsing the normative principle of expected utility maximization (<a href=\"http://pavroz.ru/files/handbookofrationalandsocialchoice.pdf\">Grant &amp; Zandt 2009</a>).</p>\n<p>According to this concept of rationality, then, an agent is rational if its beliefs are consistent with the laws of logic and probability theory and its decisions are consistent with the laws of rational choice theory. An agent is irrational to the degree that its beliefs violate the laws of logic or probability theory, or its decisions violate the laws of rational choice theory.<sup>1</sup></p>\n<p>Researchers working in the heuristics and biases tradition have shown that humans regularly violate the norms of rationality (<a href=\"http://www.amazon.com/Thinking-Reasoning-Introduction-Psychology-Judgment/dp/1841697419\">Manktelow 2012</a>; <a href=\"http://www.amazon.com/Cognitive-Illusions-Handbook-Fallacies-Judgement/dp/1841693510\">Pohl 2005</a>). These researchers tend to assume that human reasoning could be improved, and thus they have been called \"Meliorists\" (<a href=\"http://rfwest.net/Site_2/Welcome_files/Stanovich-Under-Accept-CP99.pdf\">Stanovich 1999</a>, <a href=\"http://www.amazon.com/Robots-Rebellion-Finding-Meaning-Darwin/dp/0226771253\">2004</a>), and their program of using psychological findings to make recommendations for improving human reasoning has been called \"ameliorative psychology\" (<a href=\"http://www.amazon.com/dp/0195162307\">Bishop and Trout 2004</a>).</p>\n<p>Another group of researchers, termed the \"Panglossians,\"<sup>2</sup> argue that human performance is generally \"rational\" because it manifests an evolutionary adaptation for optimal information processing (<a href=\"http://www.dangoldstein.com/papers/Czerlinski_How_Good_Are_Simple_Heuristics_Chapter_1999.pdf\">Gigerenzer et al. 1999</a>).</p>\n<p>I disagree with the Panglossian view for reasons detailed elsewhere (<a href=\"http://books.google.com/books?id=DwZTCoV3cP4C&amp;pg=PA27&amp;lpg=PA27&amp;dq=%22in+contrast+to+the+emphasis+on+errors+in+much+of+the+literature+on+judgment+and+decision-making%22&amp;source=bl&amp;ots=jyVv6JLtz_&amp;sig=In2N57M0AHAhPI3htZ23SqH5w3w&amp;hl=en&amp;sa=X&amp;ei=xV4pUay9KI3LigLhsICwAg&amp;ved=0CDMQ6AEwAA#v=onepage&amp;q=%22in%20contrast%20to%20the%20emphasis%20on%20errors%20in%20much%20of%20the%20literature%20on%20judgment%20and%20decision-making%22&amp;f=false\">Griffiths et al. 2012:27</a>; <a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/\">Stanovich 2010, ch. 1</a>; <a href=\"http://www.semioticon.com/virtuals/imitation/kstanovich_paper.pdf\">Stanovich and West 2003</a>; <a href=\"http://www.amazon.com/Without-Good-Reason-Rationality-Philosophy/dp/0198235747\">Stein 1996</a>), though I also believe the original dispute between Meliorists and Panglossians has been exaggerated (<a href=\"http://sas-space.sas.ac.uk/938/1/R_Samuels_Rationality.pdf\">Samuels et al. 2002</a>). In any case, a verbal dispute over what counts as \"normative\" for human JDM need not detain us here.<sup>3</sup> I have stipulated my definition of normative rationality \u2014 for the purposes of cognitive psychology \u2014 above. MY concern is with the question of whether cognitive change can improve human JDM in ways that enable humans to achieve their goals more effectively than without cognitive change, and it seems (as I demonstrate below) that the answer is \"yes.\"</p>\n<p>MY view of normative rationality does not imply, however, that humans ought to explicitly use the laws of rational choice theory to make every decision. Neither humans nor machines have the knowledge and resources to do so (<a href=\"http://staff.science.uva.nl/~szymanik/papers/TractableCognition.pdf\">Van Rooij 2008</a>; <a href=\"http://www.worldscientific.com/doi/pdf/10.1142/S1793843011000686\">Wang 2011</a>). Thus, in order to approximate normative rationality as best we can, we often (rationally) engage in a \"bounded rationality\" (<a href=\"http://www.math.mcgill.ca/vetta/CS764.dir/bounded.pdf\">Simon 1957</a>) or \"ecological rationality\" (<a href=\"http://www.amazon.com/Ecological-Rationality-Intelligence-Evolution-Cognition/dp/0195315448\">Gigerenzer and Todd 2012</a>) or \"grounded rationality\" (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Elqayam-Grounded-rationality-A-relativist-framework-for-normative-rationality.pdf\">Elqayam 2011</a>) that employs simple heuristics to imperfectly achieve our goals with the limited knowledge and resources at our disposal (<a href=\"http://people.csail.mit.edu/kraemer/draft-vul.pdf\">Vul 2010</a>; <a href=\"http://cocosci.berkeley.edu/tom/papers/oneanddone.pdf\">Vul et al. 2009</a>; <a href=\"http://mba.yale.edu/faculty/pdf/fredericks_Model_of_Heuristic_Judgment.pdf\">Kahneman and Frederick 2005</a>). Thus, the best prescription for human reasoning is not necessarily to always use the normative model to govern one's thinking (<a href=\"http://pavroz.ru/files/handbookofrationalandsocialchoice.pdf\">Grant &amp; Zandt 2009</a>; <a href=\"http://www.amazon.com/Who-Rational-individual-Differences-Reasoning/dp/0805824731\">Stanovich 1999</a>; <a href=\"http://www.amazon.com/Rationality-Intelligence-Jonathan-Baron/dp/052126717X\">Baron 1985</a>). Baron (<a href=\"http://www.amazon.com/Thinking-Deciding-Jonathan-Baron/dp/0521680433\">2008, ch. 2</a>) explains:</p>\n<blockquote>\n<p>In short, normative models tell us how to evaluate judgments and decisions in terms of their departure from an ideal standard. Descriptive models specify what people in a particular culture actually do and how they deviate from the normative models. Prescriptive models are designs or inventions, whose purpose is to bring the results of actual thinking into closer conformity to the normative model. If prescriptive recommendations derived in this way are successful, the study of thinking can help people to become better thinkers.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>[next, I was going to discuss the probable causes of JDM errors, tested methods for amelioration, and promising avenues for further research]</p>\n<p>&nbsp;</p>\n<h3 id=\"Notes\">Notes</h3>\n<p><small><sup>1</sup> For a survey of other conceptions of rationality, see <a href=\"http://www.amazon.com/Aspects-Rationality-Reflections-Rational-Whether/dp/1841694878/\">Nickerson (2007)</a>. Note also that our concept of rationality is personal, not subpersonal (<a href=\"http://www.open.ac.uk/Arts/philos/Systems_and_levels_preprint.pdf\">Frankish 2009</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/02/Davies-Interaction-without-Reduction-the-relationship-between-personal-and-sub-personal-levels-of-description.pdf\">Davies 2000</a>; <a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/\">Stanovich 2010</a>:5).</small></p>\n<p><small><sup>2</sup> The adjective \"Panglossian\" was originally applied by <a href=\"http://rspb.royalsocietypublishing.org/content/205/1161/581.full.pdf\">Steven Jay Gould and Richard Lewontin (1979)</a>, who used it to describe knee-jerk appeals to natural selection as the force that explains every trait. The term comes from Voltaire's character Dr. Pangloss, who said that \"our noses were made to carry spectacles\" <a href=\"http://www.amazon.com/Candide-Voltaire/dp/1478205148/\">(Voltaire 1759)</a>.</small></p>\n<p><small><sup>3</sup> To resolve such verbal disputes we can employ the \"method of elimination\" (<a href=\"http://consc.net/oxford/chap9.pdf\">Chalmers 2011</a>) or, as <a href=\"/lw/nv/replace_the_symbol_with_the_substance/\">Yudkowsky (2008)</a> put it, we can \"replace the symbol with the substance.\"</small></p>", "sections": [{"title": "1. Introduction", "anchor": "1__Introduction", "level": 1}, {"title": "2. Normative Rationality", "anchor": "2__Normative_Rationality", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2pdyL8bSGBfYsnkyS", "GKfPL6LQFgB49FEnv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-24T06:55:11.209Z", "modifiedAt": null, "url": null, "title": "Extrapolating an Obscurantist's Volition", "slug": "extrapolating-an-obscurantist-s-volition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "26wd2Xf6JTfh3CjfQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RurmwHzrcorG3NafM/extrapolating-an-obscurantist-s-volition", "pageUrlRelative": "/posts/RurmwHzrcorG3NafM/extrapolating-an-obscurantist-s-volition", "linkUrl": "https://www.lesswrong.com/posts/RurmwHzrcorG3NafM/extrapolating-an-obscurantist-s-volition", "postedAtFormatted": "Sunday, February 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Extrapolating%20an%20Obscurantist's%20Volition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExtrapolating%20an%20Obscurantist's%20Volition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRurmwHzrcorG3NafM%2Fextrapolating-an-obscurantist-s-volition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Extrapolating%20an%20Obscurantist's%20Volition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRurmwHzrcorG3NafM%2Fextrapolating-an-obscurantist-s-volition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRurmwHzrcorG3NafM%2Fextrapolating-an-obscurantist-s-volition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 310, "htmlBody": "<p>Consider the case of an obscurantist i.e. an irrational agent who is proudly ignorant and opposes the spread of (certain) knowledge even to themselves. First of all, is the existence of such an agent implausible? Not really, considering there are masochists out there and that, to some individuals, ignorance is bliss. How much, then, will be left of an obscurantist's identity upon coherently extrapolating their desires? The answers is probably not much, if anything at all.<br /><br />Do this force us to renounce to the idea of personal CEV? Hardly so. Instead, do we decry the legitimacy of the obscurantist's desires? Perhaps, but a convincing argument must be provided for the ethical aspects of such a line of thought; a utilitarian could draw support from the societal benefits of increased epistemic hygiene in the absence of obscurantists.<br /><br />In any case, this (admittedly contrived) example illustrates that there are pressing issues regarding CEV and personal identity. Also, on a related note, I recently heard a leading decision theorist say that their greatest concern with Ideal Advisor Theories was how desires become no longer the individuals' but, rather, those of their advisors; it may well be the case that personal CEV incurs in the same issues, at least under the obscurantist's conditions.<br /><br />The limiting case above also reveals a subtle interplay between knowledge and volition; our desires might (implicitly) involve not wanting to know certain propositions, wanting to not know certain propositions, not wanting to act as if we knew certain propositions, wanting to act as if we did not know certain propositions.<br /><br />What I just presented is not a rejection of the idea of personal CEV or similar desire-satisfaction theories of well-being, rather it aims to be a pointer to complications one must keep in mind when developing such proposals.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RurmwHzrcorG3NafM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -2, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "21750", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-24T19:12:57.435Z", "modifiedAt": null, "url": null, "title": "Let's make a \"Rational Immortalist Sequence\". Suggested Structure. ", "slug": "let-s-make-a-rational-immortalist-sequence-suggested", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.054Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cnmx4TAZQdqs3dwrS/let-s-make-a-rational-immortalist-sequence-suggested", "pageUrlRelative": "/posts/cnmx4TAZQdqs3dwrS/let-s-make-a-rational-immortalist-sequence-suggested", "linkUrl": "https://www.lesswrong.com/posts/cnmx4TAZQdqs3dwrS/let-s-make-a-rational-immortalist-sequence-suggested", "postedAtFormatted": "Sunday, February 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let's%20make%20a%20%22Rational%20Immortalist%20Sequence%22.%20Suggested%20Structure.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet's%20make%20a%20%22Rational%20Immortalist%20Sequence%22.%20Suggested%20Structure.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcnmx4TAZQdqs3dwrS%2Flet-s-make-a-rational-immortalist-sequence-suggested%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let's%20make%20a%20%22Rational%20Immortalist%20Sequence%22.%20Suggested%20Structure.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcnmx4TAZQdqs3dwrS%2Flet-s-make-a-rational-immortalist-sequence-suggested", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcnmx4TAZQdqs3dwrS%2Flet-s-make-a-rational-immortalist-sequence-suggested", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 797, "htmlBody": "<p>&nbsp;</p>\n<p><a href=\"http://www.overcomingbias.com/2012/12/why-dont-futurists-try-harder-to-stay-alive.html\">Why Don't Futurists Try Harder to Stay Alive?</a>, asks Rob Wiblin at Overcoming Bias</p>\n<p>Suppose you want to live for more than 10 thousand years. (I'll assume that suffices for the \"immortalist\" designation). Many here do.</p>\n<p>Suppose in addition that this is <em>by far, very far</em>, your most important goal. You'd sacrifice a lot for it. Not all, but a lot.</p>\n<p>How would you go about your daily life? In which direction would you change it?</p>\n<p>I want to examine this in a sequence, but I don't want to write it on my own, I'd like to do it with at least one person. I'll lay out the structure for the sequence here, and <strong>anyone who wants to help, by writing an entire post (these or others), or parts of many</strong>, please contact me in the comments, or message. Obviously we don't need all these posts, they are just suggestions. The sequence won't be about whether it is a good idea to do that. Just assume that the person wants to achieve some form of Longevity Escape Velocity. Take as a given that it is what an agent wants, what should she do?</p>\n<p>&nbsp;</p>\n<p>1) The Ideal Simple Egoistic Immortalist - I'll write this one, the rest is up for grabs.</p>\n<p>Describes the general goal of living long, explains it is not about living long in hell, about finding mathy or Nozickian paradoxes, about solving the moral uncertainty problem. It is just simply trying to somehow achieve a very long life worth living. Describes the two main classes of optimization 1)Optimizing your access to the resources that will grant immortality 2)Optimizing the world so that immortality happens faster.&nbsp; Sets \"3)Diminish X-risk\" aside for the moment, and moves on with a comparison of the two major classes.&nbsp;</p>\n<p>&nbsp;</p>\n<p>2) Everything else is for nothing if A is not the case -</p>\n<p>Shows the weaker points (A's) of different strategies. What if uploads don't inherit the properties in virtue of which we'd like to be preserved? What if cryonics facilities are destroyed by enraged people? What if some X-risk obtains, you die with everyone else? What if there is no personal identity in the relevant sense and immortality is a desire without a referent (a possible future world in which the desired thing obtains)? and as many other things as the poster might like to add.</p>\n<p>&nbsp;</p>\n<p>3) Immortalist Case study - Ray Kurzweil -</p>\n<p>Examines Kurzweil strategy, given his background (age, IQ, opportunities given while young etc...). Emphasis, for Kurzweil and others, on how optimal are their balances for classes one and two of optimization.</p>\n<p>&nbsp;</p>\n<p>4) Immortalist Case study - Aubrey de Grey -</p>\n<p>&nbsp;</p>\n<p>5) Immortalist Case study - Danila Medvdev -</p>\n<p>Danila has been filming everything he does hours a day. I don't know much else, but suppose he is worth examining.</p>\n<p>&nbsp;</p>\n<p>6) Immortalist Case study - Peter Thiel</p>\n<p>&nbsp;</p>\n<p>7) Immortalist Case study - Laura Deming</p>\n<p>She's been fighting death since she was 12, went to MIT to research on it, and recently got a Thiel fellowship and pivoted to fundraising. She's 20.</p>\n<p>&nbsp;</p>\n<p>8) Immortalist Case study - Ben Best</p>\n<p><a href=\"http://www.benbest.com/\">Ben Best</a> directs Cryonics Institute. He wrote extensively on mechanisms of ageing, economics and resource acquisition, and cryonics. Lots can be learned from his example.</p>\n<p>&nbsp;</p>\n<p>9) Immortalist Case study - Bill Faloon</p>\n<p>Bill is a long time cryonicist, he founded the Life Extension Foundation decades ago, and to this day makes a lot of money out of that. He's a leading figure in both the Timeship project (super-protected facility for frozen people) and in gathering the cryonics youth togheter.</p>\n<p>&nbsp;</p>\n<p>10) How old are you? How much are you worth? How that influences immortalist strategies. - This one I'd like to participate.</p>\n<p>&nbsp;</p>\n<p>11) Creating incentives for your immortalism - this one I'll write</p>\n<p>How to increase the amount of times that reality strikes you with incentives that make you more likely to pursue the strategies you should pursue, being a simple egoistic immortalist.</p>\n<p>&nbsp;</p>\n<p>12, 13, 14 .... If it suits the general topic, it could be there. Also previous posts about related things could be encompassed.</p>\n<p>&nbsp;</p>\n<p>Edit: The suggestion is not that you have to <em>really want</em> to be the ideal immortalist to take part in writing a post. <a href=\"http://www.ierfh.org/br/index.html\">My goals</a> are far from being nothing but an immortalist. But I would love to know, <em>were it the case</em>, what should I be doing? First we get the abstraction. Then we factor in everything else about us and we have learned something from the abstraction.</p>\n<p>Seems people were afraid that by taking part in the sequence they'd be signalling that their only goal is to live forever. This misses both the concept of assumption, and the idea of an informative idealized abstraction.</p>\n<p>What I'm suggesting we do here with immortality could just as well be done with some other goal like \"The Simple Ideal Anti-Malaria Fighter\" or \"The Simple Ideal Wannabe Cirque de Soleil\".&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>So who wants to play?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cnmx4TAZQdqs3dwrS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 5, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "21751", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-24T22:16:07.348Z", "modifiedAt": null, "url": null, "title": "[Link] Colonisation of Venus", "slug": "link-colonisation-of-venus", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.030Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Zwqz6uaZMhJ7uqHae", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y8moeSHderg5Bmz2u/link-colonisation-of-venus", "pageUrlRelative": "/posts/y8moeSHderg5Bmz2u/link-colonisation-of-venus", "linkUrl": "https://www.lesswrong.com/posts/y8moeSHderg5Bmz2u/link-colonisation-of-venus", "postedAtFormatted": "Sunday, February 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Colonisation%20of%20Venus&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Colonisation%20of%20Venus%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy8moeSHderg5Bmz2u%2Flink-colonisation-of-venus%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Colonisation%20of%20Venus%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy8moeSHderg5Bmz2u%2Flink-colonisation-of-venus", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy8moeSHderg5Bmz2u%2Flink-colonisation-of-venus", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<p>I was wondering what people thought of <a title=\"this paper\" href=\"http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20030022668_2003025525.pdf\" target=\"_blank\">this paper</a>&nbsp;by Geoffrey Landis on colonising Venus. In it he suggests that cloud-top Venus is one of the most benign environments in the Solar System. Temperature and gravity are similar to Earth, there's some radiation shielding and useful resources, and aerostats filled only with breathable air would float at that height. I'm no expert so can't speak to how accurate it is, but it's certainly very thought-provoking for such a short paper.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y8moeSHderg5Bmz2u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 11, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "21752", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T04:20:05.060Z", "modifiedAt": null, "url": null, "title": "Meetup : Toronto - The nature of discourse; what works, what doesn't ", "slug": "meetup-toronto-the-nature-of-discourse-what-works-what-doesn", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zk3hmtquKqz9yioSx/meetup-toronto-the-nature-of-discourse-what-works-what-doesn", "pageUrlRelative": "/posts/Zk3hmtquKqz9yioSx/meetup-toronto-the-nature-of-discourse-what-works-what-doesn", "linkUrl": "https://www.lesswrong.com/posts/Zk3hmtquKqz9yioSx/meetup-toronto-the-nature-of-discourse-what-works-what-doesn", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Toronto%20-%20The%20nature%20of%20discourse%3B%20what%20works%2C%20what%20doesn't%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Toronto%20-%20The%20nature%20of%20discourse%3B%20what%20works%2C%20what%20doesn't%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZk3hmtquKqz9yioSx%2Fmeetup-toronto-the-nature-of-discourse-what-works-what-doesn%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Toronto%20-%20The%20nature%20of%20discourse%3B%20what%20works%2C%20what%20doesn't%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZk3hmtquKqz9yioSx%2Fmeetup-toronto-the-nature-of-discourse-what-works-what-doesn", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZk3hmtquKqz9yioSx%2Fmeetup-toronto-the-nature-of-discourse-what-works-what-doesn", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jr'>Toronto - The nature of discourse; what works, what doesn't </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 February 2013 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">54 Dundas St E, Toronto, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Place: Upstairs at The Imperial Public Library 54 Dundas St. E, near Dundas Station. Enter at the door on the right marked \"library\", go upstairs and look for the paperclip sign.</p>\n\n<p>How do we avoid competitive debates where everyone loses, and achieve productive discussions where everyone wins? How do we do this when our brains seem set up to embrace \"the brawl\"?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jr'>Toronto - The nature of discourse; what works, what doesn't </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zk3hmtquKqz9yioSx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.1219778440221877e-06, "legacy": true, "legacyId": "21761", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Toronto___The_nature_of_discourse__what_works__what_doesn_t_\">Discussion article for the meetup : <a href=\"/meetups/jr\">Toronto - The nature of discourse; what works, what doesn't </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 February 2013 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">54 Dundas St E, Toronto, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Place: Upstairs at The Imperial Public Library 54 Dundas St. E, near Dundas Station. Enter at the door on the right marked \"library\", go upstairs and look for the paperclip sign.</p>\n\n<p>How do we avoid competitive debates where everyone loses, and achieve productive discussions where everyone wins? How do we do this when our brains seem set up to embrace \"the brawl\"?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Toronto___The_nature_of_discourse__what_works__what_doesn_t_1\">Discussion article for the meetup : <a href=\"/meetups/jr\">Toronto - The nature of discourse; what works, what doesn't </a></h2>", "sections": [{"title": "Discussion article for the meetup : Toronto - The nature of discourse; what works, what doesn't ", "anchor": "Discussion_article_for_the_meetup___Toronto___The_nature_of_discourse__what_works__what_doesn_t_", "level": 1}, {"title": "Discussion article for the meetup : Toronto - The nature of discourse; what works, what doesn't ", "anchor": "Discussion_article_for_the_meetup___Toronto___The_nature_of_discourse__what_works__what_doesn_t_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T05:31:28.981Z", "modifiedAt": null, "url": null, "title": "How do you not be a hater?", "slug": "how-do-you-not-be-a-hater", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.320Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2YoEvDfQvJnXo22uD/how-do-you-not-be-a-hater", "pageUrlRelative": "/posts/2YoEvDfQvJnXo22uD/how-do-you-not-be-a-hater", "linkUrl": "https://www.lesswrong.com/posts/2YoEvDfQvJnXo22uD/how-do-you-not-be-a-hater", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20you%20not%20be%20a%20hater%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20you%20not%20be%20a%20hater%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YoEvDfQvJnXo22uD%2Fhow-do-you-not-be-a-hater%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20you%20not%20be%20a%20hater%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YoEvDfQvJnXo22uD%2Fhow-do-you-not-be-a-hater", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YoEvDfQvJnXo22uD%2Fhow-do-you-not-be-a-hater", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 236, "htmlBody": "<p>I was reading <a href=\"http://www.soulriders.net/forum/index.php?topic=101858.0\">a review</a>&nbsp;of <a href=\"http://www.fanfiction.net/s/5588986/1/Trust-in-God-or-The-Riddle-of-Kyon\">Trust in God, or, The Riddle of Kyon</a>. The reviewer went through step by step and listed problems with the fic. Some of them I agreed with, and others did not. What really stood out was in a comment agreeing with it:&nbsp;\"reading that fic usually just evoked vague anger and other unpleasantness.\" Not unlike the vague feeling of anger and other unpleasantness I felt upon reading the review.</p>\n<p>I don't consider myself someone whose opinion can be trusted on the quality of the original fic. In addition to being every bit as biased as any hater, although in the opposite direction, I have Asperger's syndrome, and I don't trust myself to notice such things as people acting out of character. However, because of this, I know <em>my</em> revulsion cannot be due to the quality of the review. I looked for it in hopes of weakening any bias I have. I think I can safely say that my revulsion will prevent that from happening.</p>\n<p>So, any idea on what I can do? I've always thought haters should just stay away from what they hate. That would work fine if I just hated ponies or something, but I don't think it's such a good idea in cases where ideology is involved. And if nothing else, I don't want a vague feeling of anger and&nbsp;unpleasantness&nbsp;to ruin a perfectly good fanfic, like <a href=\"http://www.fanfiction.net/s/3413734/1/The_Death_of_Haruhi_Suzumiya\">The Death of Haruhi Suzumia</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2YoEvDfQvJnXo22uD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -2, "extendedScore": null, "score": 1.1220238216813333e-06, "legacy": true, "legacyId": "21762", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T06:19:19.713Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Cynicism in Ev-Psych (and Econ)", "slug": "seq-rerun-cynicism-in-ev-psych-and-econ", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wWkpoHQMfsLn45ws9/seq-rerun-cynicism-in-ev-psych-and-econ", "pageUrlRelative": "/posts/wWkpoHQMfsLn45ws9/seq-rerun-cynicism-in-ev-psych-and-econ", "linkUrl": "https://www.lesswrong.com/posts/wWkpoHQMfsLn45ws9/seq-rerun-cynicism-in-ev-psych-and-econ", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Cynicism%20in%20Ev-Psych%20(and%20Econ)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Cynicism%20in%20Ev-Psych%20(and%20Econ)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwWkpoHQMfsLn45ws9%2Fseq-rerun-cynicism-in-ev-psych-and-econ%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Cynicism%20in%20Ev-Psych%20(and%20Econ)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwWkpoHQMfsLn45ws9%2Fseq-rerun-cynicism-in-ev-psych-and-econ", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwWkpoHQMfsLn45ws9%2Fseq-rerun-cynicism-in-ev-psych-and-econ", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Today's post, <a href=\"/lw/yh/cynicism_in_evpsych_and_econ/\">Cynicism in Ev-Psych (and Econ?)</a> was originally published on 11 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/mediawiki/index.php?title=Less_Wrong/2009_Articles/Summaries&amp;action=edit&amp;section=43\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Evolutionary Psychology and Microeconomics seem to develop different types of cynical theories, and are cynical about different things.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gs4/seq_rerun_informers_and_persuaders/\">Informers and Persuaders</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wWkpoHQMfsLn45ws9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.1220546339976848e-06, "legacy": true, "legacyId": "21763", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KC5qGJiWSxt9zpyDy", "bhkjkvLF3swkEnYK5", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T15:31:55.865Z", "modifiedAt": null, "url": null, "title": "Can biases be used to encourage rational thinking?", "slug": "can-biases-be-used-to-encourage-rational-thinking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.607Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pinyaka", "createdAt": "2012-09-21T12:11:45.980Z", "isAdmin": false, "displayName": "pinyaka"}, "userId": "FscpDmNcKZdbeDNZ2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4aqE7wFyvTTJ5Hv6X/can-biases-be-used-to-encourage-rational-thinking", "pageUrlRelative": "/posts/4aqE7wFyvTTJ5Hv6X/can-biases-be-used-to-encourage-rational-thinking", "linkUrl": "https://www.lesswrong.com/posts/4aqE7wFyvTTJ5Hv6X/can-biases-be-used-to-encourage-rational-thinking", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20biases%20be%20used%20to%20encourage%20rational%20thinking%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20biases%20be%20used%20to%20encourage%20rational%20thinking%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4aqE7wFyvTTJ5Hv6X%2Fcan-biases-be-used-to-encourage-rational-thinking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20biases%20be%20used%20to%20encourage%20rational%20thinking%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4aqE7wFyvTTJ5Hv6X%2Fcan-biases-be-used-to-encourage-rational-thinking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4aqE7wFyvTTJ5Hv6X%2Fcan-biases-be-used-to-encourage-rational-thinking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>For example, the <a href=\"http://en.wikipedia.org/wiki/Forer_effect\">Forer (or Barnum) effect</a> leads people to believe that vague and general descriptions of personality traits are very accurate when they believe that the results are tailored to them (based on some attribute like astrological sign).<br /><br />If there were a service available to you that would only send you predictions like \"you are more likely to notice and compensate for your biases today\" would it be worth signing up for or is there a Shelling point around not deliberately engaging in irrational thought? Is there a way to calculate utility for limited self-deception?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4aqE7wFyvTTJ5Hv6X", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 1.1224106163652732e-06, "legacy": true, "legacyId": "21768", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T15:34:34.536Z", "modifiedAt": null, "url": null, "title": "Newcomb's Problem dissolved?", "slug": "newcomb-s-problem-dissolved", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:41.055Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EGI", "createdAt": "2013-02-09T19:55:36.080Z", "isAdmin": false, "displayName": "EGI"}, "userId": "pLhX7Zyo6xicZZer7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/db3vswedqyx4y6gRF/newcomb-s-problem-dissolved", "pageUrlRelative": "/posts/db3vswedqyx4y6gRF/newcomb-s-problem-dissolved", "linkUrl": "https://www.lesswrong.com/posts/db3vswedqyx4y6gRF/newcomb-s-problem-dissolved", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Newcomb's%20Problem%20dissolved%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANewcomb's%20Problem%20dissolved%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdb3vswedqyx4y6gRF%2Fnewcomb-s-problem-dissolved%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Newcomb's%20Problem%20dissolved%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdb3vswedqyx4y6gRF%2Fnewcomb-s-problem-dissolved", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdb3vswedqyx4y6gRF%2Fnewcomb-s-problem-dissolved", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 464, "htmlBody": "<p>First reading about <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb&rsquo;s Problem</a> my reaction was petty much \"wow, interesting thought\" and \"of course I would one box, I want to win $ 1 million after all\". But I had a lingering nagging feeling, that there is something wrong with the whole premise. Now, after thinking about it for a few weeks I think I have found the problem.</p>\n<p>First of all I want to point out, that I would still one box after seeing Omega predicting 50 or 100 other people correctly, since 50 to 100 bits of evidence are enough to ovecome (nearly) any prior I have about how the universe works. Only I do not think this scenario is physically possible in our universe.</p>\n<p>The mistake is nicely stated <a href=\"/lw/16i/confusion_about_newcomb_is_confusion_about/\">here:</a></p>\n<blockquote>\n<p>After all, Joe is a deterministic physical system; his current state (together with the state of his future self's past light-cone) fully determines what Joe's future action will be.&nbsp; There is no Physically Irreducible Moment of Choice, where this same Joe, with his own exact actual past, \"can\" go one way or the other.</p>\n</blockquote>\n<p>This is only true in this sense if <strong>neither</strong> <a href=\"/lw/r8/and_the_winner_is_manyworlds/\">MWI</a> is true <strong>nor</strong> there are any <a href=\"http://en.wikipedia.org/wiki/Wavefunction_collapse#The_process_of_collapse\">quantum probabilistic processes</a>, i.e., our universe allows for a true <a href=\"http://en.wikipedia.org/wiki/Laplace%27s_demon\">Laplace's demon</a> (a.k.a. Omega) to exist.</p>\n<p>If MWI is true Joe can set it up so, that \"after\" Omega filled the boxes and left there \"will\" be Everett Branches, in which Joe \"will\" twobox and different Everett Branches in which Joe \"will\" onebox.</p>\n<p>Intuitively I think Joe could even do this with his own brain by leaving it in \"undecided\" mode until Omega leaves and then using an algorithm which feels \"random\" to decide if he oneboxes or twoboxes. But of course I would not thrust my intuition here and I do not know enough about Joe's brain to decide if this really works. So Joe would use e.g. a single photon reflected/transmitted off/through a semitransparent mirror, ensuring, that he oneboxes respectively twoboxes in say 50% of the Everett Branches.</p>\n<p>If MWI is not true but there are quantum probabilistic process, Omega simply cannot predict the future state of the universe. So the same procedure used above would ensure that Omega cannot predict Joes decision due to true randomness.</p>\n<p>So I would be very <strong>very VERY</strong> surprised if I saw Omega pull this trick 100 times in a row and I could somehow rule out Stage Magic (which I could not).</p>\n<p>I am not even sure if there is <strong>any</strong> serious interpretation of quantum mechanics that allows for the strict determinism Omega would need. Would love to hear about one in the comments!</p>\n<p>Of course from an instrumental standpoint it is always rational to firmly precommit to onebox, since the extra $1000 are not worth taking the risk. Even the model uncertainity accounts for much more than 0.001.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "db3vswedqyx4y6gRF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -5, "extendedScore": null, "score": 1.1224123204020017e-06, "legacy": true, "legacyId": "21765", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B7bMmhvaufdtxBtLW", "9cgBF6BQ2TRB3Hy4E"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T15:34:40.576Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Personal Productivity", "slug": "meetup-vancouver-personal-productivity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.603Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/brqykKXpDtxWyvuCK/meetup-vancouver-personal-productivity", "pageUrlRelative": "/posts/brqykKXpDtxWyvuCK/meetup-vancouver-personal-productivity", "linkUrl": "https://www.lesswrong.com/posts/brqykKXpDtxWyvuCK/meetup-vancouver-personal-productivity", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Personal%20Productivity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Personal%20Productivity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbrqykKXpDtxWyvuCK%2Fmeetup-vancouver-personal-productivity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Personal%20Productivity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbrqykKXpDtxWyvuCK%2Fmeetup-vancouver-personal-productivity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbrqykKXpDtxWyvuCK%2Fmeetup-vancouver-personal-productivity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/js'>Vancouver Personal Productivity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 March 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 west broadway, vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to try to go somewhere more practical this week. We'll talk about how to actually get stuff done in your available time. I don't have the silver bullet; I was hoping that we collectively have a few tricks  to share.</p>\n\n<p>As usual lately, the meetup will be at Benny's Bagels (2505 west broadway) at 15:00 on Saturday.</p>\n\n<p>We can also follow up on the list of bad habits and possible improvements we came up with two weeks ago (or one week? whatever).</p>\n\n<p>The discussion has been quite theoretical for a few weeks. I've been impressed by our ability to discuss hard topics without bursting into flames, but even so, I want to take us in a more practical direction for a while. Most of us seem to be lacking in applied winning-skill more than in abstract thinking-skill.</p>\n\n<p>Join us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a></p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/js'>Vancouver Personal Productivity</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "brqykKXpDtxWyvuCK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1224123852476118e-06, "legacy": true, "legacyId": "21769", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Personal_Productivity\">Discussion article for the meetup : <a href=\"/meetups/js\">Vancouver Personal Productivity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 March 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 west broadway, vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to try to go somewhere more practical this week. We'll talk about how to actually get stuff done in your available time. I don't have the silver bullet; I was hoping that we collectively have a few tricks  to share.</p>\n\n<p>As usual lately, the meetup will be at Benny's Bagels (2505 west broadway) at 15:00 on Saturday.</p>\n\n<p>We can also follow up on the list of bad habits and possible improvements we came up with two weeks ago (or one week? whatever).</p>\n\n<p>The discussion has been quite theoretical for a few weeks. I've been impressed by our ability to discuss hard topics without bursting into flames, but even so, I want to take us in a more practical direction for a while. Most of us seem to be lacking in applied winning-skill more than in abstract thinking-skill.</p>\n\n<p>Join us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a></p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Personal_Productivity1\">Discussion article for the meetup : <a href=\"/meetups/js\">Vancouver Personal Productivity</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Personal Productivity", "anchor": "Discussion_article_for_the_meetup___Vancouver_Personal_Productivity", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Personal Productivity", "anchor": "Discussion_article_for_the_meetup___Vancouver_Personal_Productivity1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T15:52:40.847Z", "modifiedAt": null, "url": null, "title": "Why Bayes? A Wise Ruling", "slug": "why-bayes-a-wise-ruling", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:25.902Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rm6o5imQCTrkfzCgq/why-bayes-a-wise-ruling", "pageUrlRelative": "/posts/rm6o5imQCTrkfzCgq/why-bayes-a-wise-ruling", "linkUrl": "https://www.lesswrong.com/posts/rm6o5imQCTrkfzCgq/why-bayes-a-wise-ruling", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Bayes%3F%20A%20Wise%20Ruling&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Bayes%3F%20A%20Wise%20Ruling%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frm6o5imQCTrkfzCgq%2Fwhy-bayes-a-wise-ruling%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Bayes%3F%20A%20Wise%20Ruling%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frm6o5imQCTrkfzCgq%2Fwhy-bayes-a-wise-ruling", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frm6o5imQCTrkfzCgq%2Fwhy-bayes-a-wise-ruling", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 695, "htmlBody": "<p>Why is Bayes' Rule useful? Most explanations of Bayes explain the how of Bayes: they take a well-posed mathematical problem and convert given numbers to desired numbers. While Bayes is useful for calculating hard-to-estimate numbers from easy-to-estimate numbers, the quantitative use of Bayes requires the&nbsp;<em>qualitative</em>&nbsp;use of Bayes, which is noticing that such a problem exists. When you have a hard-to-estimate number that you could figure out from easy-to-estimate numbers, then you want to use Bayes. This mental process of testing beliefs and searching for easy experiments is the heart of practical Bayesian thinking. As an example, let us examine&nbsp;<a href=\"http://www.biblegateway.com/passage/?search=1%20Kings%203:16-28&amp;version=NIV\">1 Kings 3:16-28</a>:<a id=\"more\"></a></p>\n<blockquote>\n<p>Now two prostitutes came to the king and stood before him. One of them said, &ldquo;Pardon me, my lord. This woman and I live in the same house, and I had a baby while she was there with me. The third day after my child was born, this woman also had a baby. We were alone; there was no one in the house but the two of us.</p>\n<p>&ldquo;During the night this woman&rsquo;s son died because she lay on him. So she got up in the middle of the night and took my son from my side while I your servant was asleep. She put him by her breast and put her dead son by my breast. The next morning, I got up to nurse my son&mdash;and he was dead! But when I looked at him closely in the morning light, I saw that it wasn&rsquo;t the son I had borne.\"</p>\n<p>The other woman said, &ldquo;No! The living one is my son; the dead one is yours.&rdquo;</p>\n<p>But the first one insisted, &ldquo;No! The dead one is yours; the living one is mine.&rdquo; And so they argued before the king.</p>\n<p>The king said, &ldquo;This one says, &lsquo;My son is alive and your son is dead,&rsquo; while that one says, &lsquo;No! Your son is dead and mine is alive.&rsquo;&rdquo;</p>\n</blockquote>\n<p>Notice that Solomon&nbsp;<em>explicitly identified</em>&nbsp;competing hypotheses, raising them to the level of conscious attention. When each hypothesis has a personal advocate, this is easy, but it is no less important when considering other uncertainties. Often, a problem looks clearer when you branch an uncertain variable on its possible values, even if it is as simple as saying \"This is either true or not true.\"</p>\n<blockquote>\n<p>Then the king said, &ldquo;Bring me a sword.&rdquo; So they brought a sword for the king. He then gave an order: &ldquo;Cut the living child in two and give half to one and half to the other.&rdquo;</p>\n<p>The woman whose son was alive was deeply moved out of love for her son and said to the king, &ldquo;Please, my lord, give her the living baby! Don&rsquo;t kill him!&rdquo;</p>\n<p>But the other said, &ldquo;Neither I nor you shall have him. Cut him in two!&rdquo;</p>\n<p>Then the king gave his ruling: &ldquo;Give the living baby to the first woman. Do not kill him; she is his mother.&rdquo;</p>\n</blockquote>\n<p>Solomon&nbsp;<em>considers the empirical consequences</em>&nbsp;of the competing hypotheses, searching for a test which will&nbsp;<em>favor one hypothesis over another</em>. When considering one hypothesis alone, it is easy to find tests which are likely if that hypothesis is true. The true mother is likely to say the child is hers; the true mother is likely to be passionate about the issue. But that's not enough; we need to also estimate how likely those results are if the hypothesis is false. The false mother is equally likely to say the child is hers, and could generate equal passion. We need a test whose results significantly depend on which hypothesis is actually true.</p>\n<p>Witnesses or DNA tests would be more likely to support the true mother than the false mother, but they aren't available. Solomon realizes that the claimant's motivations are different, and thus putting the child in danger may cause the true mother and false mother to act differently. The test works, generates a large likelihood ratio, and now his posterior firmly favors the first claimant as the true mother.</p>\n<blockquote>\n<p>When all Israel heard the verdict the king had given, they held the king in awe, because they saw that he had wisdom from God to administer justice.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rm6o5imQCTrkfzCgq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 18, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "21766", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T17:56:06.667Z", "modifiedAt": null, "url": null, "title": "Meetup : Sunday Meetup at Buffalo Lab", "slug": "meetup-sunday-meetup-at-buffalo-lab", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "StonesOnCanvas", "createdAt": "2012-06-28T17:32:49.237Z", "isAdmin": false, "displayName": "StonesOnCanvas"}, "userId": "FAfkKGH6E8BLmXW4M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ojvJQ46CnZyzNWnZj/meetup-sunday-meetup-at-buffalo-lab", "pageUrlRelative": "/posts/ojvJQ46CnZyzNWnZj/meetup-sunday-meetup-at-buffalo-lab", "linkUrl": "https://www.lesswrong.com/posts/ojvJQ46CnZyzNWnZj/meetup-sunday-meetup-at-buffalo-lab", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sunday%20Meetup%20at%20Buffalo%20Lab&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sunday%20Meetup%20at%20Buffalo%20Lab%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FojvJQ46CnZyzNWnZj%2Fmeetup-sunday-meetup-at-buffalo-lab%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sunday%20Meetup%20at%20Buffalo%20Lab%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FojvJQ46CnZyzNWnZj%2Fmeetup-sunday-meetup-at-buffalo-lab", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FojvJQ46CnZyzNWnZj%2Fmeetup-sunday-meetup-at-buffalo-lab", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 172, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jt'>Sunday Meetup at Buffalo Lab</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 February 2013 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2727 Broadway Ave. Suite 210, Cheektowaga, NY </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everyone,</p>\n\n<p>The next post linked in this series is \"Focus you Uncertainty\". It seems to be a sort of warm up for Bayes' Theorem. I think its about time we did a whole lesson on Bayesian Reasoning.</p>\n\n<p>Instead of reading the next post in the series, I'd recommend you read \"An Intuitive Explanation of Bayes' Theorem\" <a href=\"http://yudkowsky.net/rational/bayes\" rel=\"nofollow\">http://yudkowsky.net/rational/bayes</a> OR \"Bayes' Theorem Illustrated\" <a href=\"http://lesswrong.com/lw/2b0/bayes_theorem_illustrated_my_way/\" rel=\"nofollow\">http://lesswrong.com/lw/2b0/bayes_theorem_illustrated_my_way/</a> which both do a good job explaining Bayes' Theorem in different ways. Different people learn in different ways, so if one explanation isn't working for you, try the other post and see if that makes more sense. In any case, we'll cover it at the meetup and why its important to know.</p>\n\n<p>P.S. Since Buffalo Lab is not clearly visible from the road, I recommend using google maps satellite view to make it a bit easier to find.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jt'>Sunday Meetup at Buffalo Lab</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ojvJQ46CnZyzNWnZj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "21770", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sunday_Meetup_at_Buffalo_Lab\">Discussion article for the meetup : <a href=\"/meetups/jt\">Sunday Meetup at Buffalo Lab</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 February 2013 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2727 Broadway Ave. Suite 210, Cheektowaga, NY </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everyone,</p>\n\n<p>The next post linked in this series is \"Focus you Uncertainty\". It seems to be a sort of warm up for Bayes' Theorem. I think its about time we did a whole lesson on Bayesian Reasoning.</p>\n\n<p>Instead of reading the next post in the series, I'd recommend you read \"An Intuitive Explanation of Bayes' Theorem\" <a href=\"http://yudkowsky.net/rational/bayes\" rel=\"nofollow\">http://yudkowsky.net/rational/bayes</a> OR \"Bayes' Theorem Illustrated\" <a href=\"http://lesswrong.com/lw/2b0/bayes_theorem_illustrated_my_way/\" rel=\"nofollow\">http://lesswrong.com/lw/2b0/bayes_theorem_illustrated_my_way/</a> which both do a good job explaining Bayes' Theorem in different ways. Different people learn in different ways, so if one explanation isn't working for you, try the other post and see if that makes more sense. In any case, we'll cover it at the meetup and why its important to know.</p>\n\n<p>P.S. Since Buffalo Lab is not clearly visible from the road, I recommend using google maps satellite view to make it a bit easier to find.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sunday_Meetup_at_Buffalo_Lab1\">Discussion article for the meetup : <a href=\"/meetups/jt\">Sunday Meetup at Buffalo Lab</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sunday Meetup at Buffalo Lab", "anchor": "Discussion_article_for_the_meetup___Sunday_Meetup_at_Buffalo_Lab", "level": 1}, {"title": "Discussion article for the meetup : Sunday Meetup at Buffalo Lab", "anchor": "Discussion_article_for_the_meetup___Sunday_Meetup_at_Buffalo_Lab1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CMt3ijXYuCynhPWXa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T18:00:42.153Z", "modifiedAt": null, "url": null, "title": "Meetup : Sunday Meetup at Buffalo Labs", "slug": "meetup-sunday-meetup-at-buffalo-labs", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "StonesOnCanvas", "createdAt": "2012-06-28T17:32:49.237Z", "isAdmin": false, "displayName": "StonesOnCanvas"}, "userId": "FAfkKGH6E8BLmXW4M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zozBD63qhjizphAFS/meetup-sunday-meetup-at-buffalo-labs", "pageUrlRelative": "/posts/zozBD63qhjizphAFS/meetup-sunday-meetup-at-buffalo-labs", "linkUrl": "https://www.lesswrong.com/posts/zozBD63qhjizphAFS/meetup-sunday-meetup-at-buffalo-labs", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sunday%20Meetup%20at%20Buffalo%20Labs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sunday%20Meetup%20at%20Buffalo%20Labs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzozBD63qhjizphAFS%2Fmeetup-sunday-meetup-at-buffalo-labs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sunday%20Meetup%20at%20Buffalo%20Labs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzozBD63qhjizphAFS%2Fmeetup-sunday-meetup-at-buffalo-labs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzozBD63qhjizphAFS%2Fmeetup-sunday-meetup-at-buffalo-labs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ju'>Sunday Meetup at Buffalo Labs</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 March 2013 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2727 Broadway Ave. Suite 210, Cheektowaga, NY</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everyone,</p>\n\n<p>The next post linked in this series is \"Focus you Uncertainty\" ( <a href=\"http://lesswrong.com/lw/ia/focus_your_uncertainty/\" rel=\"nofollow\">http://lesswrong.com/lw/ia/focus_your_uncertainty/</a> ). It seems to be a sort of warm up for Bayes' Theorem. I think its about time we did a whole lesson on Bayesian Reasoning.</p>\n\n<p>Instead of reading the next post in the series, I'd recommend you read \"An Intuitive Explanation of Bayes' Theorem\" <a href=\"http://yudkowsky.net/rational/bayes\" rel=\"nofollow\">http://yudkowsky.net/rational/bayes</a> OR \"Bayes' Theorem Illustrated\" <a href=\"http://lesswrong.com/lw/2b0/bayes_theorem_illustrated_my_way/\" rel=\"nofollow\">http://lesswrong.com/lw/2b0/bayes_theorem_illustrated_my_way/</a>  which both do a good job explaining Bayes' Theorem in different ways. Different people learn in different ways, so if one explanation isn't working for you, try the other post and see if that makes more sense. In any case, we'll cover it at the meetup and why its important to know.</p>\n\n<p>P.S. Since Buffalo Lab is not clearly visible from the road, I recommend using google maps satellite view to make it a bit easier to find.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ju'>Sunday Meetup at Buffalo Labs</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zozBD63qhjizphAFS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1225064878723433e-06, "legacy": true, "legacyId": "21771", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sunday_Meetup_at_Buffalo_Labs\">Discussion article for the meetup : <a href=\"/meetups/ju\">Sunday Meetup at Buffalo Labs</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 March 2013 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2727 Broadway Ave. Suite 210, Cheektowaga, NY</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everyone,</p>\n\n<p>The next post linked in this series is \"Focus you Uncertainty\" ( <a href=\"http://lesswrong.com/lw/ia/focus_your_uncertainty/\" rel=\"nofollow\">http://lesswrong.com/lw/ia/focus_your_uncertainty/</a> ). It seems to be a sort of warm up for Bayes' Theorem. I think its about time we did a whole lesson on Bayesian Reasoning.</p>\n\n<p>Instead of reading the next post in the series, I'd recommend you read \"An Intuitive Explanation of Bayes' Theorem\" <a href=\"http://yudkowsky.net/rational/bayes\" rel=\"nofollow\">http://yudkowsky.net/rational/bayes</a> OR \"Bayes' Theorem Illustrated\" <a href=\"http://lesswrong.com/lw/2b0/bayes_theorem_illustrated_my_way/\" rel=\"nofollow\">http://lesswrong.com/lw/2b0/bayes_theorem_illustrated_my_way/</a>  which both do a good job explaining Bayes' Theorem in different ways. Different people learn in different ways, so if one explanation isn't working for you, try the other post and see if that makes more sense. In any case, we'll cover it at the meetup and why its important to know.</p>\n\n<p>P.S. Since Buffalo Lab is not clearly visible from the road, I recommend using google maps satellite view to make it a bit easier to find.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sunday_Meetup_at_Buffalo_Labs1\">Discussion article for the meetup : <a href=\"/meetups/ju\">Sunday Meetup at Buffalo Labs</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sunday Meetup at Buffalo Labs", "anchor": "Discussion_article_for_the_meetup___Sunday_Meetup_at_Buffalo_Labs", "level": 1}, {"title": "Discussion article for the meetup : Sunday Meetup at Buffalo Labs", "anchor": "Discussion_article_for_the_meetup___Sunday_Meetup_at_Buffalo_Labs1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GJ4ZQm7crTzTM6xDW", "CMt3ijXYuCynhPWXa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T21:00:54.674Z", "modifiedAt": null, "url": null, "title": "What Deontology gets right", "slug": "what-deontology-gets-right", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:01.554Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ThrustVectoring", "createdAt": "2010-09-27T22:49:40.124Z", "isAdmin": false, "displayName": "ThrustVectoring"}, "userId": "hjXjrf8ESHqNfB2Tf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RkDStcAmjGs7edLZG/what-deontology-gets-right", "pageUrlRelative": "/posts/RkDStcAmjGs7edLZG/what-deontology-gets-right", "linkUrl": "https://www.lesswrong.com/posts/RkDStcAmjGs7edLZG/what-deontology-gets-right", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Deontology%20gets%20right&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Deontology%20gets%20right%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRkDStcAmjGs7edLZG%2Fwhat-deontology-gets-right%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Deontology%20gets%20right%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRkDStcAmjGs7edLZG%2Fwhat-deontology-gets-right", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRkDStcAmjGs7edLZG%2Fwhat-deontology-gets-right", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 664, "htmlBody": "<p>Let me preface this with an acknowledgement that Deontology has blind spots and that I'm not a Deontologist. Much like Logical Positivism, however, Deontology has good things to learn from that many Consequentialist decision algorithms miss.</p>\n<p>&nbsp;</p>\n<p><strong>Social Considerations</strong></p>\n<p>&nbsp;</p>\n<p>Your decision has consequences outside of the direct results. More specifically, if you decide to tell a lie, people are more likely to view you as a liar. This portion of consequences are easy to neglect when making a decision. So while Deontology over-corrects for this (for example, if you put a gun to my head and demand that I profess belief X, I'm going to say that I believe X, which a Deontological prohibition against lying forbids), it does so in a way that is better than many people's naive consequential thinking.</p>\n<p>&nbsp;</p>\n<p>Deontological arguments are also better at convincing people that you have socially valued traits. People expect truth-tellers to tell the truth, so you want to be viewed as a truth-teller. \"Lying doesn't work, so I don't lie\" is a more awkward and involved argument than \"lying is wrong\". On a related note, Deonotological reasoning is easier for other people to model. Deontology can screen off the cost-benefit analysis that someone makes when thinking about their decisions, since all you need is the rules that they are following.</p>\n<p>&nbsp;</p>\n<p><strong>Habits and Policies</strong></p>\n<p>&nbsp;</p>\n<p>Decisions aren't made in a vacuum. They also form an implicit rule that people tend to follow. In other words, people form habits. They find it easier to do the same kinds of things that they've always done. Eating one piece of cake doesn't do measurable harm to your waistline, but having a policy of eating one piece of cake whenever you want to does.</p>\n<p>&nbsp;</p>\n<p>If you're familiar with set theory, it's the distinction between {x|P(x)} and {x1, x2, x3...}. If you make decisions without consulting what policy P(x) you'd like to follow, you can make mistakes. Choosing x1 means not only having done x1, but also choosing a P(x) such that P(x1) is true.</p>\n<p>&nbsp;</p>\n<p>When I sign a gay marriage petition, it doesn't just increase the chance that gay marriage gets enacted. It also makes me more likely to do other things that support the gay marriage movement, as well as make me more likely to sign worthwhile-sounding petitions in general. This is part of why I avoid social movements: trying to fight rape culture or conservatives or racism means that I'm more likely to do similar kinds of things when they don't help (Or alternatively, convince people to join whatever movement in question even when more support for that movement isn't helpful).</p>\n<p>&nbsp;</p>\n<p>In short, the Deontological focus on following rules can help people enact the kinds of policies that they want to follow, even if they are bad at evaluating the value gained from following certain policies. It's a way of implementing a Schelling point, in other words - a way to choose a better policy even if breaking the policy this one time seems to work better.</p>\n<p>&nbsp;</p>\n<p><strong>Enforcing pro-social behavior</strong></p>\n<p>&nbsp;</p>\n<p>It's fairly straightforward to tell whether or not someone has crossed an arbitrary line separating pro-social and anti-social behavior. Evaluating someone's consequentialist reasoning, on the other hand, is much more difficult. Let's take, for example, the case of Christopher Dorner, the former LAPD officer who decided to expose and fight what he saw as a corrupt LAPD by declaring a personal war on them. A Deontological \"don't kill cops\" definitively indicts him as anti-social, whereas it's much more ambiguous whether or not trading some dead cops for a better police force is a good deal or not.</p>\n<p><br />Pro-social reasons for selfish actions are also rather cheap to make or say. If you want a millionaire lifestyle, it's easy to say that your immoral business practices are for feeding starving children in Africa. It's a lot harder to say that your immoral business practices don't violate the rule \"don't use immoral business practices\". In general, rule-breaking is much easier to detect than utility functions you don't want to have around.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RkDStcAmjGs7edLZG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "21773", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Let me preface this with an acknowledgement that Deontology has blind spots and that I'm not a Deontologist. Much like Logical Positivism, however, Deontology has good things to learn from that many Consequentialist decision algorithms miss.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Social_Considerations\">Social Considerations</strong></p>\n<p>&nbsp;</p>\n<p>Your decision has consequences outside of the direct results. More specifically, if you decide to tell a lie, people are more likely to view you as a liar. This portion of consequences are easy to neglect when making a decision. So while Deontology over-corrects for this (for example, if you put a gun to my head and demand that I profess belief X, I'm going to say that I believe X, which a Deontological prohibition against lying forbids), it does so in a way that is better than many people's naive consequential thinking.</p>\n<p>&nbsp;</p>\n<p>Deontological arguments are also better at convincing people that you have socially valued traits. People expect truth-tellers to tell the truth, so you want to be viewed as a truth-teller. \"Lying doesn't work, so I don't lie\" is a more awkward and involved argument than \"lying is wrong\". On a related note, Deonotological reasoning is easier for other people to model. Deontology can screen off the cost-benefit analysis that someone makes when thinking about their decisions, since all you need is the rules that they are following.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Habits_and_Policies\">Habits and Policies</strong></p>\n<p>&nbsp;</p>\n<p>Decisions aren't made in a vacuum. They also form an implicit rule that people tend to follow. In other words, people form habits. They find it easier to do the same kinds of things that they've always done. Eating one piece of cake doesn't do measurable harm to your waistline, but having a policy of eating one piece of cake whenever you want to does.</p>\n<p>&nbsp;</p>\n<p>If you're familiar with set theory, it's the distinction between {x|P(x)} and {x1, x2, x3...}. If you make decisions without consulting what policy P(x) you'd like to follow, you can make mistakes. Choosing x1 means not only having done x1, but also choosing a P(x) such that P(x1) is true.</p>\n<p>&nbsp;</p>\n<p>When I sign a gay marriage petition, it doesn't just increase the chance that gay marriage gets enacted. It also makes me more likely to do other things that support the gay marriage movement, as well as make me more likely to sign worthwhile-sounding petitions in general. This is part of why I avoid social movements: trying to fight rape culture or conservatives or racism means that I'm more likely to do similar kinds of things when they don't help (Or alternatively, convince people to join whatever movement in question even when more support for that movement isn't helpful).</p>\n<p>&nbsp;</p>\n<p>In short, the Deontological focus on following rules can help people enact the kinds of policies that they want to follow, even if they are bad at evaluating the value gained from following certain policies. It's a way of implementing a Schelling point, in other words - a way to choose a better policy even if breaking the policy this one time seems to work better.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Enforcing_pro_social_behavior\">Enforcing pro-social behavior</strong></p>\n<p>&nbsp;</p>\n<p>It's fairly straightforward to tell whether or not someone has crossed an arbitrary line separating pro-social and anti-social behavior. Evaluating someone's consequentialist reasoning, on the other hand, is much more difficult. Let's take, for example, the case of Christopher Dorner, the former LAPD officer who decided to expose and fight what he saw as a corrupt LAPD by declaring a personal war on them. A Deontological \"don't kill cops\" definitively indicts him as anti-social, whereas it's much more ambiguous whether or not trading some dead cops for a better police force is a good deal or not.</p>\n<p><br>Pro-social reasons for selfish actions are also rather cheap to make or say. If you want a millionaire lifestyle, it's easy to say that your immoral business practices are for feeding starving children in Africa. It's a lot harder to say that your immoral business practices don't violate the rule \"don't use immoral business practices\". In general, rule-breaking is much easier to detect than utility functions you don't want to have around.</p>", "sections": [{"title": "Social Considerations", "anchor": "Social_Considerations", "level": 1}, {"title": "Habits and Policies", "anchor": "Habits_and_Policies", "level": 1}, {"title": "Enforcing pro-social behavior", "anchor": "Enforcing_pro_social_behavior", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T22:22:39.783Z", "modifiedAt": null, "url": null, "title": "Singularity Fiction", "slug": "singularity-fiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.392Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shiftedShapes", "createdAt": "2010-02-02T21:58:21.243Z", "isAdmin": false, "displayName": "shiftedShapes"}, "userId": "fuXjZc3z2ZLEFuBAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2PedaAB7Larc8J5Kb/singularity-fiction", "pageUrlRelative": "/posts/2PedaAB7Larc8J5Kb/singularity-fiction", "linkUrl": "https://www.lesswrong.com/posts/2PedaAB7Larc8J5Kb/singularity-fiction", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Fiction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Fiction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2PedaAB7Larc8J5Kb%2Fsingularity-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Fiction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2PedaAB7Larc8J5Kb%2Fsingularity-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2PedaAB7Larc8J5Kb%2Fsingularity-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<p>I enjoy reading singularity themed sci-fi especially if it focuses on bootstrapping and power leveling. &nbsp;Most recently, I read Stross and Doctrow's Rapture of the Nerds ( http://craphound.com/rotn/download/ the third part is especially interesting), and the my little pony fanfic Friendship is Optimal ( http://www.fimfiction.net/story/62074/friendship-is-optimal&nbsp;). &nbsp;I happened upon a link to the latter on this site and it was a nice surprise. &nbsp;However, I'd prefer to be kept abreast of new releases in this genre in a more reliable way and I'm guessing many others here would as well. &nbsp;Accordingly I propose that we exchange recommendations in this thread, or if there is already such a thread that somebody link to it.</p>\n<p>&nbsp;</p>\n<p>-sS</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2PedaAB7Larc8J5Kb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -2, "extendedScore": null, "score": 1.122675336143853e-06, "legacy": true, "legacyId": "21774", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-25T23:33:54.331Z", "modifiedAt": null, "url": null, "title": "Asteroids and spaceships are kinetic bombs and how to prevent catastrophe", "slug": "asteroids-and-spaceships-are-kinetic-bombs-and-how-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:37.685Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Troshen", "createdAt": "2011-11-08T17:26:55.623Z", "isAdmin": false, "displayName": "Troshen"}, "userId": "74vNJiAARMPP4reof", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/498ccQheftkKJqTYb/asteroids-and-spaceships-are-kinetic-bombs-and-how-to", "pageUrlRelative": "/posts/498ccQheftkKJqTYb/asteroids-and-spaceships-are-kinetic-bombs-and-how-to", "linkUrl": "https://www.lesswrong.com/posts/498ccQheftkKJqTYb/asteroids-and-spaceships-are-kinetic-bombs-and-how-to", "postedAtFormatted": "Monday, February 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Asteroids%20and%20spaceships%20are%20kinetic%20bombs%20and%20how%20to%20prevent%20catastrophe&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsteroids%20and%20spaceships%20are%20kinetic%20bombs%20and%20how%20to%20prevent%20catastrophe%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F498ccQheftkKJqTYb%2Fasteroids-and-spaceships-are-kinetic-bombs-and-how-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Asteroids%20and%20spaceships%20are%20kinetic%20bombs%20and%20how%20to%20prevent%20catastrophe%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F498ccQheftkKJqTYb%2Fasteroids-and-spaceships-are-kinetic-bombs-and-how-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F498ccQheftkKJqTYb%2Fasteroids-and-spaceships-are-kinetic-bombs-and-how-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 435, "htmlBody": "<p>A reality of physics, and one that doesn't get much play in science fiction, is that as soon as humanity gains space travel, anyone in the asteroid mining or space travel business will have city-busting capabilities at their fingertips.</p>\n<p>It's there in classic sci-fi, but not so much recently.</p>\n<p>This discussion was started in the comments to:</p>\n<p>http://lesswrong.com/lw/gln/a_brief_history_of_ethically_concerned_scientists/</p>\n<p>&nbsp;</p>\n<p>In the \"Ethically Concerned Scientists\" post, Izeinwinter commented:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #ffffcc;\">, I have given some thought to this specific problem - not just asteroids, but the fact that any spaceship is potentially a weapon, and as working conditions go, extended isolation does not have the best of records on the mental stability front.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #ffffcc;\">Likely solutions: Full automation and one-time-pad locked command and control - This renders it a weapon as well controlled as nuclear arsenals, except with longer lead times on any strike, so even safer from a MAD perspective. (... and no fully private actor ever gets to run them. ) Or if full automation is not workable, a good deal of effort expended on maintaining crew sanity - Psyc/political officers - called something nice, fluffy, and utterly anodyne to make people forget just how much authority they have, backed up with a remote controlled self destruct. Again, one time pad com lock. It's not going to be a libertarian free for-all as industries go, more a case of \"Extremely well paid, to make up for the conditions and the sword that will take your head if you crack under the pressure\" Good story potential in that, though.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>A great start to a discussion here.&nbsp;</p>\n<p>You've considered people going loons and some general security, but it would then become a hacker war along the lines of who could break the security and gain control of the space ships.</p>\n<p>It doesn't address the problem of the leaders using the ships as threat weapons, since they have legitimate control, but can still make terrorist decisions.</p>\n<p>And I'm terrified of your idea of turning spaceflight, which I see as the ultimate freedom, along the lines of Niven's Belters, into a state-controlled affair like the Soviet navy with political officers.</p>\n<p>Now, one thing I think is a useful safety control that doesn't lead to worse problems is the destruct option. &nbsp;All major rockets have them right now, since if it goes out of control it's a huge hazard for a great distance. &nbsp;And although I don't like the idea of all personal spaceships being under a safety officers thumb, it might be better than the alternative of terrorist groups gaining control of asteroid mines and holding the world hostage.</p>\n<p>&nbsp;</p>\n<p>You're right about great story potential though, in any of these scenarios.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9yKbaSv9dTPWofPbD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "498ccQheftkKJqTYb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "21775", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-26T05:07:23.877Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Evolutionary-Cognitive Boundary", "slug": "seq-rerun-the-evolutionary-cognitive-boundary", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N7gwFeQ4CwHHMTiLQ/seq-rerun-the-evolutionary-cognitive-boundary", "pageUrlRelative": "/posts/N7gwFeQ4CwHHMTiLQ/seq-rerun-the-evolutionary-cognitive-boundary", "linkUrl": "https://www.lesswrong.com/posts/N7gwFeQ4CwHHMTiLQ/seq-rerun-the-evolutionary-cognitive-boundary", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Evolutionary-Cognitive%20Boundary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Evolutionary-Cognitive%20Boundary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN7gwFeQ4CwHHMTiLQ%2Fseq-rerun-the-evolutionary-cognitive-boundary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Evolutionary-Cognitive%20Boundary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN7gwFeQ4CwHHMTiLQ%2Fseq-rerun-the-evolutionary-cognitive-boundary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN7gwFeQ4CwHHMTiLQ%2Fseq-rerun-the-evolutionary-cognitive-boundary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Today's post, <a href=\"/lw/yi/the_evolutionarycognitive_boundary/\">The Evolutionary-Cognitive Boundary</a> was originally published on 12 February 2009.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2009_Articles/Summaries#The_Evolutionary-Cognitive_Boundary\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It's worth drawing a sharp boundary between ideas about evolutionary reasons for behavior, and cognitive reasons for behavior.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/gsj/seq_rerun_cynicism_in_evpsych_and_econ/\">Cynicism in Ev-Psych (and Econ)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N7gwFeQ4CwHHMTiLQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.122936298956772e-06, "legacy": true, "legacyId": "21776", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["o8Bh82hKGpRNA2q36", "wWkpoHQMfsLn45ws9", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-26T05:25:02.917Z", "modifiedAt": null, "url": null, "title": "Memory, nutrition, motivation, and genes", "slug": "memory-nutrition-motivation-and-genes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.827Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZxwXtNcw9bMQph992/memory-nutrition-motivation-and-genes", "pageUrlRelative": "/posts/ZxwXtNcw9bMQph992/memory-nutrition-motivation-and-genes", "linkUrl": "https://www.lesswrong.com/posts/ZxwXtNcw9bMQph992/memory-nutrition-motivation-and-genes", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Memory%2C%20nutrition%2C%20motivation%2C%20and%20genes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMemory%2C%20nutrition%2C%20motivation%2C%20and%20genes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZxwXtNcw9bMQph992%2Fmemory-nutrition-motivation-and-genes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Memory%2C%20nutrition%2C%20motivation%2C%20and%20genes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZxwXtNcw9bMQph992%2Fmemory-nutrition-motivation-and-genes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZxwXtNcw9bMQph992%2Fmemory-nutrition-motivation-and-genes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 453, "htmlBody": "<p>There are two confusing but potentially important papers in the Jan. 25 2013 <em>Science</em> on long-term memory (LTM) formation in fruit flies:</p>\n<p>Pierre-Yves Placais &amp; Thomas Preat. To favor survival under food shortage, the brain disables costly memory. 339:440-441.</p>\n<p>Yukinori Hirano et al. Fasting launches CRTC to facilitate long-term memory formation in Drosophila. 339:443-446.</p>\n<p>&nbsp;</p>\n<p>These papers categorize long-term memory formation along three axes.</p>\n<ul>\n<li>Aversive vs. appetitive:&nbsp; Actions that the brain interprets as helping it avoid something, vs. actions that help it attain something.</li>\n<li>Fasting-dependent (fLTM) vs. spaced training-dependent (spLTM):&nbsp; fLTM is formed in a single learning episode, but only at the time that an organism first obtains food after a long fast. spLTM does not require fasting but requires repeated training.</li>\n<li>LTM vs. ARM: Memories that require protein synthesis (LTM) vs. \"anesthesia-resistent memory\" (ARM), which does not.&nbsp; (The papers don't explain what ARM might correspond to in humans.)</li>\n</ul>\n<p>The relationship between these is unclear, particularly as each of these three axes is claimed at various times to determine whether memory can be learned in a single training cycle (appetitive, fLTM, and/or ARM) or not (aversive, spLTM, and/or LTM).&nbsp; But these things appear to be likely, or at least to be reasonable hypotheses, if these pathways are conserved in humans:</p>\n<ul>\n<li>How quickly you learn something depends on how much you've eaten recently.&nbsp; You learn most quickly immediately after ending a long fast. Your brain thinks you just learned something that saved it from starvation. (But note that a 1-day fast for a fruit fly could be compared to a human fasting for months.)</li>\n<li>How quickly you learn something depends on whether your brain thinks that this knowledge is to avoid something bad (slow learning) or to attain something good (fast learning).</li>\n<li>Almost all of the mutations that extend lifespan in organisms from yeast to humans impact the FOXO3a vs. mTORc1 axis (to use the human analogs).&nbsp; Expressing FOXO3a inhibits mTORc1 and extends lifespan in various ways; producing and assembling more mTORc1 inhibits FOXO3a and promotes protein synthesis, growth, reproduction, tissue repair, and immune response.&nbsp; We already know that extending lifespan, in general, is antithetical to building muscle.&nbsp; It may also be antithetical to forming long-term memories.&nbsp; This makes sense.</li>\n<li>Learning rate can be increased by expressing or inhibiting&nbsp; proteins involved in these responses.&nbsp; Hirano et al. focus on activating a cAMP-regulated transcriptional coactivator (CRTC) by dephosphorylating it in order to invoke fLTM.&nbsp; They were able to do this and enable flies to learn quickly without fasting followed by feeding.</li>\n</ul>\n<p>I'd really appreciate it if somebody would do a literature review and a comparison of the pathways involved to those in humans, and summarize their findings.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"92SxJsDZ78ApAGq72": 1, "3uE2pXvbcnS9nnZRE": 1, "ksdiAMKfgSyEeKMo6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZxwXtNcw9bMQph992", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 24, "extendedScore": null, "score": 1.1229476818559994e-06, "legacy": true, "legacyId": "21783", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-26T06:55:18.542Z", "modifiedAt": null, "url": null, "title": "The Center for Sustainable Nanotechnology", "slug": "the-center-for-sustainable-nanotechnology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.499Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ESRogs", "createdAt": "2011-01-05T21:50:53.170Z", "isAdmin": false, "displayName": "ESRogs"}, "userId": "dRGmZYGDzf5LFNjtz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xj9Ad3ACmMvoejSwK/the-center-for-sustainable-nanotechnology", "pageUrlRelative": "/posts/Xj9Ad3ACmMvoejSwK/the-center-for-sustainable-nanotechnology", "linkUrl": "https://www.lesswrong.com/posts/Xj9Ad3ACmMvoejSwK/the-center-for-sustainable-nanotechnology", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Center%20for%20Sustainable%20Nanotechnology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Center%20for%20Sustainable%20Nanotechnology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXj9Ad3ACmMvoejSwK%2Fthe-center-for-sustainable-nanotechnology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Center%20for%20Sustainable%20Nanotechnology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXj9Ad3ACmMvoejSwK%2Fthe-center-for-sustainable-nanotechnology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXj9Ad3ACmMvoejSwK%2Fthe-center-for-sustainable-nanotechnology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 609, "htmlBody": "<p>Those concerned about existential risks may be interested to learn that, as of last September, the National Science Foundation is funding a&nbsp;<a href=\"http://susnano.chem.wisc.edu/\">Center for Sustainable Nanotechnology</a>. &nbsp;Though I haven't yet seen anywhere where they explicitly characterize nanotechnology as an existential threat to humanity (they seem mostly to be concerned with the potential hazards of nanoparticle pollution, rather than any kind of&nbsp;<a href=\"http://en.wikipedia.org/wiki/Grey_goo\">grey goo</a>&nbsp;scenario), I was still pleased to discover that this group exists.&nbsp;</p>\n<p>Here is how they describe themselves on their <a href=\"http://susnano.chem.wisc.edu/about\">main page</a>:</p>\n<blockquote>\n<p class=\"p1\">The Center for Sustainable Nanotechnology is a multi-institutional partnership devoted to investigating the fundamental molecular mechanisms by which nanoparticles interact with biological systems.</p>\n<p class=\"p1\">...</p>\n<p class=\"p1\">While nanoparticles have a great potential to improve our society, relatively little is yet known about how nanoparticles interact with organisms, and how the unintentional release of nanoparticles from consumer or industrial products might impact the environment.</p>\n<p class=\"p1\">The goal of the Center for Sustainable Nanotechnology is to develop and utilize a molecular-level understanding of nanomaterial-biological interactions to enable development of sustainable, societally beneficial nanotechnologies. In effect, we aim to understand the molecular-level chemical and physical principles that govern how nanoparticles interact with living systems, in order to provide the scientific foundations that are needed to ensure that continued developments in nanotechnology can take place with the minimal environmental footprint and maximum benefit to society.</p>\n<p class=\"p1\">...</p>\n<p class=\"p1\">Funding for the CSN comes from the National Science Foundation Division of Chemistry through the Centers for Chemical Innovation Program.</p>\n</blockquote>\n<p>And on their <a href=\"http://sustainable-nano.com/2013/01/29/why-are-nanomaterials-so-special-and-what-is-the-center-for-sustainable-nanotechnology/\">public outreach website</a>:</p>\n<blockquote>\n<p class=\"p1\">Our &ldquo;center&rdquo; is actually a group of people who care about our environment and are doing collaborative research to help ensure that our planet will be habitable hundreds of years from now &ndash; in other words, that the things we do every day as humans will be sustainable in the long run.</p>\n<p class=\"p1\">Now you&rsquo;re probably wondering what that has to do with nanotechnology, right? Well, it turns out that nanoparticles &ndash; chunks of materials around 10,000 times smaller than the width of a human hair &ndash; may provide new and important solutions to many of the world&rsquo;s problems. For example, new kinds of nanoparticle-based solar cells are being made that could, in the future, be painted onto the sides of buildings.</p>\n<p class=\"p1\">...</p>\n<p class=\"p1\">What&rsquo;s the (potential) problem? Well, these tiny little chunks of materials are so small that they can move around and do things in ways that we don&rsquo;t fully understand. For example, really tiny particles could potentially be absorbed through skin. In the environment, nanoparticles might be able to be absorbed into insects or fish that are at the bottom of the food chain for larger animals, including us.</p>\n<p class=\"p1\">Before nanoparticles get incorporated into consumer products on a large scale, it&rsquo;s our responsibility to figure out what the downsides could be if nanoparticles were accidentally released into the environment. However, this is a huge challenge because nanoparticles can be made out of different stuff and come in many different sizes, shapes, and even internal structures.</p>\n<p class=\"p1\">Because there are so many different types of nanoparticles that could be used in the future, it&rsquo;s not practical to do a lot of testing of each kind. Instead, the people within our center are working to understand what the &ldquo;rules of behavior&rdquo; are for nanoparticles in general. If we understand the rules, then we should be able to predict what different types of nanoparticles might do, and we should be able to use this information to design and make new, safer nanoparticles.</p>\n<p class=\"p1\">In the end, it&rsquo;s all about people working together, using science to create a better, safer, more sustainable world. We hope you will join us!</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XJjvxWB68GYpts93N": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xj9Ad3ACmMvoejSwK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 1.1230058951632453e-06, "legacy": true, "legacyId": "21784", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-26T12:46:30.414Z", "modifiedAt": null, "url": null, "title": "[LINK] Westerners may be terrible experimental psychology subjects", "slug": "link-westerners-may-be-terrible-experimental-psychology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:58.989Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/syRATXbXeJxdMwQBD/link-westerners-may-be-terrible-experimental-psychology", "pageUrlRelative": "/posts/syRATXbXeJxdMwQBD/link-westerners-may-be-terrible-experimental-psychology", "linkUrl": "https://www.lesswrong.com/posts/syRATXbXeJxdMwQBD/link-westerners-may-be-terrible-experimental-psychology", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Westerners%20may%20be%20terrible%20experimental%20psychology%20subjects&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Westerners%20may%20be%20terrible%20experimental%20psychology%20subjects%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsyRATXbXeJxdMwQBD%2Flink-westerners-may-be-terrible-experimental-psychology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Westerners%20may%20be%20terrible%20experimental%20psychology%20subjects%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsyRATXbXeJxdMwQBD%2Flink-westerners-may-be-terrible-experimental-psychology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsyRATXbXeJxdMwQBD%2Flink-westerners-may-be-terrible-experimental-psychology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 249, "htmlBody": "<p><a href=\"/lw/17x/beware_of_weird_psychological_samples/\">WEIRD</a> may be weirder than you think. <a href=\"http://www.psmag.com/magazines/pacific-standard-cover-story/joe-henrich-weird-ultimatum-game-shaking-up-psychology-economics-53135/\">We Aren't The World</a> writes of psychological experiments on non-Westerners that give vastly disparate results from results that have been assumed to be hardwired, and the implications of this:</p>\n<p style=\"padding-left: 30px;\">Henrich used a &ldquo;game&rdquo;&mdash;along the lines of the famous prisoner&rsquo;s dilemma&mdash;to see whether isolated cultures shared with the West the same basic instinct for fairness. In doing so, Henrich expected to confirm one of the foundational assumptions underlying such experiments, and indeed underpinning the entire fields of economics and psychology: that humans all share the same cognitive machinery&mdash;the same evolved rational and psychological hardwiring.&nbsp; The test that Henrich introduced to the Machiguenga was called the ultimatum game.</p>\n<p style=\"padding-left: 30px;\">...<br /><br />To begin with, the offers from the first player were much lower. In addition, when on the receiving end of the game, the Machiguenga rarely refused even the lowest possible amount. &ldquo;It just seemed ridiculous to the Machiguenga that you would reject an offer of free money,&rdquo; says Henrich. &ldquo;They just didn&rsquo;t understand why anyone would sacrifice money to punish someone who had the good luck of getting to play the other role in the game.&rdquo;</p>\n<p style=\"padding-left: 30px;\">...<br /><br />At the heart of most of that research was the implicit assumption that the results revealed evolved psychological traits common to all humans, never mind that the test subjects were nearly always from the industrialized West.</p>\n<p><strong>Edit:</strong> The actual papers this article writes about are covered in <a href=\"/lw/17x/beware_of_weird_psychological_samples/\">this post by Ciphergoth</a> from a few years ago.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "syRATXbXeJxdMwQBD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 20, "extendedScore": null, "score": 1.123232450448807e-06, "legacy": true, "legacyId": "21787", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["33YYcoWwtmqzAq9QR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-26T13:07:42.902Z", "modifiedAt": null, "url": null, "title": "[Link] We aren't the world: Humans differ more than previously believed", "slug": "link-we-aren-t-the-world-humans-differ-more-than-previously", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.290Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Baughn", "createdAt": "2009-03-05T10:25:59.290Z", "isAdmin": false, "displayName": "Baughn"}, "userId": "fGazNbxCoSRJ7bEWM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QRpDvmwX98yGBX3QD/link-we-aren-t-the-world-humans-differ-more-than-previously", "pageUrlRelative": "/posts/QRpDvmwX98yGBX3QD/link-we-aren-t-the-world-humans-differ-more-than-previously", "linkUrl": "https://www.lesswrong.com/posts/QRpDvmwX98yGBX3QD/link-we-aren-t-the-world-humans-differ-more-than-previously", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20We%20aren't%20the%20world%3A%20Humans%20differ%20more%20than%20previously%20believed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20We%20aren't%20the%20world%3A%20Humans%20differ%20more%20than%20previously%20believed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQRpDvmwX98yGBX3QD%2Flink-we-aren-t-the-world-humans-differ-more-than-previously%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20We%20aren't%20the%20world%3A%20Humans%20differ%20more%20than%20previously%20believed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQRpDvmwX98yGBX3QD%2Flink-we-aren-t-the-world-humans-differ-more-than-previously", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQRpDvmwX98yGBX3QD%2Flink-we-aren-t-the-world-humans-differ-more-than-previously", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 289, "htmlBody": "<p>I found <a href=\"http://www.psmag.com/magazines/pacific-standard-cover-story/joe-henrich-weird-ultimatum-game-shaking-up-psychology-economics-53135/\">this article</a>&nbsp;last night, which I'd like to hear your thoughts on.</p>\n<p>It explains that many behaviours which psychologists have believed are human universals turn out to be specific to western culture. As the article points out, that's likely due to having easy access to western undergraduates, and assuming everyone works the same way.</p>\n<p>It's not the first time I've read something like this; in the context of an article on military training, some years ago, I read how the culture of a certain area of the world made it basically impossible to build a military to western standards of effectiveness. Unfortunately I don't have a link, so treat that as an anecdote.</p>\n<p>Some topics for discussion, off the top of my head:</p>\n<p>&nbsp;</p>\n<ul>\n<li>The article suggests that the same cultural bias that makes westerners unusual also causes us to not notice, but that's right after committing a multi-decade blooper. I'm unsure how much weight to give their negativity; western culture has been stunningly effective otherwise.</li>\n<li>Just how different <em>are</em>&nbsp;humans, anyway? One of the assumptions of CEV is that you can reasonably predict how a people would act if they \"grow up together\", which presumably requires growing up in the same culture. If culture provides this much of our basic brainware, such a scenario might be completely intractable; at worst, CEV for different cultures could produce the equivalent of baby-eaters. I'm increasingly worried that Eliezer is trying to do something that's not just impossible but also undesirable.</li>\n<li>How much of a competitive bonus does western culture provide? Is multiculturalism safe, either competitively or societally?</li>\n</ul>\n<p>I don't expect these to be answerable right away, or based on this article. It's just something I'm going to keep in mind.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QRpDvmwX98yGBX3QD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "21788", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-02-26T16:23:26.859Z", "modifiedAt": null, "url": null, "title": "Meetup : RTLW Thursday Meetup ", "slug": "meetup-rtlw-thursday-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g7RQvEoQhTLyJ7yP3/meetup-rtlw-thursday-meetup", "pageUrlRelative": "/posts/g7RQvEoQhTLyJ7yP3/meetup-rtlw-thursday-meetup", "linkUrl": "https://www.lesswrong.com/posts/g7RQvEoQhTLyJ7yP3/meetup-rtlw-thursday-meetup", "postedAtFormatted": "Tuesday, February 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20RTLW%20Thursday%20Meetup%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20RTLW%20Thursday%20Meetup%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg7RQvEoQhTLyJ7yP3%2Fmeetup-rtlw-thursday-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20RTLW%20Thursday%20Meetup%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg7RQvEoQhTLyJ7yP3%2Fmeetup-rtlw-thursday-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg7RQvEoQhTLyJ7yP3%2Fmeetup-rtlw-thursday-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/jv'>RTLW Thursday Meetup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 February 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Francesca's Dessert Caffe, 706 9th Street, Durham NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week is Meetup Activity Potpourri, including:</p>\n\n<ul>\n<li>Calibration exercises </li>\n<li>Games: The Resistance and Zendo have been mentioned </li>\n<li>Rationality Checklist check/review (<a href=\"http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/\" rel=\"nofollow\">http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/</a>) </li>\n<li>beverages </li>\n<li>etc.? </li>\n</ul>\n\n<p>We'll have a couple bags of Icehouse pyramids on the table (<a href=\"http://3.bp.blogspot.com/-NKLebKLBm2Q/TtqJ7aptmBI/AAAAAAAAALs/n1jMtUlh3Ws/s1600/Ice%2BDice%2BBag.jpg\" rel=\"nofollow\">http://3.bp.blogspot.com/-NKLebKLBm2Q/TtqJ7aptmBI/AAAAAAAAALs/n1jMtUlh3Ws/s1600/Ice%2BDice%2BBag.jpg</a>).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/jv'>RTLW Thursday Meetup </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g7RQvEoQhTLyJ7yP3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.1233724378371455e-06, "legacy": true, "legacyId": "21791", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___RTLW_Thursday_Meetup_\">Discussion article for the meetup : <a href=\"/meetups/jv\">RTLW Thursday Meetup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 February 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Francesca's Dessert Caffe, 706 9th Street, Durham NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week is Meetup Activity Potpourri, including:</p>\n\n<ul>\n<li>Calibration exercises </li>\n<li>Games: The Resistance and Zendo have been mentioned </li>\n<li>Rationality Checklist check/review (<a href=\"http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/\" rel=\"nofollow\">http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/</a>) </li>\n<li>beverages </li>\n<li>etc.? </li>\n</ul>\n\n<p>We'll have a couple bags of Icehouse pyramids on the table (<a href=\"http://3.bp.blogspot.com/-NKLebKLBm2Q/TtqJ7aptmBI/AAAAAAAAALs/n1jMtUlh3Ws/s1600/Ice%2BDice%2BBag.jpg\" rel=\"nofollow\">http://3.bp.blogspot.com/-NKLebKLBm2Q/TtqJ7aptmBI/AAAAAAAAALs/n1jMtUlh3Ws/s1600/Ice%2BDice%2BBag.jpg</a>).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___RTLW_Thursday_Meetup_1\">Discussion article for the meetup : <a href=\"/meetups/jv\">RTLW Thursday Meetup </a></h2>", "sections": [{"title": "Discussion article for the meetup : RTLW Thursday Meetup ", "anchor": "Discussion_article_for_the_meetup___RTLW_Thursday_Meetup_", "level": 1}, {"title": "Discussion article for the meetup : RTLW Thursday Meetup ", "anchor": "Discussion_article_for_the_meetup___RTLW_Thursday_Meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}